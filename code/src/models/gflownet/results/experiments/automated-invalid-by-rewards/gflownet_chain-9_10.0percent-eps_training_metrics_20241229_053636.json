{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07686890496148004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07686890496148004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07686890496148004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07662422127193874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07686890496148004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07686890496148004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.76817321777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969264507293701,
      "backward_entropy": 0.07683354616165161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.74349975585938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10968880653381348,
      "backward_entropy": 0.07663032743665907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.38514709472656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00019999989308416843,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10968492031097413,
      "backward_entropy": 0.07663638061947292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.69415283203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00030012850766070187,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10968096256256103,
      "backward_entropy": 0.07684000333150227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.26121520996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00039970866055227816,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967694520950318,
      "backward_entropy": 0.07687885231441921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.89134216308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0004990530433133245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967289209365845,
      "backward_entropy": 0.07684402333365546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.88658142089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0005992418737150729,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10966875553131103,
      "backward_entropy": 0.0768460366461012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.91912841796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0006979099125601351,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966460704803467,
      "backward_entropy": 0.07666563325458103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.35162353515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0007971200975589454,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966037511825562,
      "backward_entropy": 0.07667131556404962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.7287139892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0008975916425697505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965604782104492,
      "backward_entropy": 0.07685139444139269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.88134765625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0009990119142457843,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10965164899826049,
      "backward_entropy": 0.07668266693751018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.9957733154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0011012384202331305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10964717864990234,
      "backward_entropy": 0.07685503694746229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.0076141357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0012010432546958327,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10964267253875733,
      "backward_entropy": 0.07685674561394586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.789306640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001300614676438272,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10963832139968872,
      "backward_entropy": 0.07669907146030003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.04180908203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0014014284824952483,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10963389873504639,
      "backward_entropy": 0.07690276039971246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.58267211914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0015026837354525924,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10962941646575927,
      "backward_entropy": 0.07690513134002686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.15110778808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.001602865755558014,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10962491035461426,
      "backward_entropy": 0.0769073764483134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.45333862304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0017022084211930633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10962039232254028,
      "backward_entropy": 0.07686511675516765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.72166442871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0017995770322158933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1096158504486084,
      "backward_entropy": 0.07686665323045519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.18734741210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0018942368915304542,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10961129665374755,
      "backward_entropy": 0.07686807049645318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.16598510742188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0019878034945577383,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10960673093795777,
      "backward_entropy": 0.07673393355475532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.07020568847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0020804351661354303,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10960214138031006,
      "backward_entropy": 0.07687083880106609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.34812927246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0021729846484959126,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10959746837615966,
      "backward_entropy": 0.07674289411968654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.8022003173828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0022622409742325544,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10959280729293823,
      "backward_entropy": 0.07674716578589545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.77919006347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0023521289695054293,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958809852600097,
      "backward_entropy": 0.07692221800486247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.34934997558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002442588796839118,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958335399627686,
      "backward_entropy": 0.07687649461958143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.8347625732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0025336481630802155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10957858562469483,
      "backward_entropy": 0.07687787214914958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.1607208251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00262602255679667,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10957369804382325,
      "backward_entropy": 0.07687926292419434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.75965881347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002714041154831648,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095688819885254,
      "backward_entropy": 0.07688035567601521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.94320678710938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0028056211303919554,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10956401824951172,
      "backward_entropy": 0.07692906591627333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.08206176757812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0028926858212798834,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10955923795700073,
      "backward_entropy": 0.07677595482932197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.17835998535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002978369826450944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10955443382263183,
      "backward_entropy": 0.07693151632944743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.02349853515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00306729250587523,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954957008361817,
      "backward_entropy": 0.07693280114067926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.2582550048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0031600415240973234,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954458713531494,
      "backward_entropy": 0.07693415217929417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.95980834960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0032540103420615196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095394253730774,
      "backward_entropy": 0.07688689231872559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.61419677734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00334799918346107,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953414440155029,
      "backward_entropy": 0.07693690061569214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.6429443359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0034430017694830894,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095287561416626,
      "backward_entropy": 0.07693829801347521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.23670959472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003536788048222661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952328443527222,
      "backward_entropy": 0.07689032951990764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.19248962402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003632592735812068,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10951766967773438,
      "backward_entropy": 0.07689147525363499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.96217346191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003727173898369074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10951200723648072,
      "backward_entropy": 0.07689257462819417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.47588348388672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0038239359855651855,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1095062255859375,
      "backward_entropy": 0.07681463162104289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.98007202148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003914716187864542,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950056314468384,
      "backward_entropy": 0.07689470714992946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.02597045898438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004006059840321541,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10949480533599854,
      "backward_entropy": 0.07682173781924778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.12062072753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0040988619439303875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948894023895264,
      "backward_entropy": 0.07694734467400445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.97329711914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004192190244793892,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948300361633301,
      "backward_entropy": 0.07689774698681301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.37799072265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004286766517907381,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10947695970535279,
      "backward_entropy": 0.07689879337946574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.701416015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004379115998744965,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094709038734436,
      "backward_entropy": 0.07683569855160183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.02362060546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004468302708119154,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10946487188339234,
      "backward_entropy": 0.07695215278201634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.2287139892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004558473825454712,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945874452590942,
      "backward_entropy": 0.07695324553383721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.29673767089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004644161555916071,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945267677307129,
      "backward_entropy": 0.07684528827667236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.00559997558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004728572443127632,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10944654941558837,
      "backward_entropy": 0.07684828175438775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.96391296386719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004815572407096624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10944029092788696,
      "backward_entropy": 0.07690403196546766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.43226623535156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004898711573332548,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10943408012390136,
      "backward_entropy": 0.07685420910517375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.6207733154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004986926447600126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10942765474319457,
      "backward_entropy": 0.07690562142266168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.19520568847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005073497537523508,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10942118167877198,
      "backward_entropy": 0.076906422773997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.94125366210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005158708896487951,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094146728515625,
      "backward_entropy": 0.07690717114342584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.6584930419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00524137681350112,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10940817594528199,
      "backward_entropy": 0.07690784666273329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.5537567138672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00532675301656127,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094015121459961,
      "backward_entropy": 0.07686846786075169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.74522399902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005412430502474308,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10939478874206543,
      "backward_entropy": 0.07690933015611437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.53172302246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005498091224581003,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938796997070313,
      "backward_entropy": 0.07696331871880426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.96641540527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005583548918366432,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938107967376709,
      "backward_entropy": 0.07696417967478435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.27207946777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005672893486917019,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10937399864196777,
      "backward_entropy": 0.07696512672636244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.383056640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005761952139437199,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10936684608459472,
      "backward_entropy": 0.0769660472869873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.9816131591797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00584873091429472,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935975313186645,
      "backward_entropy": 0.07688448164198133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.3946990966797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005933043081313372,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935266017913818,
      "backward_entropy": 0.07688694530063206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.2497863769531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0060178558342158794,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10934547185897828,
      "backward_entropy": 0.07696840498182508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.3482208251953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0061091165989637375,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10933799743652343,
      "backward_entropy": 0.0768919587135315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.938232421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006200109142810106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10933043956756591,
      "backward_entropy": 0.07691578732596503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.06163787841797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006292060948908329,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10932276248931885,
      "backward_entropy": 0.07697105407714844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.66709899902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00638005044311285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10931506156921386,
      "backward_entropy": 0.0769172641966078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.46728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006466912105679512,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10930722951889038,
      "backward_entropy": 0.0769180523024665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.62266540527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006556408945471048,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1092991590499878,
      "backward_entropy": 0.07697337865829468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.80465698242188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0066446298733353615,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1092909812927246,
      "backward_entropy": 0.07690629694196913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.58218383789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006730230525135994,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10928279161453247,
      "backward_entropy": 0.07697482903798421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.57089233398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006816128734499216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10927445888519287,
      "backward_entropy": 0.07692121134863959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.1467742919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006899434141814709,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10926609039306641,
      "backward_entropy": 0.07697619332207574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.31712341308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00698052579537034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10925765037536621,
      "backward_entropy": 0.0769224829143948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.23739624023438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007061053533107042,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10924912691116333,
      "backward_entropy": 0.07691668139563666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.45726013183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0071477508172392845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1092402458190918,
      "backward_entropy": 0.07692380746205647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.78057861328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007234675344079733,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10923125743865966,
      "backward_entropy": 0.07692452271779378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.61378479003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007326632272452116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10922188758850097,
      "backward_entropy": 0.07697962390051948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.73641967773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007417935412377119,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10921237468719483,
      "backward_entropy": 0.07698045174280803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.1240692138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007511626463383436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10920264720916747,
      "backward_entropy": 0.0769813060760498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.953369140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007606239058077335,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10919277667999268,
      "backward_entropy": 0.0769821670320299,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.74154663085938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00769865931943059,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10918283462524414,
      "backward_entropy": 0.07693105936050415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.61212921142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007788262329995632,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1091728925704956,
      "backward_entropy": 0.07698373662100898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.00521850585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007870848290622234,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10916314125061036,
      "backward_entropy": 0.0769297546810574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.4800262451172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007955587469041348,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10915313959121704,
      "backward_entropy": 0.07693653636508518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.20892333984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008038070052862167,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10914309024810791,
      "backward_entropy": 0.07693097326490614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.59661865234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00811244547367096,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10913331508636474,
      "backward_entropy": 0.07693986760245429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.53578186035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008187063038349152,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10912339687347412,
      "backward_entropy": 0.07694143719143337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.782470703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008263122290372849,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10911321640014648,
      "backward_entropy": 0.0769871539539761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.29103088378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008338283747434616,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10910295248031616,
      "backward_entropy": 0.07698761092291938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.66641235351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008420578204095364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10909215211868287,
      "backward_entropy": 0.07693321837319268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.47003173828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00850854441523552,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10908079147338867,
      "backward_entropy": 0.07694751686520046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.13662719726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008595368824899197,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10906933546066284,
      "backward_entropy": 0.07694900035858154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.7271499633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008683865889906883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10905778408050537,
      "backward_entropy": 0.07693519857194689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.0717315673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008768400177359581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10904637575149537,
      "backward_entropy": 0.07693577475017971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.22357177734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008854873478412628,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10903468132019042,
      "backward_entropy": 0.07699138588375515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.20181274414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008938939310610294,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10902297496795654,
      "backward_entropy": 0.07699201504389445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.3695068359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009025235660374165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10901098251342774,
      "backward_entropy": 0.07693751653035481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.07191467285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009109379723668098,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10899895429611206,
      "backward_entropy": 0.07693806621763441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.37799072265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009193161502480507,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10898683071136475,
      "backward_entropy": 0.07696017954084608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.15199279785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009277724660933018,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10897449254989625,
      "backward_entropy": 0.07696171601613362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.03521728515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009362775832414627,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10896188020706177,
      "backward_entropy": 0.07699508137173122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.2432098388672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009450020268559456,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10894898176193238,
      "backward_entropy": 0.0769647757212321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.8866729736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009535045363008976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10893605947494507,
      "backward_entropy": 0.07694080140855578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.85297393798828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009623464196920395,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10892270803451538,
      "backward_entropy": 0.07696774270799425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.54360961914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009705045260488987,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10890967845916748,
      "backward_entropy": 0.07699741257561578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.55934143066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009794366545975208,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10889589786529541,
      "backward_entropy": 0.07699800862206353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.30590057373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009883785620331764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1088818907737732,
      "backward_entropy": 0.076943039894104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.38792419433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00996765773743391,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10886812210083008,
      "backward_entropy": 0.07694352997673883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.2642593383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0100539680570364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10885398387908936,
      "backward_entropy": 0.07694407966401842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.52926635742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01013503409922123,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10884009599685669,
      "backward_entropy": 0.07694451014200847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.7770538330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01021585799753666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1088260531425476,
      "backward_entropy": 0.0769449339972602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.87240600585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010300781577825546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10881149768829346,
      "backward_entropy": 0.07700104845894708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.7558135986328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010384961031377316,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10879678726196289,
      "backward_entropy": 0.07697926627265082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.1194305419922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010467004030942917,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10878204107284546,
      "backward_entropy": 0.07698032591078016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.50624084472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010547601617872715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10876725912094116,
      "backward_entropy": 0.07694677511850993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.312744140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010626203380525112,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10875240564346314,
      "backward_entropy": 0.0770028829574585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.52752685546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010700570419430733,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10873773097991943,
      "backward_entropy": 0.07694753011067708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.23939514160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01077509205788374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10872286558151245,
      "backward_entropy": 0.07700363794962566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.06556701660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010852843523025513,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10870758295059205,
      "backward_entropy": 0.07694819238450792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.49224853515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010930540971457958,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1086920976638794,
      "backward_entropy": 0.07698611418406169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.26263427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011010156013071537,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10867626667022705,
      "backward_entropy": 0.07694889439476861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.0326385498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011091267690062523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10866012573242187,
      "backward_entropy": 0.07694929175906712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.33238220214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011173497885465622,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10864359140396118,
      "backward_entropy": 0.07700575722588433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.80213165283203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011255722492933273,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10862691402435302,
      "backward_entropy": 0.077006200949351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.6710205078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011334437876939774,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10861030817031861,
      "backward_entropy": 0.07699090904659694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.58497619628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011413410305976868,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10859347581863403,
      "backward_entropy": 0.0770070023006863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.62633514404297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011496827937662601,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10857598781585694,
      "backward_entropy": 0.07699272367689344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.28659057617188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011574123986065388,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10855894088745117,
      "backward_entropy": 0.07695139778984918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.17845153808594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01164914295077324,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10854194164276124,
      "backward_entropy": 0.07699435949325562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.83099365234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011726452969014645,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1085245132446289,
      "backward_entropy": 0.07695195409986708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.65245056152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011807740665972233,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10850633382797241,
      "backward_entropy": 0.07699595557318793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.19245147705078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011887175031006336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10848808288574219,
      "backward_entropy": 0.07700914806789821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.90028381347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011963524855673313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1084699034690857,
      "backward_entropy": 0.0769528481695387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.85609436035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012034947983920574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10845212936401367,
      "backward_entropy": 0.0769531528155009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.3460235595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01211126521229744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1084335207939148,
      "backward_entropy": 0.07700996928744847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 254.41195678710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012188157998025417,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.108414626121521,
      "backward_entropy": 0.07695382171207005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.93008422851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012273846194148064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10839447975158692,
      "backward_entropy": 0.07695427205827501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.44561767578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012364569120109081,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10837349891662598,
      "backward_entropy": 0.07695477538638645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.02281188964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012454009614884853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10835236310958862,
      "backward_entropy": 0.0770111878712972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.16453552246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012548301368951797,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10833040475845337,
      "backward_entropy": 0.07695570256974962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.13604736328125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012641296721994877,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10830833911895751,
      "backward_entropy": 0.07700286971198188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.4900360107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01273253746330738,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10828613042831421,
      "backward_entropy": 0.07695652378929986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.00396728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012822684831917286,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10826379060745239,
      "backward_entropy": 0.07695684168073866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.8442153930664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012918705120682716,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10824041366577149,
      "backward_entropy": 0.07701263162824842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.79408264160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013009972870349884,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10821732282638549,
      "backward_entropy": 0.07700524065229628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.58568572998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013098497875034809,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10819429159164429,
      "backward_entropy": 0.07701310846540663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.07371520996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013182774186134338,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10817146301269531,
      "backward_entropy": 0.07701330052481757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.97080993652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013267945498228073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10814820528030396,
      "backward_entropy": 0.07695804701911078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.28677368164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013356899842619896,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10812408924102783,
      "backward_entropy": 0.07695821921030681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.0483169555664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013450564816594124,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10809886455535889,
      "backward_entropy": 0.07701388994852702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.25668334960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013540200889110565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10807387828826905,
      "backward_entropy": 0.07695862982008192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.62725067138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013631547801196575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10804823637008668,
      "backward_entropy": 0.07695880201127794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.351806640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013717671856284142,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1080230712890625,
      "backward_entropy": 0.07701437340842353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.55763244628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013800401240587234,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1079980969429016,
      "backward_entropy": 0.0770144992404514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.23793029785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013884559273719788,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10797256231307983,
      "backward_entropy": 0.07701461844974095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.87957763671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013966460712254047,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10794694423675537,
      "backward_entropy": 0.07695901393890381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.42762756347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014049168676137924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10792077779769897,
      "backward_entropy": 0.07695896095699734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.32212829589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014137445017695427,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10789339542388916,
      "backward_entropy": 0.0770106315612793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.25996398925781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01422329992055893,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10786601305007934,
      "backward_entropy": 0.0770109494527181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.7291259765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014299187809228897,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10783989429473877,
      "backward_entropy": 0.07695865631103516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.2787628173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014372209087014198,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10781387090682984,
      "backward_entropy": 0.07695835828781128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.89007568359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01445036381483078,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10778664350509644,
      "backward_entropy": 0.07695815960566203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.53807067871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0145284254103899,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10775901079177856,
      "backward_entropy": 0.07695794105529785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.59648895263672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014604982919991016,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10773125886917115,
      "backward_entropy": 0.07701209518644545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.060791015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014678621664643288,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10770362615585327,
      "backward_entropy": 0.07701533370547825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.71278381347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014751340262591839,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10767579078674316,
      "backward_entropy": 0.0769568681716919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.2783203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014825180172920227,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10764744281768798,
      "backward_entropy": 0.07695648405287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.78318786621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014899537898600101,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10761862993240356,
      "backward_entropy": 0.07701279719670613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.26922607421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014980493113398552,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10758826732635499,
      "backward_entropy": 0.0770154661602444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.9521713256836,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015063036233186722,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10755724906921386,
      "backward_entropy": 0.07701314820183648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.67723846435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015137535519897938,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10752737522125244,
      "backward_entropy": 0.0769549740685357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.45066833496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015207999385893345,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1074979066848755,
      "backward_entropy": 0.07701551914215088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.14666748046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015279842540621758,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10746783018112183,
      "backward_entropy": 0.0770155390103658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.63438415527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015355420298874378,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10743660926818847,
      "backward_entropy": 0.0769533978568183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.34683227539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01543267909437418,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10740461349487304,
      "backward_entropy": 0.07701555887858073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.0366668701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015516283921897411,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10737099647521972,
      "backward_entropy": 0.07701558536953396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.75619506835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015603556297719479,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10733616352081299,
      "backward_entropy": 0.07701560523774889,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.37396240234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01569507271051407,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10730018615722656,
      "backward_entropy": 0.07701563172870213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.0957794189453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015785453841090202,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10726394653320312,
      "backward_entropy": 0.07701436678568523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.4158935546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015872761607170105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10722784996032715,
      "backward_entropy": 0.07694939772288005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.48175048828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015955980867147446,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10719213485717774,
      "backward_entropy": 0.07701566484239367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.97601318359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01604151912033558,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10715543031692505,
      "backward_entropy": 0.07701566484239367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.76132202148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016126297414302826,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10711841583251953,
      "backward_entropy": 0.07694624529944526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.4764404296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01621312089264393,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10708043575286866,
      "backward_entropy": 0.07701563835144043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.61314392089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01630098558962345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10704171657562256,
      "backward_entropy": 0.07694384124543932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.1631622314453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016386423259973526,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10700263977050781,
      "backward_entropy": 0.07701494958665636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.31105041503906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016469810158014297,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1069633960723877,
      "backward_entropy": 0.07701499594582452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.22105407714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016552802175283432,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10692366361618041,
      "backward_entropy": 0.07701539993286133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.77911376953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016640206798911095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10688238143920899,
      "backward_entropy": 0.07701532046000163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.74359130859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01672874204814434,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10684038400650024,
      "backward_entropy": 0.07693666881985134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.8605194091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01681649126112461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10679798126220703,
      "backward_entropy": 0.07693517208099365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.5260772705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01690496876835823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10675486326217651,
      "backward_entropy": 0.07693362236022949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.73255920410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016997413709759712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10671012401580811,
      "backward_entropy": 0.07693211237589519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.53600311279297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017090100795030594,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10666456222534179,
      "backward_entropy": 0.07701526747809516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.55690002441406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01717400550842285,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10662070512771607,
      "backward_entropy": 0.07701528734631008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.1680679321289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017260201275348663,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10657556056976318,
      "backward_entropy": 0.07701530721452501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.94871520996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017341088503599167,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10653119087219239,
      "backward_entropy": 0.07701435354020861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.14585876464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01742650382220745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10648503303527831,
      "backward_entropy": 0.07701415485805935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.6909637451172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017510006204247475,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10643882751464843,
      "backward_entropy": 0.07692060205671522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.0748291015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017591921612620354,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.106392502784729,
      "backward_entropy": 0.07701530721452501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.8115692138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01767555996775627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10634515285491944,
      "backward_entropy": 0.07691603899002075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.22384643554688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017760492861270905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10629684925079345,
      "backward_entropy": 0.0769137011633979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.37379455566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017841985449194908,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10624892711639404,
      "backward_entropy": 0.07691119776831733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.59962463378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01791728474199772,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10620219707489013,
      "backward_entropy": 0.07690849569108751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.47392272949219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017997995018959045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10615323781967163,
      "backward_entropy": 0.0770120620727539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.46332550048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018076209351420403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10610454082489014,
      "backward_entropy": 0.07690329021877712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.98432159423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018148818984627724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1060570240020752,
      "backward_entropy": 0.07690048217773438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.93745422363281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018219226971268654,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10600956678390502,
      "backward_entropy": 0.07701073090235393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.81727600097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0182876568287611,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10596213340759278,
      "backward_entropy": 0.07701018783781263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.66524505615234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018357662484049797,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10591357946395874,
      "backward_entropy": 0.07700961165957981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.28893280029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0184212327003479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10586662292480468,
      "backward_entropy": 0.07688772678375244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.39866638183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01848064735531807,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1058205246925354,
      "backward_entropy": 0.07700822088453504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.62275695800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018540827557444572,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10577362775802612,
      "backward_entropy": 0.07700743940141466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.82939147949219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018597055226564407,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10572748184204102,
      "backward_entropy": 0.0770148237546285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.90652465820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018650835379958153,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10568146705627442,
      "backward_entropy": 0.07687225606706408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.86700439453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0187090951949358,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10563325881958008,
      "backward_entropy": 0.07700465122858684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.93592834472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01877286098897457,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10558257102966309,
      "backward_entropy": 0.07700366444057888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.88058471679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018837394192814827,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10553112030029296,
      "backward_entropy": 0.0770026445388794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.0335693359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018902625888586044,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10547893047332764,
      "backward_entropy": 0.07701438003116184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.1705551147461,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018969587981700897,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10542540550231934,
      "backward_entropy": 0.07701430055830213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.10537719726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019032355397939682,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10537301301956177,
      "backward_entropy": 0.07701420783996582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.4698257446289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019097385928034782,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10531919002532959,
      "backward_entropy": 0.07699798875384861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.86028289794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019161321222782135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10526518821716309,
      "backward_entropy": 0.07699658473332723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.3958511352539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019221095368266106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10521223545074462,
      "backward_entropy": 0.07683262560102674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.919189453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019278787076473236,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10515954494476318,
      "backward_entropy": 0.07682747973336114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.59326171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019336272031068802,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10510635375976562,
      "backward_entropy": 0.07699156469768947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.8232650756836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0193963423371315,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10505149364471436,
      "backward_entropy": 0.07681700918409559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.1160888671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01945529878139496,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1049963116645813,
      "backward_entropy": 0.07698770364125569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.11627960205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019515331834554672,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10494011640548706,
      "backward_entropy": 0.07680549886491564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.39920043945312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019574817270040512,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10488355159759521,
      "backward_entropy": 0.0770130025015937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.14097595214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019635207951068878,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10482592582702636,
      "backward_entropy": 0.07698104116651747,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.9571304321289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019696148112416267,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10476737022399903,
      "backward_entropy": 0.07697857750786676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.583984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01975647173821926,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10470854043960572,
      "backward_entropy": 0.07701243294609918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.3436279296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01982557214796543,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10464537143707275,
      "backward_entropy": 0.07697343164020115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.0917739868164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019890880212187767,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10458307266235352,
      "backward_entropy": 0.07697061697642009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.42539978027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019953187555074692,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10452141761779785,
      "backward_entropy": 0.07696757051679823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.55995178222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020016014575958252,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10445886850357056,
      "backward_entropy": 0.07696436511145698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.46611785888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020079590380191803,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10439536571502686,
      "backward_entropy": 0.07696101400587294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.23968505859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020138656720519066,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10433324575424194,
      "backward_entropy": 0.07701106203926934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.88990783691406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020203707739710808,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10426785945892333,
      "backward_entropy": 0.07701081699795193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.89556884765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02027207426726818,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10420026779174804,
      "backward_entropy": 0.07671878072950575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.7657928466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02034079283475876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10413191318511963,
      "backward_entropy": 0.07671016454696655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.515869140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02041424810886383,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10406073331832885,
      "backward_entropy": 0.0769421656926473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.4833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02048615925014019,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10398975610733033,
      "backward_entropy": 0.0769379801220364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.63189697265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02055780403316021,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10391821861267089,
      "backward_entropy": 0.07668336232503255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.1373748779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020631292834877968,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10384526252746581,
      "backward_entropy": 0.07692905267079671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.66477966308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020704783499240875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1037717342376709,
      "backward_entropy": 0.07692436377207439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.19757843017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02077772468328476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10369768142700195,
      "backward_entropy": 0.0766550964779324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.52922821044922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020847368985414505,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10362472534179687,
      "backward_entropy": 0.07700804869333903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.66543579101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020915739238262177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10355168581008911,
      "backward_entropy": 0.07663434743881226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.62922668457031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020982544869184494,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10347864627838135,
      "backward_entropy": 0.07690268092685276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.61113739013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021046815440058708,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10340638160705566,
      "backward_entropy": 0.0768964687983195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.16547393798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021106954663991928,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10333561897277832,
      "backward_entropy": 0.07688977983262804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.06696319580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021162796765565872,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1032663106918335,
      "backward_entropy": 0.07688254117965698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.71038818359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021216632798314095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10319728851318359,
      "backward_entropy": 0.07657362355126275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.4818878173828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021277155727148056,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10312408208847046,
      "backward_entropy": 0.07686727576785618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.02239227294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021341582760214806,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10304796695709229,
      "backward_entropy": 0.07685956690046522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.26963806152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02140529826283455,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10297174453735351,
      "backward_entropy": 0.07685152027342054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.99301147460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021470079198479652,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10289427042007446,
      "backward_entropy": 0.07684324847327338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.01921844482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021535541862249374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10281574726104736,
      "backward_entropy": 0.07650431659486559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.66447448730469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021599778905510902,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1027371644973755,
      "backward_entropy": 0.07682579755783081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.44036865234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021661918610334396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10265929698944092,
      "backward_entropy": 0.07647391160329182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.01400756835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021726585924625397,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10257927179336548,
      "backward_entropy": 0.0768071280585395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.08515167236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021790409460663795,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10249904394149781,
      "backward_entropy": 0.07679737938774957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.52696228027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021852334961295128,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10241948366165161,
      "backward_entropy": 0.07678718037075466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.3328857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021915271878242493,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10233860015869141,
      "backward_entropy": 0.07640884982215033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.631591796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021982388570904732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10225458145141601,
      "backward_entropy": 0.07639196846220228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.05760955810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02205311693251133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10216768980026245,
      "backward_entropy": 0.07637488842010498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.5832748413086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022122271358966827,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10208101272583008,
      "backward_entropy": 0.07635702027214898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.90795135498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022190161049365997,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1019943356513977,
      "backward_entropy": 0.07633864879608154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.63137817382812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022256769239902496,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10190736055374146,
      "backward_entropy": 0.07699117395612928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.38748931884766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02232193946838379,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10182030200958252,
      "backward_entropy": 0.07670818434821235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.45913696289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022382637485861778,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10173585414886474,
      "backward_entropy": 0.07669425010681152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.93980407714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02244465984404087,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10165024995803833,
      "backward_entropy": 0.07667994499206543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.28323364257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02250945381820202,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10156217813491822,
      "backward_entropy": 0.07623426781760322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.25128936767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022574445232748985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10147306919097901,
      "backward_entropy": 0.07621137301127116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.251708984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02263818122446537,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10138392448425293,
      "backward_entropy": 0.07663529449039036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.9720458984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022709209471940994,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10128954648971558,
      "backward_entropy": 0.07616373565461901,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.20390319824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022777380421757698,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1011965274810791,
      "backward_entropy": 0.07613942358228895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.32817077636719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02284354344010353,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10110379457473755,
      "backward_entropy": 0.07697946495480007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.55349731445312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0229057427495718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10101330280303955,
      "backward_entropy": 0.07608671983083089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.86109924316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022969596087932587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10092122554779052,
      "backward_entropy": 0.0760600831773546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.28675079345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023034797981381416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10082783699035644,
      "backward_entropy": 0.07653197977277967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.6515655517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023098893463611603,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1007344126701355,
      "backward_entropy": 0.07651236322191027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.52137756347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02316845953464508,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1006365418434143,
      "backward_entropy": 0.07649295859866673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.72753143310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023242931813001633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10053467750549316,
      "backward_entropy": 0.07594851652781169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.6065673828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023311322554945946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10043683052062988,
      "backward_entropy": 0.07591879367828369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.62704467773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023381903767585754,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.100336754322052,
      "backward_entropy": 0.07643185721503364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.96851348876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023447798565030098,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10023939609527588,
      "backward_entropy": 0.07640920745001899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.89765167236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02351108007133007,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1001432180404663,
      "backward_entropy": 0.07638524638281928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.1481475830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02357025071978569,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10004924535751343,
      "backward_entropy": 0.0763597223493788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.1188735961914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0236335638910532,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0999514102935791,
      "backward_entropy": 0.07633428441153632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.97947692871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023696132004261017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.099853515625,
      "backward_entropy": 0.0763078663084242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.0569305419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023762257769703865,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09975279569625854,
      "backward_entropy": 0.07568199104732937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.38536834716797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023831140249967575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09964978694915771,
      "backward_entropy": 0.07564671834309895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.74082946777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02389581874012947,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09954925775527954,
      "backward_entropy": 0.07622913519541423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.298828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023964403197169304,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09944520592689514,
      "backward_entropy": 0.07620159122678968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.59774780273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02403491735458374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09933902025222778,
      "backward_entropy": 0.0761733849843343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.32414245605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024107761681079865,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09923075437545777,
      "backward_entropy": 0.07614496681425306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.70782470703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024172471836209297,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09912819862365722,
      "backward_entropy": 0.07694080140855578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.60917663574219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024242961779236794,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09902087450027466,
      "backward_entropy": 0.07693860265943739,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.55109405517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024309802800416946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09891536831855774,
      "backward_entropy": 0.07604963911904229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.89446258544922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024374114349484444,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09881120920181274,
      "backward_entropy": 0.07601524723900689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.40534973144531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024437306448817253,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09870707988739014,
      "backward_entropy": 0.07597940497928196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.73814392089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024499647319316864,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09860297441482543,
      "backward_entropy": 0.07692688041263157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.6166000366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024563133716583252,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09849749803543091,
      "backward_entropy": 0.07517463631100124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.5434799194336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024624288082122803,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09839329719543458,
      "backward_entropy": 0.07586496406131321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.3784942626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024685414507985115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09828879237174988,
      "backward_entropy": 0.07506925529903835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.98424530029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024753201752901077,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09817888140678406,
      "backward_entropy": 0.07578531901041667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.58563232421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024816308170557022,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09807206392288208,
      "backward_entropy": 0.07574323150846693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.30270385742188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02488035336136818,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09796391725540161,
      "backward_entropy": 0.07690661483340794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.84325408935547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024938615038990974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09785996675491333,
      "backward_entropy": 0.07484812206692165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.1392593383789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02499685063958168,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09775564670562745,
      "backward_entropy": 0.07689827018313938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.57544708251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025054914876818657,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09765089750289917,
      "backward_entropy": 0.07472589280870226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.33447265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025115055963397026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09754409790039062,
      "backward_entropy": 0.075507918993632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.1560287475586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025174692273139954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09743720293045044,
      "backward_entropy": 0.07460033893585205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.13446807861328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025232255458831787,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09733147621154785,
      "backward_entropy": 0.0768807397948371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.54751586914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02528958208858967,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0972254991531372,
      "backward_entropy": 0.07534942362043592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.72008514404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025344453752040863,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09712073802947999,
      "backward_entropy": 0.07529245482550727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.9973373413086,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025398200377821922,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09701651334762573,
      "backward_entropy": 0.07686534192827013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.62310791015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025449147447943687,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09691425561904907,
      "backward_entropy": 0.07685965961880153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.07269287109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025507472455501556,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09680548906326295,
      "backward_entropy": 0.07417118549346924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.17058563232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025569360703229904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09669375419616699,
      "backward_entropy": 0.07409751415252686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.16851806640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025633150711655617,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09658043384552002,
      "backward_entropy": 0.07684663931528728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.87413787841797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02569742500782013,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09646605253219605,
      "backward_entropy": 0.07494240336947972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.3039093017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02576260268688202,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09635066390037536,
      "backward_entropy": 0.07386831442515056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.4659652709961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02583036571741104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0962327241897583,
      "backward_entropy": 0.07378825214174059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.32241821289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02589675784111023,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09611550569534302,
      "backward_entropy": 0.07475643025504218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.54891967773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025964315980672836,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09599740505218506,
      "backward_entropy": 0.07469126913282606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.22257995605469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026035532355308533,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09587572813034058,
      "backward_entropy": 0.07353187931908502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.14476013183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026106972247362137,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09575355052947998,
      "backward_entropy": 0.07456040382385254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.41041564941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02618403360247612,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09562691450119018,
      "backward_entropy": 0.07449618975321452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.90010070800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026262491941452026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09549900889396667,
      "backward_entropy": 0.07443083657158746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.61408996582031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02633456513285637,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09537692070007324,
      "backward_entropy": 0.07316901948716906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.29849243164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0264048483222723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09525600075721741,
      "backward_entropy": 0.07306949297587077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.19400024414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026468928903341293,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09514043331146241,
      "backward_entropy": 0.07420408725738525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.02131652832031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026530547067523003,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09502663612365722,
      "backward_entropy": 0.07411954138014051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.12602996826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0265891645103693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09491472840309143,
      "backward_entropy": 0.07403037283155653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.35353088378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026649292558431625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09480116367340088,
      "backward_entropy": 0.07262010044521755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.1202621459961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026712562888860703,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09468479752540589,
      "backward_entropy": 0.07250213623046875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.61441040039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02677261456847191,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0945716679096222,
      "backward_entropy": 0.0723819997575548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.27984619140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02683790773153305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09445385336875915,
      "backward_entropy": 0.07226492298973931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.44833374023438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02690567634999752,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09433355331420898,
      "backward_entropy": 0.07357982132169935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.05282592773438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02697337046265602,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09421327114105224,
      "backward_entropy": 0.07673954963684082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.15650177001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027036471292376518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09409657716751099,
      "backward_entropy": 0.07190155320697361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.71795654296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027094263583421707,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09398466348648071,
      "backward_entropy": 0.07328640752368504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.52973937988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02714892290532589,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09387540221214294,
      "backward_entropy": 0.07163163026173909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.2729949951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02719697542488575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09377192258834839,
      "backward_entropy": 0.07148604922824436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.4902114868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027254914864897728,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09365957975387573,
      "backward_entropy": 0.07134760750664605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.11660766601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02731347270309925,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09354668855667114,
      "backward_entropy": 0.07284112771352132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.80664825439453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027368517592549324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09343676567077637,
      "backward_entropy": 0.07106227344936794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.56688690185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027426116168498993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09332475662231446,
      "backward_entropy": 0.07091775205400255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.99301147460938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027485227212309837,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0932111382484436,
      "backward_entropy": 0.07665624883439806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.89612579345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027544863522052765,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09309723377227783,
      "backward_entropy": 0.07237107223934597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.73899841308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02760598063468933,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09298186302185059,
      "backward_entropy": 0.07224943902757433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.50293731689453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027664152905344963,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09286953210830688,
      "backward_entropy": 0.07662802934646606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.0776138305664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027721162885427475,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09275758266448975,
      "backward_entropy": 0.07016008430057102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.89776611328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027780095115303993,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09264352321624755,
      "backward_entropy": 0.07185449865129259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.85968780517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0278379637748003,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09253047108650207,
      "backward_entropy": 0.07171613640255398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.63435363769531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027892587706446648,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09242039918899536,
      "backward_entropy": 0.06966858439975315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.00204467773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027949392795562744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09230809211730957,
      "backward_entropy": 0.06949723429150051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.55616760253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028010595589876175,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09219228029251099,
      "backward_entropy": 0.07128074434068468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.55236053466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028068238869309425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09207980632781983,
      "backward_entropy": 0.06915323601828681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.44816589355469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02812761627137661,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09196555018424987,
      "backward_entropy": 0.07097591956456502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.02992248535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028184952214360237,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09185307025909424,
      "backward_entropy": 0.0708158016204834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.19451141357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028248125687241554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09173561930656433,
      "backward_entropy": 0.06860432359907362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.69882202148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028305744752287865,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0916235089302063,
      "backward_entropy": 0.07049634721544054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.60089111328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028361674398183823,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09151292443275452,
      "backward_entropy": 0.07032565275828044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.69779205322266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028424423187971115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09139606356620789,
      "backward_entropy": 0.06801622443728977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.18202209472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02848530374467373,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09128133058547974,
      "backward_entropy": 0.06998950905270046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.59121704101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028549904003739357,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09116357564926147,
      "backward_entropy": 0.06982029808892144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.68171310424805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02861930802464485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09104192852973939,
      "backward_entropy": 0.06740492582321167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.72364044189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02868124283850193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0909277081489563,
      "backward_entropy": 0.067190772957272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.3697280883789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028742888942360878,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09081388711929321,
      "backward_entropy": 0.06928717427783543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.11714935302734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028806079179048538,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09069910645484924,
      "backward_entropy": 0.06909869776831733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.024864196777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028865577653050423,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09058808088302613,
      "backward_entropy": 0.0689013401667277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.11648559570312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02892092615365982,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0904813528060913,
      "backward_entropy": 0.06869653198454115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.96666717529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028976671397686005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09037425518035888,
      "backward_entropy": 0.06603286663691203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.68157958984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029032781720161438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0902668595314026,
      "backward_entropy": 0.06578655375374688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.153831481933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02909429371356964,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09015469551086426,
      "backward_entropy": 0.06807312700483534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.9919891357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02914866991341114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09004957675933838,
      "backward_entropy": 0.06785321235656738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.54109191894531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029208308085799217,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0899397611618042,
      "backward_entropy": 0.06502768728468153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.9080581665039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02927039936184883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08982839584350585,
      "backward_entropy": 0.0647740364074707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.27632141113281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029330281540751457,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0897194504737854,
      "backward_entropy": 0.06720225016276042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.4929428100586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029392117634415627,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08960923552513123,
      "backward_entropy": 0.06697843472162883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.16460418701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02945392206311226,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08949956893920899,
      "backward_entropy": 0.06675003634558783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.42425537109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029512086883187294,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08939343094825744,
      "backward_entropy": 0.07632084687550862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.74754333496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02955983206629753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08929750919342042,
      "backward_entropy": 0.06625623173183864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.79732513427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029612742364406586,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08919761180877686,
      "backward_entropy": 0.06600879298316108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.34170532226562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02966291643679142,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08910049796104431,
      "backward_entropy": 0.06575109561284383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.32183074951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02971442975103855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08900250196456909,
      "backward_entropy": 0.062491039435068764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.52254867553711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029769128188490868,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08890211582183838,
      "backward_entropy": 0.065237561861674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.82819366455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0298197902739048,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08880597949028016,
      "backward_entropy": 0.0649713675181071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.52857971191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02986929751932621,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08871108293533325,
      "backward_entropy": 0.06469757027096218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.34182739257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02992435172200203,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08861186504364013,
      "backward_entropy": 0.06443591912587483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.21659851074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029976852238178253,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08851544857025147,
      "backward_entropy": 0.060955663522084556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.44404602050781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030027063563466072,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08842154741287231,
      "backward_entropy": 0.06388297345903185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.99993896484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030075056478381157,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08833006620407105,
      "backward_entropy": 0.06358721521165636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.65426635742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030118733644485474,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08824280500411988,
      "backward_entropy": 0.06327672137154473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.034889221191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030163664370775223,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08815467357635498,
      "backward_entropy": 0.06296301550335354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.95450210571289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03020544908940792,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0880698800086975,
      "backward_entropy": 0.06263934241400824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.1722526550293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03024241328239441,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08798986673355103,
      "backward_entropy": 0.05889897214041816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.23773956298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03027687966823578,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08791245222091675,
      "backward_entropy": 0.058527105384402804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.20802307128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03031640127301216,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08783090114593506,
      "backward_entropy": 0.061615420712365046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.2138442993164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030355535447597504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08775019645690918,
      "backward_entropy": 0.06127527025010851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.33173370361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0303996242582798,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08766567707061768,
      "backward_entropy": 0.060947285758124456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.33910369873047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03044222667813301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08758295774459839,
      "backward_entropy": 0.057092216279771596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.9627914428711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030486809089779854,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08749921321868896,
      "backward_entropy": 0.0602794885635376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.19816589355469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030531371012330055,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0874161958694458,
      "backward_entropy": 0.056362801127963595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.63936614990234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.030579563230276108,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08733024597167968,
      "backward_entropy": 0.07583600944942898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.49569702148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030628420412540436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08724424242973328,
      "backward_entropy": 0.059275772836473256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.86909484863281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03067738562822342,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08715873956680298,
      "backward_entropy": 0.05894210603502062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.52597045898438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03072531893849373,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08707468509674073,
      "backward_entropy": 0.07578065660264757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.16517639160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030773822218179703,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08699087500572204,
      "backward_entropy": 0.05826992458767361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.983829498291016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030818693339824677,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08691104650497436,
      "backward_entropy": 0.05414331621593899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.66100311279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030859502032399178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08683533668518066,
      "backward_entropy": 0.0537555283970303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.87295532226562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030898958444595337,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08676112294197083,
      "backward_entropy": 0.057175258795420326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.1859359741211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03093755804002285,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08668801784515381,
      "backward_entropy": 0.056793914900885686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.69369506835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030979909002780914,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08661231994628907,
      "backward_entropy": 0.05642556481891208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.14927673339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03102712519466877,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0865332007408142,
      "backward_entropy": 0.056072519885169134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.40098571777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03107476234436035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08645431995391846,
      "backward_entropy": 0.05179997947480944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.58558654785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031117655336856842,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08638004660606384,
      "backward_entropy": 0.055346382988823786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.64506530761719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031155599281191826,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0863104522228241,
      "backward_entropy": 0.05099474721484714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.54400634765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03119676373898983,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08623919486999512,
      "backward_entropy": 0.05457282728619046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.674949645996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031239023432135582,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08616735935211181,
      "backward_entropy": 0.05419168869654337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.24321746826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03127444162964821,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08610198497772217,
      "backward_entropy": 0.04975419905450609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.09323120117188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03131156787276268,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08603548407554626,
      "backward_entropy": 0.05338352256351047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.38423919677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03135025128722191,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08596808314323426,
      "backward_entropy": 0.04890626006656223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.72667694091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031388361006975174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08590162992477417,
      "backward_entropy": 0.04848167631361219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.10356903076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0314309224486351,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08583192825317383,
      "backward_entropy": 0.04806154304080539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.79621887207031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03147256746888161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08576353788375854,
      "backward_entropy": 0.047636568546295166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.81359100341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03151920810341835,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08569160699844361,
      "backward_entropy": 0.04722709457079569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.110111236572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03156456723809242,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08562188744544982,
      "backward_entropy": 0.04680879248513116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.04586791992188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031605131924152374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08555684685707092,
      "backward_entropy": 0.04637465543217129,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.77960968017578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031648702919483185,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08549064993858338,
      "backward_entropy": 0.05023828479978773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.91167068481445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03169170767068863,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0854255199432373,
      "backward_entropy": 0.04983586735195584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.49955749511719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03173239529132843,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08536267876625062,
      "backward_entropy": 0.04942506551742554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.02788543701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03177449852228165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08529895544052124,
      "backward_entropy": 0.04901860157648722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.3371353149414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03181971609592438,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08523368835449219,
      "backward_entropy": 0.048622687657674156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.8316650390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03186524659395218,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08516888618469239,
      "backward_entropy": 0.04822322063975864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.951337814331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03191668540239334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0851003110408783,
      "backward_entropy": 0.043371650907728404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.53182601928711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03195925056934357,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08503960371017456,
      "backward_entropy": 0.04742900199360318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.69758605957031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03199785575270653,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08498297333717346,
      "backward_entropy": 0.0750357707341512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.77538299560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03204183652997017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08492261171340942,
      "backward_entropy": 0.04659830861621433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.00117492675781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03209049627184868,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08485930562019348,
      "backward_entropy": 0.041626161999172635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.91433715820312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032141584903001785,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08479451537132263,
      "backward_entropy": 0.04583240548769633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.91411590576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03219418600201607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08472993969917297,
      "backward_entropy": 0.040796528259913124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.38359069824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03225227817893028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08466236591339112,
      "backward_entropy": 0.040389812654919095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.3914794921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032315485179424286,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08459222316741943,
      "backward_entropy": 0.04473412699169583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.986488342285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03237519785761833,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08452597856521607,
      "backward_entropy": 0.04437245925267538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.56285858154297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0324319563806057,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08446240425109863,
      "backward_entropy": 0.04400352636973063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.73332977294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032489921897649765,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08439890146255494,
      "backward_entropy": 0.043632709317737155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.0400161743164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032545916736125946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08433761596679687,
      "backward_entropy": 0.04325282242563036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.3634033203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03260050714015961,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08427831530570984,
      "backward_entropy": 0.03797722193929884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.5522689819336,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032650768756866455,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08422274589538574,
      "backward_entropy": 0.07487776544358996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.93543243408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03270239382982254,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0841666579246521,
      "backward_entropy": 0.04209993282953898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.16309356689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03275422379374504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08411104679107666,
      "backward_entropy": 0.041720188326305814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.79237365722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03280533850193024,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08405646085739135,
      "backward_entropy": 0.036363558636771307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.61967468261719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03286968544125557,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08399404287338257,
      "backward_entropy": 0.04101592302322388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.92259979248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0329354964196682,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08393217325210571,
      "backward_entropy": 0.04069662094116211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.51461791992188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033000342547893524,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0838718831539154,
      "backward_entropy": 0.035297897126939565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.15548706054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03306255489587784,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08381401300430298,
      "backward_entropy": 0.03492465284135607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.02109146118164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03312757983803749,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.083754962682724,
      "backward_entropy": 0.03968846797943115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.3813705444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03318890556693077,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08369938135147095,
      "backward_entropy": 0.03417565094100104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.85747528076172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0332496203482151,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08364499807357788,
      "backward_entropy": 0.07482634650336371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.03031921386719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03330889716744423,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08359215259552003,
      "backward_entropy": 0.03861944874127706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.479825973510742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03336850926280022,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08353986740112304,
      "backward_entropy": 0.033023443486955434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.61565399169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03341950476169586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08349401950836181,
      "backward_entropy": 0.032626615630255804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.92508697509766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03346993029117584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08344919681549072,
      "backward_entropy": 0.037495666080051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.61030578613281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033522382378578186,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08340364694595337,
      "backward_entropy": 0.037116037474738225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.05183410644531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03357469663023949,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08335858583450317,
      "backward_entropy": 0.03674566083484226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.50086975097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033627476543188095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08331433534622193,
      "backward_entropy": 0.03106470240486993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.45626831054688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03367570415139198,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08327388763427734,
      "backward_entropy": 0.0359946952925788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.456504821777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03372503072023392,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08323315382003785,
      "backward_entropy": 0.035616536935170494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.79405975341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033774301409721375,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08319286108016968,
      "backward_entropy": 0.03524458739492628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.26708984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03381802886724472,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08315688371658325,
      "backward_entropy": 0.03485793206426832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.87283706665039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03386053815484047,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08312206864356994,
      "backward_entropy": 0.034474187427096896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.50654602050781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033903058618307114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08308749794960021,
      "backward_entropy": 0.03409266471862793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.563838958740234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03394579887390137,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08305304050445557,
      "backward_entropy": 0.028343399365743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.894845962524414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0339885950088501,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08301914930343628,
      "backward_entropy": 0.033340897825029164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.6585235595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0340254083275795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08298962116241455,
      "backward_entropy": 0.027576772703064814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.08602905273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.034073833376169205,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08295291662216187,
      "backward_entropy": 0.07447066571977404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.66197967529297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03412342444062233,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08291617035865784,
      "backward_entropy": 0.03228475650151571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.44667053222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03417585417628288,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08287820219993591,
      "backward_entropy": 0.031959354877471924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.61613845825195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034226275980472565,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08284202814102173,
      "backward_entropy": 0.03163169158829583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.840782165527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03427616506814957,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08280694484710693,
      "backward_entropy": 0.025852368937598333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.20502471923828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0343252569437027,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08277311325073242,
      "backward_entropy": 0.03096637460920546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.17863464355469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03437645733356476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08273822069168091,
      "backward_entropy": 0.025165637334187824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.96536254882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03443463146686554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08270058035850525,
      "backward_entropy": 0.02485424280166626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.02883911132812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034494418650865555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08266270160675049,
      "backward_entropy": 0.024538225597805448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.72948455810547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03456496819853783,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08262007236480713,
      "backward_entropy": 0.02980202105310228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.1507568359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03463435545563698,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0825793445110321,
      "backward_entropy": 0.02953319748242696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.006839752197266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03470044583082199,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08254097700119019,
      "backward_entropy": 0.029260675112406414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.367313385009766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03476359322667122,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08250465393066406,
      "backward_entropy": 0.028985387749142118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.425479888916016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03482585400342941,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08246909379959107,
      "backward_entropy": 0.023066156440311007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.87596130371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03488369286060333,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08243647813796998,
      "backward_entropy": 0.02277000082863702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.03218078613281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034941401332616806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08240418434143067,
      "backward_entropy": 0.022482431597179837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.259979248046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034995369613170624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08237447142601013,
      "backward_entropy": 0.02219388551182217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.19334411621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03504565730690956,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08234729766845703,
      "backward_entropy": 0.0744484265645345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.01498413085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03509907424449921,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08231889009475708,
      "backward_entropy": 0.027319303817219205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.7455997467041,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03516571596264839,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.082284814119339,
      "backward_entropy": 0.027086353964275785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.59832763671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03522560000419617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.082255619764328,
      "backward_entropy": 0.021090305513805814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.9673957824707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035284608602523804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08222728967666626,
      "backward_entropy": 0.0208222187227673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.545284271240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035343363881111145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08219934105873108,
      "backward_entropy": 0.026341423392295837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.50955581665039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03540020063519478,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08217263221740723,
      "backward_entropy": 0.026099974910418194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.36846923828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035457223653793335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08214600086212158,
      "backward_entropy": 0.020055380132463243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.91250610351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03551049157977104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08212172985076904,
      "backward_entropy": 0.019801442821820576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.86392593383789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03556743264198303,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08209621906280518,
      "backward_entropy": 0.025402519438001845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.08268737792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035621024668216705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08207283616065979,
      "backward_entropy": 0.019316267636087205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.87688446044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03567693382501602,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08204800486564637,
      "backward_entropy": 0.02496161725785997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.75698852539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03573935851454735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08201996088027955,
      "backward_entropy": 0.018880726562605962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.89830017089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035807475447654724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08199077844619751,
      "backward_entropy": 0.018680873844358657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.75801086425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03587191179394722,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08196375370025635,
      "backward_entropy": 0.02440215316083696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.87572479248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035931870341300964,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0819395899772644,
      "backward_entropy": 0.024205138285954792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.29376220703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03599275276064873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08191553354263306,
      "backward_entropy": 0.018061142828729417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.00246810913086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03605498746037483,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08189120292663574,
      "backward_entropy": 0.023815820614496868,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.6361198425293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03611694276332855,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0818674087524414,
      "backward_entropy": 0.02362863885031806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.22153854370117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03617730736732483,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08184467554092408,
      "backward_entropy": 0.023445326420995925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.307899475097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036233726888895035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08182440400123596,
      "backward_entropy": 0.023249735434850056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.05489349365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03629084676504135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08180404901504516,
      "backward_entropy": 0.02306481699148814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.23567199707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03635047748684883,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0817832589149475,
      "backward_entropy": 0.022875539130634732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.31304168701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036413222551345825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0817617654800415,
      "backward_entropy": 0.016657032900386386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.33891677856445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036476943641901016,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08173998594284057,
      "backward_entropy": 0.022525148259268865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.71749877929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03653701767325401,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08172025680541992,
      "backward_entropy": 0.022349557942814298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.893672943115234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03659870848059654,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08169999718666077,
      "backward_entropy": 0.022182368569903903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.664005279541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036660030484199524,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08168009519577027,
      "backward_entropy": 0.015903760989507038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.62024688720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03671986609697342,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08166110515594482,
      "backward_entropy": 0.01572372019290924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.10585021972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03678472340106964,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08163984417915345,
      "backward_entropy": 0.021714177396562364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.25094604492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036849454045295715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08161895871162414,
      "backward_entropy": 0.015397147999869453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.49765014648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0369197241961956,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08159657716751098,
      "backward_entropy": 0.015240670906172859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.55819702148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03698936477303505,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08157507181167603,
      "backward_entropy": 0.02130754788716634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.14960479736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03705960512161255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08155353665351868,
      "backward_entropy": 0.0149064130253262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.86475372314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03713172301650047,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0815317988395691,
      "backward_entropy": 0.014742442303233676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.89834594726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03720124065876007,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08151145577430725,
      "backward_entropy": 0.02089621623357137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.4576187133789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037272412329912186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08149086236953736,
      "backward_entropy": 0.014415173066986932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.18492126464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0373486764729023,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0814688742160797,
      "backward_entropy": 0.014259348313013712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.0659408569336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03741999715566635,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08144962787628174,
      "backward_entropy": 0.020494525631268818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.645687103271484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03749608248472214,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08142846822738647,
      "backward_entropy": 0.02036584085888333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.62734985351562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03756844252347946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08140963315963745,
      "backward_entropy": 0.020224998394648235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.552162170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03764238581061363,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08139019012451172,
      "backward_entropy": 0.020087964004940458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.401573181152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03771203011274338,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0813729703426361,
      "backward_entropy": 0.01345841172668669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.05254364013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037782732397317886,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08135535717010497,
      "backward_entropy": 0.019829862647586398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.58099365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037857163697481155,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08133591413497925,
      "backward_entropy": 0.019720729854371812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.69422912597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037930890917778015,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08131697177886962,
      "backward_entropy": 0.019622885518603854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.30702209472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0380014069378376,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08130004405975341,
      "backward_entropy": 0.01951522297329373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.432857513427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038069531321525574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08128442168235779,
      "backward_entropy": 0.019406745831171673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.751140594482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038136985152959824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08126944303512573,
      "backward_entropy": 0.012619528505537245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.32525634765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03819912299513817,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08125721216201783,
      "backward_entropy": 0.01918496357070075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.02950668334961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038262683898210526,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08124470114707946,
      "backward_entropy": 0.012337792250845168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.2578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03832414001226425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08123337626457214,
      "backward_entropy": 0.012194856173462339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.002197265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038391780108213425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08121914863586426,
      "backward_entropy": 0.018855103188090853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.527240753173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038459859788417816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08120551109313964,
      "backward_entropy": 0.011925347977214389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.481422424316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03852236270904541,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08119476437568665,
      "backward_entropy": 0.011783800191349454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.366703033447266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038586169481277466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08118329048156739,
      "backward_entropy": 0.011644428802861108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.556556701660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038651008158922195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08117121458053589,
      "backward_entropy": 0.01150537199444241,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.978492736816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03871385008096695,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08115983009338379,
      "backward_entropy": 0.018315388096703425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.98696517944336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038775112479925156,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08114937543869019,
      "backward_entropy": 0.018225269185172185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.623044967651367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03883465752005577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08114017248153686,
      "backward_entropy": 0.011135396030214097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.45892333984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038892004638910294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08113195896148681,
      "backward_entropy": 0.01101610561211904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.948753356933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03894997760653496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08112313747406005,
      "backward_entropy": 0.010898976690239377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.03052520751953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03901058807969093,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0811126708984375,
      "backward_entropy": 0.010791723926862081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.626928329467773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03907085210084915,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08110282421112061,
      "backward_entropy": 0.017828666501575045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.43299865722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03912799060344696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08109481334686279,
      "backward_entropy": 0.017761276827918157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.76152420043945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03918885439634323,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08108532428741455,
      "backward_entropy": 0.01769820186826918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.440000534057617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0392492301762104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08107599020004272,
      "backward_entropy": 0.01039550370640225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.098346710205078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03930553048849106,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08106852769851684,
      "backward_entropy": 0.07552195919884576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.214086532592773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03935961797833443,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08106220960617065,
      "backward_entropy": 0.01753126747078366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.798988342285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03941059112548828,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08105738759040833,
      "backward_entropy": 0.017476010653707717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.9490852355957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03946467861533165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08105131983757019,
      "backward_entropy": 0.017427869968944125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.771100997924805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039520297199487686,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08104456663131714,
      "backward_entropy": 0.01738807393444909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.09715270996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039573829621076584,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0810389757156372,
      "backward_entropy": 0.009848462210761176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.526939392089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039631228893995285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08103182315826415,
      "backward_entropy": 0.009763635694980621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.430519104003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03969009593129158,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08102407455444335,
      "backward_entropy": 0.009673175712426504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.3429946899414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039750050753355026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0810155987739563,
      "backward_entropy": 0.017205393976635404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.431753158569336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03981383517384529,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08100579977035523,
      "backward_entropy": 0.0171701494190428,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.72223663330078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03987162187695503,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08099893927574157,
      "backward_entropy": 0.017126412855254278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.07854652404785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0399286150932312,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08099254369735717,
      "backward_entropy": 0.017080770598517522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.701749801635742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03998377174139023,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08098692893981933,
      "backward_entropy": 0.009260919358995225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.127281188964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040035415440797806,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08098286390304565,
      "backward_entropy": 0.016993029250038996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.43601989746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04008505120873451,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08097968101501465,
      "backward_entropy": 0.00909039792087343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.843046188354492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04013902321457863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08097482919692993,
      "backward_entropy": 0.009005086289511787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.499534606933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040191225707530975,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0809707522392273,
      "backward_entropy": 0.016832977533340454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.881957054138184,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04024440795183182,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08096615672111511,
      "backward_entropy": 0.01677561302979787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.20889663696289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040293797850608826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08096360564231872,
      "backward_entropy": 0.016731715864605375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.82308578491211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04034461826086044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08096022605895996,
      "backward_entropy": 0.008677986760934195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.47940444946289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04039532318711281,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08095715045928956,
      "backward_entropy": 0.00859785493877199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.01673889160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0404469296336174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08095350861549377,
      "backward_entropy": 0.008527321947945489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.639381408691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04050133749842644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08094860315322876,
      "backward_entropy": 0.016571533348825242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.77793884277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040557004511356354,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08094322681427002,
      "backward_entropy": 0.016540911462571885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.51137924194336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04061690717935562,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08093621134757996,
      "backward_entropy": 0.016516404019461736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.68255615234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04067893698811531,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08092828392982483,
      "backward_entropy": 0.016492224401897855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.732959747314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04074707627296448,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0809178113937378,
      "backward_entropy": 0.016467955377366807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.80974197387695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04081131890416145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08090921640396118,
      "backward_entropy": 0.016453067461649578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.75355529785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04087807610630989,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08089941740036011,
      "backward_entropy": 0.016452888647715252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.63451385498047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04094501584768295,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08088971376419067,
      "backward_entropy": 0.016453097263971966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.40207290649414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0410119853913784,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08088006973266601,
      "backward_entropy": 0.016451327337159052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.19498062133789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04107939079403877,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08087023496627807,
      "backward_entropy": 0.016455431779225666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.330951690673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041149694472551346,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08085918426513672,
      "backward_entropy": 0.016459160380893283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.955284118652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04121703281998634,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0808496356010437,
      "backward_entropy": 0.007849748763773177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.66378402709961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04128476232290268,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.080839604139328,
      "backward_entropy": 0.0164728926287757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.204814910888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0413542278110981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08082865476608277,
      "backward_entropy": 0.007766023278236389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.39234924316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04141955450177193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08081981539726257,
      "backward_entropy": 0.007726529406176673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.160181045532227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04148644953966141,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08081037998199463,
      "backward_entropy": 0.016510566075642902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.20062255859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041549064218997955,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08080276846885681,
      "backward_entropy": 0.016515509949790105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.5072021484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04160729795694351,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08079724311828614,
      "backward_entropy": 0.01650809579425388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.41123962402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04166477546095848,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0807918906211853,
      "backward_entropy": 0.007542057169808282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.310829162597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04172611981630325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0807847261428833,
      "backward_entropy": 0.01648796432548099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.77138137817383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041786279529333115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08077846765518189,
      "backward_entropy": 0.007440223462051815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.788707733154297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04184715077280998,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08077176809310913,
      "backward_entropy": 0.016458072596126132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.52309036254883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041904516518116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08076654672622681,
      "backward_entropy": 0.016446034113566082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.51808166503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04196275398135185,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08076081871986389,
      "backward_entropy": 0.016432510481940374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.30453109741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0420280285179615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0807519793510437,
      "backward_entropy": 0.0072426290975676644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.06383514404297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042093001306056976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08074350953102112,
      "backward_entropy": 0.007193097637759315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.41988754272461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04216392710804939,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08073259592056274,
      "backward_entropy": 0.016405575805240206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.90667724609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04223325476050377,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08072242140769958,
      "backward_entropy": 0.01639713181389703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.030452728271484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04229939356446266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08071370720863343,
      "backward_entropy": 0.016383836666742962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.720191955566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04236702248454094,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08070408701896667,
      "backward_entropy": 0.016368902391857572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.664854049682617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04243164509534836,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08069568872451782,
      "backward_entropy": 0.006952534119288127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.17274856567383,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042493317276239395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08068867921829223,
      "backward_entropy": 0.016325776775677998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.55791091918945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04255557060241699,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08068130016326905,
      "backward_entropy": 0.0163030293252733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.57269287109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042621683329343796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08067237138748169,
      "backward_entropy": 0.016292760769526165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.751155853271484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042686827480793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08066365718841553,
      "backward_entropy": 0.006763169748915566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.935447692871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042752090841531754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806545615196228,
      "backward_entropy": 0.006720891015397178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.27556610107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0428210124373436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08064389824867249,
      "backward_entropy": 0.006684509416421254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.284767150878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04288819804787636,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08063390851020813,
      "backward_entropy": 0.016293240918053523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.94516372680664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042955413460731506,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806238293647766,
      "backward_entropy": 0.006606965843174193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.985836029052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0430217944085598,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08061404228210449,
      "backward_entropy": 0.006572763952944014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.869327545166016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04308828338980675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08060393929481506,
      "backward_entropy": 0.01630961729420556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.627681732177734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04315657168626785,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08059319257736205,
      "backward_entropy": 0.006505351099703047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.53264617919922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0432235486805439,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08058280944824218,
      "backward_entropy": 0.07654800017674764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.35188293457031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04328763484954834,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08057371377944947,
      "backward_entropy": 0.07655902703603108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.37921714782715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04335726425051689,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08056201934814453,
      "backward_entropy": 0.016341028942002192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.16050720214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04342252016067505,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08055232763290406,
      "backward_entropy": 0.01634037494659424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.90715789794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043486639857292175,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08054308891296387,
      "backward_entropy": 0.006327826115820143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.40999984741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043551430106163025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08053355216979981,
      "backward_entropy": 0.006293950809372796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.30057144165039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04362045228481293,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08052213191986084,
      "backward_entropy": 0.0163505838976966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.885112762451172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043692175298929214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08050915002822875,
      "backward_entropy": 0.006225699351893531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.316593170166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043760549277067184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08049758672714233,
      "backward_entropy": 0.006190852986441718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.15245819091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04382891580462456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08048582077026367,
      "backward_entropy": 0.006156896551450093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.97873306274414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043897390365600586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08047401905059814,
      "backward_entropy": 0.006124652094311184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.52469253540039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04396618530154228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08046206831932068,
      "backward_entropy": 0.016380477282736037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.709407806396484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044036462903022766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08044922351837158,
      "backward_entropy": 0.016398198074764676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.30767250061035,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04410640895366669,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0804362416267395,
      "backward_entropy": 0.07667574617597792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.776857376098633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04417330026626587,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08042473196983338,
      "backward_entropy": 0.016425400972366333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.107742309570312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044239215552806854,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08041363954544067,
      "backward_entropy": 0.005984942118326823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.562959671020508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04430270567536354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08040369749069214,
      "backward_entropy": 0.005959315018521415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.430411338806152,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04436532407999039,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08039406538009644,
      "backward_entropy": 0.016483792000346713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.880962371826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04442441090941429,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08038619756698609,
      "backward_entropy": 0.005908472256528007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.677364349365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04448140785098076,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08037923574447632,
      "backward_entropy": 0.01651872197786967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.562992095947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04453982412815094,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08037124872207642,
      "backward_entropy": 0.016539992557631597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.01787567138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04459928348660469,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0803627073764801,
      "backward_entropy": 0.01656278471151988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.903175354003906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.044658370316028595,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08035443425178528,
      "backward_entropy": 0.07674955659442478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.820966720581055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04471722990274429,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08034605979919433,
      "backward_entropy": 0.016618112723032635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.67927169799805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044775549322366714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08033773303031921,
      "backward_entropy": 0.005767928229437934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.580095291137695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0448378250002861,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08032724857330323,
      "backward_entropy": 0.0057483526567618055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.212160110473633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04489953815937042,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08031687140464783,
      "backward_entropy": 0.01671288079685635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.3999080657959,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04495910555124283,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0803076148033142,
      "backward_entropy": 0.07678874333699544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.034664154052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04501781985163689,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08029870986938477,
      "backward_entropy": 0.005688948349820243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.1738224029541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04507489874958992,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08029054403305054,
      "backward_entropy": 0.005668139292134179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.23360061645508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045131586492061615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08028247356414794,
      "backward_entropy": 0.005646360831128227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.819156646728516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045189425349235535,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08027368783950806,
      "backward_entropy": 0.01686502330833011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.458120822906494,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04524538293480873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08026578426361083,
      "backward_entropy": 0.005602711604701148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.94723129272461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04529747739434242,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08026010990142822,
      "backward_entropy": 0.016927181018723383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.70432662963867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045352671295404434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08025245070457458,
      "backward_entropy": 0.01696475346883138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.69331359863281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045409563928842545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08024365305900574,
      "backward_entropy": 0.01700981126891242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.396744728088379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04546831175684929,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08023365139961243,
      "backward_entropy": 0.005530251810948054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.330543518066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045523885637521744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0802254319190979,
      "backward_entropy": 0.017069151004155476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.27477264404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04558088630437851,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08021643161773681,
      "backward_entropy": 0.017103061079978943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.104637145996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045637041330337524,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08020764589309692,
      "backward_entropy": 0.017124725712670222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.188528060913086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045694030821323395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08019790649414063,
      "backward_entropy": 0.017141203085581463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.154590606689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04574791342020035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08019012212753296,
      "backward_entropy": 0.017157223489549425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.943361282348633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045798689126968384,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08018405437469482,
      "backward_entropy": 0.01716556813981798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.43937301635742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045848678797483444,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0801783561706543,
      "backward_entropy": 0.017181525627772015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.29207992553711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04590189829468727,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08017075061798096,
      "backward_entropy": 0.01720185743437873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.548429489135742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0459580197930336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0801612138748169,
      "backward_entropy": 0.017225705915027194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.863000869750977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046013686805963516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08015169501304627,
      "backward_entropy": 0.017244199911753338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.314348220825195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04606669396162033,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08014371395111083,
      "backward_entropy": 0.01726811793115404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.241046905517578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04612015560269356,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08013534545898438,
      "backward_entropy": 0.07688062058554755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.456083297729492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046173520386219025,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0801268458366394,
      "backward_entropy": 0.017329070303175185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.367280960083008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04622502252459526,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08011935353279113,
      "backward_entropy": 0.017344822486241657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.280902862548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046275146305561066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08011261224746705,
      "backward_entropy": 0.00519529605905215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.214811325073242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04632433131337166,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08010636568069458,
      "backward_entropy": 0.01737036969926622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.37074279785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04637257009744644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08010053634643555,
      "backward_entropy": 0.005148505998982323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.45350646972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04642279073596001,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08009332418441772,
      "backward_entropy": 0.017402264806959365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.991615295410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04647711664438248,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08008350133895874,
      "backward_entropy": 0.017416354682710435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.0015983581543,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04653008282184601,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08007444143295288,
      "backward_entropy": 0.017433701290024653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.313185691833496,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04658454284071922,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08006424903869629,
      "backward_entropy": 0.017454713582992554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.7457160949707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04663645848631859,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08005574941635132,
      "backward_entropy": 0.017481292287508648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.52324295043945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04669256508350372,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08004412055015564,
      "backward_entropy": 0.017508010069529217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.82414245605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04675454646348953,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08002876043319702,
      "backward_entropy": 0.017523234089215595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.75811767578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046820446848869324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08001081347465515,
      "backward_entropy": 0.004981112976868947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.553749084472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04688718542456627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0799920916557312,
      "backward_entropy": 0.00495417457487848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.01804733276367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04695514217019081,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07997244596481323,
      "backward_entropy": 0.004928336375289493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.90985107421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047022756189107895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07995293140411378,
      "backward_entropy": 0.01751891440815396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.707019805908203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047086045145988464,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07993602156639099,
      "backward_entropy": 0.017515187462170918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.777654647827148,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04714954271912575,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07991834878921508,
      "backward_entropy": 0.017514506975809734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.11405944824219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04720950871706009,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07990314960479736,
      "backward_entropy": 0.017520892951223586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.467159271240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047273606061935425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07988499402999878,
      "backward_entropy": 0.017521629730860393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.112852096557617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04733904451131821,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798657238483429,
      "backward_entropy": 0.01752264963255988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.960466384887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04740436002612114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07984648942947388,
      "backward_entropy": 0.017523528801070318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.91850662231445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04746946692466736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982698082923889,
      "backward_entropy": 0.004730268898937438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.72348403930664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04753551259636879,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980614900588989,
      "backward_entropy": 0.017513884438408747,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.48478889465332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04760294407606125,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978443503379821,
      "backward_entropy": 0.017514278491338093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.33354949951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04767048731446266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976250648498535,
      "backward_entropy": 0.004660492555962669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.173688888549805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0477374829351902,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974058389663696,
      "backward_entropy": 0.004638833718167411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.063230514526367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04780425503849983,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07971850633621216,
      "backward_entropy": 0.004618008931477864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.713321685791016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04786951467394829,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07969751358032226,
      "backward_entropy": 0.017550528049468994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.81316566467285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04793693497776985,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07967404127120972,
      "backward_entropy": 0.017553438742955525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.965302467346191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04800276830792427,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07965173721313476,
      "backward_entropy": 0.0045538196961085005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.904239654541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048064667731523514,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07963213324546814,
      "backward_entropy": 0.004533264372083876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.64577865600586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04813048988580704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0796093463897705,
      "backward_entropy": 0.004512882894939846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.10162353515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048193756490945816,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07958825826644897,
      "backward_entropy": 0.017587814066145156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.467329025268555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048256758600473404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0795669674873352,
      "backward_entropy": 0.004473631994591819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.09392547607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04831765964627266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07954713702201843,
      "backward_entropy": 0.004456013441085815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.294151306152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04837723821401596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07952792048454285,
      "backward_entropy": 0.004436629099978341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.19624710083008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04843461140990257,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07951018810272217,
      "backward_entropy": 0.017634906702571444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.761445999145508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04849390685558319,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07949070930480957,
      "backward_entropy": 0.004398927092552185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.045087814331055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04855237901210785,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07947157621383667,
      "backward_entropy": 0.01766380336549547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.542890548706055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0486089251935482,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07945381999015808,
      "backward_entropy": 0.004364275683959325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.88743782043457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04866482689976692,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07943617701530456,
      "backward_entropy": 0.017700117495324876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.919158935546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04871930927038193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07941958904266358,
      "backward_entropy": 0.01772701409127977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.724029541015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048776958137750626,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07939990162849427,
      "backward_entropy": 0.017748691969447665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.11760711669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04883265867829323,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07938172817230224,
      "backward_entropy": 0.017771000663439434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.02248191833496,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04888760298490524,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07936380505561828,
      "backward_entropy": 0.07693478796217176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.911741256713867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048942551016807556,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07934575080871582,
      "backward_entropy": 0.017818030383851793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.20642852783203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048996977508068085,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07932788133621216,
      "backward_entropy": 0.017846682005458407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.331369400024414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04905201494693756,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07930905818939209,
      "backward_entropy": 0.017869873179329768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.34650802612305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04910534620285034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0792914628982544,
      "backward_entropy": 0.004228542662329144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.8487548828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04916427284479141,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07926877737045288,
      "backward_entropy": 0.017911508679389954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.503621578216553,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049219850450754166,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07924877405166626,
      "backward_entropy": 0.017930669917000666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.01878547668457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04927141219377518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07923250794410705,
      "backward_entropy": 0.004183212916056315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.439695358276367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049321677535772324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07921720743179321,
      "backward_entropy": 0.004170257598161697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.54926300048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04937300831079483,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07920004725456238,
      "backward_entropy": 0.01801384323173099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.19620381295681,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0494268499314785,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07918029427528381,
      "backward_entropy": 0.01804499824841817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.43492889404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04947535693645477,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07916561961174011,
      "backward_entropy": 0.018076489369074505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.96051788330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04952770099043846,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0791472315788269,
      "backward_entropy": 0.004120392931832207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.955228805541992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04958118870854378,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07912758588790894,
      "backward_entropy": 0.018144720130496554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.500077247619629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049636516720056534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07910583019256592,
      "backward_entropy": 0.004097906665669547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.57550621032715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04969021677970886,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07908540368080139,
      "backward_entropy": 0.018208980560302734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.346392631530762,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049744732677936554,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07906386852264405,
      "backward_entropy": 0.01824356946680281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.292573928833008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04979768767952919,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07904360294342042,
      "backward_entropy": 0.018282011151313782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.204864501953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04985012859106064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0790234625339508,
      "backward_entropy": 0.0183159824874666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.096986770629883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04990477114915848,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07900092601776124,
      "backward_entropy": 0.01835219727622138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.94921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049959033727645874,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07897852659225464,
      "backward_entropy": 0.01839222510655721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.964315414428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05001423880457878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07895501852035522,
      "backward_entropy": 0.004028329418765174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.989205360412598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05006777495145798,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07893290519714355,
      "backward_entropy": 0.004021119740274217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.4478816986084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05011874809861183,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0789135217666626,
      "backward_entropy": 0.01853922340604994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.034302234649658,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05017167702317238,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07889119982719421,
      "backward_entropy": 0.018583675225575764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.504146575927734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050220802426338196,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0788726806640625,
      "backward_entropy": 0.0186325427558687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.22319221496582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0502699539065361,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0788536787033081,
      "backward_entropy": 0.018680926826265123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.528167724609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05032039433717728,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07883300185203553,
      "backward_entropy": 0.003986691849099265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.223007202148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050369273871183395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07881374955177307,
      "backward_entropy": 0.018773767683241103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.616117477416992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0504179410636425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07879428863525391,
      "backward_entropy": 0.003968702422247993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.623958587646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05046894773840904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07877213954925537,
      "backward_entropy": 0.003958905736605327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.54823112487793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05052552372217178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07874389290809632,
      "backward_entropy": 0.003948686023553212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.49602699279785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05057886615395546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07871872782707215,
      "backward_entropy": 0.018912759092119005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.092398643493652,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05063285306096077,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07869254350662232,
      "backward_entropy": 0.018944342931111652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.09897994995117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050685420632362366,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07866768836975098,
      "backward_entropy": 0.01898535920514001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.931164741516113,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050742439925670624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07863786220550537,
      "backward_entropy": 0.0039121607939402265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.984479904174805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05079726502299309,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07861019372940063,
      "backward_entropy": 0.019071676664882235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.853052139282227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05085252970457077,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0785814642906189,
      "backward_entropy": 0.01911030047469669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.20937156677246,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05090802535414696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07855165004730225,
      "backward_entropy": 0.019142753548092313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.59183120727539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05096254497766495,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07852274179458618,
      "backward_entropy": 0.019169989559385512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.9249267578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05101719871163368,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07849313020706176,
      "backward_entropy": 0.07699341244167751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.33168601989746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05107320100069046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07846153378486634,
      "backward_entropy": 0.003846351471212175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.0142879486084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05112938955426216,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07842925190925598,
      "backward_entropy": 0.01921121610535516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.202720642089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05118797719478607,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07839395999908447,
      "backward_entropy": 0.019219121999210782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.914756774902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05125000327825546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07835411429405212,
      "backward_entropy": 0.019230448537402682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.089887619018555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051311805844306946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07831428050994874,
      "backward_entropy": 0.003789211312929789,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.48910903930664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051374465227127075,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07827303409576417,
      "backward_entropy": 0.019267363680733576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.196815490722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051439639180898666,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07822818756103515,
      "backward_entropy": 0.0192750526799096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.845715522766113,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05150272324681282,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07818513512611389,
      "backward_entropy": 0.01927897996372647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.759467124938965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051562853157520294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07814512252807618,
      "backward_entropy": 0.00373048335313797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.042808532714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05162031948566437,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07810776829719543,
      "backward_entropy": 0.0192820496029324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.746065139770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051678162068128586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07806975841522217,
      "backward_entropy": 0.0037008850938744014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.398311614990234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051734764128923416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07803274393081665,
      "backward_entropy": 0.019310762484868366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.64373016357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05178820341825485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07799939513206482,
      "backward_entropy": 0.01933099329471588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.298379898071289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05184223875403404,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07796529531478882,
      "backward_entropy": 0.019358413086997137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.240715980529785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051893435418605804,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07793463468551635,
      "backward_entropy": 0.019392455617586773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.3121280670166,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05194170027971268,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07790707349777222,
      "backward_entropy": 0.019422004620234173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.165815353393555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051992155611515045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07787587642669677,
      "backward_entropy": 0.01945696108871036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.090426445007324,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052043382078409195,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07784299850463867,
      "backward_entropy": 0.019495162698957656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.067224502563477,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05209320783615112,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07781222462654114,
      "backward_entropy": 0.019540997015105352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.817420959472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052140798419713974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07778438329696655,
      "backward_entropy": 0.003620300441980362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.624719619750977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052189186215400696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0777548909187317,
      "backward_entropy": 0.019649750656551786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.928257465362549,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052239593118429184,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07772221565246581,
      "backward_entropy": 0.01970322098996904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.351285934448242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05228760465979576,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07769263982772827,
      "backward_entropy": 0.01976445482836829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.68014144897461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05233749374747276,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07765993475914001,
      "backward_entropy": 0.0198213044140074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.60791301727295,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052385974675416946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07762892246246338,
      "backward_entropy": 0.019881036546495225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.7476487159729,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05243288725614548,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07759957313537598,
      "backward_entropy": 0.019934601253933378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9209253787994385,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05247770994901657,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07757300138473511,
      "backward_entropy": 0.019995220833354525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.662724018096924,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05251940339803696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07755078077316284,
      "backward_entropy": 0.02005806565284729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.100574493408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052559223026037216,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0775308609008789,
      "backward_entropy": 0.020118089185820684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.586255073547363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05260373651981354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07750287652015686,
      "backward_entropy": 0.00358295523458057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.545842170715332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0526462197303772,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07747820019721985,
      "backward_entropy": 0.02022377649943034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.196578025817871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052686795592308044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07745597958564758,
      "backward_entropy": 0.0035738986399438647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.811925888061523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052726730704307556,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0774342656135559,
      "backward_entropy": 0.0035692122247483996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.69196891784668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05276716127991676,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07741153240203857,
      "backward_entropy": 0.020383540127012465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.5491943359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05281098559498787,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07738317251205444,
      "backward_entropy": 0.0035577966935104793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.17789649963379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052857644855976105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07734991908073426,
      "backward_entropy": 0.003548121286763085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.072771072387695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052904896438121796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07731524705886841,
      "backward_entropy": 0.020475816395547655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.960649490356445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052953220903873444,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07727868556976318,
      "backward_entropy": 0.020507540967729356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.214660167694092,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0530024953186512,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07724042534828186,
      "backward_entropy": 0.0205488039387597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.73777961730957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05304934084415436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07720561027526855,
      "backward_entropy": 0.0035167551702923244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.127342224121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05309705063700676,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07716906666755677,
      "backward_entropy": 0.020646025737126667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.079200744628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05314631015062332,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07712945938110352,
      "backward_entropy": 0.02068748904599084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.492762565612793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05319284275174141,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07709375619888306,
      "backward_entropy": 0.0207271890507804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.867768287658691,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0532379075884819,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07705993056297303,
      "backward_entropy": 0.02076326310634613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.370687484741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053282830864191055,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07702587842941284,
      "backward_entropy": 0.0034842946463161046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.702463150024414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05332683399319649,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0769930362701416,
      "backward_entropy": 0.020845789048406813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.496584892272949,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05337044224143028,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07696024179458619,
      "backward_entropy": 0.02088018258412679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.54746150970459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05341107398271561,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07693224549293518,
      "backward_entropy": 0.003464693824450175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.475846290588379,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05345156043767929,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07690370082855225,
      "backward_entropy": 0.020950309104389615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.758711338043213,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05349251255393028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07687414288520814,
      "backward_entropy": 0.003448023564285702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.719906806945801,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053531717509031296,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07684726119041443,
      "backward_entropy": 0.021026164293289185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.685771465301514,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05356888100504875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07682337760925292,
      "backward_entropy": 0.021056526237063937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.924845695495605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053604334592819214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07680199146270753,
      "backward_entropy": 0.0034229742983977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.38174819946289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05363974720239639,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07678053975105285,
      "backward_entropy": 0.021116778254508972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.060198783874512,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05367685854434967,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07675573825836182,
      "backward_entropy": 0.02114921311537425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.42814826965332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053714584559202194,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.076729416847229,
      "backward_entropy": 0.021183732483122084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.513552665710449,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05375446379184723,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0766987919807434,
      "backward_entropy": 0.02120493021276262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.293269395828247,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05379261076450348,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07667099833488464,
      "backward_entropy": 0.021229957540829975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.44381856918335,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05382809787988663,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07664780616760254,
      "backward_entropy": 0.021257473362816706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.412303447723389,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053862154483795166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07662697434425354,
      "backward_entropy": 0.003362199084626304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.6570405960083,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05389498174190521,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07660789489746093,
      "backward_entropy": 0.02131434612803989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.83790397644043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05392874404788017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07658650875091552,
      "backward_entropy": 0.021344211366441514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.10566644370555878,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05396512523293495,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07655973434448242,
      "backward_entropy": 0.021365433931350708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.374778747558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0539981909096241,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07653924822807312,
      "backward_entropy": 0.0213954895734787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.635156631469727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05403100699186325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07651885747909545,
      "backward_entropy": 0.021422704060872395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.395648002624512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05406789854168892,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07649083137512207,
      "backward_entropy": 0.003310623061325815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.224051475524902,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05410604923963547,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07645999193191529,
      "backward_entropy": 0.021477147936820984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.273109436035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05414330214262009,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07643032670021058,
      "backward_entropy": 0.021494234601656597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.129019737243652,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054184019565582275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07639394402503967,
      "backward_entropy": 0.021512645814153884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.046422958374023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054225772619247437,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07635525465011597,
      "backward_entropy": 0.0032711223595672185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.871557235717773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054267510771751404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07631645202636719,
      "backward_entropy": 0.003261722210380766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.888659477233887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054312095046043396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07627147436141968,
      "backward_entropy": 0.0032517901725239223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.733713150024414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05435648560523987,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07622671723365784,
      "backward_entropy": 0.021590171588791743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.4411678314209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05440162122249603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07618018388748168,
      "backward_entropy": 0.0032351002511050967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.868284225463867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05444914847612381,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07612855434417724,
      "backward_entropy": 0.021636436382929485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.556078910827637,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05449417978525162,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07608139514923096,
      "backward_entropy": 0.0032184448921018178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.784498691558838,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05453893169760704,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07603440880775451,
      "backward_entropy": 0.02169502278168996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.395870208740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05458126589655876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07599166631698609,
      "backward_entropy": 0.02172625230418311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.126053810119629,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0546233206987381,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07594896554946899,
      "backward_entropy": 0.021754023101594713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.031198501586914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05466608703136444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07590421438217163,
      "backward_entropy": 0.0031887928230894935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.399310111999512,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05470966547727585,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0758574664592743,
      "backward_entropy": 0.07701598935657078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0934210941195488,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05475214123725891,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07581260204315185,
      "backward_entropy": 0.021841767761442397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.021744728088379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054790396243333817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07577599287033081,
      "backward_entropy": 0.0031684736410776773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.380277633666992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05482902750372887,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07573829889297486,
      "backward_entropy": 0.021916119588745966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.877754211425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054869793355464935,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07569581270217896,
      "backward_entropy": 0.02195799516306983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.448205471038818,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05491041764616966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07565308809280395,
      "backward_entropy": 0.02199743191401164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.398362159729004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054948918521404266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07561452388763427,
      "backward_entropy": 0.003146444343858295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.951862335205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054988499730825424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07557271718978882,
      "backward_entropy": 0.003140227041310734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.46919059753418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05502974987030029,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07552692890167237,
      "backward_entropy": 0.02209462391005622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.5150785446167,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055073801428079605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07547484636306763,
      "backward_entropy": 0.02212543785572052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.262462615966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055117569863796234,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07542294263839722,
      "backward_entropy": 0.02216287122832404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.359818458557129,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05515887215733528,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07537590265274048,
      "backward_entropy": 0.022196787926885817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.385547637939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05519959703087807,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07532933950424195,
      "backward_entropy": 0.003105261880490515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.154188632965088,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055241551250219345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07527931928634643,
      "backward_entropy": 0.0030940864235162735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.655966758728027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0552816316485405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07523322701454163,
      "backward_entropy": 0.003085365725888146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.09074990451335907,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05532264709472656,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07518465518951416,
      "backward_entropy": 0.022271311945385404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.523608207702637,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05535987764596939,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07514442205429077,
      "backward_entropy": 0.02230593893263075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.938048362731934,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055396195501089096,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07510575056076049,
      "backward_entropy": 0.022336954871813457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.875481605529785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05543261766433716,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07506625652313233,
      "backward_entropy": 0.022364641229311626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.098804473876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05546936020255089,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07502571344375611,
      "backward_entropy": 0.022396840982966952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.3280792236328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055508676916360855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0749785304069519,
      "backward_entropy": 0.0030418485403060913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.067850112915039,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05554714426398277,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07493318915367127,
      "backward_entropy": 0.07701621452967326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.843910217285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05558644235134125,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07488501071929932,
      "backward_entropy": 0.02246790462070041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.808867931365967,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055623721331357956,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07484127283096313,
      "backward_entropy": 0.0030192782481511435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.778326034545898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055658940225839615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07480190396308899,
      "backward_entropy": 0.003010113620095783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.08135986328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05569241940975189,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07476620674133301,
      "backward_entropy": 0.02252311838997735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.719202041625977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055725302547216415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07473145127296447,
      "backward_entropy": 0.0029904190450906754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.301461219787598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05575673654675484,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07470000982284546,
      "backward_entropy": 0.0029803531037436593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.954414367675781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055788855999708176,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0746662974357605,
      "backward_entropy": 0.022564681039916143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.461823463439941,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055820681154727936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07463313937187195,
      "backward_entropy": 0.02258643342389001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.654397964477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055853746831417084,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07459696531295776,
      "backward_entropy": 0.022602102822727628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.298723220825195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05588911101222038,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0745538353919983,
      "backward_entropy": 0.002945046457979414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.226253509521484,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0559290386736393,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07449923753738404,
      "backward_entropy": 0.0770160092247857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.295279026031494,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05596959963440895,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07444270253181458,
      "backward_entropy": 0.07701598273383246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.468488693237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05600738897919655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07439305782318115,
      "backward_entropy": 0.0029224111802048152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.974471092224121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05604315921664238,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07434792518615722,
      "backward_entropy": 0.02271269592973921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.734918594360352,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056079648435115814,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07430030703544617,
      "backward_entropy": 0.022730082273483276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.521942138671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05611618235707283,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.074252188205719,
      "backward_entropy": 0.0028988433380921683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.61034107208252,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056151747703552246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0742060661315918,
      "backward_entropy": 0.022769699494043987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.546171188354492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05618748813867569,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07415902614593506,
      "backward_entropy": 0.0028831188877423606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.38731050491333,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056223202496767044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07411166429519653,
      "backward_entropy": 0.02281402548154195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.421849250793457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05625835806131363,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0740657091140747,
      "backward_entropy": 0.022846062978108723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.435760498046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05629335716366768,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07401913404464722,
      "backward_entropy": 0.02286890149116516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1373517513275146,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05632924288511276,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07396976947784424,
      "backward_entropy": 0.02288940217759874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.249617576599121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05636279657483101,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0739266335964203,
      "backward_entropy": 0.022921340333090887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1594343185424805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056396838277578354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07388191223144532,
      "backward_entropy": 0.0028429575678375033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.07855575531721115,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05643010511994362,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0738387405872345,
      "backward_entropy": 0.023000071446100872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.082650661468506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05646036937832832,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07380366325378418,
      "backward_entropy": 0.0028335555560059017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.053625583648682,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05649049952626228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07376870512962341,
      "backward_entropy": 0.023096260097291734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.98350715637207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05651947110891342,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0737365186214447,
      "backward_entropy": 0.02314725849363539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.8666410446167,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05654904618859291,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07370216846466064,
      "backward_entropy": 0.023194871015018888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.879719257354736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05658102035522461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07366082668304444,
      "backward_entropy": 0.0028200726956129074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.889342784881592,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05661310628056526,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07361847162246704,
      "backward_entropy": 0.002815316534704632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.7759175300598145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056644659489393234,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0735772728919983,
      "backward_entropy": 0.002810501183072726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.634699821472168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05667662248015404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07353441715240479,
      "backward_entropy": 0.002805944946077135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.462546348571777,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05670973286032677,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07348794937133789,
      "backward_entropy": 0.023409277200698853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.839592695236206,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056744419038295746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07343615293502807,
      "backward_entropy": 0.002794987211624781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.681931018829346,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056777384132146835,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07338926196098328,
      "backward_entropy": 0.02346681555112203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.070788383483887,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056809570640325546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0733443021774292,
      "backward_entropy": 0.023491968711217243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.282382011413574,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05684449523687363,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07328965663909912,
      "backward_entropy": 0.023517409960428875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.517927169799805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05688009411096573,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0732332706451416,
      "backward_entropy": 0.002767696562740538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.692840814590454,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056919537484645844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07316532731056213,
      "backward_entropy": 0.02355476717154185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.226926803588867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05695686861872673,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07310311198234558,
      "backward_entropy": 0.023575107256571453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.404860019683838,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05699765309691429,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07303013801574706,
      "backward_entropy": 0.023583839337031048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.358318328857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05703688785433769,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07296135425567626,
      "backward_entropy": 0.002735128419266807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.053346157073975,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05707505717873573,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07289527654647827,
      "backward_entropy": 0.002728292925490273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.643178939819336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05711262673139572,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07283024787902832,
      "backward_entropy": 0.02363255288865831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.211422920227051,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057153623551130295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07275446653366088,
      "backward_entropy": 0.002712201327085495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.857763290405273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05719313025474548,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07268276214599609,
      "backward_entropy": 0.023658292161093816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.795801162719727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05723186954855919,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07261263132095337,
      "backward_entropy": 0.0026958860043022367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4026339054107666,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057270124554634094,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07254379987716675,
      "backward_entropy": 0.023685779836442735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.673506259918213,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05730639770627022,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07248085737228394,
      "backward_entropy": 0.02370890478293101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.249785423278809,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05734235420823097,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07241753339767457,
      "backward_entropy": 0.023734056287341647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.314309597015381,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05737863853573799,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0723528504371643,
      "backward_entropy": 0.02375386158625285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.891016483306885,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057413119822740555,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07229370474815369,
      "backward_entropy": 0.02378090222676595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.444700241088867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057446420192718506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07223764657974244,
      "backward_entropy": 0.023801333374447294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.393657207489014,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057479433715343475,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07218185067176819,
      "backward_entropy": 0.023815434839990404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.342503070831299,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05751245841383934,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07212557792663574,
      "backward_entropy": 0.002638634087310897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.287868022918701,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05754570662975311,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07206854820251465,
      "backward_entropy": 0.002633065606156985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.154078483581543,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057578977197408676,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07201072573661804,
      "backward_entropy": 0.023903888132837083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5999259948730469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05761087313294411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07195740938186646,
      "backward_entropy": 0.0026252042088243696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.621402740478516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05764075368642807,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07191067337989807,
      "backward_entropy": 0.024009656574991014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.586603164672852,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05767018347978592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07186502814292908,
      "backward_entropy": 0.002621762247549163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5641531944274902,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05769908055663109,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07182078957557678,
      "backward_entropy": 0.02412648167875078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.006782531738281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05772623419761658,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07178246974945068,
      "backward_entropy": 0.024191718962457445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.490275859832764,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057753708213567734,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07174180746078491,
      "backward_entropy": 0.024248575170834858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.923634052276611,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05778069794178009,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07170230746269227,
      "backward_entropy": 0.02429796920882331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5199942588806152,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057808056473731995,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07166090607643127,
      "backward_entropy": 0.02434287468592326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.286632061004639,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05783373862504959,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07162506580352783,
      "backward_entropy": 0.024394641319910686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.367036819458008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057860542088747025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0715849220752716,
      "backward_entropy": 0.0026018621606959235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9123198986053467,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057886891067028046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07154592275619506,
      "backward_entropy": 0.02447400987148285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.7214555740356445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057912345975637436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07150949835777283,
      "backward_entropy": 0.002592171852787336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.276773452758789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05793816223740578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07147141695022582,
      "backward_entropy": 0.024549171328544617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.852384328842163,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05796397849917412,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0714330792427063,
      "backward_entropy": 0.002582461883624395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8333237171173096,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05798877030611038,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07139784097671509,
      "backward_entropy": 0.024631967147191364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4379117488861084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058012526482343674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07136586904525757,
      "backward_entropy": 0.002572357240650389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.533849716186523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05803487449884415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07133863568305969,
      "backward_entropy": 0.0025675389915704727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.498712062835693,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058058008551597595,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07130811214447022,
      "backward_entropy": 0.02474147578080495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.812877178192139,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05808178707957268,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07127481698989868,
      "backward_entropy": 0.024773471885257296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.764484882354736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058106929063797,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07123613953590394,
      "backward_entropy": 0.002550880735119184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.383320331573486,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05813349783420563,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07119209766387939,
      "backward_entropy": 0.024840437703662448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.3422441482543945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05816039815545082,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0711462140083313,
      "backward_entropy": 0.024875735243161518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.990419626235962,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058187637478113174,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07109870910644531,
      "backward_entropy": 0.024911502997080486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.960441827774048,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05821434035897255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07105282545089722,
      "backward_entropy": 0.0025308579206466675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9292855262756348,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05824034661054611,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07100878357887268,
      "backward_entropy": 0.0249626487493515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.46247673034668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058266133069992065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07096529006958008,
      "backward_entropy": 0.024987351563241746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.139959335327148,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058292847126722336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0709177553653717,
      "backward_entropy": 0.025004491209983826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.317384958267212,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058319926261901855,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0708687961101532,
      "backward_entropy": 0.02502544555399153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8093483448028564,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058344919234514236,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07082724571228027,
      "backward_entropy": 0.0024959432582060495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2973642349243164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058369770646095276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07078565955162049,
      "backward_entropy": 0.002489320312937101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.221386909484863,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05839301273226738,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07075029611587524,
      "backward_entropy": 0.025089571873346966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2788230180740356,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05841759219765663,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07070857882499695,
      "backward_entropy": 0.002477452572849062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.348654270172119,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05844053626060486,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07067319750785828,
      "backward_entropy": 0.07701524098714192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6731696128845215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05846527963876724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0706299901008606,
      "backward_entropy": 0.02516121334499783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.645658016204834,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05848987400531769,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07058751583099365,
      "backward_entropy": 0.0251845700873269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.42997145652771,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058514129370450974,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0705458641052246,
      "backward_entropy": 0.025205504563119676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5921759605407715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0585373230278492,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07050796151161194,
      "backward_entropy": 0.025222417381074693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2235885858535767,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05856020748615265,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07047077417373657,
      "backward_entropy": 0.02523389955361684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.705606460571289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05858162045478821,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07043907642364503,
      "backward_entropy": 0.025247299008899264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.361758232116699,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.058603737503290176,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07040426135063171,
      "backward_entropy": 0.07701574431525336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.640540599822998,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0586250014603138,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07037237882614136,
      "backward_entropy": 0.0024147344132264457,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 6.200019814297557,
    "avg_log_Z": -0.057204989045858384,
    "success_rate": 1.0,
    "avg_reward": 37.5,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.05,
      "1": 0.6,
      "2": 0.35
    },
    "avg_forward_entropy": 0.07260340386629104,
    "avg_backward_entropy": 0.019140287693589924,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}