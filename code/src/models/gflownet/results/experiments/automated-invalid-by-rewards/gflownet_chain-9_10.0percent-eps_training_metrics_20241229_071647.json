{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07682135370042589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07688921027713352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.61880493164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938756465911866,
      "backward_entropy": 0.07694670889112684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.59217834472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938428640365601,
      "backward_entropy": 0.07694782151116265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.00497436523438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0001999999803956598,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938091278076172,
      "backward_entropy": 0.07683375146653917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.53900146484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0003002662560902536,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10937745571136474,
      "backward_entropy": 0.07683979802661473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.57191467285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00040012889076024294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10937389135360717,
      "backward_entropy": 0.07689421706729466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.4860076904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005005108541809022,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10937024354934692,
      "backward_entropy": 0.07695215278201634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.65463256835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0006003712769597769,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10936648845672607,
      "backward_entropy": 0.07689677344428168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.2561492919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0006985856452956796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10936264991760254,
      "backward_entropy": 0.07689789930979411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.80734252929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0007957267807796597,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10935873985290527,
      "backward_entropy": 0.07689895894792345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.38162231445312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0008902662666514516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10935486555099487,
      "backward_entropy": 0.07689984639485677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.77662658691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0009857574477791786,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10935087203979492,
      "backward_entropy": 0.07690076695548163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.11859130859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0010830044047906995,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10934677124023437,
      "backward_entropy": 0.07695767614576551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.6780242919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0011775882449001074,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10934258699417114,
      "backward_entropy": 0.07695846425162421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.45021057128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.001272945781238377,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10933827161788941,
      "backward_entropy": 0.07695923911200629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.62632751464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001365851960144937,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10933383703231811,
      "backward_entropy": 0.07689563433329265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.070556640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0014598461566492915,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10932928323745728,
      "backward_entropy": 0.07689985964033338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.98333740234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0015536153223365545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10932505130767822,
      "backward_entropy": 0.07690530353122288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.96269226074219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0016495808959007263,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10932073593139649,
      "backward_entropy": 0.07696198092566596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.75120544433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001741034328006208,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10931632518768311,
      "backward_entropy": 0.07690640290578206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.58257293701172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0018323456170037389,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10931181907653809,
      "backward_entropy": 0.07691501908832127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.392822265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00192202755715698,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10930728912353516,
      "backward_entropy": 0.07690701219770643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.4802703857422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0020061589311808348,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10930273532867432,
      "backward_entropy": 0.07692091994815403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.92015075683594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00209423853084445,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10929805040359497,
      "backward_entropy": 0.07692376772562663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.93838500976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0021828701719641685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10929329395294189,
      "backward_entropy": 0.0769071380297343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.03053283691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00227577262558043,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10928837060928345,
      "backward_entropy": 0.07690728372997707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.02059936523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002371377544477582,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10928335189819335,
      "backward_entropy": 0.07690751552581787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.59120178222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0024705210234969854,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10927822589874267,
      "backward_entropy": 0.07690789302190144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.31007385253906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0025700407568365335,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1092730164527893,
      "backward_entropy": 0.07693735758463542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.30577087402344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0026696822606027126,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10926762819290162,
      "backward_entropy": 0.07693994045257568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.14576721191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0027722117956727743,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10926206111907959,
      "backward_entropy": 0.07694252994325426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.49237060546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0028747140895575285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10925641059875488,
      "backward_entropy": 0.0769092771742079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.97081756591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002977085066959262,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10925065279006958,
      "backward_entropy": 0.07690958182017009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.81410217285156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003074365435168147,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10924487113952637,
      "backward_entropy": 0.07694972885979547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.27304077148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003172047436237335,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10923895835876465,
      "backward_entropy": 0.07696906725565593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.28407287597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003273979527875781,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10923285484313965,
      "backward_entropy": 0.07696945799721612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.53683471679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0033744166139513254,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10922672748565673,
      "backward_entropy": 0.07696979575686985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.07432556152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003465628484264016,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10922058820724487,
      "backward_entropy": 0.07696995470258924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.04583740234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0035608652979135513,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10921432971954345,
      "backward_entropy": 0.07690942287445068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.91685485839844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003659555222839117,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10920791625976563,
      "backward_entropy": 0.07696115970611572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.81289672851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0037600405048578978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10920145511627197,
      "backward_entropy": 0.07690940962897407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.97178649902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0038653993979096413,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10919473171234131,
      "backward_entropy": 0.07690955532921685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.69906616210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003971298690885305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10918787717819214,
      "backward_entropy": 0.07690966791576809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.1774139404297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004075874108821154,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10918081998825073,
      "backward_entropy": 0.07696819967693752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.43186950683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004181272350251675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1091737151145935,
      "backward_entropy": 0.07690959506564671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.33624267578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004286018665879965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10916657447814941,
      "backward_entropy": 0.07690954208374023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.93995666503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004390942398458719,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10915932655334473,
      "backward_entropy": 0.07697138521406385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.7804946899414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004496116191148758,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10915193557739258,
      "backward_entropy": 0.07697143819597033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.36427307128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004597009159624577,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10914456844329834,
      "backward_entropy": 0.07697138521406385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.68162536621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004696004092693329,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10913724899291992,
      "backward_entropy": 0.0769712660047743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.98652648925781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004794939421117306,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10912988185882569,
      "backward_entropy": 0.07697850465774536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.37225341796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004890455864369869,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10912251472473145,
      "backward_entropy": 0.07697090837690565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.94950866699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004986089188605547,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10911502838134765,
      "backward_entropy": 0.07690591944588555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.052490234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005078635644167662,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10910753011703492,
      "backward_entropy": 0.07697033882141113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.02284240722656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005176061764359474,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10909976959228515,
      "backward_entropy": 0.07698286904229058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.97308349609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00527770072221756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10909174680709839,
      "backward_entropy": 0.07690372069676717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.97494506835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005380164831876755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10908360481262207,
      "backward_entropy": 0.07690299881829156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.95536804199219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0054783751256763935,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10907542705535889,
      "backward_entropy": 0.07698591550191243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.53558349609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005572755821049213,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10906717777252198,
      "backward_entropy": 0.0769867632124159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.76575469970703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005671842955052853,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10905871391296387,
      "backward_entropy": 0.07698763741387261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.84422302246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005765794310718775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10905036926269532,
      "backward_entropy": 0.07689861456553142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.5171890258789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005861430894583464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10904184579849244,
      "backward_entropy": 0.0768973429997762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.58978271484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005949968937784433,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10903338193893433,
      "backward_entropy": 0.07696629895104302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.05162048339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00603533536195755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10902478694915771,
      "backward_entropy": 0.07689404487609863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.0518035888672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006120309233665466,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10901615619659424,
      "backward_entropy": 0.07699092891481188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.58570861816406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006209677085280418,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10900729894638062,
      "backward_entropy": 0.07699148522482978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.3008270263672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006300204899162054,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10899844169616699,
      "backward_entropy": 0.07699202166663276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.96868896484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006396032869815826,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10898927450180054,
      "backward_entropy": 0.07699257797665066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.73443603515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006494948174804449,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10897990465164184,
      "backward_entropy": 0.07699314753214519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.31576538085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006594652775675058,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10897035598754883,
      "backward_entropy": 0.07696081532372369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.22662353515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006693747825920582,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1089607834815979,
      "backward_entropy": 0.07699418730205959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.76031494140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006799532566219568,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10895079374313354,
      "backward_entropy": 0.07695929209391277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.33229064941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006907461676746607,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089407205581665,
      "backward_entropy": 0.07695859008365208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.89273071289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007021274883300066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10893036127090454,
      "backward_entropy": 0.07695799403720432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.3848876953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0071281613782048225,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089201807975769,
      "backward_entropy": 0.07695725891325209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.85890197753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007235386874526739,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089099407196045,
      "backward_entropy": 0.07695649729834662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.42803955078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007348568644374609,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10889945030212403,
      "backward_entropy": 0.07687502437167698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.93359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007462269626557827,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10888911485671997,
      "backward_entropy": 0.0768735342555576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.1648178100586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007574193645268679,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1088788628578186,
      "backward_entropy": 0.07687201764848497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.43936157226562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00767915602773428,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10886873006820678,
      "backward_entropy": 0.07695353031158447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.58644104003906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007790050935000181,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10885823965072632,
      "backward_entropy": 0.07699921396043566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.82855224609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007899156771600246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1088477611541748,
      "backward_entropy": 0.07695188124974568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.53018188476562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008009534329175949,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10883710384368897,
      "backward_entropy": 0.0770000351799859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.8647003173828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008119499310851097,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10882635116577148,
      "backward_entropy": 0.0770004325442844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.76402282714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008228978142142296,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10881544351577759,
      "backward_entropy": 0.07694907320870294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 258.927978515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008336987346410751,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10880460739135742,
      "backward_entropy": 0.07700112793180677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.66197204589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008451411500573158,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10879319906234741,
      "backward_entropy": 0.07700150542789036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.00942993164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008567620068788528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10878154039382934,
      "backward_entropy": 0.076855911148919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.28160095214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008684022352099419,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10876971483230591,
      "backward_entropy": 0.07694513267940944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.11839294433594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00879160687327385,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10875825881958008,
      "backward_entropy": 0.07700251208411323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.58477783203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008902084082365036,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10874662399291993,
      "backward_entropy": 0.0769428014755249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.94793701171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009012121707201004,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10873475074768066,
      "backward_entropy": 0.07700303528043959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.43051147460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009120313450694084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10872286558151245,
      "backward_entropy": 0.07684479819403754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.12982940673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009223608300089836,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10871104001998902,
      "backward_entropy": 0.07693891392813788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.1927490234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00932226236909628,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10869919061660767,
      "backward_entropy": 0.077003366417355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.1645965576172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009421530179679394,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10868711471557617,
      "backward_entropy": 0.07700338628556994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.53997802734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009521345607936382,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10867478847503662,
      "backward_entropy": 0.07683331436581081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.41958618164062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009620163589715958,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10866234302520753,
      "backward_entropy": 0.0770033531718784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.25543212890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009720820002257824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10864949226379395,
      "backward_entropy": 0.07682689030965169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.9732666015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00981875043362379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10863660573959351,
      "backward_entropy": 0.07682343324025472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.41981506347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009917701594531536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10862360000610352,
      "backward_entropy": 0.0768200159072876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.21551513671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010021605528891087,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10861010551452636,
      "backward_entropy": 0.0768166979153951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.25694274902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010127070359885693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10859631299972534,
      "backward_entropy": 0.07692205243640476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.86492919921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010233595035970211,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10858217477798462,
      "backward_entropy": 0.07680994934505886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.75746154785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010341157205402851,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10856771469116211,
      "backward_entropy": 0.07691789997948541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.36634826660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010449946857988834,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10855305194854736,
      "backward_entropy": 0.07680280341042413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.69549560546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010556379333138466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10853824615478516,
      "backward_entropy": 0.07679898209042019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.3703155517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010659526102244854,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10852349996566772,
      "backward_entropy": 0.0767949554655287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.85098266601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010764160193502903,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10850844383239747,
      "backward_entropy": 0.07679086923599243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.14971923828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010867299512028694,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10849335193634033,
      "backward_entropy": 0.07678670353359646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.9419403076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010973541997373104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1084779143333435,
      "backward_entropy": 0.07678258419036865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.72085571289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011085028760135174,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10846180915832519,
      "backward_entropy": 0.0769012173016866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.5372314453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01119090337306261,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10844587087631226,
      "backward_entropy": 0.0770028829574585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.22203063964844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01129347924143076,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10843000411987305,
      "backward_entropy": 0.07700274387995402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.76991271972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011392867192626,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10841411352157593,
      "backward_entropy": 0.07689274681939019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.6691436767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01149417832493782,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10839791297912597,
      "backward_entropy": 0.07688972022798327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.51365661621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011597514152526855,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10838170051574707,
      "backward_entropy": 0.07688670688205296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.63540649414062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011700772680342197,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10836541652679443,
      "backward_entropy": 0.0770020882288615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.81028747558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011809571646153927,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1083484172821045,
      "backward_entropy": 0.07688058747185601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.7710418701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011920705437660217,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1083310604095459,
      "backward_entropy": 0.07673801316155328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.67323303222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012033890001475811,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10831410884857177,
      "backward_entropy": 0.07673275470733643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.75579833984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012146074324846268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10829733610153199,
      "backward_entropy": 0.07672733730740017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.3284454345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012259854935109615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1082801103591919,
      "backward_entropy": 0.07672171460257636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.50994873046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012369086965918541,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10826288461685181,
      "backward_entropy": 0.07700158490075006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.34223175048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01247871108353138,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10824530124664307,
      "backward_entropy": 0.0767090916633606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.8351593017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012583133764564991,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1082280158996582,
      "backward_entropy": 0.0767023033565945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.24114990234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012690482661128044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10821040868759155,
      "backward_entropy": 0.07685456011030409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.30679321289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012796854600310326,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10819247961044312,
      "backward_entropy": 0.07700084977679783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.75169372558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012904204428195953,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10817427635192871,
      "backward_entropy": 0.07668174637688531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.06975555419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0130080571398139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10815620422363281,
      "backward_entropy": 0.07667442162831624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.10369110107422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013105614110827446,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10813852548599243,
      "backward_entropy": 0.07700007491641575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.66700744628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013197151944041252,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10812106132507324,
      "backward_entropy": 0.07665801048278809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.04989624023438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013289795257151127,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10810332298278809,
      "backward_entropy": 0.07682947317759196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.53042602539062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013379992917180061,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10808541774749755,
      "backward_entropy": 0.07699864440494114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.5704803466797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013466457836329937,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10806753635406494,
      "backward_entropy": 0.07699799537658691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.61865234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013554584234952927,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10804930925369263,
      "backward_entropy": 0.0768142541249593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.45471954345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013651452027261257,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10803003311157226,
      "backward_entropy": 0.07661228709750706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.94981384277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013743900693953037,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10801082849502563,
      "backward_entropy": 0.07680429352654351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.52972412109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013838809914886951,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10799117088317871,
      "backward_entropy": 0.07699596881866455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.77528381347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013931136578321457,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10797221660614013,
      "backward_entropy": 0.07658289538489448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.42405700683594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014029117301106453,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10795243978500366,
      "backward_entropy": 0.07699506812625462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.8554916381836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014124522916972637,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10793266296386719,
      "backward_entropy": 0.0767834981282552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.6010513305664,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01421409659087658,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10791378021240235,
      "backward_entropy": 0.07699411445193821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.8727569580078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014300196431577206,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10789499282836915,
      "backward_entropy": 0.07699349191453722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.81956481933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014388395473361015,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10787607431411743,
      "backward_entropy": 0.07699297534094916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.83071899414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014476212672889233,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10785678625106812,
      "backward_entropy": 0.07675970925225152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.42181396484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014570423401892185,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10783689022064209,
      "backward_entropy": 0.07650844256083171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.380126953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014668245799839497,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10781623125076294,
      "backward_entropy": 0.07674813270568848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.3953094482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014769276604056358,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10779485702514649,
      "backward_entropy": 0.07674235767788357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.80625915527344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01487497054040432,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10777209997177124,
      "backward_entropy": 0.07699146535661486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.5767364501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014981870539486408,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10774881839752197,
      "backward_entropy": 0.07646811008453369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.23348999023438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015086730942130089,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10772533416748047,
      "backward_entropy": 0.07699131965637207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.98384094238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01518954522907734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10770153999328613,
      "backward_entropy": 0.07644716236326429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.8026885986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015287630259990692,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10767791271209717,
      "backward_entropy": 0.07643555932574803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.62867736816406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015390774235129356,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10765345096588134,
      "backward_entropy": 0.07699084281921387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 253.89071655273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015495053492486477,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10762836933135986,
      "backward_entropy": 0.0769907898373074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.8969497680664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015606289729475975,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10760209560394288,
      "backward_entropy": 0.07669222354888916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.93443298339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015711583197116852,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10757598876953126,
      "backward_entropy": 0.07639080948299831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.01007843017578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015815360471606255,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10754998922348022,
      "backward_entropy": 0.07667797141604954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.20803833007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01591087505221367,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10752451419830322,
      "backward_entropy": 0.07666997114817302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.33332824707031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016003523021936417,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1074991226196289,
      "backward_entropy": 0.07699006133609348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.42758178710938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01609088107943535,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10747475624084472,
      "backward_entropy": 0.07665254672368367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.4198760986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016174454241991043,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10745017528533936,
      "backward_entropy": 0.07631901899973552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.91131591796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016256598755717278,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10742530822753907,
      "backward_entropy": 0.07698769039577907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.08660888671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016341861337423325,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10739948749542236,
      "backward_entropy": 0.07698681619432238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.82192993164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01642693765461445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10737332105636596,
      "backward_entropy": 0.07661318778991699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.47393798828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016514835879206657,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1073462724685669,
      "backward_entropy": 0.07660303513209026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.1484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016606930643320084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10731825828552247,
      "backward_entropy": 0.07623105578952366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.87184143066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01669379696249962,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10729069709777832,
      "backward_entropy": 0.07621241940392388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.83692932128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01677902601659298,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10726301670074463,
      "backward_entropy": 0.07657141817940606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.06574249267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016862789168953896,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10723519325256348,
      "backward_entropy": 0.07617354393005371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.59048461914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016942156478762627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1072077751159668,
      "backward_entropy": 0.07615294059117635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.77207946777344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017028138041496277,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10717889070510864,
      "backward_entropy": 0.07697953118218316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.35499572753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017115812748670578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10714954137802124,
      "backward_entropy": 0.07652516497506036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.97898864746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017201891168951988,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10712019205093384,
      "backward_entropy": 0.07697792847951253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.99069213867188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017286650836467743,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10709078311920166,
      "backward_entropy": 0.07607268624835545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.24627685546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017371203750371933,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10706079006195068,
      "backward_entropy": 0.07648665375179714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.57611083984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017454493790864944,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10703076124191284,
      "backward_entropy": 0.07697524627049764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.50733947753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01754266582429409,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10699950456619263,
      "backward_entropy": 0.07645952701568604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.12400817871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01763393171131611,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1069675087928772,
      "backward_entropy": 0.07697394159105089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.742919921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017730869352817535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10693423748016358,
      "backward_entropy": 0.07596736484103733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.70262145996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01782985031604767,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1069002628326416,
      "backward_entropy": 0.0759474966261122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.3486328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01793033815920353,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10686542987823486,
      "backward_entropy": 0.07592707210116917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.581298828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01802782341837883,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10683075189590455,
      "backward_entropy": 0.07697270313898723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.1746063232422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018127288669347763,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1067953109741211,
      "backward_entropy": 0.07637784216139051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.88560485839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01822986640036106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10675884485244751,
      "backward_entropy": 0.07586220900217693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.79202270507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018329327926039696,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10672259330749512,
      "backward_entropy": 0.07583953274620904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 268.1761474609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01842855103313923,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10668561458587647,
      "backward_entropy": 0.07633288701375325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.38922119140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01853681541979313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1066465973854065,
      "backward_entropy": 0.0757934848467509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.19764709472656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01864241436123848,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10660734176635742,
      "backward_entropy": 0.07697104083167182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.994873046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018750300630927086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10656698942184448,
      "backward_entropy": 0.07574583424462213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.45140075683594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01885470375418663,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10652691125869751,
      "backward_entropy": 0.07627132866117689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.72576904296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018960485234856606,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10648629665374756,
      "backward_entropy": 0.07625514268875122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.46156311035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01906738616526127,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1064450979232788,
      "backward_entropy": 0.07567294438680013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.1321258544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019172267988324165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10640392303466797,
      "backward_entropy": 0.07564806938171387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.54110717773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01928120292723179,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10636143684387207,
      "backward_entropy": 0.07562372419569227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.88856506347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019390590488910675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10631821155548096,
      "backward_entropy": 0.07618802123599583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.58067321777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01949872449040413,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1062745213508606,
      "backward_entropy": 0.07557134495841132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.46015167236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019604265689849854,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.106230628490448,
      "backward_entropy": 0.076151000128852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.22821044921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019702764227986336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10618745088577271,
      "backward_entropy": 0.07551002502441406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.563232421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01979784108698368,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10614407062530518,
      "backward_entropy": 0.07696632544199626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.69602966308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019895020872354507,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10609986782073974,
      "backward_entropy": 0.07608630259831746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.20573425292969,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019990619271993637,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10605517625808716,
      "backward_entropy": 0.07696359687381321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.14170837402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02008211612701416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10601109266281128,
      "backward_entropy": 0.07536676857206556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.29916381835938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020178886130452156,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10596518516540528,
      "backward_entropy": 0.07696054379145305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.19818115234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02027726359665394,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10591821670532227,
      "backward_entropy": 0.07695927884843615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.1947479248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020381564274430275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1058692216873169,
      "backward_entropy": 0.07525628805160522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.15951538085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02048727124929428,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10581985712051392,
      "backward_entropy": 0.07522063785129124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.5422821044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02058902010321617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10577117204666138,
      "backward_entropy": 0.07518197430504693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 265.5917053222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020693160593509674,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10572119951248168,
      "backward_entropy": 0.07589834928512573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.32905578613281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02080576680600643,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10566893815994263,
      "backward_entropy": 0.0758754809697469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.17713928222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020911134779453278,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10561842918395996,
      "backward_entropy": 0.07585066556930542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.42227172851562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021020250394940376,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10556647777557374,
      "backward_entropy": 0.07582624753316243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.49981689453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021126998588442802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1055147409439087,
      "backward_entropy": 0.07499055067698161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.14425659179688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021232763305306435,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10546246767044068,
      "backward_entropy": 0.07577437824673122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.3452606201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021328506991267204,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1054121732711792,
      "backward_entropy": 0.07574502627054851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.65916442871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021427376195788383,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10536034107208252,
      "backward_entropy": 0.07695085472530788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.18524932861328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021522918716073036,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10530848503112793,
      "backward_entropy": 0.07694899373584324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.7147674560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021610965952277184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10525801181793212,
      "backward_entropy": 0.07475605938169691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 217.20318603515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02169857546687126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10520713329315186,
      "backward_entropy": 0.0756186114417182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.16314697265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021791905164718628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10515418052673339,
      "backward_entropy": 0.07465129428439671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.92007446289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021887067705392838,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10509991645812988,
      "backward_entropy": 0.07555402649773492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.37100219726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02198570780456066,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10504422187805176,
      "backward_entropy": 0.0769387616051568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.0466766357422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022085821256041527,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10498749017715454,
      "backward_entropy": 0.07693762249416775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.77589416503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02218405157327652,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1049304485321045,
      "backward_entropy": 0.0744457377327813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.2333984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022286836057901382,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10487155914306641,
      "backward_entropy": 0.07439621951844957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.33779907226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02238735370337963,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10481233596801758,
      "backward_entropy": 0.07434331046210395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 232.26515197753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022488735616207123,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10475181341171265,
      "backward_entropy": 0.07535519864824083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.56086730957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022595804184675217,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10468909740447999,
      "backward_entropy": 0.07423641946580675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.25340270996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0226973257958889,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10462739467620849,
      "backward_entropy": 0.074180006980896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.64474487304688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022795433178544044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10456604957580566,
      "backward_entropy": 0.07524975140889485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.92250061035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022885795682668686,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10450646877288819,
      "backward_entropy": 0.0769254101647271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.7537078857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02297690510749817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10444594621658325,
      "backward_entropy": 0.07398845752080281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.73492431640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023065757006406784,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10438560247421265,
      "backward_entropy": 0.07512655523088244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.64840698242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023154057562351227,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1043247938156128,
      "backward_entropy": 0.07508354716830784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.68544006347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023243026807904243,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10426266193389892,
      "backward_entropy": 0.07377802001105414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.23968505859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02333320863544941,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10419982671737671,
      "backward_entropy": 0.07499577601750691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.02813720703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02342427894473076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10413545370101929,
      "backward_entropy": 0.07363800870047675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.79383850097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023514075204730034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10406997203826904,
      "backward_entropy": 0.07356403933631049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.62199401855469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023607797920703888,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10400242805480957,
      "backward_entropy": 0.07486085096995036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.88198852539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023695936426520348,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10393624305725098,
      "backward_entropy": 0.0734160410033332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.23414611816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023778943344950676,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10387115478515625,
      "backward_entropy": 0.0733354820145501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.79786682128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023863790556788445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10380496978759765,
      "backward_entropy": 0.0747109784020318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.05699157714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02395472303032875,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10373599529266357,
      "backward_entropy": 0.07688985930548774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.57899475097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024043437093496323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10366716384887695,
      "backward_entropy": 0.07310435507032606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.6510009765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02413143403828144,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1035974383354187,
      "backward_entropy": 0.07455964883168538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.98894500732422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024219399318099022,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10352751016616821,
      "backward_entropy": 0.07450711727142334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.3046417236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024300960823893547,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10345962047576904,
      "backward_entropy": 0.07286317480934991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.28033447265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024384019896388054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10338966846466065,
      "backward_entropy": 0.07277646329667833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.7193374633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024466954171657562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10331647396087647,
      "backward_entropy": 0.07268636756473118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.35975646972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02454700693488121,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10324329137802124,
      "backward_entropy": 0.07686787181430393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.59363555908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02462863363325596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10316753387451172,
      "backward_entropy": 0.07249740097257826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.7495574951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02470751851797104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10309206247329712,
      "backward_entropy": 0.0723995566368103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.357627868652344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024787135422229767,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1030154824256897,
      "backward_entropy": 0.07685443427827623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.11030578613281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02485760673880577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10294176340103149,
      "backward_entropy": 0.07219552993774414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.90176391601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024924740195274353,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10286847352981568,
      "backward_entropy": 0.0739453633626302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.10546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024998363107442856,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10279154777526855,
      "backward_entropy": 0.07197942998674181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.64329528808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02507769502699375,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10271106958389283,
      "backward_entropy": 0.07380867666668361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.32676696777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025159409269690514,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10262908935546874,
      "backward_entropy": 0.07374203867382473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.78904724121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02524443343281746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10254453420639038,
      "backward_entropy": 0.07367598348193699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.67706298828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02533397637307644,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10245691537857056,
      "backward_entropy": 0.07682689030965169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.95547485351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0254275593906641,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10236647129058837,
      "backward_entropy": 0.07149506939782037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.09352111816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025527695193886757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10227208137512207,
      "backward_entropy": 0.07140722539689806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.32240295410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02562493085861206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10217850208282471,
      "backward_entropy": 0.0713150766160753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 213.31822204589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025720536708831787,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10208415985107422,
      "backward_entropy": 0.07121576203240289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.2400360107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025820855051279068,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10198637247085571,
      "backward_entropy": 0.07111777199639215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.14256286621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02591952122747898,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1018884539604187,
      "backward_entropy": 0.07101507319344415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.24874877929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026016704738140106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10179028511047364,
      "backward_entropy": 0.07090768549177381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.45713806152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026114212349057198,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10169119834899902,
      "backward_entropy": 0.07079912556542291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.19810485839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0262087881565094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10159251689910889,
      "backward_entropy": 0.07068450583351983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.15293884277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026301126927137375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10149396657943725,
      "backward_entropy": 0.07056807809405857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.1377410888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026390766724944115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10139487981796265,
      "backward_entropy": 0.07044496801164415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.26153564453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02648717351257801,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10129086971282959,
      "backward_entropy": 0.07276994652218288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.0525665283203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02658242918550968,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10118646621704101,
      "backward_entropy": 0.07268889745076497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.2906265258789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026677722111344337,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10108017921447754,
      "backward_entropy": 0.07260515954759386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.3838653564453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02676423452794552,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10097823143005372,
      "backward_entropy": 0.07251399755477905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.7131805419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02685079723596573,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10087580680847168,
      "backward_entropy": 0.06980044311947292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.1702423095703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026938294991850853,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10077031850814819,
      "backward_entropy": 0.07679895559946696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.62757873535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02702711895108223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1006629228591919,
      "backward_entropy": 0.06951695680618286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.01841735839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027116945013403893,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10055351257324219,
      "backward_entropy": 0.07213589880201551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.7715606689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02721085399389267,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10044068098068237,
      "backward_entropy": 0.06923315260145399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.99159240722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027302410453557968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10032838582992554,
      "backward_entropy": 0.06908857822418213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.24502563476562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027394786477088928,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10021440982818604,
      "backward_entropy": 0.07678086227840847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.11569213867188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027481313794851303,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10010193586349488,
      "backward_entropy": 0.07677508725060357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.6576690673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02756771631538868,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09998865127563476,
      "backward_entropy": 0.06862184736463758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.59880828857422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0276524405926466,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09987550973892212,
      "backward_entropy": 0.07151773240831164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.56927490234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02773258090019226,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09976429939270019,
      "backward_entropy": 0.07675511969460382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.63662719726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027815021574497223,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09965125918388366,
      "backward_entropy": 0.07127990987565783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.53546142578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027896102517843246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09953781366348266,
      "backward_entropy": 0.06794051329294841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.27507781982422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027975967153906822,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09942388534545898,
      "backward_entropy": 0.07673393355475532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.80923461914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028051752597093582,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09931180477142335,
      "backward_entropy": 0.06757032208972508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.77601623535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028128741309046745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0991986095905304,
      "backward_entropy": 0.07076969411638048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.24945068359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02820475585758686,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09908407926559448,
      "backward_entropy": 0.07063501411014134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.13353729248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028288012370467186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09896408915519714,
      "backward_entropy": 0.06700789928436279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.9170379638672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02836546115577221,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09884781837463379,
      "backward_entropy": 0.07669599850972493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.36825561523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02844209410250187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0987308144569397,
      "backward_entropy": 0.07023184167014228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.2111358642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028522439301013947,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09860947132110595,
      "backward_entropy": 0.07009330060746935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.6273193359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028603173792362213,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09848648309707642,
      "backward_entropy": 0.0766729646258884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.96961975097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02868109755218029,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09836405515670776,
      "backward_entropy": 0.06599543491999309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.5795135498047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028759673237800598,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09823987483978272,
      "backward_entropy": 0.07665484481387669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.45314025878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02883763797581196,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09811580181121826,
      "backward_entropy": 0.06950520144568549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.09861755371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02891765534877777,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09798848628997803,
      "backward_entropy": 0.07663758595784505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.57928466796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0289970263838768,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09786165952682495,
      "backward_entropy": 0.06511340538660686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.74055480957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029080163687467575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0977311372756958,
      "backward_entropy": 0.06489468945397271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.86270141601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029160385951399803,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09760161638259887,
      "backward_entropy": 0.06889178355534871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.39002990722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029242830350995064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0974697232246399,
      "backward_entropy": 0.06873471207088894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.5166473388672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02932864800095558,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09733412265777588,
      "backward_entropy": 0.06421870655483669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.16930389404297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02942057140171528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09719303250312805,
      "backward_entropy": 0.06400543451309204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.92991638183594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02950713038444519,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09705489873886108,
      "backward_entropy": 0.07659657796223958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.0614013671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02959342859685421,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0969154953956604,
      "backward_entropy": 0.06353761090172662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.24722290039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0296768955886364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09677833318710327,
      "backward_entropy": 0.06329459614223903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.091552734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029762176796793938,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09663876295089721,
      "backward_entropy": 0.0630504157808092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.6968231201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029844650998711586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09650123119354248,
      "backward_entropy": 0.06280079152848986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.42218017578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02992878295481205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09636053442955017,
      "backward_entropy": 0.0674197408888075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.45001220703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03001759946346283,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09621473550796508,
      "backward_entropy": 0.06724828481674194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.52516174316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030109338462352753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09606627225875855,
      "backward_entropy": 0.067078431447347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.40542602539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03020360879600048,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09591499567031861,
      "backward_entropy": 0.06182125541898939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.11309051513672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.030303195118904114,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09575881958007812,
      "backward_entropy": 0.07655229833390978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.43557739257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030398286879062653,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09560531377792358,
      "backward_entropy": 0.0665669043858846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.87987518310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030490372329950333,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09545180201530457,
      "backward_entropy": 0.0610837803946601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.45001220703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03057555854320526,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09530295729637146,
      "backward_entropy": 0.06617616282569037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.60773468017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03066093660891056,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09515352845191956,
      "backward_entropy": 0.06052140394846598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.14838409423828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030743269249796867,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09500579833984375,
      "backward_entropy": 0.06576039393742879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.18295288085938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0308182742446661,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09486403465270996,
      "backward_entropy": 0.07649701171451145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.30766296386719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030889790505170822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09472459554672241,
      "backward_entropy": 0.05961334705352783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.12028503417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030956126749515533,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09458750486373901,
      "backward_entropy": 0.05928288565741645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.90338134765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0310213603079319,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09445050954818726,
      "backward_entropy": 0.058948642677730985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.30451965332031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03108913078904152,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09431166648864746,
      "backward_entropy": 0.05862453911039564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.43238830566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031152399256825447,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09417561888694763,
      "backward_entropy": 0.05828583240509033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.27490234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03121965192258358,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09403595924377442,
      "backward_entropy": 0.06408917903900146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.7384262084961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03129463642835617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09388567209243774,
      "backward_entropy": 0.05763673120074802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.5030975341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03136647120118141,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09373931884765625,
      "backward_entropy": 0.06361624929640028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.22576904296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03144161030650139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09358986616134643,
      "backward_entropy": 0.05699705415301853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.2850570678711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03151937946677208,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0934367299079895,
      "backward_entropy": 0.0631456110212538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.64468383789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03159681707620621,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09328444004058838,
      "backward_entropy": 0.06290825208028157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.48057556152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03168009966611862,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09312644600868225,
      "backward_entropy": 0.05606683095296224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.52423095703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03176528587937355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09296550750732421,
      "backward_entropy": 0.05576961570315891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.56238555908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031853560358285904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09280061721801758,
      "backward_entropy": 0.055472248130374484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.96834564208984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031937163323163986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09263973236083985,
      "backward_entropy": 0.055157641569773354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.96131896972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03201296180486679,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09248418211936951,
      "backward_entropy": 0.05481240153312683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.81377410888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03209136426448822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09232510924339295,
      "backward_entropy": 0.054471247726016574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.99394989013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032165851444005966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09216865301132202,
      "backward_entropy": 0.06120672490861681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.58787536621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03223857283592224,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09201360940933227,
      "backward_entropy": 0.07626552051968044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.911376953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03231457993388176,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09185526371002198,
      "backward_entropy": 0.06066576639811198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.14942932128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03239190950989723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0916956603527069,
      "backward_entropy": 0.053052392270829946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.47132110595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03247027471661568,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09153425097465515,
      "backward_entropy": 0.0601295895046658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.0746078491211,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032543208450078964,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09137701392173767,
      "backward_entropy": 0.07622143957349989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.8311767578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03261467441916466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09122164249420166,
      "backward_entropy": 0.051962190204196505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.45779418945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0326877236366272,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09106265306472779,
      "backward_entropy": 0.05159564812978109,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.65691375732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03276228904724121,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09090201258659363,
      "backward_entropy": 0.05123065577612983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.29879760742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0328366681933403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.090740966796875,
      "backward_entropy": 0.050858755906422935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.75086212158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0329095683991909,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09058172702789306,
      "backward_entropy": 0.05048400163650513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.71502685546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0329827219247818,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09042249917984009,
      "backward_entropy": 0.05010961823993259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.46737670898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033058833330869675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09025931358337402,
      "backward_entropy": 0.0577791068289015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.6763916015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033140767365694046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09008914232254028,
      "backward_entropy": 0.0574892693095737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.2638397216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03322639316320419,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08991495370864869,
      "backward_entropy": 0.04902891649140252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.66439819335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03331540897488594,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0897371530532837,
      "backward_entropy": 0.0486823750866784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.44225311279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03340762108564377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08955661058425904,
      "backward_entropy": 0.048345880375968084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.7446517944336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03349476307630539,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08938047885894776,
      "backward_entropy": 0.04798551731639438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.732177734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03357758745551109,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08920897245407104,
      "backward_entropy": 0.047611978318956166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.117431640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03365957736968994,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08903815746307372,
      "backward_entropy": 0.055736932489607066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.47854614257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03374536335468292,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08886312246322632,
      "backward_entropy": 0.04686920510398017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.54045867919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03382692486047745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08869234919548034,
      "backward_entropy": 0.055116507742140025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.01852416992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033907655626535416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08852162361145019,
      "backward_entropy": 0.05479468570815192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.538761138916016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.033993784338235855,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08834500312805176,
      "backward_entropy": 0.076066878106859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.53421783447266,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0340728722512722,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0881771206855774,
      "backward_entropy": 0.07605613602532281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.33921813964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03414861112833023,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08801326751708985,
      "backward_entropy": 0.04495915770530701,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.01353454589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034225620329380035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08784660100936889,
      "backward_entropy": 0.04457132683859931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.2712860107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03430677950382233,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08767467141151428,
      "backward_entropy": 0.05317572752634684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.71385192871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03438898175954819,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08750196695327758,
      "backward_entropy": 0.07601820098029242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.00599670410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034475177526474,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08732564449310302,
      "backward_entropy": 0.05253847440083822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.22877502441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034563515335321426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08714783191680908,
      "backward_entropy": 0.04310558239618937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.0096664428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03465522080659866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08696699142456055,
      "backward_entropy": 0.04276507761743334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.61131286621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0347454734146595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08678832054138183,
      "backward_entropy": 0.04241591029696994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.3949432373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03484012186527252,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0866042971611023,
      "backward_entropy": 0.04207179612583584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.92208099365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03493592143058777,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08641927242279053,
      "backward_entropy": 0.050995965798695884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.56724548339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035025209188461304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08624235391616822,
      "backward_entropy": 0.0413651532597012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.99055480957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03511476516723633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08606449365615845,
      "backward_entropy": 0.04100434647666083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.42009735107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035201530903577805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08589005470275879,
      "backward_entropy": 0.04063467515839471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.91526794433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03528711199760437,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08571643829345703,
      "backward_entropy": 0.04025620884365506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.57305145263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035374950617551804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08554115295410156,
      "backward_entropy": 0.039889531003104314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.95027160644531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035461731255054474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08536744117736816,
      "backward_entropy": 0.0395216080877516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.81939697265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035547297447919846,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08519385457038879,
      "backward_entropy": 0.048653129074308604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.10448455810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035637933760881424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08501486778259278,
      "backward_entropy": 0.03878413968616062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.78931427001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03572719171643257,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08483760356903076,
      "backward_entropy": 0.03841841220855713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.139404296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03581218421459198,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08466514945030212,
      "backward_entropy": 0.038038677639431424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.2955093383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03589770570397377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08449132442474365,
      "backward_entropy": 0.03765598601765103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.01145935058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03597790375351906,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08432358503341675,
      "backward_entropy": 0.04690810375743442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.96391296386719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03605492785573006,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0841593861579895,
      "backward_entropy": 0.03685996929804484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.81141662597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03613056614995003,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08399770259857178,
      "backward_entropy": 0.03646034995714823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.9611587524414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03620944172143936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08383113145828247,
      "backward_entropy": 0.04579455984963311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.7207794189453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03628360107541084,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08366951942443848,
      "backward_entropy": 0.04541672931777106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.4354248046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036360982805490494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08350319862365722,
      "backward_entropy": 0.03528411520851983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.49996948242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03643983602523804,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08333485126495362,
      "backward_entropy": 0.04467860526508755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.1020736694336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0365140363574028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08317173719406128,
      "backward_entropy": 0.03449081381162008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.56829071044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03658711165189743,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08300957679748536,
      "backward_entropy": 0.04391257630454169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.10440063476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03665792942047119,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08285129070281982,
      "backward_entropy": 0.03367843561702304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.47439575195312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03673401474952698,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08268741965293884,
      "backward_entropy": 0.03329441282484266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.6967315673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036807455122470856,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0825271725654602,
      "backward_entropy": 0.032906042204962835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.0368423461914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03688585385680199,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08236135244369507,
      "backward_entropy": 0.032534519831339516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.1889877319336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03696285933256149,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08219749331474305,
      "backward_entropy": 0.03216174244880676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.666013717651367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03703567013144493,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08203926086425781,
      "backward_entropy": 0.041677243179745145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.526180267333984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037100210785865784,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08189166784286499,
      "backward_entropy": 0.04128429955906338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.83802795410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03715873137116432,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08175072669982911,
      "backward_entropy": 0.040877974695629544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.15476989746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03722357377409935,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08159984946250916,
      "backward_entropy": 0.030566632747650146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.41049194335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03729434311389923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08144322037696838,
      "backward_entropy": 0.030193335480160184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.51046752929688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03736753389239311,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08128434419631958,
      "backward_entropy": 0.07560071680280897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.35204315185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0374414287507534,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08112524151802063,
      "backward_entropy": 0.03941243555810717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.27876281738281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037517230957746506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0809630036354065,
      "backward_entropy": 0.039058668745888606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.37808227539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03759339451789856,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08080004453659058,
      "backward_entropy": 0.028759155008527968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.95836639404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03767280653119087,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08063348531723022,
      "backward_entropy": 0.038357287645339966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.98995971679688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0377478301525116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08047211170196533,
      "backward_entropy": 0.02805124388800727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.00283813476562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03782065212726593,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0803148627281189,
      "backward_entropy": 0.037639121214548744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.64599609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03789140284061432,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08016107082366944,
      "backward_entropy": 0.027335337466663785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.95806121826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03796042129397392,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08001051545143127,
      "backward_entropy": 0.026981254418690998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.39958572387695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038030609488487244,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07985810041427613,
      "backward_entropy": 0.026632676521937054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.23167419433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0380961149930954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07971224784851075,
      "backward_entropy": 0.026277323563893635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.06039428710938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03816630691289902,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07956159114837646,
      "backward_entropy": 0.03583906094233195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.45468139648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03823777288198471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07941052913665772,
      "backward_entropy": 0.025613857640160456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.96707153320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03831177204847336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07925683259963989,
      "backward_entropy": 0.025294227732552424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.44882202148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038388222455978394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07910164594650268,
      "backward_entropy": 0.03484162025981479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.0477066040039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03846076875925064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07895049452781677,
      "backward_entropy": 0.03450501627392239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.7685546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0385327972471714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07879971861839294,
      "backward_entropy": 0.034169909026887685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.2637939453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038604430854320526,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07864983081817627,
      "backward_entropy": 0.03383699390623304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.320512771606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038675934076309204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07850126028060914,
      "backward_entropy": 0.023720549212561712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.18069458007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03874001279473305,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07836336493492127,
      "backward_entropy": 0.0331649813387129,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.41090393066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0388045571744442,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07822476625442505,
      "backward_entropy": 0.0230875164270401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.72151184082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03887521103024483,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07807856798171997,
      "backward_entropy": 0.022787387172381084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.00890350341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038945768028497696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07793263196945191,
      "backward_entropy": 0.03218717376391093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.18726348876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039019182324409485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0777843713760376,
      "backward_entropy": 0.03188131252924601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.65584564208984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03909235820174217,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07763768434524536,
      "backward_entropy": 0.02193820310963525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.66498565673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03916235268115997,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0774953007698059,
      "backward_entropy": 0.031264285246531166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.57063293457031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039232585579156876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07735415101051331,
      "backward_entropy": 0.02138162487083011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.20807647705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039301399141550064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07721498608589172,
      "backward_entropy": 0.021103188395500183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.990753173828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03936910256743431,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07707886695861817,
      "backward_entropy": 0.03033217125468784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.93665313720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03943117335438728,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07694934010505676,
      "backward_entropy": 0.020538707574208576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.9349136352539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03949280455708504,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07681995630264282,
      "backward_entropy": 0.020256208048926458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.541847229003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039556991308927536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07668794393539428,
      "backward_entropy": 0.01998768084579044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.23368072509766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039617594331502914,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07656035423278809,
      "backward_entropy": 0.029068453444374934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.28598022460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03967664763331413,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0764360785484314,
      "backward_entropy": 0.028758194711473253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.25106811523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039735663682222366,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07631322741508484,
      "backward_entropy": 0.01919000181886885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.05333709716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039798952639102936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07618567943572999,
      "backward_entropy": 0.028162373436821833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.83052062988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03986320272088051,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07605725526809692,
      "backward_entropy": 0.018702636162439983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.46556854248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03992978855967522,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07592612504959106,
      "backward_entropy": 0.018468840254677668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.042224884033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0399986207485199,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07579397559165954,
      "backward_entropy": 0.018244895670149062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.34485626220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040059056133031845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0756709098815918,
      "backward_entropy": 0.018000861008961994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.15360260009766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040122140198946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07554378509521484,
      "backward_entropy": 0.01776297887166341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.65113067626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04018356651067734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07542006969451905,
      "backward_entropy": 0.017528706126742892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.66710662841797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04024915397167206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07529211044311523,
      "backward_entropy": 0.017307367589738634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.85917663574219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040316980332136154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0751618504524231,
      "backward_entropy": 0.017091421617401972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.624755859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040381330996751785,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07503713965415955,
      "backward_entropy": 0.01687395903799269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.05227279663086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04044394567608833,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07491497993469239,
      "backward_entropy": 0.016657170322206285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.46884155273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040501996874809265,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07479863762855529,
      "backward_entropy": 0.016433101561334398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.0664291381836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04056468605995178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07467683553695678,
      "backward_entropy": 0.016223576333787706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.09672546386719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04062720760703087,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07455583810806274,
      "backward_entropy": 0.024678289890289307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.39098358154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04069387540221214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07443013191223144,
      "backward_entropy": 0.015821066167619493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.00030517578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040761496871709824,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07430398464202881,
      "backward_entropy": 0.02420785692003038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.61915588378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04082697257399559,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07417985200881957,
      "backward_entropy": 0.015436859594451057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.154483795166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04089925065636635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07404875755310059,
      "backward_entropy": 0.015259938107596504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.160736083984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04096636921167374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07392547130584717,
      "backward_entropy": 0.015076642235120138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.60852813720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04102875664830208,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07380828857421876,
      "backward_entropy": 0.014886478583017984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.0240707397461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041095465421676636,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07368676662445069,
      "backward_entropy": 0.014708858397271898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.4074592590332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041160475462675095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07356838583946228,
      "backward_entropy": 0.014533460140228271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.07852172851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041222475469112396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07345405817031861,
      "backward_entropy": 0.014356116453806559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.04370880126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041288718581199646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07333404421806336,
      "backward_entropy": 0.014186370703909133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.96177673339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041354864835739136,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07321561574935913,
      "backward_entropy": 0.022200885746214125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.38187408447266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04141805320978165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0731020987033844,
      "backward_entropy": 0.021987746159235638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.163917541503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04148268327116966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07298698425292968,
      "backward_entropy": 0.013696485095553927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.42227172851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04154597967863083,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07287421226501464,
      "backward_entropy": 0.013538362251387702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.77049255371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0416121631860733,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07275777459144592,
      "backward_entropy": 0.013385464747746786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.5552978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04167960584163666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07264059782028198,
      "backward_entropy": 0.013235559066136679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.99893188476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041748229414224625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07252224683761596,
      "backward_entropy": 0.013090316620137956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.125328063964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041816551238298416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07240526676177979,
      "backward_entropy": 0.02079396777682834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.40239334106445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041881874203681946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07229312658309936,
      "backward_entropy": 0.012803057001696693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.6311264038086,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.041945815086364746,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07218282222747803,
      "backward_entropy": 0.07548899120754665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.78779220581055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042009830474853516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07207173109054565,
      "backward_entropy": 0.012516617774963379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.60544204711914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04207133501768112,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0719649314880371,
      "backward_entropy": 0.012374780244297452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.51045227050781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04212898015975952,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07186213731765748,
      "backward_entropy": 0.019838200675116643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.0025634765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042190294712781906,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0717553436756134,
      "backward_entropy": 0.012089851001898447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.456520080566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042252033948898315,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07164738774299621,
      "backward_entropy": 0.011954044302304586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.41054916381836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04231143370270729,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0715422809123993,
      "backward_entropy": 0.01181683854924308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.200096130371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0423702709376812,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07143889665603638,
      "backward_entropy": 0.011682903601063622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.53331756591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04242715984582901,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07133830785751342,
      "backward_entropy": 0.011548634204599593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.62457275390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04248788580298424,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07123327851295472,
      "backward_entropy": 0.018762454390525818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.04766082763672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042556166648864746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07111921906471252,
      "backward_entropy": 0.011306776768631406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.82121658325195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04262431710958481,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07100518345832825,
      "backward_entropy": 0.011191344923443265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.66014862060547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04268956556916237,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07089440822601319,
      "backward_entropy": 0.01107190383805169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.43871307373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04275651276111603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07078243494033813,
      "backward_entropy": 0.010957908299234178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.22409057617188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.042827777564525604,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07066631317138672,
      "backward_entropy": 0.07563108868069118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.409875869750977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0429028756916523,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07054611444473266,
      "backward_entropy": 0.017856693930096097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.89694213867188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04297177121043205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07043457627296448,
      "backward_entropy": 0.010645202464527555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.47653198242188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04304079711437225,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0703241229057312,
      "backward_entropy": 0.010544786850611368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.39889526367188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04311393201351166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07020901441574097,
      "backward_entropy": 0.010451407896147834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.44725799560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04318934679031372,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07009146213531495,
      "backward_entropy": 0.01036053564813402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.83053970336914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04326417297124863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06997547149658204,
      "backward_entropy": 0.010270964768197801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.94855499267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043334394693374634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06986652612686158,
      "backward_entropy": 0.01704002751244439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.22787094116211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04340584576129913,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06975556015968323,
      "backward_entropy": 0.016908206873469882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.384525299072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04347569867968559,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0696467101573944,
      "backward_entropy": 0.009998605483108096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.6651840209961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04354270547628403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06954092979431152,
      "backward_entropy": 0.009905447562535604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.49366760253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043609946966171265,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06943503618240357,
      "backward_entropy": 0.00981472349829144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.015987396240234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0436774343252182,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06932849287986756,
      "backward_entropy": 0.009726290901501974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.92715072631836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04374242201447487,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06922524571418762,
      "backward_entropy": 0.016256549292140536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.61949157714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043805137276649475,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06912482976913452,
      "backward_entropy": 0.009547256761127047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.8941879272461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0438699834048748,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06902202367782592,
      "backward_entropy": 0.01600435045030382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.73567962646484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04393531754612923,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06891810297966003,
      "backward_entropy": 0.015882031785117254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.050968170166016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044001106172800064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0688132643699646,
      "backward_entropy": 0.015761733055114746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.91944122314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0440659299492836,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06870911121368409,
      "backward_entropy": 0.009207050833437178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.24087524414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044132672250270844,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06860235333442688,
      "backward_entropy": 0.009125438001420762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.01394653320312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04419980198144913,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06849526166915894,
      "backward_entropy": 0.01541117661529117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.485694885253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04426998272538185,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06838420629501343,
      "backward_entropy": 0.008968181080288358,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.57112121582031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04433885216712952,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06827507019042969,
      "backward_entropy": 0.008890471524662442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.432769775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04441062733530998,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0681618332862854,
      "backward_entropy": 0.00881660564078225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.071678161621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044478245079517365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06805437803268433,
      "backward_entropy": 0.008739776081509061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.60433959960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04454483464360237,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06794779300689698,
      "backward_entropy": 0.008662376138899062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.09706115722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04461320862174034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06783908605575562,
      "backward_entropy": 0.008588052458233304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.65238952636719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04468187689781189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06772957444190979,
      "backward_entropy": 0.008515456484423744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.515316009521484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04474946856498718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06762154102325439,
      "backward_entropy": 0.008443308373292288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.9158821105957,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04481610283255577,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06751472949981689,
      "backward_entropy": 0.07611532343758477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.249820709228516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044879186898469925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0674127221107483,
      "backward_entropy": 0.008299522929721408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.66519165039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044941794127225876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06731131076812744,
      "backward_entropy": 0.014249576462639702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.82959747314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04500802978873253,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06720536947250366,
      "backward_entropy": 0.014151571525467766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.98643493652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04507214576005936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06710198521614075,
      "backward_entropy": 0.014053910970687866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.93252563476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04513707011938095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06699758172035217,
      "backward_entropy": 0.008023829095893435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.56912612915039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04520408809185028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06689072847366333,
      "backward_entropy": 0.007960923843913607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.382957458496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04527034983038902,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06678562164306641,
      "backward_entropy": 0.007899604737758636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.337158203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0453345850110054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06668358445167541,
      "backward_entropy": 0.007838427192635007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.17961120605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045399706810712814,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06658092737197877,
      "backward_entropy": 0.013600370950169034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.02607727050781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04546291381120682,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06648072004318237,
      "backward_entropy": 0.07621560494105022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.87849426269531,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04552841559052467,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06637719869613648,
      "backward_entropy": 0.07622665166854858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.5193099975586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045594677329063416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06627275347709656,
      "backward_entropy": 0.0076071106725268895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.742774963378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045665595680475235,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06616202592849732,
      "backward_entropy": 0.007554127938217587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.9032211303711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04573407769203186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06605496406555175,
      "backward_entropy": 0.00750080578856998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.36265563964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04580831155180931,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06593967080116273,
      "backward_entropy": 0.007451347178883023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.40030288696289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04588112607598305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06582673192024231,
      "backward_entropy": 0.007402114570140839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.84436798095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045951325446367264,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06571726799011231,
      "backward_entropy": 0.007352551652325524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.16084671020508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04602842032909393,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06559902429580688,
      "backward_entropy": 0.007307893700069851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.784690856933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046102505177259445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06548510193824768,
      "backward_entropy": 0.0072627787788709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.76522827148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046175193041563034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06537283658981323,
      "backward_entropy": 0.007217298779222701,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.89160919189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04625058174133301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06525716781616211,
      "backward_entropy": 0.007173524962531196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.68447494506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04633099585771561,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06513398885726929,
      "backward_entropy": 0.007132780220773485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.27993774414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04640808328986168,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06501559019088746,
      "backward_entropy": 0.012545307477315268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.20048522949219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04649001359939575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06489073038101197,
      "backward_entropy": 0.007051703830560048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.746828079223633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04657239466905594,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06476508975028991,
      "backward_entropy": 0.007012834979428185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.76493835449219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046649958938360214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0646463930606842,
      "backward_entropy": 0.00697282784514957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.053932189941406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04672841727733612,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06452625393867492,
      "backward_entropy": 0.07647313674290974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.33795928955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04680637642741203,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06440646648406982,
      "backward_entropy": 0.01223202794790268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.248355865478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04688518866896629,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.064285147190094,
      "backward_entropy": 0.006854843762185838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.68959426879883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046962179243564606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0641664743423462,
      "backward_entropy": 0.006815752221478356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.950416564941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04703623056411743,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06405181884765625,
      "backward_entropy": 0.006776410672399733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.106164932250977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04710894078016281,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0639394223690033,
      "backward_entropy": 0.011983957555558946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.66897201538086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04717784747481346,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0638322651386261,
      "backward_entropy": 0.0066984788411193425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.345354080200195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04724593833088875,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06372599601745606,
      "backward_entropy": 0.006660449835989211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.22014617919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047308098524808884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06362850666046142,
      "backward_entropy": 0.006621473365359836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.01177215576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047374021261930466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06352506875991822,
      "backward_entropy": 0.00658444066842397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.78813171386719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04744331166148186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0634167492389679,
      "backward_entropy": 0.006548759837945302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.938720703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047515641897916794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06330368518829346,
      "backward_entropy": 0.006514459434482787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.99507141113281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04759325087070465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0631829023361206,
      "backward_entropy": 0.006482010500298606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.20159149169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04767048731446266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06306260824203491,
      "backward_entropy": 0.006450101319286559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.5215950012207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04775119945406914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06293747425079346,
      "backward_entropy": 0.006420116871595383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.429447174072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04782995581626892,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06281504631042481,
      "backward_entropy": 0.006390172988176346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.104764938354492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047908246517181396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06269280910491944,
      "backward_entropy": 0.006360636817084419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.07461929321289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04798096418380737,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06257905960083007,
      "backward_entropy": 0.00633078482415941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.89906311035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048053789883852005,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0624650776386261,
      "backward_entropy": 0.01124571594927046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.774600982666016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048126716166734695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06235076785087586,
      "backward_entropy": 0.006273064762353897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.23114776611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04819846525788307,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06223806142807007,
      "backward_entropy": 0.006244537317090564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.590965270996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048275500535964966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06211721897125244,
      "backward_entropy": 0.006217011974917518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.18851089477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04834964498877525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06200094223022461,
      "backward_entropy": 0.006189460555712382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.4997329711914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04842375963926315,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06188454627990723,
      "backward_entropy": 0.006162376453479131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.22333526611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048501644283533096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061762118339538576,
      "backward_entropy": 0.006136159516043133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.85662841796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048582881689071655,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061634612083435056,
      "backward_entropy": 0.01089457008573744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.96827697753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04866213724017143,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06150984764099121,
      "backward_entropy": 0.006085853609773848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.13725471496582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048738300800323486,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06138997077941895,
      "backward_entropy": 0.010799795389175415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.07965087890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0488104447722435,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06127602458000183,
      "backward_entropy": 0.006036277446481917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.55043029785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04888274520635605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06116170883178711,
      "backward_entropy": 0.006011989381578233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.104698181152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04895645007491112,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06104491949081421,
      "backward_entropy": 0.005988243139452404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.72048950195312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049028921872377396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06092970967292786,
      "backward_entropy": 0.005965001881122589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.686134338378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049104053527116776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0608099102973938,
      "backward_entropy": 0.005942693601051967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.13422393798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049175333231687546,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06069589257240295,
      "backward_entropy": 0.005920930455128352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.5278377532959,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04924435168504715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060585391521453855,
      "backward_entropy": 0.005899426837762197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.916866302490234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049310021102428436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06048039197921753,
      "backward_entropy": 0.005877923634317186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.913829803466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049373965710401535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06037806868553162,
      "backward_entropy": 0.005856581860118442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.4034423828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0494338721036911,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06028230190277099,
      "backward_entropy": 0.005835539764828152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.79290008544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04949764162302017,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06017990112304687,
      "backward_entropy": 0.005814897517363231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.753828048706055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04956367239356041,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060073405504226685,
      "backward_entropy": 0.0057952714463075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.39754104614258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04962550103664398,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05997365713119507,
      "backward_entropy": 0.005776266256968181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.29850387573242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04968598484992981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05987619161605835,
      "backward_entropy": 0.0057576050360997515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.36785888671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04974524304270744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059780669212341306,
      "backward_entropy": 0.005739208724763658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.35087585449219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04980846494436264,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.059678077697753906,
      "backward_entropy": 0.010111997524897257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.97895812988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04987155646085739,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05957524180412292,
      "backward_entropy": 0.005705050710174773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.30726623535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04993328079581261,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.059474396705627444,
      "backward_entropy": 0.01003866559929318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.1541633605957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0499962642788887,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05937095880508423,
      "backward_entropy": 0.005673954056368934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.664093017578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05006032809615135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05926547050476074,
      "backward_entropy": 0.0056590984265009565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.83514404296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0501229353249073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05916221141815185,
      "backward_entropy": 0.005644936528470781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.00770568847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050186675041913986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05905669927597046,
      "backward_entropy": 0.005631135155757268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.66847229003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05025508627295494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05894312858581543,
      "backward_entropy": 0.005617448025279575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.42826843261719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05032647028565407,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0588243842124939,
      "backward_entropy": 0.0056038544409804875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.103458404541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05040053278207779,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05870097279548645,
      "backward_entropy": 0.005590473612149556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.985185623168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050472039729356766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05858200788497925,
      "backward_entropy": 0.0055771490765942466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.9012451171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05054011568427086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05846880674362183,
      "backward_entropy": 0.0055647918747531045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.75592041015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050605084747076035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05836084485054016,
      "backward_entropy": 0.005553194218211704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.18653106689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05066846311092377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058255481719970706,
      "backward_entropy": 0.005541932251718309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.650634765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050736524164676666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058141696453094485,
      "backward_entropy": 0.005530702571074168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.12754821777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05080152302980423,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05803303718566895,
      "backward_entropy": 0.00961561832163069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.14809799194336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05086737126111984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05792264342308044,
      "backward_entropy": 0.0055105239152908325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.99538803100586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0509326197206974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0578133761882782,
      "backward_entropy": 0.005499589360422558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.10052490234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05099751800298691,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057704377174377444,
      "backward_entropy": 0.0054894669188393485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.75966739654541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05106073245406151,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05759847164154053,
      "backward_entropy": 0.005478982296254899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.036617279052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051118891686201096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05750151872634888,
      "backward_entropy": 0.005469656652874417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.47303009033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05117977783083916,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05739935636520386,
      "backward_entropy": 0.00546029872364468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.02416120283305645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0512407012283802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05729683637619019,
      "backward_entropy": 0.005451402730411953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.946565628051758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05129551514983177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057205700874328615,
      "backward_entropy": 0.005443219509389665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.737098693847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051348499953746796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05711773633956909,
      "backward_entropy": 0.005435322721799214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.400856018066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05140341445803642,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057026028633117676,
      "backward_entropy": 0.005426647348536385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.020912170410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0514577254652977,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05693517923355103,
      "backward_entropy": 0.0054183585776223075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.20992660522461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05151510238647461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05683837532997131,
      "backward_entropy": 0.005409842977921168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.593732833862305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05157160386443138,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05674309730529785,
      "backward_entropy": 0.005401538064082463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.509422302246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0516260601580143,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05665143728256226,
      "backward_entropy": 0.005393404927518632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.446449279785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05168123170733452,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05655816793441772,
      "backward_entropy": 0.005386127365960015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.926233291625977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0517345666885376,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056468135118484496,
      "backward_entropy": 0.005379423499107361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.592735290527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05178504064679146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0563834547996521,
      "backward_entropy": 0.005373491181267632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.45146179199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05183782801032066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05629394054412842,
      "backward_entropy": 0.005368030733532376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.556034088134766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05189274623990059,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056199944019317626,
      "backward_entropy": 0.005363493329948849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.17233657836914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05194699764251709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05610715746879578,
      "backward_entropy": 0.005358685221936967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.028470993041992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05200311914086342,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0560105562210083,
      "backward_entropy": 0.005354021158483293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.573123931884766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05205727741122246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055917668342590335,
      "backward_entropy": 0.005349524733093049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.16750717163086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052112117409706116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05582324266433716,
      "backward_entropy": 0.005345170696576436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.070594787597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05216636881232262,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055729794502258304,
      "backward_entropy": 0.005341297222508324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.741893768310547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052220117300748825,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05563701987266541,
      "backward_entropy": 0.076900667614407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.313961029052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05227215960621834,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05554748773574829,
      "backward_entropy": 0.005334842536184523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.74115753173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052326302975416183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05545358657836914,
      "backward_entropy": 0.005331799801852968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.84309768676758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05238591134548187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0553489089012146,
      "backward_entropy": 0.008870638906955719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.26830291748047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05244574695825577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05524346828460693,
      "backward_entropy": 0.005325838923454285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.681575775146484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052510421723127365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05512866973876953,
      "backward_entropy": 0.005322568946414524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.511966705322266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05257592722773552,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055012094974517825,
      "backward_entropy": 0.005319613135523266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.34048080444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05264211446046829,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05489416122436523,
      "backward_entropy": 0.005316386620203654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.11481857299805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05270884558558464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05477526187896729,
      "backward_entropy": 0.005312385244501961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.000465393066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052773792296648026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05465980172157288,
      "backward_entropy": 0.005309037036365933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.77863693237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05283818393945694,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05454538464546203,
      "backward_entropy": 0.005305216958125432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.50175476074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052904509007930756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054427146911621094,
      "backward_entropy": 0.005300947775443395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.26351547241211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05297379195690155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05430301427841187,
      "backward_entropy": 0.0052965304089917075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.132102966308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053045593202114105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05417411327362061,
      "backward_entropy": 0.00864047971036699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.06825256347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053118545562028885,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05404294729232788,
      "backward_entropy": 0.00528497373064359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.840822219848633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053191326558589935,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05391218662261963,
      "backward_entropy": 0.005278830313020282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.68745040893555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053258173167705536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0537932276725769,
      "backward_entropy": 0.005274597555398941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.272825241088867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05332554131746292,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.053672826290130614,
      "backward_entropy": 0.008536156680848863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.19872283935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05338984727859497,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05355826616287231,
      "backward_entropy": 0.008513404263390435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.439835548400879,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05345122143626213,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.053449618816375735,
      "backward_entropy": 0.008489147656493716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.34165954589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.053508613258600235,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05334940552711487,
      "backward_entropy": 0.07693458928002252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.306382179260254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05356968194246292,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053241515159606935,
      "backward_entropy": 0.0052542297376526725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.30488204956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05362717807292938,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05314062833786011,
      "backward_entropy": 0.005250484579139286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.365331649780273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05368713662028313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05303448438644409,
      "backward_entropy": 0.005245871427986357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.820404052734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05374591425061226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05293044447898865,
      "backward_entropy": 0.005242129994763268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.741127014160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0538046695291996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05282634496688843,
      "backward_entropy": 0.005237918347120285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.042436599731445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053865697234869,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05271764993667603,
      "backward_entropy": 0.0052325911819934845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.86901092529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05392538756132126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05261119604110718,
      "backward_entropy": 0.00522816926240921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.4878010749816895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05398627743124962,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05250205993652344,
      "backward_entropy": 0.00522464844915602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.2352523803711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05404241755604744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05240275859832764,
      "backward_entropy": 0.005223107006814744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.165897369384766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054104436188936234,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.052291488647460936,
      "backward_entropy": 0.005218817955917782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.84003829956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05416952446103096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05217394232749939,
      "backward_entropy": 0.005213478373156654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.032501220703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054234057664871216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05205751061439514,
      "backward_entropy": 0.005209354062875112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.138099670410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05429907515645027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051939791440963744,
      "backward_entropy": 0.005204797618918949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.73397827148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054365839809179306,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05181839466094971,
      "backward_entropy": 0.005200934078958299,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.994178771972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05443746596574783,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051687192916870114,
      "backward_entropy": 0.005195947984854381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.07936477661133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05450664460659027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051561111211776735,
      "backward_entropy": 0.005191732197999954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.567110061645508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054574623703956604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05143768787384033,
      "backward_entropy": 0.00518658094935947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.33171081542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054639413952827454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051320832967758176,
      "backward_entropy": 0.005183008809884389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.71304702758789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05470803380012512,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05119611024856567,
      "backward_entropy": 0.005178280174732208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.68476867675781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054776765406131744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05107108950614929,
      "backward_entropy": 0.007916555636458926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.19993019104004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05484773963689804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05094156265258789,
      "backward_entropy": 0.005168625050120884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.14132308959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05491508170962334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05081979632377624,
      "backward_entropy": 0.005163779689206017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.893375396728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054982587695121765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05069766640663147,
      "backward_entropy": 0.005159365634123485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.8743953704834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05505364388227463,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05056807994842529,
      "backward_entropy": 0.005154842303858863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.730857849121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05512203276157379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050444507598876955,
      "backward_entropy": 0.005149127708541023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.47700500488281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05518822371959686,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05032550096511841,
      "backward_entropy": 0.0051444148023923235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.19746017456055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055253759026527405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050207394361495974,
      "backward_entropy": 0.005142302148871952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.84200668334961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05531958118081093,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050088626146316526,
      "backward_entropy": 0.005140048762162526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.81886672973633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05538671091198921,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04996716380119324,
      "backward_entropy": 0.0051369162069426644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.591070175170898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055454034358263016,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04984513819217682,
      "backward_entropy": 0.00513461646106508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.18642807006836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05551702156662941,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04973239302635193,
      "backward_entropy": 0.005133831666575538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.48465633392334,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05558169260621071,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04961585998535156,
      "backward_entropy": 0.007590002483791775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.4378776550293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05564214289188385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049508672952651975,
      "backward_entropy": 0.005132704973220825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.23333740234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055702224373817444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049402153491973876,
      "backward_entropy": 0.00513215528594123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.14206314086914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055765241384506226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049289488792419435,
      "backward_entropy": 0.005130424267715878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.34022521972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055827707052230835,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049177467823028564,
      "backward_entropy": 0.005129979302485784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.293529510498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05589401349425316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04905738234519959,
      "backward_entropy": 0.0051287876235114205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.22674560546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05595823749899864,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04894178211688995,
      "backward_entropy": 0.005128139009078343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.54648208618164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05602269992232323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04882583022117615,
      "backward_entropy": 0.00512668283449279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.08122945576906204,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056086331605911255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04871161580085755,
      "backward_entropy": 0.0051253313819567365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.69890594482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05614372342824936,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048611265420913694,
      "backward_entropy": 0.005125685698456234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.12471008300781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05620205029845238,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048508751392364505,
      "backward_entropy": 0.005125495294729869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.61060333251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0562601312994957,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048406726121902464,
      "backward_entropy": 0.0051253073745303685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.90884780883789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05631685256958008,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.048307621479034425,
      "backward_entropy": 0.007302404277854496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.381053924560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05637678876519203,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048201388120651244,
      "backward_entropy": 0.005124495261245304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.564208984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05643526464700699,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04809820055961609,
      "backward_entropy": 0.005124856614404255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.17009925842285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05649353936314583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04799523651599884,
      "backward_entropy": 0.005126068161593543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.783626556396484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05655040219426155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047895461320877075,
      "backward_entropy": 0.0051269253922833335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.51489543914795,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05660929158329964,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04779117703437805,
      "backward_entropy": 0.0071877191464106245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.47301959991455,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0566646009683609,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.047694772481918335,
      "backward_entropy": 0.07696834537718031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.922273635864258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05671657621860504,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04760596752166748,
      "backward_entropy": 0.005129841052823597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.482824325561523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05676870048046112,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04751706123352051,
      "backward_entropy": 0.007118335200680627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.641149520874023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05681917816400528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04743122458457947,
      "backward_entropy": 0.005131799313757155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.268242835998535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056870296597480774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04734345078468323,
      "backward_entropy": 0.005135688516828749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.290401458740234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.056918587535619736,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.047261974215507506,
      "backward_entropy": 0.0769720474878947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.149291515350342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05696538835763931,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047183775901794435,
      "backward_entropy": 0.005145000086890327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.250484466552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05700855329632759,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04711400866508484,
      "backward_entropy": 0.005149248159594006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.12265396118164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057055070996284485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04703645706176758,
      "backward_entropy": 0.007003534171316359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.0782575607299805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057104334235191345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0469531774520874,
      "backward_entropy": 0.00515332818031311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.990683555603027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05714976415038109,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04687862992286682,
      "backward_entropy": 0.005154249568780263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.919422149658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05719301104545593,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04680849015712738,
      "backward_entropy": 0.005157113075256348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.571109771728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057235024869441986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.046741583943367006,
      "backward_entropy": 0.005158235215478473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.43632507324219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05727941170334816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.046668946743011475,
      "backward_entropy": 0.005159429791900847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.33000183105469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057325977832078934,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04659106731414795,
      "backward_entropy": 0.006860098077191247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.40594482421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05737423524260521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.046509623527526855,
      "backward_entropy": 0.005160752269956801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.28310775756836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05742943659424782,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04641297161579132,
      "backward_entropy": 0.0051582881973849404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.26346206665039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057489875704050064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.046304899454116824,
      "backward_entropy": 0.005155032293664085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.912811279296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05754854530096054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04620120525360107,
      "backward_entropy": 0.005151676634947459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.759483337402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05760665982961655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04609876275062561,
      "backward_entropy": 0.005147987769709693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.943326950073242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05766437202692032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04599737823009491,
      "backward_entropy": 0.005144784847895305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.20964050292969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057720404118299484,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04590031206607818,
      "backward_entropy": 0.005139862497647603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.372936248779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057780247181653976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04579530358314514,
      "backward_entropy": 0.005132797691557143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.18583869934082,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05783915892243385,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04569310247898102,
      "backward_entropy": 0.006582210461298625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.890331268310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05789761617779732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04559168219566345,
      "backward_entropy": 0.005116043405400382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.500335693359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05795331671833992,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04549677968025208,
      "backward_entropy": 0.005108555985821618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.7077579498291,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058009613305330276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.045400920510292056,
      "backward_entropy": 0.005099662062194612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.641376495361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058063533157110214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.045310336351394656,
      "backward_entropy": 0.0050926001535521615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.026363372802734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05811741575598717,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.045219641923904416,
      "backward_entropy": 0.0050868843164708875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.361854553222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05817023664712906,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04513120353221893,
      "backward_entropy": 0.005082722753286362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.11790466308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058223288506269455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.045041757822036746,
      "backward_entropy": 0.005081238845984141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.935401916503906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.058278340846300125,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0449479877948761,
      "backward_entropy": 0.07696729236178929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.60112190246582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05833512544631958,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04485074281692505,
      "backward_entropy": 0.005075102051099141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.51744270324707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0583905354142189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044756531715393066,
      "backward_entropy": 0.005073062247700161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.359432220458984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058444466441869736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04466604888439178,
      "backward_entropy": 0.0050699176887671156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.27806282043457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0585002563893795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04457156956195831,
      "backward_entropy": 0.005066360864374373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.19626235961914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0585547499358654,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.044479915499687196,
      "backward_entropy": 0.00616800660888354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.803985595703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058607831597328186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04439182877540589,
      "backward_entropy": 0.005061949706739849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.97805404663086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058662764728069305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04429989457130432,
      "backward_entropy": 0.005058349006705814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.43601608276367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05871628597378731,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04421133995056152,
      "backward_entropy": 0.005054732577668296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.046958923339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0587715283036232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04411934614181519,
      "backward_entropy": 0.005049541592597961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.01372528076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05882766842842102,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04402506947517395,
      "backward_entropy": 0.005047132571538289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.02894592285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0588855966925621,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.043926388025283813,
      "backward_entropy": 0.005047272476885054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.49476432800293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05894703418016434,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04382023811340332,
      "backward_entropy": 0.005047708749771118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.213134765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05900735780596733,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04371700882911682,
      "backward_entropy": 0.00504748812980122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.26143264770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05906511843204498,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04361904263496399,
      "backward_entropy": 0.005051636861430274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.027099609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05912499129772186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04351723790168762,
      "backward_entropy": 0.005052261882358127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.988497734069824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05918417498469353,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04341680407524109,
      "backward_entropy": 0.00505479011270735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.843294143676758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05924057587981224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.043322688341140746,
      "backward_entropy": 0.005843072301811642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.63252067565918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059295378625392914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.043232393264770505,
      "backward_entropy": 0.005061212927103043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.266944885253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059349771589040756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04314314723014832,
      "backward_entropy": 0.005064210957951016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.536231994628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059405602514743805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04305131435394287,
      "backward_entropy": 0.005064791275395287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.83027648925781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059459734708070755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04296367168426514,
      "backward_entropy": 0.005063919971386592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.8878116607666,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059515804052352905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04287120699882507,
      "backward_entropy": 0.005066201504733827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.985445022583008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0595722571015358,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04277817606925964,
      "backward_entropy": 0.005067818694644504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.603715896606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05962781980633736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04268780648708344,
      "backward_entropy": 0.005066545473204719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.696409225463867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05968340486288071,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04259842038154602,
      "backward_entropy": 0.005632913774914212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.199176788330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059738293290138245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04251086711883545,
      "backward_entropy": 0.005054372052351634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.08006477355957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05979388579726219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.042421311140060425,
      "backward_entropy": 0.005051011426581277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.512874603271484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05984971672296524,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04233167171478271,
      "backward_entropy": 0.0050464388397004865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.55146026611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059906959533691406,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.042238938808441165,
      "backward_entropy": 0.005042907678418689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.71922302246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059962280094623566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04215088784694672,
      "backward_entropy": 0.005038425326347351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.216848373413086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060019783675670624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04205853939056396,
      "backward_entropy": 0.00503252860572603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.204235076904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060073625296354294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04197420477867127,
      "backward_entropy": 0.005029011931684282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.11575698852539,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06012820824980736,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.04188781678676605,
      "backward_entropy": 0.07696908050113255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.421886444091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06018109992146492,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041805621981620786,
      "backward_entropy": 0.005026732467942768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.78606414794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060233861207962036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04172326326370239,
      "backward_entropy": 0.005028214305639267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.360604286193848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06028696894645691,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04164074063301086,
      "backward_entropy": 0.005027017245690028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.803260803222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06033783406019211,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041563010215759276,
      "backward_entropy": 0.005027780102358924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.923439025878906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.060390669852495193,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.04148054122924805,
      "backward_entropy": 0.07697085539499919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.140292167663574,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0604432038962841,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.04139865338802338,
      "backward_entropy": 0.07697184880574544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.378267288208008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060493458062410355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041321825981140134,
      "backward_entropy": 0.005040805372926924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.987105369567871,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06054257974028587,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.04124760329723358,
      "backward_entropy": 0.07697385549545288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.719324111938477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060589808970689774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04117743372917175,
      "backward_entropy": 0.0050521766146024065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.819011688232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06063803285360336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04110531210899353,
      "backward_entropy": 0.005057293507787917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.4143123626709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06068805977702141,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04102961421012878,
      "backward_entropy": 0.005060806456539366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.874008178710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060738906264305115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04095210433006287,
      "backward_entropy": 0.00506485543317265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.812870025634766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06078870967030525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04087655544281006,
      "backward_entropy": 0.005071227335267597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.995838165283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060837287455797195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04080402851104736,
      "backward_entropy": 0.005076431565814548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.723093032836914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060886651277542114,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04073007702827454,
      "backward_entropy": 0.005080396930376689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.7836856842041,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0609358586370945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040656501054763795,
      "backward_entropy": 0.005084509650866191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.6983528137207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060986801981925964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040579354763031004,
      "backward_entropy": 0.005088346699873607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.314416885375977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061040058732032776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040497702360153195,
      "backward_entropy": 0.005089637719922596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.25933265686035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06109175831079483,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040419644117355345,
      "backward_entropy": 0.005090925842523575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.110124588012695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06114482134580612,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04033898711204529,
      "backward_entropy": 0.005091468907064862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.029258728027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061196327209472656,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040261849761009216,
      "backward_entropy": 0.00509232696559694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.80669403076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061246275901794434,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04018855094909668,
      "backward_entropy": 0.005092545929882262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.61853790283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061296675354242325,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04011470675468445,
      "backward_entropy": 0.005091520647207896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.0554423332214355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061347752809524536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040039241313934326,
      "backward_entropy": 0.005092456522915099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.892244338989258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06139460578560829,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039973095059394836,
      "backward_entropy": 0.005093659791681502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.382904052734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06143861636519432,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039913100004196164,
      "backward_entropy": 0.005095406124989192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.795793056488037,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06148280203342438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03985285758972168,
      "backward_entropy": 0.0050967757900555926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.356582641601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06152447313070297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03979787230491638,
      "backward_entropy": 0.005099598318338394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.817216873168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06156571954488754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039743620157241824,
      "backward_entropy": 0.0051033637589878505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.417302131652832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06160855293273926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039685577154159546,
      "backward_entropy": 0.005109177281459172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.15273255109786987,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06165001541376114,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03963014483451843,
      "backward_entropy": 0.00511684517065684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.178739547729492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06168737635016441,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0395835280418396,
      "backward_entropy": 0.0051253363490104675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.243545532226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0617276206612587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03953026533126831,
      "backward_entropy": 0.005135478244887458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.221607208251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06176672503352165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03947919011116028,
      "backward_entropy": 0.005146825479136573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.187793731689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06180763617157936,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03942382633686066,
      "backward_entropy": 0.005160068058305317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.715341567993164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06184958294034004,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03936717808246613,
      "backward_entropy": 0.00516817428999477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.377377510070801,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06189106032252312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039311358332633974,
      "backward_entropy": 0.005176977564891179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.168960571289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06193036213517189,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.039259839057922366,
      "backward_entropy": 0.004581102894412147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.663311004638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061970304697752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039206933975219724,
      "backward_entropy": 0.0051977332267496325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.543432235717773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062011681497097015,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039151090383529666,
      "backward_entropy": 0.005207166075706482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.791682243347168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062054332345724106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039092648029327395,
      "backward_entropy": 0.005215824478202396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.770246505737305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06209537759423256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03903774619102478,
      "backward_entropy": 0.005223573495944341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.159658432006836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06213683634996414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038981962203979495,
      "backward_entropy": 0.0052309127317534555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.091934204101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06217779591679573,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03892711102962494,
      "backward_entropy": 0.005238511496120029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.0949482917785645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06221822276711464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038873481750488284,
      "backward_entropy": 0.0052453867263264125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.464130401611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06225629895925522,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03882505297660828,
      "backward_entropy": 0.005251069863637288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.010477066040039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0622933954000473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03877824544906616,
      "backward_entropy": 0.005258538242843416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.76517105102539,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0623285286128521,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.038735613226890564,
      "backward_entropy": 0.07699476348029242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.989253520965576,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0623638890683651,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038692066073417665,
      "backward_entropy": 0.005274396389722824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.655661582946777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06239715591073036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038653564453125,
      "backward_entropy": 0.0052791258527172934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.970869064331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062430672347545624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03861437737941742,
      "backward_entropy": 0.005284206734763252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.8427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06246518716216087,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038573211431503295,
      "backward_entropy": 0.005288180791669422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.154727935791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06249796971678734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038535648584365846,
      "backward_entropy": 0.005291742583115895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.076379776000977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06253258138895035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03849462270736694,
      "backward_entropy": 0.005292842785517375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4274590015411377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06256868690252304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038451164960861206,
      "backward_entropy": 0.005290098488330841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.14855755865573883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06260212510824203,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03841291069984436,
      "backward_entropy": 0.005289240429798762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.936144828796387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06263216584920883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038381725549697876,
      "backward_entropy": 0.005288930402861701,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.404043197631836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06266182661056519,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03835110366344452,
      "backward_entropy": 0.005289391511016422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.0198974609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06269282847642899,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03831772208213806,
      "backward_entropy": 0.005289374954170651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.49859619140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06272461265325546,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03828163146972656,
      "backward_entropy": 0.005294272469149696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.157577514648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06275820732116699,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03824237585067749,
      "backward_entropy": 0.0052961135903994245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.095497131347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06279279291629791,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03820098042488098,
      "backward_entropy": 0.005298016799820794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.348350524902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06282812356948853,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03815837800502777,
      "backward_entropy": 0.0052982742587725324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.07343292236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06286585330963135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0381111204624176,
      "backward_entropy": 0.005296734472115834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.639636039733887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06290483474731445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038061752915382385,
      "backward_entropy": 0.005293210347493489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.608223915100098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06294355541467667,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03801239728927612,
      "backward_entropy": 0.005292660660213894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.579071044921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0629817470908165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03796441555023193,
      "backward_entropy": 0.005291360947820876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.414657592773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06302064657211304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03791449666023254,
      "backward_entropy": 0.0052932674686114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.395537376403809,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06305927038192749,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03786473870277405,
      "backward_entropy": 0.005297469596068065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.2224760055542,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.063097283244133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037816762924194336,
      "backward_entropy": 0.005299598806434208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.327085494995117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06313427537679672,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037770426273345946,
      "backward_entropy": 0.005304754608207279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.177978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06317228823900223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03772298991680145,
      "backward_entropy": 0.005305122170183394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.12276554107666016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06320972740650177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03767714500427246,
      "backward_entropy": 0.005303727255927192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.905641555786133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06324352324008942,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03763840794563293,
      "backward_entropy": 0.005304387874073452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.035650253295898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06327994167804718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037594228982925415,
      "backward_entropy": 0.005306094057030148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.829955101013184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06331496685743332,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03755350708961487,
      "backward_entropy": 0.00530458324485355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.498729705810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06335056573152542,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03751192390918732,
      "backward_entropy": 0.0053017714785204995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.649781227111816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06338915973901749,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03746472597122193,
      "backward_entropy": 0.005297345005803638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.385648727416992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06342796236276627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03741743564605713,
      "backward_entropy": 0.005292276541392009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.717903137207031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0634680986404419,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037367004156112674,
      "backward_entropy": 0.005290483021073871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.46530818939209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06350673735141754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0373196005821228,
      "backward_entropy": 0.005289542592234082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.242083549499512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0635451152920723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0372721791267395,
      "backward_entropy": 0.005292494263913896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.41090202331543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06358389556407928,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03722383677959442,
      "backward_entropy": 0.005296612779299418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.669686317443848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06362605094909668,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03716958165168762,
      "backward_entropy": 0.005297755201657613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.218151092529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06366590410470963,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03711938261985779,
      "backward_entropy": 0.005302484664652083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.637737274169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0637049674987793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03707112073898315,
      "backward_entropy": 0.00530579396420055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.745776176452637,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06374495476484299,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03702141642570496,
      "backward_entropy": 0.003631544609864553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.530609130859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06378526985645294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0369706928730011,
      "backward_entropy": 0.0053122635516855456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.922371864318848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06382337212562561,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03692401647567749,
      "backward_entropy": 0.005319194661246406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.863954544067383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06386081129312515,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03687904179096222,
      "backward_entropy": 0.0053243984778722124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.444878578186035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06389757245779037,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03683588802814484,
      "backward_entropy": 0.00532705792122417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.725101470947266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0639345645904541,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03679271936416626,
      "backward_entropy": 0.005327781041463216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.621414184570312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0639709010720253,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03675122857093811,
      "backward_entropy": 0.005326297548082139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.966673374176025,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06400686502456665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03671038746833801,
      "backward_entropy": 0.0053258558942212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.116044044494629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06404156982898712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036672082543373105,
      "backward_entropy": 0.005324899322456784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.586804389953613,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06407659500837326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03663366436958313,
      "backward_entropy": 0.005321598715252346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.51782512664795,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0641128197312355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0365931361913681,
      "backward_entropy": 0.00531763666205936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.89699363708496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0641498938202858,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036551672220230105,
      "backward_entropy": 0.005310413738091786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.244078636169434,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06418885290622711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03650656640529633,
      "backward_entropy": 0.005304401119550069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.157214164733887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06422873586416245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036459612846374514,
      "backward_entropy": 0.005300150977240669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.062394142150879,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06426611542701721,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036417645215988156,
      "backward_entropy": 0.005295799838172065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.462750434875488,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0643029734492302,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036376529932022096,
      "backward_entropy": 0.005293115145630307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.5377302169799805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06434009969234467,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03633490800857544,
      "backward_entropy": 0.00529139530327585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.432077407836914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06437551975250244,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03629725575447083,
      "backward_entropy": 0.005285706784990098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.813895225524902,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0644097700715065,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03626169562339783,
      "backward_entropy": 0.005281344883971744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.921337604522705,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06444363296031952,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03622700870037079,
      "backward_entropy": 0.0052767470479011536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.678853988647461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06447580456733704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03619515299797058,
      "backward_entropy": 0.005275020996729533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.882179260253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0645078644156456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036163538694381714,
      "backward_entropy": 0.005273967567417357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.572577476501465,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06453826278448105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036134999990463254,
      "backward_entropy": 0.005273528811004426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.861157417297363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06456863880157471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03610655665397644,
      "backward_entropy": 0.005272664957576328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.067935943603516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06459973007440567,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036077046394348146,
      "backward_entropy": 0.0052710796395937605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.365673065185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06463256478309631,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036043742299079896,
      "backward_entropy": 0.005272772990994983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.626969337463379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06466533988714218,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03601033091545105,
      "backward_entropy": 0.005276666747199165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.275092124938965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06469863653182983,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035976064205169675,
      "backward_entropy": 0.005279879603121016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.943785190582275,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06473159044981003,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03594263792037964,
      "backward_entropy": 0.005282181832525466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.124935150146484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06476350128650665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03591127991676331,
      "backward_entropy": 0.005284152925014496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.570276260375977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06479541212320328,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03587961196899414,
      "backward_entropy": 0.005288336012098525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.005876541137695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06482861936092377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03584582209587097,
      "backward_entropy": 0.005291739271746742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.383181571960449,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06486167758703232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035811984539031984,
      "backward_entropy": 0.005297298646635479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.535704612731934,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06489191949367523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03578395843505859,
      "backward_entropy": 0.005299394743310081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.508881092071533,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06492055952548981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03575900197029114,
      "backward_entropy": 0.005300614568922255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.83822250366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06494775414466858,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03573680520057678,
      "backward_entropy": 0.005301305817233192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.737733840942383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06497500836849213,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03571485579013824,
      "backward_entropy": 0.0052997801038954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.031155586242676,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0650026723742485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03569175004959106,
      "backward_entropy": 0.0053008538153436445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.711023330688477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06503176689147949,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035666531324386595,
      "backward_entropy": 0.005298716326554616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.6181001663208,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06506050378084183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03564269244670868,
      "backward_entropy": 0.005291900700993008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.573299407958984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06508921086788177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03561900854110718,
      "backward_entropy": 0.005284556084209018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.49177074432373,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06511781364679337,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03559573292732239,
      "backward_entropy": 0.005276040898429023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.429333686828613,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0651465430855751,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0355720043182373,
      "backward_entropy": 0.005269329994916916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.544892311096191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06517545133829117,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035547617077827456,
      "backward_entropy": 0.005265123314327664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.47839069366455,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06520691514015198,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03551801443099976,
      "backward_entropy": 0.005265297989050548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.23391056060791,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06523925065994263,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03548759818077087,
      "backward_entropy": 0.005260611987776226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1937150955200195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06527160853147507,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03545646369457245,
      "backward_entropy": 0.0052612705363167655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.175629615783691,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06530293077230453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03542722463607788,
      "backward_entropy": 0.00526229374938541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.067763328552246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06533622741699219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03539445400238037,
      "backward_entropy": 0.005263454384273953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.997925758361816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06536936014890671,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03536157608032227,
      "backward_entropy": 0.00526816522081693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.753389358520508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06540294736623764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035327720642089847,
      "backward_entropy": 0.005274591346581777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.901131629943848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06544036418199539,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03528733849525452,
      "backward_entropy": 0.005280105604065789,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.700399398803711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06547728925943375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03524866700172424,
      "backward_entropy": 0.005280492620335685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.727481842041016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06551486253738403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03520887196063995,
      "backward_entropy": 0.005281773706277211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.73646354675293,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06555196642875671,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.035170567035675046,
      "backward_entropy": 0.07697165012359619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.6548686027526855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06558803468942642,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03513436019420624,
      "backward_entropy": 0.00527572300699022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.575111389160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06562330573797226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03509966731071472,
      "backward_entropy": 0.005272366106510162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.089272499084473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06565798819065094,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.035065779089927675,
      "backward_entropy": 0.002629797077841229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.107306480407715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06569400429725647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03502987623214722,
      "backward_entropy": 0.005269163598616918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.411335468292236,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06573082506656647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034992295503616336,
      "backward_entropy": 0.00527009947432412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.73867130279541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06576666235923767,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034956687688827516,
      "backward_entropy": 0.005270290705892775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7145895957946777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06580381095409393,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03491886854171753,
      "backward_entropy": 0.0052712418966823155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.432661056518555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0658385381102562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03488549888134003,
      "backward_entropy": 0.005271539505985048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6466078758239746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0658719465136528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03485423922538757,
      "backward_entropy": 0.005273337165514628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.560468673706055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06590336561203003,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034826481342315675,
      "backward_entropy": 0.005274895992543962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.322868824005127,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06593595445156097,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03479638695716858,
      "backward_entropy": 0.005279124197032716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.992441177368164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06596727669239044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03476864695549011,
      "backward_entropy": 0.005282544427447849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2590532302856445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06599817425012589,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03474169373512268,
      "backward_entropy": 0.005285489062468211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.195284843444824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06602781265974045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03471730053424835,
      "backward_entropy": 0.005285585920015971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.191061019897461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06605652719736099,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034694325923919675,
      "backward_entropy": 0.005286358710792329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.765610694885254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06608641147613525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03466939330101013,
      "backward_entropy": 0.0052872175971666975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.024957656860352,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06611812114715576,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03464125394821167,
      "backward_entropy": 0.0052893997894393075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.67716646194458,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06615074723958969,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034611523151397705,
      "backward_entropy": 0.005292221903800964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.233514785766602,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06618276983499527,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03458291292190552,
      "backward_entropy": 0.005294766690995958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.979267597198486,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06621500849723816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034553781151771545,
      "backward_entropy": 0.005298348764578502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.867053985595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06624581664800644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03452747464179993,
      "backward_entropy": 0.0023824210382170146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3058929443359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06627596914768219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03450148105621338,
      "backward_entropy": 0.005306355655193329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.978065490722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06630432605743408,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034478724002838135,
      "backward_entropy": 0.005311773469050725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.8055009841918945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06633321940898895,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034454941749572754,
      "backward_entropy": 0.005317863490846422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.811305522918701,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0663611963391304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03443271517753601,
      "backward_entropy": 0.0053236546615759535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.275851249694824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06639014929533005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03440803289413452,
      "backward_entropy": 0.0053358301520347595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.758472919464111,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06641878187656403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03438403606414795,
      "backward_entropy": 0.00534644474585851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.156627178192139,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06644773483276367,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034359729290008544,
      "backward_entropy": 0.005355091972483529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.623865127563477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06647655367851257,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03433538675308227,
      "backward_entropy": 0.005365260773234897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.54758882522583,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0665057823061943,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03431035280227661,
      "backward_entropy": 0.005375231305758159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.0219221115112305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06653551012277603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03428422808647156,
      "backward_entropy": 0.00538693947924508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.948611736297607,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06656491756439209,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03425859808921814,
      "backward_entropy": 0.005398654689391454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.412641525268555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06659422814846039,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03423267602920532,
      "backward_entropy": 0.005413026445441776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5737197399139404,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06662362813949585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03420705795288086,
      "backward_entropy": 0.005423501133918762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.59117603302002,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06665059179067612,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03418567180633545,
      "backward_entropy": 0.0022649827102820077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.382138729095459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06667980551719666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03416092991828919,
      "backward_entropy": 0.005436554965045717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.365429401397705,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06670793890953064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034138023853302,
      "backward_entropy": 0.00544085767534044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.911980390548706,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06673495471477509,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034117239713668826,
      "backward_entropy": 0.005442317989137437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.816695213317871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06676049530506134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03409879207611084,
      "backward_entropy": 0.005443761332167519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4696898460388184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0667879730463028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034076982736587526,
      "backward_entropy": 0.005446496523088879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.433052659034729,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06681346893310547,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03405804932117462,
      "backward_entropy": 0.005451400660806232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.586014270782471,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0668373852968216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.034040987491607666,
      "backward_entropy": 0.005461103386349148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.791276454925537,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06686102598905563,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03402501940727234,
      "backward_entropy": 0.005464724782440398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.842036247253418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0668836459517479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03401044309139252,
      "backward_entropy": 0.005469777517848545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.07726526260376,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0669088065624237,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.033992046117782594,
      "backward_entropy": 0.005472085542149014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.346110820770264,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06693344563245773,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03397416472434998,
      "backward_entropy": 0.005476433369848464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.588476181030273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06695839762687683,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.033955180644989015,
      "backward_entropy": 0.0054846952358881635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6907854080200195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06698574870824814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03393247723579407,
      "backward_entropy": 0.00549191940161917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.248414039611816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06701168417930603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03391187191009522,
      "backward_entropy": 0.005499871654642953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4181166887283325,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06703756004571915,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03389122486114502,
      "backward_entropy": 0.07699796226289538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9137449264526367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0670611634850502,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03387470841407776,
      "backward_entropy": 0.005512195328871409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.1684651374816895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06708420813083649,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03385900855064392,
      "backward_entropy": 0.005515562991301219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.14004135131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.067107193171978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.033843708038330075,
      "backward_entropy": 0.00551578195558654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.542260646820068,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06713254004716873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03382492661476135,
      "backward_entropy": 0.005513673855198754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3167636394500732,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06715907156467438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.033804047107696536,
      "backward_entropy": 0.005513427158196767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3232251405715942,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06718366593122482,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03378596007823944,
      "backward_entropy": 0.005515417291058434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.958556652069092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06720637530088425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.033770793676376344,
      "backward_entropy": 0.0055175113181273145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.139759540557861,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06722915917634964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03375552892684937,
      "backward_entropy": 0.005519219570689731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4625558853149414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0672525018453598,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03373958468437195,
      "backward_entropy": 0.005519017577171326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.660099983215332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06727492064237595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03372461199760437,
      "backward_entropy": 0.005522249887386958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.634442090988159,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06729685515165329,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.033710372447967527,
      "backward_entropy": 0.005525265716844135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.601492404937744,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06731835752725601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03369678854942322,
      "backward_entropy": 0.0055279429588052965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6001548767089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06733953207731247,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0336835503578186,
      "backward_entropy": 0.00553142237994406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.332042694091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06736020743846893,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03367131352424622,
      "backward_entropy": 0.005532561077011956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.352851152420044,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0673835352063179,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03365488648414612,
      "backward_entropy": 0.0055341530177328326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.178067207336426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06740598380565643,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03363928198814392,
      "backward_entropy": 0.005539970265494453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.891818523406982,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0674310103058815,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03361944556236267,
      "backward_entropy": 0.005547661334276199,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 7.342180162668228,
    "avg_log_Z": -0.06607231207191944,
    "success_rate": 1.0,
    "avg_reward": 85.8,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.02,
      "1": 0.03,
      "2": 0.95
    },
    "avg_forward_entropy": 0.034729025602340695,
    "avg_backward_entropy": 0.006705971643742588,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}