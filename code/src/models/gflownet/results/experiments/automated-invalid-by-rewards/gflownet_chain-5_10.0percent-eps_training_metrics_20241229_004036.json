{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 0.45454545454545453
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13862848281860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.21873474121094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18285759290059408,
      "backward_entropy": 0.13859095573425292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.81382751464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285415569941202,
      "backward_entropy": 0.13862884044647217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.78753662109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00019883509958162904,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18285053968429565,
      "backward_entropy": 0.13861751556396484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.30809020996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0002974918461404741,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18284672498703003,
      "backward_entropy": 0.13862931728363037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.00877380371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0003962336922995746,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18284289042154947,
      "backward_entropy": 0.13860552310943602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.7082061767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.000495718908496201,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18283881743748984,
      "backward_entropy": 0.1386096715927124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.8597869873047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0005948232719674706,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1828346053759257,
      "backward_entropy": 0.13861145973205566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.6403350830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0006950717070139945,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18283013502756754,
      "backward_entropy": 0.13862900733947753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.0804901123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0007964021642692387,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18282542626063028,
      "backward_entropy": 0.13859968185424804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.95584106445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0008962485590018332,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18282057841618857,
      "backward_entropy": 0.13859596252441406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.182861328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0009962975746020675,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18281551202138266,
      "backward_entropy": 0.13862695693969726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.09519958496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.001097245840355754,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18281020720799765,
      "backward_entropy": 0.1385880708694458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.86708068847656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0011969894403591752,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18280474344889322,
      "backward_entropy": 0.13862394094467162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.3213653564453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0012969046365469694,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18279900153477988,
      "backward_entropy": 0.13862525224685668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.03819274902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013990546576678753,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18279290199279785,
      "backward_entropy": 0.1385751724243164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.22866821289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0015025802422314882,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1827864646911621,
      "backward_entropy": 0.138627290725708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.29530334472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0016052244463935494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18277986844380698,
      "backward_entropy": 0.1386149525642395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.50100708007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0017073187045753002,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1827730337778727,
      "backward_entropy": 0.13856101036071777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.23495483398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0018098452128469944,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1827659805615743,
      "backward_entropy": 0.13860843181610108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.77944946289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0019118107156828046,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18275868892669678,
      "backward_entropy": 0.1386047124862671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.40182495117188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0020117508247494698,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18275129795074463,
      "backward_entropy": 0.1385455012321472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.88746643066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0021124689374119043,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18274366855621338,
      "backward_entropy": 0.13854007720947265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.4757080078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0022151635494083166,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18273556232452393,
      "backward_entropy": 0.1385917067527771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.302490234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0023143989965319633,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18272745609283447,
      "backward_entropy": 0.13858678340911865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.28994750976562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002414448419585824,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18271907170613608,
      "backward_entropy": 0.13862886428833007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.78602600097656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002512945095077157,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1827105482419332,
      "backward_entropy": 0.13862853050231932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.68576049804688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0026124746073037386,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18270172675450644,
      "backward_entropy": 0.13862814903259277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.96414184570312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00271037220954895,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1826927661895752,
      "backward_entropy": 0.13850507736206055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.28114318847656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002808237913995981,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18268356720606485,
      "backward_entropy": 0.1386272668838501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.0535125732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0029050405137240887,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18267414967219034,
      "backward_entropy": 0.1385509967803955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.36880493164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0030004067812114954,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18266459306081137,
      "backward_entropy": 0.13854389190673827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.37197875976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0030984249897301197,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18265469868977866,
      "backward_entropy": 0.13847817182540895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.29544067382812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0032007067929953337,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18264424800872803,
      "backward_entropy": 0.13862533569335939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.3314971923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003304581856355071,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18263347943623862,
      "backward_entropy": 0.13852168321609498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.933837890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0034076659940183163,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18262247244517008,
      "backward_entropy": 0.13851382732391357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.89892578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0035110199823975563,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18261122703552246,
      "backward_entropy": 0.13845096826553344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.0320281982422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003614602144807577,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18259974320729574,
      "backward_entropy": 0.1384438991546631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.384033203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0037169267889112234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18258814016977945,
      "backward_entropy": 0.1384366273880005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.4473114013672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0038219999987632036,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18257604042689005,
      "backward_entropy": 0.13862307071685792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.74903869628906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0039273593574762344,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18256362279256186,
      "backward_entropy": 0.13862278461456298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.10719299316406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004034932237118483,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18255076805750528,
      "backward_entropy": 0.13862257003784179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.13247680664062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004141119308769703,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18253771464029947,
      "backward_entropy": 0.13840749263763427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.0413360595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00424695061519742,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1825244426727295,
      "backward_entropy": 0.13839980363845825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.98751831054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0043515716679394245,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18251097202301025,
      "backward_entropy": 0.13842875957489015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.9759063720703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004453055094927549,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18249754110972086,
      "backward_entropy": 0.1383833885192871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.64569854736328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004553815349936485,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1824838916460673,
      "backward_entropy": 0.13862085342407227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.1077117919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00465147290378809,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18247012297312418,
      "backward_entropy": 0.13839478492736818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.52569580078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004747236147522926,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1824562152226766,
      "backward_entropy": 0.13838266134262084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.0204315185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004841180052608252,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18244234720865884,
      "backward_entropy": 0.13834609985351562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.02017211914062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004937393590807915,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18242804209391275,
      "backward_entropy": 0.13861883878707887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.18202209472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0050318618305027485,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18241365750630698,
      "backward_entropy": 0.1383427619934082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.8802490234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005129897501319647,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18239867687225342,
      "backward_entropy": 0.13832881450653076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.677978515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005228794179856777,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1823833187421163,
      "backward_entropy": 0.1383068561553955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.45443725585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005322976037859917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18236804008483887,
      "backward_entropy": 0.13829972743988037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.09866333007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005415724590420723,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18235262235005698,
      "backward_entropy": 0.13828481435775758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.74049377441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005508610978722572,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18233690659205118,
      "backward_entropy": 0.1382732629776001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.59402465820312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0056008449755609035,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18232103188832602,
      "backward_entropy": 0.1386152386665344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.2177734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00569823756814003,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18230438232421875,
      "backward_entropy": 0.13823416233062744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.40969848632812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0057967049069702625,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18228729565938315,
      "backward_entropy": 0.13823792934417725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.1623077392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005894524045288563,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1822699507077535,
      "backward_entropy": 0.13819921016693115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.95777130126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0059902239590883255,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18225252628326416,
      "backward_entropy": 0.13821253776550294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.51417541503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006077699828892946,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1822356383005778,
      "backward_entropy": 0.13819797039031984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.82931518554688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006167253945022821,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18221823374430338,
      "backward_entropy": 0.1381387233734131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.88450622558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006257263012230396,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18220035235087076,
      "backward_entropy": 0.1381683588027954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.4022674560547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0063489628955721855,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1821818749109904,
      "backward_entropy": 0.13861082792282103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.36592102050781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0064422995783388615,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18216252326965332,
      "backward_entropy": 0.13807451725006104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.5413055419922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0065313782542943954,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18214285373687744,
      "backward_entropy": 0.1381213903427124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.89913177490234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0066217523999512196,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18212254842122397,
      "backward_entropy": 0.13810455799102783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.615478515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006705829873681068,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18210254112879434,
      "backward_entropy": 0.13808618783950805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.46426391601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006790990941226482,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18208187818527222,
      "backward_entropy": 0.13806748390197754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.38453674316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0068768602795898914,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1820606787999471,
      "backward_entropy": 0.13804832696914673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.5044403076172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006957006175071001,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18203977743784586,
      "backward_entropy": 0.138604736328125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.0846710205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007038699928671122,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18201820055643717,
      "backward_entropy": 0.13800532817840577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.2083511352539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007120848633348942,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18199610710144043,
      "backward_entropy": 0.1379828929901123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.90191650390625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007201300468295813,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1819737950960795,
      "backward_entropy": 0.13860114812850952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.58901977539062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007282089442014694,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18195088704427084,
      "backward_entropy": 0.138599956035614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.11536407470703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007364999037235975,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18192692597707114,
      "backward_entropy": 0.13791108131408691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.96519470214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0074461642652750015,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18190266688664755,
      "backward_entropy": 0.137884259223938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.7104949951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007525477092713118,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.181878129641215,
      "backward_entropy": 0.1376955509185791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.45870208740234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007607800420373678,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18185269832611084,
      "backward_entropy": 0.13859494924545288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.71546936035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007688318379223347,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18182700872421265,
      "backward_entropy": 0.13779838085174562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.69541931152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007770658936351538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1818005641301473,
      "backward_entropy": 0.13758424520492554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.6063690185547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007856014184653759,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18177314599355063,
      "backward_entropy": 0.1385906457901001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.9187469482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007947608828544617,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18174423774083456,
      "backward_entropy": 0.13770968914031984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.7153549194336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00804076250642538,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18171455462773642,
      "backward_entropy": 0.1376805782318115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.07284545898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00812940951436758,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18168512980143228,
      "backward_entropy": 0.13764926195144653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.01747131347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008223123848438263,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1816542943318685,
      "backward_entropy": 0.13761870861053466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.61170196533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008317365311086178,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18162288268407187,
      "backward_entropy": 0.13735561370849608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.0419921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00840842816978693,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18159147103627524,
      "backward_entropy": 0.13731274604797364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.3047637939453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008501945994794369,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18155908584594727,
      "backward_entropy": 0.1375211000442505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.53657531738281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00859723798930645,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18152586619059244,
      "backward_entropy": 0.13748760223388673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.48397064208984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008686313405632973,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18149320284525552,
      "backward_entropy": 0.137450909614563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.78929138183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008771263994276524,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18146081765492758,
      "backward_entropy": 0.13741166591644288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.01229858398438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008850539103150368,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18142950534820557,
      "backward_entropy": 0.13857946395874024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.5688018798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008931489661335945,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18139684200286865,
      "backward_entropy": 0.13702731132507323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.8316192626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009012672118842602,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18136338392893472,
      "backward_entropy": 0.13697357177734376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.75569152832031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009097756817936897,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1813285748163859,
      "backward_entropy": 0.13857345581054686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.20795440673828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00917897466570139,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18129390478134155,
      "backward_entropy": 0.13857132196426392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.57666015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009255040436983109,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18125967184702554,
      "backward_entropy": 0.1368089199066162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.6663360595703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009338349103927612,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18122337261835733,
      "backward_entropy": 0.13856655359268188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.19180297851562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009421218186616898,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18118651707967123,
      "backward_entropy": 0.1370531439781189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.14488220214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009506992995738983,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1811483899752299,
      "backward_entropy": 0.13663504123687745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.7413558959961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009592236950993538,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18110974629720053,
      "backward_entropy": 0.13856072425842286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.166259765625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009675704874098301,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18107088406880698,
      "backward_entropy": 0.1385585308074951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.87648010253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00976234395056963,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1810306708017985,
      "backward_entropy": 0.13644704818725586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.22035217285156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009848089888691902,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18098998069763184,
      "backward_entropy": 0.13855447769165039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.96109008789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009936063550412655,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18094823757807413,
      "backward_entropy": 0.13674509525299072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.81044006347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010025051422417164,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18090597788492838,
      "backward_entropy": 0.13624155521392822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.25465393066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010116365738213062,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1808625857035319,
      "backward_entropy": 0.13663535118103026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.2057342529297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010206473991274834,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18081887563069662,
      "backward_entropy": 0.13854689598083497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.81670379638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010295509360730648,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18077439069747925,
      "backward_entropy": 0.13601310253143312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.07837677001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010380469262599945,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18073026339213052,
      "backward_entropy": 0.13592748641967772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.38829040527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010464182123541832,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18068567911783853,
      "backward_entropy": 0.13639031648635863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.55892181396484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010553251951932907,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18063892920811972,
      "backward_entropy": 0.13632738590240479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.239501953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010637937113642693,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18059271574020386,
      "backward_entropy": 0.13567194938659669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.34707641601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010720954276621342,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.180546243985494,
      "backward_entropy": 0.13619050979614258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.29871368408203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010808722116053104,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18049776554107666,
      "backward_entropy": 0.13549150228500367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.65977478027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010893014259636402,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18044954538345337,
      "backward_entropy": 0.13539788722991944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.91807556152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010978342965245247,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18040027221043906,
      "backward_entropy": 0.13597630262374877,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.5471954345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011061650700867176,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18035084009170532,
      "backward_entropy": 0.1351994752883911,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.4691162109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011146122589707375,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18030027548472086,
      "backward_entropy": 0.13582198619842528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.52701568603516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011233393102884293,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18024806181589761,
      "backward_entropy": 0.1349881649017334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.51634216308594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011317106895148754,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18019616603851318,
      "backward_entropy": 0.13850934505462648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.68599700927734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01139577105641365,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18014504512151083,
      "backward_entropy": 0.1355752468109131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.91307067871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01147312019020319,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18009358644485474,
      "backward_entropy": 0.13548556566238404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.8863983154297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011548832058906555,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18004179000854492,
      "backward_entropy": 0.13849411010742188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.96920013427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011628802865743637,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1799878478050232,
      "backward_entropy": 0.13530094623565675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.65215301513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011707198806107044,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1799336075782776,
      "backward_entropy": 0.13426769971847535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.41337203979492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011784656904637814,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1798789103825887,
      "backward_entropy": 0.1341375946998596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.1533432006836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011853879317641258,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1798262596130371,
      "backward_entropy": 0.1339990496635437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.62247085571289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01192027609795332,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17977388699849448,
      "backward_entropy": 0.13385199308395385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.92372131347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011979004368185997,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17972340186436972,
      "backward_entropy": 0.1347801923751831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.16314697265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01205034926533699,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17966785033543906,
      "backward_entropy": 0.133551287651062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.47389221191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012121940031647682,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17961150407791138,
      "backward_entropy": 0.13340835571289061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.17129516601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012192511931061745,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17955474058787027,
      "backward_entropy": 0.1384269952774048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.37102508544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012267671525478363,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17949525515238443,
      "backward_entropy": 0.13311407566070557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.8732147216797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012341471388936043,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1794354518254598,
      "backward_entropy": 0.1384108543395996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.48416137695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012419244274497032,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1793732245763143,
      "backward_entropy": 0.13281092643737794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.68858337402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012498931027948856,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1793092886606852,
      "backward_entropy": 0.1340203046798706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.518798828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012578831985592842,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1792443792025248,
      "backward_entropy": 0.13390812873840333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.15328979492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012656483799219131,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17917962869008383,
      "backward_entropy": 0.13234615325927734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.26429748535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012734738178551197,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17911372582117716,
      "backward_entropy": 0.13218857049942018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.37071228027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012813856825232506,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.179046630859375,
      "backward_entropy": 0.13203125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.3849639892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012897231616079807,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1789769728978475,
      "backward_entropy": 0.13344323635101318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.39299774169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012980494648218155,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17890634139378866,
      "backward_entropy": 0.13172028064727784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.8117446899414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013062599115073681,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17883535226186117,
      "backward_entropy": 0.1332033634185791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.31095123291016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013142730109393597,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17876404523849487,
      "backward_entropy": 0.13307671546936034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.25707244873047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013219046406447887,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17869303623835245,
      "backward_entropy": 0.13834511041641234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.2480926513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013291933573782444,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1786213517189026,
      "backward_entropy": 0.1328059673309326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.4784393310547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013366158120334148,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17854775985081991,
      "backward_entropy": 0.13832805156707764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.08837890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013442516326904297,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17847174406051636,
      "backward_entropy": 0.13063220977783202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.449462890625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013515465892851353,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17839582761128744,
      "backward_entropy": 0.1383113980293274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.75846099853516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013587426394224167,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1783190369606018,
      "backward_entropy": 0.13223358392715454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.54489135742188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01365914847701788,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17824129263559976,
      "backward_entropy": 0.13829247951507567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.42236328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013731769286096096,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17816181977589926,
      "backward_entropy": 0.13192853927612305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.20419311523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013800101354718208,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17808322111765543,
      "backward_entropy": 0.13176480531692505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.52561950683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013873765245079994,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17800098657608032,
      "backward_entropy": 0.1293494701385498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.841552734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013952046632766724,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17791531483332315,
      "backward_entropy": 0.12913625240325927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.56605529785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014032931998372078,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.177827258904775,
      "backward_entropy": 0.12892448902130127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.0438232421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014115848578512669,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1777369181315104,
      "backward_entropy": 0.13113584518432617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.57191467285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014204234816133976,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17764262358347574,
      "backward_entropy": 0.13098559379577637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.54702758789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014295543543994427,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17754550774892172,
      "backward_entropy": 0.1282944679260254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.18592834472656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014386731199920177,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1774474779764811,
      "backward_entropy": 0.1382291793823242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.3286361694336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014481005258858204,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17734670639038086,
      "backward_entropy": 0.13052651882171631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.9990997314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014568735845386982,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17724802096684775,
      "backward_entropy": 0.13035936355590821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.95823669433594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014659922569990158,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.177146315574646,
      "backward_entropy": 0.13821666240692138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.48747253417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014752080664038658,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17704315980275473,
      "backward_entropy": 0.1272168517112732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.57852935791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014839984476566315,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1769425868988037,
      "backward_entropy": 0.126983380317688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.3698272705078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014920370653271675,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1768449346224467,
      "backward_entropy": 0.13819580078125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.3133544921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015000849962234497,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17674525578816733,
      "backward_entropy": 0.12647862434387208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.20352172851562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015088298358023167,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17664007345835367,
      "backward_entropy": 0.13817853927612306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.4614028930664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015175120905041695,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17653338114420572,
      "backward_entropy": 0.12596776485443115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.08412170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015256325714290142,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17642861604690552,
      "backward_entropy": 0.12889227867126465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.94786071777344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015338337048888206,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1763222614924113,
      "backward_entropy": 0.13814980983734132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.0523681640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015420307405292988,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17621425787607828,
      "backward_entropy": 0.12514781951904297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.28172302246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015506415627896786,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1761025587717692,
      "backward_entropy": 0.12487568855285644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.14332580566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01558779925107956,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17599302530288696,
      "backward_entropy": 0.12459585666656495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.91162109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015669848769903183,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17588182290395102,
      "backward_entropy": 0.1381068229675293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.7748260498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015755603089928627,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1757666269938151,
      "backward_entropy": 0.12403857707977295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.34697723388672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015842987224459648,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1756487488746643,
      "backward_entropy": 0.1274164915084839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.06169891357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015924587845802307,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17553309599558511,
      "backward_entropy": 0.12718672752380372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.47744750976562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016002435237169266,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17541809876759848,
      "backward_entropy": 0.13806207180023194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.628173828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01608104258775711,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1753011147181193,
      "backward_entropy": 0.12670187950134276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.91940307617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01616237498819828,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17518101135889688,
      "backward_entropy": 0.12645903825759888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.45120239257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01624428853392601,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17505915959676108,
      "backward_entropy": 0.12219523191452027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.0886688232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016326889395713806,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1749355991681417,
      "backward_entropy": 0.12187770605087281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.48257446289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01641152612864971,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17480899890263876,
      "backward_entropy": 0.12572147846221923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.66917419433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016498608514666557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17467967669169107,
      "backward_entropy": 0.12124669551849365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.88079833984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016591085121035576,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1745451291402181,
      "backward_entropy": 0.13798205852508544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.54098510742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016681157052516937,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17441083987553915,
      "backward_entropy": 0.12499672174453735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.1415252685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016775857657194138,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1742713451385498,
      "backward_entropy": 0.1247552752494812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.54499053955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01687026210129261,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17413107554117838,
      "backward_entropy": 0.12451047897338867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.6728515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01695955917239189,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1739924152692159,
      "backward_entropy": 0.12424740791320801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.4845733642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017054162919521332,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17384850978851318,
      "backward_entropy": 0.11934915781021119,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.69808197021484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017149874940514565,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17370245854059854,
      "backward_entropy": 0.12373976707458496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.4808807373047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01724272221326828,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17355668544769287,
      "backward_entropy": 0.12347394227981567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.1112060546875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017334720119833946,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17340985933939615,
      "backward_entropy": 0.13792057037353517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.19844055175781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017429349943995476,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1732589602470398,
      "backward_entropy": 0.1229284167289734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.0106964111328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017515821382403374,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1731161673863729,
      "backward_entropy": 0.13789676427841185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.10321807861328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017603818327188492,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17297112941741943,
      "backward_entropy": 0.13788197040557862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.07955932617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017687704414129257,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1728276014328003,
      "backward_entropy": 0.12201881408691406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.48519897460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01777692139148712,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17267791430155435,
      "backward_entropy": 0.11643878221511841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.7307891845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017865844070911407,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17252697547276816,
      "backward_entropy": 0.1214057207107544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.11141204833984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01796000450849533,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.172369917233785,
      "backward_entropy": 0.12110793590545654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.8591537475586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018051808699965477,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17221341530481973,
      "backward_entropy": 0.11527820825576782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.13725280761719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018137400969862938,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17206033070882162,
      "backward_entropy": 0.11486510038375855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.6955795288086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018219780176877975,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17190887530644736,
      "backward_entropy": 0.12012584209442138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.42916107177734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018296897411346436,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17176022132237753,
      "backward_entropy": 0.11400028467178344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.76991271972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01837058551609516,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1716123421986898,
      "backward_entropy": 0.11940100193023681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.22705078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01845671609044075,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17145244280497232,
      "backward_entropy": 0.11312012672424317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.77580261230469,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01854102872312069,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17129274209340414,
      "backward_entropy": 0.1376858353614807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.22151184082031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018623871728777885,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.171133021513621,
      "backward_entropy": 0.11837222576141357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.36727905273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018704429268836975,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1709752082824707,
      "backward_entropy": 0.1376492738723755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.78032684326172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018791167065501213,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17081032196680704,
      "backward_entropy": 0.11142394542694092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.7667694091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018876515328884125,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17064515749613443,
      "backward_entropy": 0.11100692749023437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.91728973388672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018965288996696472,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17047448952992758,
      "backward_entropy": 0.11698193550109863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.4769744873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019046632573008537,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1703095237414042,
      "backward_entropy": 0.11660882234573364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.0943489074707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019128726795315742,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17014235258102417,
      "backward_entropy": 0.10970051288604736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.57443237304688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01920275390148163,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16998271147410074,
      "backward_entropy": 0.1158284068107605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.74131774902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019278598949313164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16982016960779825,
      "backward_entropy": 0.11542978286743164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.43458557128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019359055906534195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1696510116259257,
      "backward_entropy": 0.10832343101501465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.23683166503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01943754218518734,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16948360204696655,
      "backward_entropy": 0.11465117931365967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.92157745361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019517425447702408,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1693135897318522,
      "backward_entropy": 0.11426199674606323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.08648681640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01959511637687683,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1691451072692871,
      "backward_entropy": 0.10697832107543945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.20938110351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019672738388180733,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16897592941919962,
      "backward_entropy": 0.10653034448623658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.56971740722656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019750839099287987,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1688034733136495,
      "backward_entropy": 0.1374314546585083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.64027404785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019833549857139587,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16862420241038004,
      "backward_entropy": 0.1126660943031311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.1869659423828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019916841760277748,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16844276587168375,
      "backward_entropy": 0.13741307258605956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.87743377685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020000817254185677,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16825886567433676,
      "backward_entropy": 0.10467537641525268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.5823516845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020083993673324585,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1680746873219808,
      "backward_entropy": 0.1042067289352417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.92976379394531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02016759291291237,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16788792610168457,
      "backward_entropy": 0.10372939109802246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.72216033935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020246360450983047,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.167705237865448,
      "backward_entropy": 0.11060953140258789,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.91146850585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02032330259680748,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16752437750498453,
      "backward_entropy": 0.10272234678268433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.83949279785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02040194161236286,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16734031836191812,
      "backward_entropy": 0.10972428321838379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.06842041015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020486922934651375,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16714719931284586,
      "backward_entropy": 0.1093027114868164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.81859588623047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02057405561208725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16694982846577963,
      "backward_entropy": 0.10126465559005737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.09308624267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020656365901231766,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1667573849360148,
      "backward_entropy": 0.10076082944869995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.1494369506836,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020734649151563644,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16656955083211264,
      "backward_entropy": 0.13728611469268798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.5997543334961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02080678753554821,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16638763745625815,
      "backward_entropy": 0.10748543739318847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.89962768554688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020876087248325348,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1662090023358663,
      "backward_entropy": 0.13722928762435913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.43511962890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02095322124660015,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16601953903834024,
      "backward_entropy": 0.09864647388458252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.86968231201172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021031374111771584,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1658264398574829,
      "backward_entropy": 0.13719465732574462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.43782806396484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021105371415615082,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16563702623049417,
      "backward_entropy": 0.09757851362228394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.10719299316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021177249029278755,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1654483675956726,
      "backward_entropy": 0.10506999492645264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.2450408935547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021254463121294975,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16525126496950784,
      "backward_entropy": 0.09646760225296021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.10662841796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02133631892502308,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.165046493212382,
      "backward_entropy": 0.09592781662940979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.98070526123047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021417200565338135,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16484159231185913,
      "backward_entropy": 0.09537360668182374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.03899383544922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02149909734725952,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16463427742322287,
      "backward_entropy": 0.13708267211914063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.08980560302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021582799032330513,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1644239624341329,
      "backward_entropy": 0.09427650570869446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.45281219482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021667547523975372,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1642105976740519,
      "backward_entropy": 0.10221216678619385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.72027587890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021750641986727715,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16399668653806052,
      "backward_entropy": 0.0931879699230194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.04903411865234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021832561120390892,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16378241777420044,
      "backward_entropy": 0.10123505592346191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.951904296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021911846473813057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16357019543647766,
      "backward_entropy": 0.1007237195968628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.299560546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02199195884168148,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1633546749750773,
      "backward_entropy": 0.09143953323364258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.39321899414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022068044170737267,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16314415136973062,
      "backward_entropy": 0.09966871738433838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.12132263183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022147763520479202,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1629276474316915,
      "backward_entropy": 0.09024670124053955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.50462341308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02223259210586548,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16270363330841064,
      "backward_entropy": 0.0986465871334076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.16175842285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022321710363030434,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16247283418973288,
      "backward_entropy": 0.09816466569900513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.1133575439453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022412989288568497,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1622382402420044,
      "backward_entropy": 0.08860903978347778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.40016174316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022507892921566963,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16199779510498047,
      "backward_entropy": 0.09722952842712403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.62047576904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02260281890630722,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16175705194473267,
      "backward_entropy": 0.09676216840744019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.80902862548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02269706316292286,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16151545445124307,
      "backward_entropy": 0.09627768993377686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.2993621826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022789284586906433,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16127576430638632,
      "backward_entropy": 0.0957763135433197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.53950500488281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02288505807518959,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1610301931699117,
      "backward_entropy": 0.08586630821228028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.53995513916016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022980375215411186,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16078408559163412,
      "backward_entropy": 0.08530260324478149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.1441879272461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02307000383734703,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16054527958234152,
      "backward_entropy": 0.08470790386199951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.01832580566406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023154666647315025,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1603131095568339,
      "backward_entropy": 0.1368553638458252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.590110778808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023236611858010292,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1600841482480367,
      "backward_entropy": 0.08346431255340576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.23701477050781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02331336960196495,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15986289580663046,
      "backward_entropy": 0.13679325580596924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.29882049560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023389678448438644,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15963908036549887,
      "backward_entropy": 0.08218388557434082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.62611389160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023464879021048546,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1594169537226359,
      "backward_entropy": 0.08154879808425904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.9280776977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023540543392300606,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15919329722722372,
      "backward_entropy": 0.0809219241142273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.12992858886719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023612454533576965,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15897014737129211,
      "backward_entropy": 0.08027347922325134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.96530151367188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023679684847593307,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15875277916590372,
      "backward_entropy": 0.07960790395736694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.5322723388672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02375168167054653,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15852638085683188,
      "backward_entropy": 0.13656692504882811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.52652740478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023828107863664627,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1582921544710795,
      "backward_entropy": 0.07836555242538452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.31499481201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023902952671051025,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1580598553021749,
      "backward_entropy": 0.0777522087097168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.78273010253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023979701101779938,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1578227678934733,
      "backward_entropy": 0.07713813185691834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.7779541015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024061571806669235,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1575756867726644,
      "backward_entropy": 0.13647537231445311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.4278793334961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024146905168890953,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15732319156328836,
      "backward_entropy": 0.08615806698799133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.27320098876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02422954887151718,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1570734977722168,
      "backward_entropy": 0.0855968713760376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.08362579345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024309787899255753,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15682613849639893,
      "backward_entropy": 0.07474614381790161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.972190856933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0243857279419899,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1565832495689392,
      "backward_entropy": 0.13641049861907958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.86439514160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024456210434436798,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15634759267171225,
      "backward_entropy": 0.0837908148765564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.26959228515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02452397160232067,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1561154822508494,
      "backward_entropy": 0.0727340817451477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.56668090820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02458958514034748,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15588646133740744,
      "backward_entropy": 0.07205132246017457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.57144165039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0246597807854414,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15564765532811484,
      "backward_entropy": 0.08189224004745484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.78353118896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02473607286810875,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15539793173472086,
      "backward_entropy": 0.08130462169647217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.6766128540039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024808872491121292,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15515291690826416,
      "backward_entropy": 0.07010743618011475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.8347625732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024882148951292038,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15490608414014181,
      "backward_entropy": 0.06945526599884033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.21395874023438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024959830567240715,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1546522080898285,
      "backward_entropy": 0.07950880527496337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.54432678222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025043154135346413,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15438932180404663,
      "backward_entropy": 0.07896320819854737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.31072998046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02513319067656994,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15411599477132162,
      "backward_entropy": 0.06772035956382752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.73563766479492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02522098459303379,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1538483202457428,
      "backward_entropy": 0.07793459296226501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.13676452636719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025302933529019356,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15359145402908325,
      "backward_entropy": 0.07737538814544678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.20516967773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02538478560745716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15333451827367148,
      "backward_entropy": 0.06602439880371094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.18034362792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025471746921539307,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15306875109672546,
      "backward_entropy": 0.06546969413757324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.95417785644531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02555786818265915,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1528035302956899,
      "backward_entropy": 0.07572036385536193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.16680145263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02564128302037716,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15254129966100058,
      "backward_entropy": 0.07514910697937012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.20072174072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0257226824760437,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1522823969523112,
      "backward_entropy": 0.06369893550872803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.59412384033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025800174102187157,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15202889839808145,
      "backward_entropy": 0.06306777596473694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.4628677368164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025879686698317528,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1517715851465861,
      "backward_entropy": 0.07336188554763794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.05617904663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02595776878297329,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15151719252268472,
      "backward_entropy": 0.07275747060775757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.64864349365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026028886437416077,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15127456188201904,
      "backward_entropy": 0.07211451530456543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.559783935546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026101035997271538,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15102970600128174,
      "backward_entropy": 0.07148233652114869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.66080474853516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02616899088025093,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1507927974065145,
      "backward_entropy": 0.0598623514175415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.49806594848633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026238087564706802,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15055246154467264,
      "backward_entropy": 0.07019867300987244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.79496765136719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026303213089704514,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15031940738360086,
      "backward_entropy": 0.13592708110809326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.00216674804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026372035965323448,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15008030335108438,
      "backward_entropy": 0.0579306960105896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.52323913574219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02644575759768486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14983266592025757,
      "backward_entropy": 0.05733137130737305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.34773254394531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0265185609459877,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14958658814430237,
      "backward_entropy": 0.05672774314880371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.1254119873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026588721200823784,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14934494098027548,
      "backward_entropy": 0.06710119247436523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.21835327148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026665624231100082,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14909271399180093,
      "backward_entropy": 0.05554302334785462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.2460823059082,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026748323813080788,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14883089065551758,
      "backward_entropy": 0.13581944704055787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.24951171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026826132088899612,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14857948819796243,
      "backward_entropy": 0.0654178261756897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.7043228149414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026908252388238907,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14832190672556558,
      "backward_entropy": 0.0538779616355896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.0447769165039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026990290731191635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14806391795476279,
      "backward_entropy": 0.05332405567169189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.28385162353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02707432396709919,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14780322710673013,
      "backward_entropy": 0.052780479192733765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.74616241455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027158120647072792,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14754254619280496,
      "backward_entropy": 0.05222194790840149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.68408203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027243642136454582,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14727930227915445,
      "backward_entropy": 0.06275676488876343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.05170440673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027332985773682594,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14701215426127115,
      "backward_entropy": 0.051153695583343504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.02919006347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027420422062277794,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14674997329711914,
      "backward_entropy": 0.13589664697647094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.32408142089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027509251609444618,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14648553729057312,
      "backward_entropy": 0.0612694263458252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.38987731933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027599703520536423,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14621996879577637,
      "backward_entropy": 0.04959840178489685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.44759368896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02769138477742672,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14595305919647217,
      "backward_entropy": 0.04909589886665344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.7198486328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027782408520579338,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14568785826365152,
      "backward_entropy": 0.04858752191066742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.35321044921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027876267209649086,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14541844526926676,
      "backward_entropy": 0.05935611128807068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.45696258544922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02796749770641327,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1451540986696879,
      "backward_entropy": 0.05887041091918945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.57479095458984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02806132659316063,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1448851227760315,
      "backward_entropy": 0.04707788527011871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.6061019897461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028155745938420296,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1446149249871572,
      "backward_entropy": 0.0465699315071106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.40584564208984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028247646987438202,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1443504492441813,
      "backward_entropy": 0.04605490565299988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.25227355957031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028337301686406136,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14409097035725912,
      "backward_entropy": 0.05692766904830933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.13704681396484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02842847630381584,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14383027950922647,
      "backward_entropy": 0.04503549039363861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.46797180175781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028520913794636726,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14356855551401773,
      "backward_entropy": 0.05596805214881897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.113346099853516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02860921621322632,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14331438144048056,
      "backward_entropy": 0.04403737485408783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.78777313232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028686976060271263,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1430798868338267,
      "backward_entropy": 0.04348784685134888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.394065856933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028767067939043045,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14284064372380575,
      "backward_entropy": 0.04294564723968506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.41157913208008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028841443359851837,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14261426528294882,
      "backward_entropy": 0.05379247069358826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.042572021484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028910581022500992,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14239904284477234,
      "backward_entropy": 0.1360584616661072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.66324996948242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028974343091249466,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1421919067700704,
      "backward_entropy": 0.041279032826423645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.6598129272461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029035659506917,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14199044307072958,
      "backward_entropy": 0.05198380947113037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.15757751464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029101114720106125,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14178040623664856,
      "backward_entropy": 0.04016891121864319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.864013671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029172614216804504,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14156150817871094,
      "backward_entropy": 0.13588497638702393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.22870635986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02924758940935135,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14133707682291666,
      "backward_entropy": 0.03918178677558899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.9248275756836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029327113181352615,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1411044398943583,
      "backward_entropy": 0.049859476089477536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.06092834472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02940584160387516,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1408737301826477,
      "backward_entropy": 0.03825732469558716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.81834411621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029483553022146225,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14064393440882364,
      "backward_entropy": 0.03778505325317383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.97323226928711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029559491202235222,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14042005936304727,
      "backward_entropy": 0.03732512891292572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.54790496826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029629670083522797,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14020532369613647,
      "backward_entropy": 0.04786621928215027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.36199951171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029700741171836853,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13999231656392416,
      "backward_entropy": 0.03637841641902924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.324954986572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029770780354738235,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1397823691368103,
      "backward_entropy": 0.04688510298728943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.97279357910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02983606420457363,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1395810047785441,
      "backward_entropy": 0.04638186693191528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.21978759765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029901055619120598,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1393826206525167,
      "backward_entropy": 0.0350229799747467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.2326889038086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029965383931994438,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1391857167085012,
      "backward_entropy": 0.03458358645439148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.01786804199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030032534152269363,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13898414373397827,
      "backward_entropy": 0.0341619074344635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.00394439697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03009875677525997,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13878411054611206,
      "backward_entropy": 0.044476255774497986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.44063568115234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030170906335115433,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13857348759969076,
      "backward_entropy": 0.044042402505874635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.70301055908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030241990461945534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1383663217226664,
      "backward_entropy": 0.032954704761505124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.40098571777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030314991250634193,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13815486431121826,
      "backward_entropy": 0.032571914792060855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.81195831298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03038821369409561,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13794302940368652,
      "backward_entropy": 0.03219165802001953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.6043701171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0304605383425951,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13773528734842935,
      "backward_entropy": 0.04237450957298279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.36930084228516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030536843463778496,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.137521763642629,
      "backward_entropy": 0.03148396015167236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.92308044433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03061673417687416,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13730311393737793,
      "backward_entropy": 0.03116070628166199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.81970977783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03070303425192833,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13707390427589417,
      "backward_entropy": 0.030863577127456666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.88980484008789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030793391168117523,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13683821757634482,
      "backward_entropy": 0.030581581592559814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.929534912109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03087725304067135,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1366141438484192,
      "backward_entropy": 0.030272907018661498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.41168975830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030955083668231964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13639983534812927,
      "backward_entropy": 0.029934123158454895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.120126724243164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031034676358103752,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13618342081705728,
      "backward_entropy": 0.029606997966766357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.88300704956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03110552951693535,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1359827220439911,
      "backward_entropy": 0.02925240695476532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.22285461425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031173940747976303,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13578717907269797,
      "backward_entropy": 0.03918523490428925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.91800689697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031244544312357903,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13558598359425864,
      "backward_entropy": 0.03880148828029632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.33230209350586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031316109001636505,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13538441061973572,
      "backward_entropy": 0.028208932280540465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.58194732666016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031382933259010315,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13518903652826944,
      "backward_entropy": 0.03804527223110199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.659461975097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03145575150847435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13498310248057047,
      "backward_entropy": 0.02751726806163788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.46546936035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03152795135974884,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13478084405263266,
      "backward_entropy": 0.03734250664710999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.34221649169922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03160065785050392,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1345771849155426,
      "backward_entropy": 0.03700327277183533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.596099853515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03167375177145004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13437199592590332,
      "backward_entropy": 0.02657431662082672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.747074127197266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03174237906932831,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1341747244199117,
      "backward_entropy": 0.026253610849380493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.91353607177734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0318089984357357,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13398231069246927,
      "backward_entropy": 0.035970157384872435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.94085693359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03187883645296097,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1337855656941732,
      "backward_entropy": 0.03564788997173309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.1265754699707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03195122256875038,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1335840622584025,
      "backward_entropy": 0.0253598690032959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.30897521972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03201941028237343,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13339032729466757,
      "backward_entropy": 0.025068917870521547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.75443649291992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03209070488810539,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13319247961044312,
      "backward_entropy": 0.034726780652999875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.38880157470703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03216099366545677,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13299513856569925,
      "backward_entropy": 0.0245242640376091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.06022644042969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03223076090216637,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1327991783618927,
      "backward_entropy": 0.03413560390472412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.73887634277344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03230501711368561,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1325958569844564,
      "backward_entropy": 0.13591870069503784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.67169189453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032381389290094376,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13238770763079324,
      "backward_entropy": 0.033603519201278687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.19374084472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03246142342686653,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13217292229334512,
      "backward_entropy": 0.033354449272155764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.91687774658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032543182373046875,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13195516665776572,
      "backward_entropy": 0.023292887210845947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.00263214111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03262658417224884,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13173491756121317,
      "backward_entropy": 0.023072578012943268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.36693572998047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032714586704969406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13150691986083984,
      "backward_entropy": 0.022865557670593263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.55924224853516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032800134271383286,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13128270705540976,
      "backward_entropy": 0.022651556134223937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.695444107055664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0328880250453949,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13105247418085733,
      "backward_entropy": 0.02243543565273285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.86634063720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03296898677945137,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1308363676071167,
      "backward_entropy": 0.022212666273117066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.542823791503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0330498144030571,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13061968485514322,
      "backward_entropy": 0.0317690372467041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.43360900878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03312920406460762,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13040625055631003,
      "backward_entropy": 0.03154166340827942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.12212371826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033210210502147675,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1301889717578888,
      "backward_entropy": 0.021563681960105895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.053955078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03329283744096756,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12996870279312134,
      "backward_entropy": 0.021359333395957948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.81059265136719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03337521478533745,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12974838415781656,
      "backward_entropy": 0.02115616500377655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.776451110839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03345745801925659,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12952792644500732,
      "backward_entropy": 0.0209548681974411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.10829544067383,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033536769449710846,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12931430339813232,
      "backward_entropy": 0.03049280643463135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.39715576171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03361126407980919,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12910819053649902,
      "backward_entropy": 0.030270582437515257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.697574615478516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033684972673654556,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12890314062436423,
      "backward_entropy": 0.030049777030944823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.95547866821289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03375491872429848,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12870601812998453,
      "backward_entropy": 0.02013162225484848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.2643928527832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0338248647749424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1285101274649302,
      "backward_entropy": 0.01993606090545654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.53165054321289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03389288857579231,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12831715742746988,
      "backward_entropy": 0.029399192333221434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.40769958496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033957477658987045,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12812980016072592,
      "backward_entropy": 0.019537922739982606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.90091705322266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03403184935450554,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1279240349928538,
      "backward_entropy": 0.019362238049507142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.83561325073242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03411177918314934,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1277072528998057,
      "backward_entropy": 0.028821468353271484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.16155242919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03418863192200661,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12749494115511575,
      "backward_entropy": 0.028642100095748902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.99897003173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034267865121364594,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12727813919385275,
      "backward_entropy": 0.018872717022895814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.880855560302734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03434588015079498,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12706299622853598,
      "backward_entropy": 0.028304970264434813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.779239654541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03441963344812393,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12685577074686685,
      "backward_entropy": 0.028126704692840575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.22712516784668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03448956459760666,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1266555388768514,
      "backward_entropy": 0.01837233155965805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.96733474731445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03455457463860512,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12646515170733133,
      "backward_entropy": 0.02774880826473236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.60224914550781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03461829572916031,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12627669175465903,
      "backward_entropy": 0.018019826710224153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.40733337402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03468397632241249,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12608325481414795,
      "backward_entropy": 0.017850574851036072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.28172302246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03475622832775116,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12587612867355347,
      "backward_entropy": 0.027210378646850587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.38787841796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03482947498559952,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12566552559534708,
      "backward_entropy": 0.13673391342163085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.092472076416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03491165116429329,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12543744842211405,
      "backward_entropy": 0.01741204410791397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.4552001953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03498927503824234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12521827220916748,
      "backward_entropy": 0.02677323520183563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.07622146606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03506583720445633,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.125,
      "backward_entropy": 0.017133447527885436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.89581298828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03513997420668602,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12478588024775188,
      "backward_entropy": 0.02648821771144867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.61962890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035221125930547714,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12455634276072185,
      "backward_entropy": 0.026361754536628722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.6488151550293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03529805317521095,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12433629234631856,
      "backward_entropy": 0.016728782653808595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.77808380126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03537260740995407,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12412063280741374,
      "backward_entropy": 0.01659664511680603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.52466583251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03545285016298294,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12389268477757771,
      "backward_entropy": 0.016476690769195557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.22640609741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035539619624614716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12365057071050008,
      "backward_entropy": 0.016366931796073913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.03932189941406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.035624563694000244,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12341171503067017,
      "backward_entropy": 0.13702819347381592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.03908157348633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03570926934480667,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12317194541295369,
      "backward_entropy": 0.016143810749053956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.55781555175781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03578921779990196,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12294178207715352,
      "backward_entropy": 0.016025865077972413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.8780517578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03586965426802635,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12271012862523396,
      "backward_entropy": 0.1371155261993408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.18527221679688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03594456613063812,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1224917471408844,
      "backward_entropy": 0.015799698233604432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.43126678466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03602037578821182,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12227008740107219,
      "backward_entropy": 0.015688782930374144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.726491928100586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036093819886446,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12205204367637634,
      "backward_entropy": 0.025113591551780702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.62642669677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036162253469228745,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12184536457061768,
      "backward_entropy": 0.01545756459236145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.2139663696289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03623220697045326,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12163367867469788,
      "backward_entropy": 0.01534423530101776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.85582733154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03630974516272545,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12140440940856934,
      "backward_entropy": 0.01524365395307541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.05272674560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03638961538672447,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12116962671279907,
      "backward_entropy": 0.024685165286064147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.18097686767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03646976128220558,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12093228101730347,
      "backward_entropy": 0.015057983994483947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.60410690307617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03654889017343521,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12069688240687053,
      "backward_entropy": 0.014966388046741486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.76423454284668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03662851080298424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12045915921529134,
      "backward_entropy": 0.014875940978527069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.672332763671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03670411929488182,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1202306052049001,
      "backward_entropy": 0.014783549308776855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.94163513183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03677607327699661,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12001015742619832,
      "backward_entropy": 0.014688840508460999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.97990798950195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03685230761766434,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11977830529212952,
      "backward_entropy": 0.01459949165582657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.599586486816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03692604601383209,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11954987049102783,
      "backward_entropy": 0.014502310752868652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.319400787353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0370008684694767,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11931778987248738,
      "backward_entropy": 0.01440834254026413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.94026184082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03707189857959747,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11909304062525432,
      "backward_entropy": 0.014308184385299683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.33990478515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037147097289562225,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11885611216227214,
      "backward_entropy": 0.023735921084880828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.5107536315918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03722463920712471,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1186120609442393,
      "backward_entropy": 0.01411799043416977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.595149993896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0373014397919178,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11836957931518555,
      "backward_entropy": 0.014028392732143402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.40140151977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03737901896238327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11812432607014973,
      "backward_entropy": 0.013942340016365051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.01077651977539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03745722398161888,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11787629127502441,
      "backward_entropy": 0.02337360829114914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.79819107055664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03753451630473137,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11762955784797668,
      "backward_entropy": 0.013773436844348907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.84648132324219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03761119395494461,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11738457282384236,
      "backward_entropy": 0.01369410902261734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.353883743286133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03769029304385185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11713320016860962,
      "backward_entropy": 0.013621851801872253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.30532455444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037765298038721085,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11689096689224243,
      "backward_entropy": 0.013544195890426635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.16138458251953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037839777767658234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11664928992589314,
      "backward_entropy": 0.022970962524414062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.8685531616211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037918247282505035,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11639589071273804,
      "backward_entropy": 0.013397228717803956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.86334228515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03800167888402939,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11612800757090251,
      "backward_entropy": 0.01333044171333313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.61637496948242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03808208182454109,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11586640278498332,
      "backward_entropy": 0.01325942724943161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.28248977661133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0381612628698349,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11560632785161336,
      "backward_entropy": 0.013186682760715485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.39883041381836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0382409431040287,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11534406741460164,
      "backward_entropy": 0.01311640590429306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.860107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038318146020174026,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11508800586064656,
      "backward_entropy": 0.013046583533287049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.09946060180664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038396015763282776,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11482880512873332,
      "backward_entropy": 0.012978583574295044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.71794891357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03847168758511543,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11457587281862895,
      "backward_entropy": 0.012915492057800293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.91602325439453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038546666502952576,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11432333787282307,
      "backward_entropy": 0.022304943203926085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.35767364501953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.038624074310064316,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11406294504801433,
      "backward_entropy": 0.13778891563415527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.43790435791016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03870068117976189,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11380375425020854,
      "backward_entropy": 0.022165633738040924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.42329788208008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03877944499254227,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11353668570518494,
      "backward_entropy": 0.022098131477832794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.48112487792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03885572403669357,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1132754385471344,
      "backward_entropy": 0.01261860877275467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.68903350830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038935717195272446,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11300243933995564,
      "backward_entropy": 0.01256435215473175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.94416427612305,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039017461240291595,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11272239685058594,
      "backward_entropy": 0.1378633737564087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.265567779541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039099328219890594,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11244025826454163,
      "backward_entropy": 0.021832607686519623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.49667739868164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03917991369962692,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11216108997662862,
      "backward_entropy": 0.021766571700572966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.00659942626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03926071524620056,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11187936862309773,
      "backward_entropy": 0.012349729239940644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.703968048095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039344608783721924,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1115867296854655,
      "backward_entropy": 0.012298506498336793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.21868896484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0394226498901844,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11131229003270467,
      "backward_entropy": 0.01224469393491745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.85803985595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03949833661317825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11104460557301839,
      "backward_entropy": 0.012190863490104675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.05194854736328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0395762175321579,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11076870560646057,
      "backward_entropy": 0.021428288519382478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.33540344238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03965890780091286,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11047661304473877,
      "backward_entropy": 0.012092772126197814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.920082092285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039743050932884216,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11017882823944092,
      "backward_entropy": 0.012047921121120454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.68506622314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039827026426792145,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10987999041875203,
      "backward_entropy": 0.021243180334568023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.24247932434082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03991083800792694,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10958009958267212,
      "backward_entropy": 0.011958011239767075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.28952026367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03999020904302597,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10929377873738606,
      "backward_entropy": 0.021113970875740053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.00128173828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040072694420814514,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10899551709493001,
      "backward_entropy": 0.011864768713712693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.896780014038086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040153708308935165,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10870037476221721,
      "backward_entropy": 0.011818074434995652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.62689971923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040230561047792435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10841850439707439,
      "backward_entropy": 0.011769660562276841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.79572296142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04030649736523628,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10813797513643901,
      "backward_entropy": 0.011720851063728333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.42224884033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04037734121084213,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10787417491277058,
      "backward_entropy": 0.011669743061065673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.92130661010742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04044647887349129,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10761586825052898,
      "backward_entropy": 0.020686791837215425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.7186164855957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04051690921187401,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10735221703847249,
      "backward_entropy": 0.0115704745054245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.50395965576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040588490664958954,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10708341995875041,
      "backward_entropy": 0.011523394286632538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.155458450317383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04066668078303337,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10679004589716594,
      "backward_entropy": 0.01148044615983963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.07154083251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04074098542332649,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10650964577992757,
      "backward_entropy": 0.011436179280281067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.21554183959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040816012769937515,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10622590780258179,
      "backward_entropy": 0.011393167078495026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.830570220947266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04089028015732765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10594354073206584,
      "backward_entropy": 0.011350434273481369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.86222457885742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040961045771837234,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10567363103230794,
      "backward_entropy": 0.01130683571100235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.623193740844727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04103147238492966,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10540308554967244,
      "backward_entropy": 0.011263643205165864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.52451705932617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04109877720475197,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10514338811238606,
      "backward_entropy": 0.011219729483127595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.828731536865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04116608574986458,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10488218069076538,
      "backward_entropy": 0.011176757514476776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.05896759033203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04123476520180702,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1046149730682373,
      "backward_entropy": 0.019863314926624298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.61601638793945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041306063532829285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1043367584546407,
      "backward_entropy": 0.011095837503671647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.74810218811035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0413755401968956,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10406472285588582,
      "backward_entropy": 0.011056222766637803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.01193618774414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04144059494137764,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10380903879801433,
      "backward_entropy": 0.011015434563159943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.21308517456055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041507236659526825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10354578495025635,
      "backward_entropy": 0.010975948721170425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.282644271850586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04157254472374916,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10328635573387146,
      "backward_entropy": 0.010936814546585082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.67434310913086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04163246974349022,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10304765899976094,
      "backward_entropy": 0.010896511375904083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.03799819946289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041695840656757355,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10279413064320882,
      "backward_entropy": 0.010858213901519776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.88309860229492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041759587824344635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10253774126370747,
      "backward_entropy": 0.010821355879306794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.855201721191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0418236143887043,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10227972269058228,
      "backward_entropy": 0.019183842837810515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.551780700683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04188932850956917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10201328992843628,
      "backward_entropy": 0.010751036554574966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.463829040527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04195515438914299,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1017453670501709,
      "backward_entropy": 0.019044655561447143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.26087188720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04202239215373993,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10147120555241902,
      "backward_entropy": 0.010685492306947708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.03490447998047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042090874165296555,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10119158029556274,
      "backward_entropy": 0.010653634369373322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.90508460998535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04215782508254051,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1009172797203064,
      "backward_entropy": 0.010622464865446091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.950546741485596,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04222331941127777,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10064856211344402,
      "backward_entropy": 0.018764407932758333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.644472122192383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04228348657488823,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1004015604654948,
      "backward_entropy": 0.01055944710969925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.27365493774414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042342912405729294,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10015696287155151,
      "backward_entropy": 0.01052841693162918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.708197593688965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04240439459681511,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09990260998408,
      "backward_entropy": 0.010499157011508942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.64863109588623,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04246225953102112,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0996633768081665,
      "backward_entropy": 0.010469317436218262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.728694915771484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04251686856150627,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09943787256876628,
      "backward_entropy": 0.010439113527536393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.530065536499023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04257407411932945,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09919949372609456,
      "backward_entropy": 0.010411892831325532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.11490249633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042628128081560135,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09897434711456299,
      "backward_entropy": 0.010384784638881683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.418785095214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042686112225055695,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09873094161351521,
      "backward_entropy": 0.010359805822372437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.362953186035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04274081066250801,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09850200017293294,
      "backward_entropy": 0.0181498721241951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.22028350830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04279669374227524,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09826663136482239,
      "backward_entropy": 0.010309875011444092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.071556091308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04285360127687454,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09802597761154175,
      "backward_entropy": 0.010286948829889297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.352081298828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04291142523288727,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09778063495953877,
      "backward_entropy": 0.01026494950056076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.773521423339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04296857491135597,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09753827253977458,
      "backward_entropy": 0.010240992903709412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.096193313598633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04302655905485153,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09729198614756267,
      "backward_entropy": 0.010217584669589996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.973237991333008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04308402165770531,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09704739848772685,
      "backward_entropy": 0.010195538401603699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.414581298828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04314103722572327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09680374463399251,
      "backward_entropy": 0.010175120085477829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.02436065673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04319604113698006,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0965699553489685,
      "backward_entropy": 0.010151611268520355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.621198654174805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04325482249259949,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09631802638371785,
      "backward_entropy": 0.010130835324525833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.85540008544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043312884867191315,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0960692564646403,
      "backward_entropy": 0.010109736025333405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.700927734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04337175190448761,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09581591685612996,
      "backward_entropy": 0.010090576857328415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.8484992980957,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04343131184577942,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09555858373641968,
      "backward_entropy": 0.13814928531646728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.192237854003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04349277541041374,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09529195229212443,
      "backward_entropy": 0.01733226329088211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.20890808105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04355859011411667,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.095004936059316,
      "backward_entropy": 0.010040771961212159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.43378829956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043624311685562134,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09471819798151652,
      "backward_entropy": 0.010025730729103089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.527080535888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04369258135557175,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09441955884297688,
      "backward_entropy": 0.01001170203089714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.79969024658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043757837265729904,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09413514534632365,
      "backward_entropy": 0.009996776282787324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.485652923583984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043824441730976105,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09384365876515706,
      "backward_entropy": 0.00998399332165718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.497474670410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04389104247093201,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09355124831199646,
      "backward_entropy": 0.009974569082260132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.167911529541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04396131634712219,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09324175119400024,
      "backward_entropy": 0.01696300506591797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.95390319824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04403216019272804,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09293014804522197,
      "backward_entropy": 0.009953594952821731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.813894271850586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044102270156145096,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09262197216351827,
      "backward_entropy": 0.009940674901008606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.866812705993652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04417051747441292,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09232260783513387,
      "backward_entropy": 0.0099283367395401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.543384552001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044234320521354675,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0920452078183492,
      "backward_entropy": 0.009913449734449386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.755760192871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04429672658443451,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.091774582862854,
      "backward_entropy": 0.009896899759769439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.650288581848145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04436312988400459,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0914844274520874,
      "backward_entropy": 0.009881535172462463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.136215209960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044425398111343384,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09121429920196533,
      "backward_entropy": 0.009866301715373994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.45840072631836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04448635131120682,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09095081686973572,
      "backward_entropy": 0.009848729521036149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.25814437866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044548891484737396,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0906787912050883,
      "backward_entropy": 0.009832939505577088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.044031143188477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04461280629038811,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0903994341691335,
      "backward_entropy": 0.009817969053983688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.57522964477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04467417299747467,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09013219674428304,
      "backward_entropy": 0.01628517210483551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.65679931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04473454877734184,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08986916144688924,
      "backward_entropy": 0.009793063998222351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.30276107788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04479643702507019,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08959837754567464,
      "backward_entropy": 0.009781406819820404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.846641540527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04485734924674034,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08933150768280029,
      "backward_entropy": 0.009773112088441848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.559438705444336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0449235700070858,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08903835217158,
      "backward_entropy": 0.016064348816871642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.017603874206543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04498676210641861,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08876099189122517,
      "backward_entropy": 0.009756296873092651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.05814943090081215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04504569619894028,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08850649992624919,
      "backward_entropy": 0.009739194065332413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.858829498291016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04509866237640381,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08828256527582805,
      "backward_entropy": 0.009722666442394256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.38101577758789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0451551228761673,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08804068962732951,
      "backward_entropy": 0.015796658396720887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.39198875427246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04521718993782997,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08777048190434773,
      "backward_entropy": 0.00969470590353012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.96503257751465,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045278120785951614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08750547965367635,
      "backward_entropy": 0.009683774411678314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.860849380493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045336686074733734,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08725233872731526,
      "backward_entropy": 0.00967339426279068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.785417556762695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045393206179142,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08700915177663167,
      "backward_entropy": 0.00966522991657257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.121057510375977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0454476922750473,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0867764949798584,
      "backward_entropy": 0.00965581089258194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.657196044921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04550271853804588,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08654151360193889,
      "backward_entropy": 0.009643420577049255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.65865707397461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04556336998939514,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08627790212631226,
      "backward_entropy": 0.00963192731142044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.94321823120117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04562268406152725,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08602174123128255,
      "backward_entropy": 0.015294232964515686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.285717010498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04568597674369812,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08574534455935161,
      "backward_entropy": 0.009605373442173003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.26414680480957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04574671760201454,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08548184235890706,
      "backward_entropy": 0.00959581956267357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.10491943359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04580765217542648,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0852168599764506,
      "backward_entropy": 0.009589235484600066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.92449188232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04586604982614517,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08496546745300293,
      "backward_entropy": 0.00958104282617569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.788970947265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04592590779066086,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08470672369003296,
      "backward_entropy": 0.009572827816009521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.730365753173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04598579928278923,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08444809913635254,
      "backward_entropy": 0.009563712030649185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.21104049682617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04604433849453926,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0841972827911377,
      "backward_entropy": 0.009551094472408294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.64908218383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04610542580485344,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08393380045890808,
      "backward_entropy": 0.00953773781657219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.673834800720215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04617122933268547,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08364699284235637,
      "backward_entropy": 0.014733389019966125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.166873931884766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046233028173446655,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08338071902592976,
      "backward_entropy": 0.009514949470758437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.757505416870117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04629339277744293,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08312227825323741,
      "backward_entropy": 0.009504936635494232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.600996017456055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046353764832019806,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08286373813947041,
      "backward_entropy": 0.009496074169874191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.115060806274414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04641401395201683,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08260629574457805,
      "backward_entropy": 0.009486294537782668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.995943069458008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04647158458828926,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08236382901668549,
      "backward_entropy": 0.00947294682264328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.92108154296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046526968479156494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0821328063805898,
      "backward_entropy": 0.009460510313510894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.5238037109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.046580202877521515,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.08191356062889099,
      "backward_entropy": 0.13827779293060302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.294170379638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04663533717393875,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08168389896551768,
      "backward_entropy": 0.009433481097221374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.654375076293945,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04668939486145973,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.08146097759405772,
      "backward_entropy": 0.13827300071716309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.51827621459961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04674395173788071,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08123528957366943,
      "backward_entropy": 0.009401103109121322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.002644538879395,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046798765659332275,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08100886642932892,
      "backward_entropy": 0.009383302927017213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.222562789916992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046850401908159256,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08079945544401805,
      "backward_entropy": 0.013862156867980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.666044235229492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04690250754356384,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08058812220891316,
      "backward_entropy": 0.009346343576908112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.5487003326416,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046954043209552765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0803799827893575,
      "backward_entropy": 0.009327103197574616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.444366455078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0470050610601902,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08017468949158986,
      "backward_entropy": 0.009308610856533051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.31721305847168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04705548286437988,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07997289299964905,
      "backward_entropy": 0.009288665652275086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.202880859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04710548743605614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07977332671483357,
      "backward_entropy": 0.00926986038684845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.611943244934082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04715511202812195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0795758565266927,
      "backward_entropy": 0.009252214431762695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.758515357971191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04720194265246391,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07939372460047404,
      "backward_entropy": 0.009233838319778443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.691750526428223,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04724761098623276,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07921757300694783,
      "backward_entropy": 0.00921819508075714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.312979698181152,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04729209840297699,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07904791831970215,
      "backward_entropy": 0.13821194171905518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.39117431640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047332894057035446,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07889927426973979,
      "backward_entropy": 0.013030390441417693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.816787719726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04737197235226631,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07875942687193553,
      "backward_entropy": 0.009165024757385254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.665266036987305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04741416499018669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07860226432482402,
      "backward_entropy": 0.009151224046945572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.309593200683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04745921865105629,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07842923700809479,
      "backward_entropy": 0.009141793847084046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.331377029418945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04750334471464157,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07826093832651775,
      "backward_entropy": 0.009135892242193222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.184200286865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04754878208041191,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07808571060498555,
      "backward_entropy": 0.009130352735519409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.050512313842773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047594256699085236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07791045804818471,
      "backward_entropy": 0.009125229716300965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.089224815368652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04764215275645256,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0777222067117691,
      "backward_entropy": 0.00912267565727234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.861560821533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047687020152807236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07755204041798909,
      "backward_entropy": 0.009111206233501434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.841346740722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047732021659612656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07738102475802104,
      "backward_entropy": 0.00910220742225647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.541990280151367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04777607321739197,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07721486190954845,
      "backward_entropy": 0.009097089618444442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.428401947021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04782145470380783,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07704148689905803,
      "backward_entropy": 0.009093662351369857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.149633407592773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04786781966686249,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07686352729797363,
      "backward_entropy": 0.009087887406349183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.334152221679688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047915905714035034,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07667775948842366,
      "backward_entropy": 0.009074724465608596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.6485013961792,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04796377196907997,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07649323344230652,
      "backward_entropy": 0.012094798684120178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.12894058227539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048009466379880905,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0763196349143982,
      "backward_entropy": 0.009062132984399795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.829979419708252,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04805946722626686,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07612394789854686,
      "backward_entropy": 0.009057963639497757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.203722953796387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04810582473874092,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07594768206278484,
      "backward_entropy": 0.009059032797813416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.4390926361084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048150982707738876,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07577796777089436,
      "backward_entropy": 0.00906166285276413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.971725463867188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04819754511117935,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07560012241204579,
      "backward_entropy": 0.009070327132940292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.965119361877441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04824617877602577,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07541172703107198,
      "backward_entropy": 0.009078747779130935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.2435417175293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048293352127075195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07523140807946523,
      "backward_entropy": 0.009087730944156647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.10125606507062912,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04834344983100891,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07503655552864075,
      "backward_entropy": 0.1382249712944031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.766939163208008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048388902097940445,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0748662253220876,
      "backward_entropy": 0.011677592992782593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.15003204345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04843557998538017,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07468901077906291,
      "backward_entropy": 0.009121249616146087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.637001991271973,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048482030630111694,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07451341549555461,
      "backward_entropy": 0.009135746210813523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.410388946533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04852469265460968,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07435927291711171,
      "backward_entropy": 0.011574779450893403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.67928695678711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048568516969680786,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07419936855634053,
      "backward_entropy": 0.009151089191436767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.93884563446045,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04861482232809067,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0740257700284322,
      "backward_entropy": 0.00916178673505783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.13462448120117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048658713698387146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07386533419291179,
      "backward_entropy": 0.009172125160694123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.952850341796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04870712384581566,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0736812154452006,
      "backward_entropy": 0.00918399840593338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.121760368347168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048759207129478455,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07347936928272247,
      "backward_entropy": 0.009190152585506439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.305139541625977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048809122294187546,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07329028348128001,
      "backward_entropy": 0.009191484749317169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.719837188720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048858366906642914,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0731051762898763,
      "backward_entropy": 0.009192121773958206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.320049285888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04890913516283035,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07291281223297119,
      "backward_entropy": 0.011227165907621383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.37092399597168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048960112035274506,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07272020479043324,
      "backward_entropy": 0.009187901020050048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.340511322021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04901240020990372,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0725215623776118,
      "backward_entropy": 0.009183336049318314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.709888458251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04906713217496872,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07231061160564423,
      "backward_entropy": 0.009181790053844452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.502663612365723,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049120720475912094,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07210613787174225,
      "backward_entropy": 0.011013002693653106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.489763259887695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04917197674512863,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07191505531469981,
      "backward_entropy": 0.00917695090174675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.35777759552002,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049222249537706375,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07172988851865132,
      "backward_entropy": 0.009170874953269958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.33264923095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04927026107907295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07155799865722656,
      "backward_entropy": 0.00915738046169281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.177896499633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04932207614183426,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07136700550715129,
      "backward_entropy": 0.009147553890943527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.097575187683105,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04936952888965607,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07119973500569661,
      "backward_entropy": 0.00913448929786682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.098849296569824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04941505193710327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07104368011156718,
      "backward_entropy": 0.009115911275148391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.910414695739746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04945707693696022,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07090575496355693,
      "backward_entropy": 0.009099900722503662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.928133487701416,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04949802905321121,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07077337304751079,
      "backward_entropy": 0.009085483849048615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.927742958068848,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.049537140876054764,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07065000136693318,
      "backward_entropy": 0.13826427459716797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.778623580932617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04957418888807297,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07053800920645396,
      "backward_entropy": 0.00906333103775978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.791082859039307,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04961000010371208,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07043419281641643,
      "backward_entropy": 0.009039627015590667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.411824226379395,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04964457079768181,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07033577561378479,
      "backward_entropy": 0.1382357358932495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.524510383605957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04967966675758362,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07023487488428752,
      "backward_entropy": 0.009006975591182709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.265220642089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049714379012584686,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07013574242591858,
      "backward_entropy": 0.008992411941289902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.20453356206417084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04974944144487381,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07003548741340637,
      "backward_entropy": 0.008973515778779983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.55340003967285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049780577421188354,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06995667020479839,
      "backward_entropy": 0.00894930139183998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.888746976852417,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0498146191239357,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06986308097839355,
      "backward_entropy": 0.008924655616283417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.593379974365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04984612017869949,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06978282332420349,
      "backward_entropy": 0.009743712097406387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.167768478393555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049879882484674454,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06968992948532104,
      "backward_entropy": 0.008884337544441224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.712383270263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04991668835282326,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06958070397377014,
      "backward_entropy": 0.008875694870948792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.818164825439453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04995698109269142,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06945382555325826,
      "backward_entropy": 0.008868513256311416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.382358074188232,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04999397322535515,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06934498747189839,
      "backward_entropy": 0.008856884390115737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.099281311035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05002924054861069,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06924521923065186,
      "backward_entropy": 0.008845958858728409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.292145252227783,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05006595700979233,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06913831830024719,
      "backward_entropy": 0.008835448324680329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.2709269523620605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050101060420274734,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0690397967894872,
      "backward_entropy": 0.008827531337738037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.180740833282471,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05013446882367134,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06895042459170024,
      "backward_entropy": 0.008817535638809205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.19602394104004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05016700550913811,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06886452436447144,
      "backward_entropy": 0.008818528056144715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.621262550354004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0502023920416832,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0687640110651652,
      "backward_entropy": 0.00882144421339035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.949739456176758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05023728683590889,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06866611043612163,
      "backward_entropy": 0.008825850486755372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.37626075744629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05027485266327858,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06855446596940358,
      "backward_entropy": 0.008833977580070495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.855375289916992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05031382292509079,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06843562920888265,
      "backward_entropy": 0.008845241367816925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.945581436157227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050352901220321655,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06831662853558858,
      "backward_entropy": 0.008855978399515152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.281464576721191,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05039532110095024,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06818067034085591,
      "backward_entropy": 0.008958543837070464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.615909576416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050436656922101974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06804994245370229,
      "backward_entropy": 0.008890148252248764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.783069610595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05048106238245964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06790401538213094,
      "backward_entropy": 0.008911997079849243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.360339164733887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05052625760436058,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06775425871213277,
      "backward_entropy": 0.008937805145978927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.555910110473633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05057109147310257,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06760663787523906,
      "backward_entropy": 0.00896586999297142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4301047325134277,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05061637610197067,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06745766599973042,
      "backward_entropy": 0.00899137631058693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.829544067382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05065801739692688,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.067327747742335,
      "backward_entropy": 0.009012872725725174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.77899169921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05069870874285698,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.06720201174418132,
      "backward_entropy": 0.13823845386505126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.748392105102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05073835700750351,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06708156069119771,
      "backward_entropy": 0.009064757078886033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.851289749145508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050776708871126175,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06696862975756328,
      "backward_entropy": 0.009083520621061325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.695940971374512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050814833492040634,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06685773531595866,
      "backward_entropy": 0.009094923734664917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.709676742553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0508534200489521,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06674349308013916,
      "backward_entropy": 0.00911382883787155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.357735633850098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05089147016406059,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06663367648919423,
      "backward_entropy": 0.009119249880313873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.408166885375977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050927724689245224,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06653252243995667,
      "backward_entropy": 0.009126301854848862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.234170436859131,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05096321552991867,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0664351483186086,
      "backward_entropy": 0.009132705628871918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.346677780151367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05099603533744812,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06635115544001262,
      "backward_entropy": 0.009137677401304245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.288249015808105,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051029272377491,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06626532971858978,
      "backward_entropy": 0.008480239659547806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.167292594909668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0510639026761055,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06617270906766255,
      "backward_entropy": 0.009140098094940185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.061487197875977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0510990135371685,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06607731183369954,
      "backward_entropy": 0.009143563359975815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.129790306091309,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05113641172647476,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06597057978312175,
      "backward_entropy": 0.009148678928613662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.071910858154297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05117161199450493,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0658756544192632,
      "backward_entropy": 0.13827369213104249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.951876640319824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05120500177145004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0657900869846344,
      "backward_entropy": 0.009141892194747925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.476943969726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05123777315020561,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0657076636950175,
      "backward_entropy": 0.13826401233673097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.947103023529053,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051274195313453674,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06560660401980083,
      "backward_entropy": 0.009137680381536483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.226848602294922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0513087660074234,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06551490227381389,
      "backward_entropy": 0.009138115495443345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.38398265838623,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051346514374017715,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06540780266125996,
      "backward_entropy": 0.009140178561210632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.641888618469238,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05138527601957321,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0652957558631897,
      "backward_entropy": 0.009144361317157745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.18181037902832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05142293497920036,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06518936653931935,
      "backward_entropy": 0.009147490561008453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.282094955444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05146149918437004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06507884462674458,
      "backward_entropy": 0.009150777012109756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.443721771240234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05150004103779793,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0649682879447937,
      "backward_entropy": 0.009156567603349685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.885557174682617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05153755471110344,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06486279765764873,
      "backward_entropy": 0.007876967638731002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.328778266906738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05157576501369476,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06475496292114258,
      "backward_entropy": 0.009164822101593018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.771799087524414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05161287635564804,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06465267141660054,
      "backward_entropy": 0.009165973216295243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.5228400230407715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05165346711874008,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06453466415405273,
      "backward_entropy": 0.009164060652256011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.090871810913086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05169180780649185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06442774335543315,
      "backward_entropy": 0.009162195026874542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.466382026672363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05173185467720032,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0643129050731659,
      "backward_entropy": 0.009162312000989914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.625526428222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051769424229860306,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0642108569542567,
      "backward_entropy": 0.00762077122926712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.3178486824035645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051806796342134476,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06411006053288777,
      "backward_entropy": 0.009150520712137223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.614492416381836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05184256657958031,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0640160044034322,
      "backward_entropy": 0.009153091162443162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.792202472686768,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05188005790114403,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06391450266043346,
      "backward_entropy": 0.009153688699007035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.314933776855469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05191680043935776,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06381582220395406,
      "backward_entropy": 0.009161514043807984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.703234910964966,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05195329338312149,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06371907889842987,
      "backward_entropy": 0.009165066480636596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.151937961578369,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05198683589696884,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.06363675991694133,
      "backward_entropy": 0.1382519006729126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.615257740020752,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05201886221766472,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.06356143951416016,
      "backward_entropy": 0.13824996948242188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.449568748474121,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05205020681023598,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06348979473114014,
      "backward_entropy": 0.009163696318864822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.076199531555176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05208301916718483,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06341082851092021,
      "backward_entropy": 0.009166327118873597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.868749618530273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052114106714725494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06334049503008525,
      "backward_entropy": 0.009165027737617492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.041852951049805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05214562639594078,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06326803068319957,
      "backward_entropy": 0.009164825081825256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.75444507598877,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052180010825395584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06318292021751404,
      "backward_entropy": 0.0071352913975715635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.642555236816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05221421644091606,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06309941411018372,
      "backward_entropy": 0.007088767737150193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.951067924499512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05224860832095146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06301479538281758,
      "backward_entropy": 0.009151971340179444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.837403297424316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052283741533756256,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06292744477589925,
      "backward_entropy": 0.009145554900169373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.424036026000977,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052319709211587906,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.06283629933993022,
      "backward_entropy": 0.1382209300994873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.356200218200684,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0523555725812912,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06274584929148357,
      "backward_entropy": 0.009136377274990082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.53000545501709,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05239124596118927,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06265655159950256,
      "backward_entropy": 0.006871652603149414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.746049880981445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05242783948779106,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06256282826264699,
      "backward_entropy": 0.009132225811481477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.598996162414551,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052465543150901794,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0624654491742452,
      "backward_entropy": 0.009125523269176483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.5288724899292,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0525016263127327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06237444778283437,
      "backward_entropy": 0.00913001224398613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.766650199890137,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05253865197300911,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06228103240331014,
      "backward_entropy": 0.009121853858232498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.88923454284668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052574485540390015,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06219316522280375,
      "backward_entropy": 0.009114138782024384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.947047233581543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052610062062740326,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.062106808026631675,
      "backward_entropy": 0.009106501936912537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.001594543457031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052646487951278687,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06201633314291636,
      "backward_entropy": 0.009104679524898528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.404749393463135,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05268438905477524,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06191946566104889,
      "backward_entropy": 0.009105600416660309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.712620735168457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05272025614976883,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06183166801929474,
      "backward_entropy": 0.00910879224538803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.487929344177246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05275635048747063,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.061744168400764465,
      "backward_entropy": 0.009103649109601975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.505316734313965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05279228463768959,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06165717542171478,
      "backward_entropy": 0.009102749824523925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.391863822937012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0528285913169384,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.061569273471832275,
      "backward_entropy": 0.009099131077528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.24270486831665,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0528654009103775,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06147933999697367,
      "backward_entropy": 0.009096845239400863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1504340171813965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05290105566382408,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06139460702737173,
      "backward_entropy": 0.009096619486808778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.085823059082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05293602868914604,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.061312069495519005,
      "backward_entropy": 0.009106656908988953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.079322814941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05297186225652695,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06122540434201559,
      "backward_entropy": 0.009122532606124879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.071516513824463,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053007110953330994,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06114226579666138,
      "backward_entropy": 0.009131264686584473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.1143341064453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0530405230820179,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06106713910897573,
      "backward_entropy": 0.00914095938205719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.609376907348633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053071532398462296,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.061004310846328735,
      "backward_entropy": 0.006167799234390259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.892995357513428,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053105007857084274,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.060931007067362465,
      "backward_entropy": 0.009128949046134949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.578947067260742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0531374029815197,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06086275974909464,
      "backward_entropy": 0.009121748059988022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.763305187225342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05317073315382004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06079001724720001,
      "backward_entropy": 0.00912037268280983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.722926139831543,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05320326238870621,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.060720483462015785,
      "backward_entropy": 0.006016073003411293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.567736625671387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05323496833443642,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06065446138381958,
      "backward_entropy": 0.009127236157655715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.466240882873535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05326613411307335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06059214472770691,
      "backward_entropy": 0.00911911129951477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.524279594421387,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.053297173231840134,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.06053060293197632,
      "backward_entropy": 0.13818491697311402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.102928161621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053328078240156174,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06046818196773529,
      "backward_entropy": 0.00911366268992424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.788548469543457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053359899669885635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06040174265702566,
      "backward_entropy": 0.00912117213010788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.94011116027832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053393375128507614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06032808621724447,
      "backward_entropy": 0.009132243692874908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.124211311340332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053427476435899734,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06025191148122152,
      "backward_entropy": 0.00914502888917923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6749634742736816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053461283445358276,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06017715732256571,
      "backward_entropy": 0.009157340973615646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.003830909729004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05349266901612282,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06011421978473663,
      "backward_entropy": 0.00915542095899582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2576189041137695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05352390930056572,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06005210677782694,
      "backward_entropy": 0.009152378141880035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.880298614501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05355418100953102,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05999431014060974,
      "backward_entropy": 0.009147484600543977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8135662078857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053584400564432144,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.059936851263046265,
      "backward_entropy": 0.00914226621389389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.727806091308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05361255630850792,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.059887503584225975,
      "backward_entropy": 0.009142759442329406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.054494380950928,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05364122614264488,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05983530978361765,
      "backward_entropy": 0.009150643646717072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.939784049987793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053669434040784836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05978486935297648,
      "backward_entropy": 0.005533741042017937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.2103094905614853,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05369874835014343,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.059731115897496544,
      "backward_entropy": 0.00915793478488922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.669825553894043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053724709898233414,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.059692194064458214,
      "backward_entropy": 0.009145520627498627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.486006736755371,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0537530817091465,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05964197218418121,
      "backward_entropy": 0.009147562831640244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.005361557006836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05378153547644615,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0595915416876475,
      "backward_entropy": 0.00914880856871605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.01366901397705,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05381067842245102,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05953864256540934,
      "backward_entropy": 0.009146456420421601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.857131004333496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053842127323150635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05947620670000712,
      "backward_entropy": 0.00914621278643608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.216121673583984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0538739413022995,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.059412782390912376,
      "backward_entropy": 0.00914289355278015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.694445610046387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05390582233667374,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05934843420982361,
      "backward_entropy": 0.009147941321134567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.14552561938762665,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053938087075948715,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.059282888968785606,
      "backward_entropy": 0.009151309728622437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.01603889465332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053967367857694626,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05922907590866089,
      "backward_entropy": 0.009160254895687104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6241295337677002,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05399804562330246,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05916982889175415,
      "backward_entropy": 0.0091684490442276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.550795555114746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05402639880776405,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05912002424399058,
      "backward_entropy": 0.009176675230264664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.880906105041504,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05405358225107193,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05907603104909261,
      "backward_entropy": 0.009172778576612473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.413328647613525,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.054081130772829056,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.05903009573618571,
      "backward_entropy": 0.1381835460662842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9639065265655518,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05410821735858917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0589855561653773,
      "backward_entropy": 0.009181089699268341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.526867866516113,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054134197533130646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05894513428211212,
      "backward_entropy": 0.0091915562748909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.449604988098145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05416199192404747,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05889710783958435,
      "backward_entropy": 0.0092054083943367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.622692584991455,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05419139936566353,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05884248514970144,
      "backward_entropy": 0.009222046285867692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.585155487060547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054220885038375854,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05878728628158569,
      "backward_entropy": 0.009242076426744461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.514453887939453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.054250240325927734,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.058732827504475914,
      "backward_entropy": 0.1382183313369751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.171139717102051,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054279644042253494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0586780309677124,
      "backward_entropy": 0.00928233116865158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.719887733459473,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05430794879794121,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0586282213528951,
      "backward_entropy": 0.138232958316803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.373149871826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05433712899684906,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05857439339160919,
      "backward_entropy": 0.009314173460006714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.009354114532471,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05436613783240318,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.058521399895350136,
      "backward_entropy": 0.00933099389076233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.761502981185913,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05439450219273567,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.058470552166303,
      "backward_entropy": 0.009351292252540588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.256776332855225,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05442088097333908,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05842882891496023,
      "backward_entropy": 0.004868563637137413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.183329582214355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05444696173071861,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05838895837465922,
      "backward_entropy": 0.009354502707719804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.110739707946777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05447591841220856,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.058337713281313576,
      "backward_entropy": 0.009353907406330108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6140434741973877,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05450475588440895,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05828657249609629,
      "backward_entropy": 0.009355488419532775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.445176124572754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0545319989323616,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0582416703303655,
      "backward_entropy": 0.00935596451163292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.136185646057129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05456055700778961,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.058191776275634766,
      "backward_entropy": 0.009357231110334397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.694183588027954,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05458991602063179,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05813798805077871,
      "backward_entropy": 0.00936613827943802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6995410919189453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.054618608206510544,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.05808614691098531,
      "backward_entropy": 0.13825666904449463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4775350093841553,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054646238684654236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.058038920164108276,
      "backward_entropy": 0.009389814734458924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.200003623962402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054672449827194214,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05799713730812073,
      "backward_entropy": 0.009398501366376877,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.7166852951049805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05470087751746178,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05794626474380493,
      "backward_entropy": 0.009412756562232972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.658570289611816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05472904071211815,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057896584272384644,
      "backward_entropy": 0.009424891322851181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5349769592285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05475703999400139,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057847450176874794,
      "backward_entropy": 0.009437325596809387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.733626842498779,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05478380620479584,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057803998390833534,
      "backward_entropy": 0.009439362585544587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.51344633102417,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05481220409274101,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05775324503580729,
      "backward_entropy": 0.009452440589666367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3612284660339355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05484037101268768,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.057703365882237755,
      "backward_entropy": 0.004531612619757652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.401264190673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05486645922064781,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057662700613339744,
      "backward_entropy": 0.009463571757078171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.407775402069092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05489270016551018,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05762078364690145,
      "backward_entropy": 0.00946761891245842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3048317432403564,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0549197643995285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05757512152194977,
      "backward_entropy": 0.009478657692670821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.273645401000977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05494580790400505,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05753381053606669,
      "backward_entropy": 0.009483703970909118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.210793972015381,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054971881210803986,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05749201774597168,
      "backward_entropy": 0.009491795301437378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1616833209991455,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0549972802400589,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057452638943990074,
      "backward_entropy": 0.009500609338283538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.149810791015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055021464824676514,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05741756161053976,
      "backward_entropy": 0.009510698914527892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.088618278503418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05504446104168892,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057386964559555054,
      "backward_entropy": 0.009519272297620774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.033377170562744,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055067919194698334,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05735393365224203,
      "backward_entropy": 0.009534313529729842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.931042671203613,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05509286746382713,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05731502175331116,
      "backward_entropy": 0.009551440179347993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.085416555404663,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05511990189552307,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05726770559946696,
      "backward_entropy": 0.009574420750141144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.064563512802124,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05514524504542351,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05722728371620178,
      "backward_entropy": 0.009589822590351104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9998505115509033,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05516905337572098,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05719307065010071,
      "backward_entropy": 0.009598439186811447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.907249927520752,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05519196391105652,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05716292063395182,
      "backward_entropy": 0.009598466008901596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.7068939208984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05521468073129654,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05713261663913727,
      "backward_entropy": 0.009606318175792694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.656536102294922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055238496512174606,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05709867179393768,
      "backward_entropy": 0.009607626497745514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9459894895553589,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05526319518685341,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.057062183817227684,
      "backward_entropy": 0.009601263701915741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9605293273925781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05528658255934715,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05703038970629374,
      "backward_entropy": 0.00959477573633194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7957236766815186,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05530843883752823,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05700487891832987,
      "backward_entropy": 0.004135402292013169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0076550245285034,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055329833179712296,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05698095758756002,
      "backward_entropy": 0.009564952552318573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.726593255996704,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055349696427583694,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056962390740712486,
      "backward_entropy": 0.009553001821041107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8453714847564697,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05536947399377823,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05694359540939331,
      "backward_entropy": 0.009546477347612381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9571841359138489,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05538853630423546,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056927079955736794,
      "backward_entropy": 0.00954342857003212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.362941741943359,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05540657415986061,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05691343545913696,
      "backward_entropy": 0.009548229724168777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.489703416824341,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05542571842670441,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05689568320910136,
      "backward_entropy": 0.009553053975105285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.120328426361084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055445220321416855,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056876689195632935,
      "backward_entropy": 0.009556376188993455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.569267988204956,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055466216057538986,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056852211554845176,
      "backward_entropy": 0.009559632837772369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.1812944412231445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055487167090177536,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05682705839474996,
      "backward_entropy": 0.009571298211812972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3357017040252686,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05550915002822876,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05679752926031748,
      "backward_entropy": 0.009589232504367828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3374133110046387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055531423538923264,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056766500075658165,
      "backward_entropy": 0.009611036628484726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.440167427062988,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05555351823568344,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05673673748970032,
      "backward_entropy": 0.0038906048983335497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6539033651351929,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05557800456881523,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05669806400934855,
      "backward_entropy": 0.009640839695930482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.548209190368652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05560149997472763,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05666234095891317,
      "backward_entropy": 0.00966375321149826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.411802053451538,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05562632530927658,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05662241578102112,
      "backward_entropy": 0.009678875654935836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3617513179779053,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055650293827056885,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05658569931983948,
      "backward_entropy": 0.00969165563583374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8543686270713806,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05567372962832451,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05655046304066976,
      "backward_entropy": 0.009708847105503082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7975592613220215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05569545924663544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056521132588386536,
      "backward_entropy": 0.00972646102309227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.867660403251648,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05571781098842621,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05648943285147349,
      "backward_entropy": 0.00974445641040802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2844927310943604,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055738210678100586,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05646530290444692,
      "backward_entropy": 0.009753652662038804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2615511417388916,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05575813353061676,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05644292632738749,
      "backward_entropy": 0.00976005420088768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2315728664398193,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055777620524168015,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056422059734662376,
      "backward_entropy": 0.00976426675915718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9195258617401123,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0557967908680439,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05640203754107157,
      "backward_entropy": 0.009768854826688766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.528319001197815,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05581609904766083,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05638161301612854,
      "backward_entropy": 0.009770998358726501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5295519828796387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05583418160676956,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05636624495188395,
      "backward_entropy": 0.009763660281896592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7946707010269165,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05585325509309769,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056346784035364784,
      "backward_entropy": 0.009761325269937515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4892864227294922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05587087199091911,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05633249878883362,
      "backward_entropy": 0.009757197648286819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.448132872581482,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055887360125780106,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056322917342185974,
      "backward_entropy": 0.009742428362369538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.405953884124756,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05590316653251648,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056315730015436806,
      "backward_entropy": 0.009727176278829575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.390368103981018,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05592009425163269,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05630406737327576,
      "backward_entropy": 0.009716439992189407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7217392921447754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05593670532107353,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05629261831442515,
      "backward_entropy": 0.009715791046619415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.370895266532898,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05595346540212631,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.056281069914499916,
      "backward_entropy": 0.1382564663887024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.276716709136963,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05596984922885895,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056270221869150795,
      "backward_entropy": 0.00970890074968338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.261709690093994,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05598736181855202,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05625482896963755,
      "backward_entropy": 0.009714589267969132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9748777151107788,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05600564554333687,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05623667438824972,
      "backward_entropy": 0.009718628972768784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1746554374694824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056023646146059036,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05621946851412455,
      "backward_entropy": 0.009722666442394256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1516144275665283,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056042663753032684,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05619800090789795,
      "backward_entropy": 0.009733716398477555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5203235149383545,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056062426418066025,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05617366234461466,
      "backward_entropy": 0.009746882319450378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7021681666374207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056082215160131454,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.056149293979008995,
      "backward_entropy": 0.009758306294679641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.694915235042572,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05610043555498123,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05613060792287191,
      "backward_entropy": 0.009765854477882386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4424357414245605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05611724406480789,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05611705780029297,
      "backward_entropy": 0.00977003276348114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0106050968170166,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05613434687256813,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05610238512357076,
      "backward_entropy": 0.009773547947406768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5503313541412354,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05615203082561493,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.05608590443929037,
      "backward_entropy": 0.13825652599334717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2165464162826538,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05617076903581619,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05606578787167867,
      "backward_entropy": 0.003340528905391693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4771320819854736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05618881806731224,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05604755878448486,
      "backward_entropy": 0.009770426154136657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1945265531539917,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056207794696092606,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05602645377318064,
      "backward_entropy": 0.003305337578058243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9588053226470947,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0562259778380394,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05600776274998983,
      "backward_entropy": 0.0032883815467357634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1762810945510864,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056245364248752594,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05598542590936025,
      "backward_entropy": 0.009758807718753815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3100204467773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056263770908117294,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055966357390085854,
      "backward_entropy": 0.009750916808843612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.06951497495174408,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05628318339586258,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05594368278980255,
      "backward_entropy": 0.00974370688199997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6571638584136963,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05630100890994072,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05592587093512217,
      "backward_entropy": 0.009746641665697098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5999215841293335,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056318528950214386,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05590895811716715,
      "backward_entropy": 0.009751105308532714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1079269647598267,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056334882974624634,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05589585999647776,
      "backward_entropy": 0.009757800400257111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6206727027893066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056350626051425934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05588465929031372,
      "backward_entropy": 0.009765946865081787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0853251218795776,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056367214769124985,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05587032934029897,
      "backward_entropy": 0.009775449335575104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0683083534240723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0563831627368927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05585801601409912,
      "backward_entropy": 0.009786088764667512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5493748188018799,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05639947950839996,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05584414303302765,
      "backward_entropy": 0.0031347203999757766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.540850281715393,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05641574785113335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055830041567484535,
      "backward_entropy": 0.009814874082803727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5612130165100098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05643182620406151,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055816431840260826,
      "backward_entropy": 0.009831073135137558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4992656707763672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05644679069519043,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05580660700798035,
      "backward_entropy": 0.00984623059630394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.441514253616333,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0564618855714798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05579577883084615,
      "backward_entropy": 0.009866826236248016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4656732082366943,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056477807462215424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05578188598155975,
      "backward_entropy": 0.009887434542179108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4182980060577393,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056493811309337616,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05576701958974203,
      "backward_entropy": 0.009914641827344894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9932630658149719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05651004612445831,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05575212836265564,
      "backward_entropy": 0.003075243905186653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4236328601837158,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056525543332099915,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05573971072832743,
      "backward_entropy": 0.009938035160303116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.401523470878601,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05654106289148331,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055726587772369385,
      "backward_entropy": 0.009954296052455902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.947998583316803,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05655672773718834,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055712148547172546,
      "backward_entropy": 0.009979000687599182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.38766348361969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05657195672392845,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05569870273272196,
      "backward_entropy": 0.01000787764787674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2488200664520264,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05658706650137901,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05568540096282959,
      "backward_entropy": 0.010035918653011322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.799975037574768,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05660288408398628,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055669585863749184,
      "backward_entropy": 0.01006178855895996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9009568095207214,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05661877989768982,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05565356214841207,
      "backward_entropy": 0.01008211523294449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8990737795829773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05663429945707321,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.055638005336125694,
      "backward_entropy": 0.0030271394178271295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7329208850860596,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056649334728717804,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055623615781466164,
      "backward_entropy": 0.01014002487063408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8844862580299377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05666467174887657,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05560790499051412,
      "backward_entropy": 0.010169301182031631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5157485008239746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05667944997549057,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05559375882148743,
      "backward_entropy": 0.010198535025119781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6748980283737183,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05669540539383888,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05557522177696228,
      "backward_entropy": 0.010227078199386596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8606405854225159,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0567115843296051,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05555561184883118,
      "backward_entropy": 0.010255721211433411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6684702634811401,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05672701448202133,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055538361271222435,
      "backward_entropy": 0.010281737148761749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.45764246582984924,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056742239743471146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05552257100741068,
      "backward_entropy": 0.010295010358095168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6205111742019653,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05675625801086426,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055510823925336204,
      "backward_entropy": 0.010304028540849686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8330777287483215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056770388036966324,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055498833457628884,
      "backward_entropy": 0.010307222604751587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3532207012176514,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05678381770849228,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05548924207687378,
      "backward_entropy": 0.010306335985660553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5808333158493042,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05679813399910927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0554769237836202,
      "backward_entropy": 0.010298573970794677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.170286774635315,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056812334805727005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055465454856554665,
      "backward_entropy": 0.010282377898693084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7848663330078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056826356798410416,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055454189578692116,
      "backward_entropy": 0.010268034040927887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.237671136856079,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05683989077806473,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05544408162434896,
      "backward_entropy": 0.010257681459188461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8684511184692383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056854479014873505,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055430278182029724,
      "backward_entropy": 0.010247041285037995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8184043169021606,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05686931684613228,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05541602770487467,
      "backward_entropy": 0.010228271782398223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4975316524505615,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05688486993312836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05539871255556742,
      "backward_entropy": 0.010216654092073441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.754253089427948,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05690182000398636,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05537621180216471,
      "backward_entropy": 0.010210732370615006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0918757915496826,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056917704641819,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05535762012004852,
      "backward_entropy": 0.010200542211532593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3863677978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05693449452519417,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05533559123675028,
      "backward_entropy": 0.010194652527570725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.380443811416626,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05695140361785889,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05531264344851176,
      "backward_entropy": 0.0101955346763134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7002764344215393,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056967124342918396,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055293540159861244,
      "backward_entropy": 0.010198882222175598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3384157419204712,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05698219686746597,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05527630945046743,
      "backward_entropy": 0.010205688327550888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.36675411462783813,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05699741095304489,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05525844295819601,
      "backward_entropy": 0.010214396566152573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6721566915512085,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057011596858501434,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055243879556655884,
      "backward_entropy": 0.010224741697311402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.597273588180542,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057025328278541565,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05523044864336649,
      "backward_entropy": 0.010239429771900177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.581018090248108,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0570397824048996,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05521425108114878,
      "backward_entropy": 0.010258036851882934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9556242227554321,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05705481395125389,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05519583821296692,
      "backward_entropy": 0.010278396308422089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2535334825515747,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0570695735514164,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055178165435791016,
      "backward_entropy": 0.01029941812157631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2380874156951904,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057084258645772934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05516099433104197,
      "backward_entropy": 0.01031511276960373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.684130907058716,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057098858058452606,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055144319931666054,
      "backward_entropy": 0.010325948148965836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4906328916549683,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057115186005830765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0551220973332723,
      "backward_entropy": 0.010331441462039948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8971030116081238,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057131651788949966,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055099502205848694,
      "backward_entropy": 0.01033405214548111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1704655885696411,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05714759975671768,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05507848163445791,
      "backward_entropy": 0.010336801409721375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1446857452392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0571632981300354,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.055058449506759644,
      "backward_entropy": 0.01033589467406273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5904240012168884,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05717894062399864,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05503834287325541,
      "backward_entropy": 0.0026677580550312995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.386698842048645,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057193730026483536,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05502102772394816,
      "backward_entropy": 0.002657279558479786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6254370212554932,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05720875784754753,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05500305195649465,
      "backward_entropy": 0.010336846113204956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3409255743026733,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057224441319704056,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05498275657494863,
      "backward_entropy": 0.010337289422750473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5606530904769897,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05724037066102028,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05496137340863546,
      "backward_entropy": 0.010339364409446716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3097963333129883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057255350053310394,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05494330326716105,
      "backward_entropy": 0.010339114069938659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.2878965437412262,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05727044865489006,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05492503444353739,
      "backward_entropy": 0.010335299372673034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5065935850143433,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05728452280163765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05490981042385101,
      "backward_entropy": 0.010335616767406464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5270286798477173,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05729934200644493,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05489182472229004,
      "backward_entropy": 0.002583744376897812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5198373198509216,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05731336772441864,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.05487644672393799,
      "backward_entropy": 0.13833754062652587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.28083935379981995,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057326674461364746,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05486339827378591,
      "backward_entropy": 0.010343582183122636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7332958579063416,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057338960468769073,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05485370755195618,
      "backward_entropy": 0.010344135016202927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.497804194688797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05735112726688385,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05484418570995331,
      "backward_entropy": 0.010347452759742738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7206336259841919,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05736280977725983,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05483607451121012,
      "backward_entropy": 0.0025362372398376465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9260285496711731,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05737431347370148,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054828494787216187,
      "backward_entropy": 0.010356176644563675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1381837129592896,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05738617852330208,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054819345474243164,
      "backward_entropy": 0.01036555990576744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3466423749923706,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05739861726760864,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05480795602003733,
      "backward_entropy": 0.010377468168735504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6876739859580994,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05741175636649132,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05479429165522257,
      "backward_entropy": 0.0025071771815419197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6809179186820984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05742448940873146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054782082637151085,
      "backward_entropy": 0.010394628345966338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.46017932891845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05743680149316788,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0547713836034139,
      "backward_entropy": 0.01039717048406601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.863372266292572,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057448480278253555,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054762755831082664,
      "backward_entropy": 0.010398095846176148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6503502130508423,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057460296899080276,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05475353201230367,
      "backward_entropy": 0.01039985716342926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.2411220520734787,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057471878826618195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05474506815274557,
      "backward_entropy": 0.010401389002799988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.03681117296218872,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057482607662677765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054739286502202354,
      "backward_entropy": 0.010402370244264603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6192290186882019,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05749232694506645,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05473662416140238,
      "backward_entropy": 0.010404688864946365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.4314593970775604,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057502225041389465,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054732879002889,
      "backward_entropy": 0.010412984341382981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.4188818037509918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05751162767410278,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0547309418519338,
      "backward_entropy": 0.010416269302368164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.21961276233196259,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0575207881629467,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05472955107688904,
      "backward_entropy": 0.010421401262283326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3549381494522095,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057529546320438385,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05472909410794576,
      "backward_entropy": 0.010431747138500213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5909624695777893,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05753961205482483,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054724156856536865,
      "backward_entropy": 0.010442141443490982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.39733272790908813,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057549700140953064,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054718861977259316,
      "backward_entropy": 0.010454607009887696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.3982069790363312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05755956470966339,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05471389492352804,
      "backward_entropy": 0.010470932722091675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9359659552574158,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057569075375795364,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.05471000075340271,
      "backward_entropy": 0.002392491512000561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.20831288397312164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0575791597366333,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.054704089959462486,
      "backward_entropy": 0.010500402003526688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.20942020416259766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057588621973991394,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.05470001697540283,
      "backward_entropy": 0.010515395551919937,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 1.2353780084848405,
    "avg_log_Z": -0.056928150095045565,
    "success_rate": 1.0,
    "avg_reward": 78.6,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.02,
      "1": 0.12,
      "2": 0.86
    },
    "avg_forward_entropy": 0.05531613414486248,
    "avg_backward_entropy": 0.011879424260929227,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}