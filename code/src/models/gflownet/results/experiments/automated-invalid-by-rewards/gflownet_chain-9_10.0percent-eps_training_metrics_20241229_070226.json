{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07696053716871473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07697655094994439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.10260009765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980520248413086,
      "backward_entropy": 0.07698466380437215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.43304443359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00010000001202570274,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980212688446045,
      "backward_entropy": 0.07696310016844007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.04771423339844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00020012189634144306,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097989559173584,
      "backward_entropy": 0.07696559694078234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.37353515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0002998841810040176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10979571342468261,
      "backward_entropy": 0.07698653803931342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.34359741210938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0004000056069344282,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10979237556457519,
      "backward_entropy": 0.0769703918033176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.39022827148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005003479891456664,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978891849517822,
      "backward_entropy": 0.0769821736547682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.3944091796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0006001120782457292,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978538990020752,
      "backward_entropy": 0.07698326640658909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.25428771972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.000697591807693243,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109781813621521,
      "backward_entropy": 0.0769770277871026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.94854736328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0007960305665619671,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977811813354492,
      "backward_entropy": 0.07698531283272637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.95945739746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0008953238138929009,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097743034362793,
      "backward_entropy": 0.07698982291751438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.5015106201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0009956281865015626,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977036952972412,
      "backward_entropy": 0.07698733276791042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.94818115234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0010967570124194026,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10976629257202149,
      "backward_entropy": 0.07698497507307264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.86167907714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0011972846696153283,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976213216781616,
      "backward_entropy": 0.07699144548839992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.4031219482422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0012985649518668652,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975784063339233,
      "backward_entropy": 0.07698853810628255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.79576110839844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0014004958793520927,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975341796875,
      "backward_entropy": 0.07699020703633626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.91062927246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0015028993366286159,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10974887609481812,
      "backward_entropy": 0.07699183622996013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.12818908691406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0016057866159826517,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10974420309066772,
      "backward_entropy": 0.07699339257346259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.16293334960938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0017084230203181505,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10973938703536987,
      "backward_entropy": 0.07699487606684367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 258.4437255859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018121206667274237,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097344160079956,
      "backward_entropy": 0.07699488931232029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.97531127929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0019172359025105834,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10972929000854492,
      "backward_entropy": 0.07699576351377699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.0532684326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0020199890714138746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10972433090209961,
      "backward_entropy": 0.0769965714878506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.8229522705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002123728860169649,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971922874450683,
      "backward_entropy": 0.07699625359641181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 259.1423645019531,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0022267699241638184,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10971403121948242,
      "backward_entropy": 0.07700155840979682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.33380126953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0023312873672693968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10970865488052368,
      "backward_entropy": 0.07699729998906453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.99966430664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0024350008461624384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097032904624939,
      "backward_entropy": 0.07699780331717597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.15354919433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0025371478404849768,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969791412353516,
      "backward_entropy": 0.07699824704064263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.37477111816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002640177495777607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969239473342896,
      "backward_entropy": 0.07699866427315606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.37466430664062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0027422159910202026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10968680381774902,
      "backward_entropy": 0.07700191603766547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.60498046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002845393493771553,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10968106985092163,
      "backward_entropy": 0.07700255182054308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.43785095214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0029478068463504314,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1096752405166626,
      "backward_entropy": 0.07700314786699083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.9809875488281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0030504583846777678,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966928005218506,
      "backward_entropy": 0.0770089758767022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.65594482421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003154843347147107,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966321229934692,
      "backward_entropy": 0.07700966464148627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.9140167236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003259959165006876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965707302093505,
      "backward_entropy": 0.07700068420834011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.5935516357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0033633036073297262,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965087413787841,
      "backward_entropy": 0.07700096236334907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 259.1080017089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0034659055527299643,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10964457988739014,
      "backward_entropy": 0.07701145278082953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.9775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003570063039660454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10963810682296753,
      "backward_entropy": 0.07700147893693712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.78106689453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0036714747548103333,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10963163375854493,
      "backward_entropy": 0.0770065254635281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 273.9816589355469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0037721102125942707,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10962507724761963,
      "backward_entropy": 0.07700688309139675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.89718627929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0038755328860133886,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10961825847625732,
      "backward_entropy": 0.0770072407192654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.45864868164062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003976223058998585,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10961143970489502,
      "backward_entropy": 0.07701359854804145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.91304016113281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004073279909789562,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10960469245910645,
      "backward_entropy": 0.07700785663392809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.41099548339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0041655185632407665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10959799289703369,
      "backward_entropy": 0.07700243923399183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.16676330566406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004254986997693777,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10959134101867676,
      "backward_entropy": 0.07701441976759169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.53831481933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004345097579061985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095845341682434,
      "backward_entropy": 0.07700243261125353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.16744995117188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004436919931322336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10957751274108887,
      "backward_entropy": 0.07700240612030029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.31613159179688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004527173936367035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10957045555114746,
      "backward_entropy": 0.07700233989291722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.11497497558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004614862613379955,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10956341028213501,
      "backward_entropy": 0.07700893614027235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.98553466796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004701443016529083,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10955629348754883,
      "backward_entropy": 0.07701528734631008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.79998779296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004786123987287283,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954915285110474,
      "backward_entropy": 0.07701539331012303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.37840270996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004873201251029968,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954174995422364,
      "backward_entropy": 0.07700918780432807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.3046417236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0049612256698310375,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953419208526612,
      "backward_entropy": 0.07700925403171116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.22055053710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005051376763731241,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952637195587159,
      "backward_entropy": 0.07700126700931126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.51797485351562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005141273606568575,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1095184326171875,
      "backward_entropy": 0.07701573769251506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.07080078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005232865922152996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10951027870178223,
      "backward_entropy": 0.07700055837631226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.273681640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005323157645761967,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950202941894531,
      "backward_entropy": 0.07700013452106053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.09803771972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005413382314145565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10949363708496093,
      "backward_entropy": 0.07699967755211724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.94662475585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005503382068127394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948508977890015,
      "backward_entropy": 0.07700942622290717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.89036560058594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005595221184194088,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094762921333313,
      "backward_entropy": 0.07701598273383246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.8649444580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005686495453119278,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10946738719940186,
      "backward_entropy": 0.07700937324100071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.54331970214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005777898244559765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945827960968017,
      "backward_entropy": 0.0769974258210924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.65574645996094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00586906960234046,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10944905281066894,
      "backward_entropy": 0.07701604896121556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.62651824951172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005960224196314812,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094396471977234,
      "backward_entropy": 0.07700922754075792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.17066955566406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006046582013368607,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10943039655685424,
      "backward_entropy": 0.07701607545216878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.2190704345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006131186615675688,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10942108631134033,
      "backward_entropy": 0.07700902885860866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.8521728515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0062127485871315,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10941182374954224,
      "backward_entropy": 0.07700890302658081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.54157257080078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006290857680141926,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10940264463424683,
      "backward_entropy": 0.07701607545216878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.3227081298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006365451030433178,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1093935489654541,
      "backward_entropy": 0.0770085718896654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.2388916015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0064417277462780476,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938417911529541,
      "backward_entropy": 0.07700839307573107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.88348388671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006518546957522631,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10937459468841552,
      "backward_entropy": 0.07701604896121556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.1806640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006597924046218395,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10936462879180908,
      "backward_entropy": 0.0769881076282925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.57752990722656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006677552126348019,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935425758361816,
      "backward_entropy": 0.07701602909300062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.58853149414062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006757325027137995,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10934362411499024,
      "backward_entropy": 0.0770160092247857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.14524841308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0068382378667593,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10933266878128052,
      "backward_entropy": 0.07700742615593804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 256.2472839355469,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006916749756783247,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10932167768478393,
      "backward_entropy": 0.07701596948835585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.235107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007001142017543316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10930999517440795,
      "backward_entropy": 0.07698134581247966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.69735717773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0070874630473554134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10929793119430542,
      "backward_entropy": 0.0769798755645752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.47389221191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0071729738265275955,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10928572416305542,
      "backward_entropy": 0.07701591650644939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.00653076171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007261648774147034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1092730164527893,
      "backward_entropy": 0.07697688208685981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.7178955078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007347141392529011,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10926034450531005,
      "backward_entropy": 0.07697538534800212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.03228759765625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0074311126954853535,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10924758911132812,
      "backward_entropy": 0.07701583041085137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.00543212890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00751712778583169,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10923441648483276,
      "backward_entropy": 0.07697225941552056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.67884826660156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0076059396378695965,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10922077894210816,
      "backward_entropy": 0.07700521416134304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.33111572265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007697523105889559,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10920664072036743,
      "backward_entropy": 0.07701575093799168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.57940673828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007790239527821541,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10919216871261597,
      "backward_entropy": 0.07700467109680176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.79827880859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00788435060530901,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10917731523513793,
      "backward_entropy": 0.07696570290459527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.90386962890625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007975761778652668,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10916246175765991,
      "backward_entropy": 0.07701567146513197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.7248077392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008067958056926727,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1091472864151001,
      "backward_entropy": 0.07696208688947889,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.53208923339844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008161342702805996,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10913174152374268,
      "backward_entropy": 0.07701560523774889,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.63218688964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008255599066615105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1091158390045166,
      "backward_entropy": 0.07700281673007542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.91539001464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008350828662514687,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10909957885742187,
      "backward_entropy": 0.07695611980226305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.30397033691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008441902697086334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10908346176147461,
      "backward_entropy": 0.07695386144849989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.36849975585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008530895225703716,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10906730890274048,
      "backward_entropy": 0.07695151699913873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.63027954101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008620264008641243,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10905067920684815,
      "backward_entropy": 0.07694910632239448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.2827606201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008701300248503685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10903455018997192,
      "backward_entropy": 0.07694647709528606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.29148864746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008783555589616299,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10901792049407959,
      "backward_entropy": 0.07699967755211724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.41664123535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00886412151157856,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10900110006332397,
      "backward_entropy": 0.07699888282352024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.61872100830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008948401547968388,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10898308753967285,
      "backward_entropy": 0.07699812783135308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.00209045410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009027251973748207,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10896511077880859,
      "backward_entropy": 0.07693517870373195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.71058654785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009106348268687725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10894613265991211,
      "backward_entropy": 0.07693205277125041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.24371337890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009184204041957855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10892668962478638,
      "backward_entropy": 0.07692873477935791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.90121459960938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009261063300073147,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10890681743621826,
      "backward_entropy": 0.07701459858152601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.4204559326172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009340712800621986,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10888603925704957,
      "backward_entropy": 0.07701444625854492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.42982482910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009422967210412025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1088644027709961,
      "backward_entropy": 0.0769180523024665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.64381408691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0095065301284194,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10884208679199218,
      "backward_entropy": 0.07691444291008843,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.30048370361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009588651359081268,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10881965160369873,
      "backward_entropy": 0.07699018054538304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 289.9433288574219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009664222598075867,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10879795551300049,
      "backward_entropy": 0.07698892884784275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.92465209960938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009748109616339207,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1087746262550354,
      "backward_entropy": 0.07698776986863878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.79725646972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009835680946707726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10875029563903808,
      "backward_entropy": 0.07689842912885878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.58558654785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009922340512275696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10872564315795899,
      "backward_entropy": 0.07698551813761394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.09878540039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010010984726250172,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10870022773742676,
      "backward_entropy": 0.07689015070597331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.05121612548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010101508349180222,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1086740493774414,
      "backward_entropy": 0.07688609759012859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.80023193359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010186530649662018,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10864835977554321,
      "backward_entropy": 0.07698203457726373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.0319366455078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01026877760887146,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10862267017364502,
      "backward_entropy": 0.0770126117600335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.25487518310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010353099554777145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10859614610671997,
      "backward_entropy": 0.07697933912277222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.94036865234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01043352484703064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10856983661651612,
      "backward_entropy": 0.07697786225212945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.90216064453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010516964830458164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1085425853729248,
      "backward_entropy": 0.07686285177866618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.48606872558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010604331269860268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10851418972015381,
      "backward_entropy": 0.07685849401685926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.95587158203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010692319832742214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1084851861000061,
      "backward_entropy": 0.07685399717754787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.78709411621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010777142830193043,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.108456289768219,
      "backward_entropy": 0.07684916920132107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.0275115966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010862765833735466,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1084267258644104,
      "backward_entropy": 0.07697064346737331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.08456420898438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01094826776534319,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10839672088623047,
      "backward_entropy": 0.07701114813486735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.01576232910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011029216460883617,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10836708545684814,
      "backward_entropy": 0.07696725262535943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.1444854736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011113811284303665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1083362340927124,
      "backward_entropy": 0.07682753933800592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.19375610351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011195487342774868,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10830544233322144,
      "backward_entropy": 0.07682139343685573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.91644287109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011279916390776634,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10827362537384033,
      "backward_entropy": 0.07701026731067234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.72044372558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011363302357494831,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10824151039123535,
      "backward_entropy": 0.07701008849673802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 253.1613311767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011450113728642464,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10820808410644531,
      "backward_entropy": 0.07695757018195258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.82586669921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011542530730366707,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10817289352416992,
      "backward_entropy": 0.0767977237701416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.5374755859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01163383200764656,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10813734531402588,
      "backward_entropy": 0.07679168383280437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.744873046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011723172850906849,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10810161828994751,
      "backward_entropy": 0.07678540547688802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.31719970703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011810433119535446,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10806578397750854,
      "backward_entropy": 0.07694921228620741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.5372543334961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011894626542925835,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10803015232086181,
      "backward_entropy": 0.07694670226838854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.60336303710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011973616667091846,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10799521207809448,
      "backward_entropy": 0.07676398754119873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.02401733398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012054416351020336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10795927047729492,
      "backward_entropy": 0.07675646411048041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.94863891601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012135443277657032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10792267322540283,
      "backward_entropy": 0.07674871550665961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.63322448730469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01221382524818182,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1078860878944397,
      "backward_entropy": 0.07693492703967625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.01146697998047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012287243269383907,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10785014629364013,
      "backward_entropy": 0.07673112551371257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.35198974609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012356070801615715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1078147530555725,
      "backward_entropy": 0.07672113842434353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.6694107055664,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012426505796611309,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10777838230133056,
      "backward_entropy": 0.0770064393679301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 236.923095703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012492991983890533,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10774247646331787,
      "backward_entropy": 0.07691793309317695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.68397521972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012566049583256245,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10770422220230103,
      "backward_entropy": 0.07691359519958496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.201045989990234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012644005008041859,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10766347646713256,
      "backward_entropy": 0.07700468434227838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.42640686035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012713585048913956,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10762404203414917,
      "backward_entropy": 0.07700403531392415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.79771423339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012782258912920952,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10758407115936279,
      "backward_entropy": 0.07665930853949653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.14613342285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012852372601628304,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10754282474517822,
      "backward_entropy": 0.07689502504136828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.40679931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012923681177198887,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10750035047531128,
      "backward_entropy": 0.076637069384257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.30970764160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012996519915759563,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10745666027069092,
      "backward_entropy": 0.07662592993842231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.70115661621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01306917890906334,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10741219520568848,
      "backward_entropy": 0.07700104183620876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.32818603515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01314469613134861,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10736615657806396,
      "backward_entropy": 0.07687485218048096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.5229949951172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01321842335164547,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10731984376907348,
      "backward_entropy": 0.0768695871035258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.3695297241211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013291794806718826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1072728157043457,
      "backward_entropy": 0.07686407698525323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.38365173339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013362232595682144,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10722591876983642,
      "backward_entropy": 0.07685812314351399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.0246124267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013434250839054585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10717774629592895,
      "backward_entropy": 0.0765533447265625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 233.90455627441406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013509820215404034,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1071274995803833,
      "backward_entropy": 0.07699771722157796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.19732666015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013591630384325981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10707447528839112,
      "backward_entropy": 0.07652791341145833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.67669677734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013674863614141941,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10702011585235596,
      "backward_entropy": 0.07683528794182672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.83982849121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013756925240159035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10696530342102051,
      "backward_entropy": 0.07682953940497504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.5317840576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01384017989039421,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10690913200378419,
      "backward_entropy": 0.07648897171020508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.69196319580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013925835490226746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1068511962890625,
      "backward_entropy": 0.07647524939643012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.40015411376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01400609128177166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10679438114166259,
      "backward_entropy": 0.07646014955308703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.34539794921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01408182829618454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1067385196685791,
      "backward_entropy": 0.07644409603542751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.94510650634766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014161546714603901,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10668030977249146,
      "backward_entropy": 0.07642886373731825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.79071044921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01423528790473938,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10662333965301514,
      "backward_entropy": 0.07699374357859294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.38605499267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014312789775431156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1065639853477478,
      "backward_entropy": 0.07639604806900024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.97120666503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014386748895049095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10650491714477539,
      "backward_entropy": 0.07637841171688503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.66485595703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014460517093539238,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10644497871398925,
      "backward_entropy": 0.07699144548839992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.21810913085938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014538299292325974,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10638256072998047,
      "backward_entropy": 0.07699082295099895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.3311767578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01461540162563324,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10631861686706542,
      "backward_entropy": 0.07674517896440294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.45552062988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014694957062602043,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10625240802764893,
      "backward_entropy": 0.07630748218960232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.1603775024414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014770668931305408,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1061863660812378,
      "backward_entropy": 0.07628817028469509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.00100708007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014843269251286983,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10612044334411622,
      "backward_entropy": 0.07626743449105157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.66088104248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014914123341441154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10605397224426269,
      "backward_entropy": 0.07624473836686876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.46664428710938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0149795301258564,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10598886013031006,
      "backward_entropy": 0.07669255468580458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.61880493164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015041682869195938,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10592424869537354,
      "backward_entropy": 0.07619439231024848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.09607696533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015104790218174458,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10585803985595703,
      "backward_entropy": 0.07616817951202393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.80088806152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015163333155214787,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.105793035030365,
      "backward_entropy": 0.07698149151272243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.00633239746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015221805311739445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10572693347930909,
      "backward_entropy": 0.07611166106330024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.1660385131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015283041633665562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10565841197967529,
      "backward_entropy": 0.07608304421106975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.62411499023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015341461636126041,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10559040307998657,
      "backward_entropy": 0.07605326837963527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.21090698242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015402446500957012,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10551991462707519,
      "backward_entropy": 0.0765913791126675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.05067443847656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01545763947069645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10545152425765991,
      "backward_entropy": 0.07657452424367268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.12086486816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015520280227065086,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10537847280502319,
      "backward_entropy": 0.076558960808648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.60279846191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015584136359393597,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10530387163162232,
      "backward_entropy": 0.07697017987569173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 231.45272827148438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015654318034648895,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10522500276565552,
      "backward_entropy": 0.07696935203340319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.8053741455078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01573139987885952,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10514156818389893,
      "backward_entropy": 0.07696904076470269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.80070495605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015809517353773117,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10505660772323608,
      "backward_entropy": 0.0765017138587104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.58071899414062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01588471420109272,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10497241020202637,
      "backward_entropy": 0.07696834537718031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.519775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01596115529537201,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10488651990890503,
      "backward_entropy": 0.07578743828667535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 213.15879821777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016036860644817352,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10479972362518311,
      "backward_entropy": 0.07575613922542995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.07887268066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01611812599003315,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1047091007232666,
      "backward_entropy": 0.0764419502682156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.49485778808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016202550381422043,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10461562871932983,
      "backward_entropy": 0.0756983028517829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.89976501464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016284961253404617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10452253818511963,
      "backward_entropy": 0.07566845417022705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.77500915527344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01636696606874466,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10442887544631958,
      "backward_entropy": 0.07696703407499525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.43780517578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016448035836219788,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10433456897735596,
      "backward_entropy": 0.07696674929724799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.8122100830078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01653139293193817,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10423800945281983,
      "backward_entropy": 0.07696663008795844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.93531799316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016614850610494614,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10413966178894044,
      "backward_entropy": 0.0763433641857571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.7646942138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01669955998659134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10403883457183838,
      "backward_entropy": 0.07550685935550266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.99711608886719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01678447052836418,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10393657684326171,
      "backward_entropy": 0.0763062768512302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.50965881347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016860056668519974,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10383940935134887,
      "backward_entropy": 0.0769642326566908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.40496826171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01693677343428135,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10374022722244262,
      "backward_entropy": 0.0769629610909356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.63760375976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01701601967215538,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10363826751708985,
      "backward_entropy": 0.0762373407681783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.55551147460938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017090363427996635,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1035394549369812,
      "backward_entropy": 0.076960113313463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.67184448242188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017160290852189064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10344231128692627,
      "backward_entropy": 0.07525551981396145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.9960479736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017229583114385605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10334455966949463,
      "backward_entropy": 0.07520706123775905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.0279998779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01730486936867237,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10323991775512695,
      "backward_entropy": 0.07612382041083442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.78888702392578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017378520220518112,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10313466787338257,
      "backward_entropy": 0.07609395186106364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.3285675048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017449509352445602,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10302979946136474,
      "backward_entropy": 0.07506038082970513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.61550903320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017524605616927147,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1029202938079834,
      "backward_entropy": 0.0750088029437595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.56932830810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01759255863726139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10281486511230468,
      "backward_entropy": 0.07495276133219402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.6884765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017656860873103142,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1027107834815979,
      "backward_entropy": 0.07595747047000462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.27305603027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01772633008658886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10260162353515626,
      "backward_entropy": 0.07483587000105116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.97103881835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017802858725190163,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10248596668243408,
      "backward_entropy": 0.07478001382615831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.53111267089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017883388325572014,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10236632823944092,
      "backward_entropy": 0.0758507317966885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.22430419921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017966115847229958,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10224400758743286,
      "backward_entropy": 0.07467138767242432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.7246551513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01804671809077263,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10212225914001465,
      "backward_entropy": 0.0757798088921441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.20574951171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01812671311199665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10199985504150391,
      "backward_entropy": 0.07455698649088542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.2757568359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018205100670456886,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10187786817550659,
      "backward_entropy": 0.07692618502510919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.97876739501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018284643068909645,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10175397396087646,
      "backward_entropy": 0.07443842622968885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.78948974609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018358435481786728,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10163404941558837,
      "backward_entropy": 0.07437532477908665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.66828918457031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01843244396150112,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10151270627975464,
      "backward_entropy": 0.0769182840983073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.5259246826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01850186660885811,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10139365196228027,
      "backward_entropy": 0.07552843623691136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.17774200439453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01857573911547661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10126967430114746,
      "backward_entropy": 0.07417349020640056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.1077880859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018647560849785805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10114697217941285,
      "backward_entropy": 0.07410424285464817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.72068786621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018718387931585312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10102406740188599,
      "backward_entropy": 0.07403296894497341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.9191665649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018789878115057945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10089973211288453,
      "backward_entropy": 0.07396137714385986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.49099731445312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018855944275856018,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10077900886535644,
      "backward_entropy": 0.07388411627875434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.90689086914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018921371549367905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10065755844116211,
      "backward_entropy": 0.07380433877309163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.79508972167969,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018984898924827576,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10053666830062866,
      "backward_entropy": 0.07689083947075738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.39073181152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0190467219799757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10041617155075074,
      "backward_entropy": 0.07363514105478923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.1978759765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019102972000837326,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10030004978179932,
      "backward_entropy": 0.07504589690102471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.92710876464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019158372655510902,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10018380880355834,
      "backward_entropy": 0.07498013973236084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.5870361328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019217142835259438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10006321668624878,
      "backward_entropy": 0.07335183355543348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.24423217773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019273478537797928,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09994410276412964,
      "backward_entropy": 0.0732530156771342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.81259155273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01933032087981701,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09982353448867798,
      "backward_entropy": 0.0747789806789822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.60989379882812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019394608214497566,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09969521760940551,
      "backward_entropy": 0.07471158107121785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.84844970703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01945873349905014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09956629276275634,
      "backward_entropy": 0.07294874721103245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.8613739013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01952260173857212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09943671226501465,
      "backward_entropy": 0.07284413443671332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.24851989746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019589385017752647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09930391311645508,
      "backward_entropy": 0.07274129655626085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.4713897705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019656874239444733,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09916936159133911,
      "backward_entropy": 0.07263538572523329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.6416015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019729435443878174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09902939796447754,
      "backward_entropy": 0.07253214385774401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.88190460205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019799694418907166,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09889112710952759,
      "backward_entropy": 0.07429969310760498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.99253845214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01986776478588581,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09875438809394836,
      "backward_entropy": 0.07422728008694118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.90093994140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019935214892029762,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09861726760864258,
      "backward_entropy": 0.07415268156263563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.1080780029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02000800520181656,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09847439527511596,
      "backward_entropy": 0.07408179177178277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.19561767578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020082369446754456,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09832915663719177,
      "backward_entropy": 0.07400982909732395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.6173210144043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02015634998679161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09818294048309326,
      "backward_entropy": 0.07184126642015246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.49385070800781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020221993327140808,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09804470539093017,
      "backward_entropy": 0.07681175072987874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.58995056152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02028297260403633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09791054725646972,
      "backward_entropy": 0.07156294584274292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.49041748046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020345287397503853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09777406454086304,
      "backward_entropy": 0.07366304927402073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.41839599609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02041218802332878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09763280153274537,
      "backward_entropy": 0.07127527395884196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.0865020751953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020478736609220505,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09749142527580261,
      "backward_entropy": 0.07347828812069362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.10377502441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02054944820702076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09734572172164917,
      "backward_entropy": 0.07098866171307033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.99851989746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020619435235857964,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09720032215118408,
      "backward_entropy": 0.07678013377719456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.05772399902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02069011703133583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09705368280410767,
      "backward_entropy": 0.0706954730881585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.65446472167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020763350650668144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09690449237823487,
      "backward_entropy": 0.07055091195636326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.16604614257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020832562819123268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0967586874961853,
      "backward_entropy": 0.0703990724351671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.65072631835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0208969134837389,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09661744832992554,
      "backward_entropy": 0.07023900747299194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.77690124511719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020961081609129906,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09647557735443116,
      "backward_entropy": 0.07675388124254015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.39207458496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021025333553552628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09633316993713378,
      "backward_entropy": 0.0699087381362915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.30770874023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021089067682623863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09619019031524659,
      "backward_entropy": 0.06973641448550755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.05120849609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021156612783670425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09604237675666809,
      "backward_entropy": 0.0724288887447781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.3603057861328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021223323419690132,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0958945631980896,
      "backward_entropy": 0.07231014304690891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.61163330078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021291116252541542,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09574534893035888,
      "backward_entropy": 0.07219021850162083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.90701293945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021365556865930557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09558924436569213,
      "backward_entropy": 0.06902987427181667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.9316635131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02143773064017296,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09543578624725342,
      "backward_entropy": 0.06885241799884373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.61146545410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02150909975171089,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09528310298919677,
      "backward_entropy": 0.06867051786846584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.32106018066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021580738946795464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09512946605682374,
      "backward_entropy": 0.06848283608754475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.94960021972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02165333926677704,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0949752151966095,
      "backward_entropy": 0.07157046927346124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.88168334960938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021731527522206306,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09481451511383057,
      "backward_entropy": 0.07669882641898261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.79181671142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021808084100484848,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09465541839599609,
      "backward_entropy": 0.07131045394473606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.57931518554688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02187737636268139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09450396299362182,
      "backward_entropy": 0.06770871745215522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.30679321289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021948739886283875,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09435007572174073,
      "backward_entropy": 0.07668293846978082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.09075927734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022019166499376297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09419704675674438,
      "backward_entropy": 0.06728373633490668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.29916381835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02209191396832466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09404193758964538,
      "backward_entropy": 0.06706938478681776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.2872772216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022160647436976433,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0938909411430359,
      "backward_entropy": 0.06684149636162652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.81002807617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022232001647353172,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09373773336410522,
      "backward_entropy": 0.07038884030448066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.31710815429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02230355143547058,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09358386993408203,
      "backward_entropy": 0.07022351688808864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.00371551513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022374529391527176,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09343111515045166,
      "backward_entropy": 0.07005400127834743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.96927642822266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022441569715738297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09328224658966064,
      "backward_entropy": 0.06590355104870266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.30699157714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022503457963466644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09313841462135315,
      "backward_entropy": 0.0656425886683994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.87692260742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022559329867362976,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09300073385238647,
      "backward_entropy": 0.06948118739657932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.02302551269531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02261337824165821,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09286555051803588,
      "backward_entropy": 0.0650869541698032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.900390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02266804873943329,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09272929430007934,
      "backward_entropy": 0.06906076272328694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.66957092285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02272055298089981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09259523749351502,
      "backward_entropy": 0.06450710031721327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.21530151367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022778399288654327,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09245538711547852,
      "backward_entropy": 0.06863012578752306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.85881042480469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022838113829493523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0923137128353119,
      "backward_entropy": 0.06393078962961833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.38842010498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022896913811564445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09217355251312256,
      "backward_entropy": 0.0636382367875841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.2272491455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02295614592730999,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0920331358909607,
      "backward_entropy": 0.06797909736633301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.25763702392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02301725558936596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09189108610153199,
      "backward_entropy": 0.06304579973220825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.81575012207031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023075491189956665,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09175212383270263,
      "backward_entropy": 0.07648332913716634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.98916625976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023133115842938423,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0916145920753479,
      "backward_entropy": 0.06242928902308146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.00292205810547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023187091574072838,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09148141145706176,
      "backward_entropy": 0.06703770160675049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.82850646972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023241795599460602,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09134728908538818,
      "backward_entropy": 0.06678556071387397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.12496566772461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023306196555495262,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09120417833328247,
      "backward_entropy": 0.06655452648798625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.62360382080078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02336575835943222,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09106838703155518,
      "backward_entropy": 0.06631029314464992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.02861785888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023422887548804283,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09093543291091918,
      "backward_entropy": 0.06605690717697144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.84321594238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023479443043470383,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09080359935760499,
      "backward_entropy": 0.060535311698913574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.1916732788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02353549376130104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09067286252975464,
      "backward_entropy": 0.06020835373136732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.16259002685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02359270304441452,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09054169058799744,
      "backward_entropy": 0.06526843706766765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.1737060546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02364948019385338,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09041174054145813,
      "backward_entropy": 0.06499848100874159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.60841369628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02370438724756241,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09028435349464417,
      "backward_entropy": 0.05922053919898139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.8733673095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02376614138484001,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09015070796012878,
      "backward_entropy": 0.05889929665459527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.14126968383789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02383146435022354,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09001463651657104,
      "backward_entropy": 0.0641971230506897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.56848907470703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02388971857726574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08988672494888306,
      "backward_entropy": 0.0639159811867608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.63534164428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02394496649503708,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08976325988769532,
      "backward_entropy": 0.05791335635715061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.26319122314453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023992883041501045,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08964838981628417,
      "backward_entropy": 0.07628546158472697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.53585815429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024040214717388153,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08953499794006348,
      "backward_entropy": 0.06299432781007555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.909385681152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02408781833946705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08942102193832398,
      "backward_entropy": 0.056838777330186635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.41200256347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024130025878548622,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08931255340576172,
      "backward_entropy": 0.06234170330895318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.02320861816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02417602576315403,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08920010328292846,
      "backward_entropy": 0.06201574537489149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.62364959716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02422591671347618,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08908456563949585,
      "backward_entropy": 0.06169914537005954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.1283416748047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024274615570902824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08897066712379456,
      "backward_entropy": 0.055354012383355036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.6429672241211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024328438565135002,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08885291814804078,
      "backward_entropy": 0.05499811967213949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.13247680664062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02438356727361679,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08873435258865356,
      "backward_entropy": 0.07616731193330553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.12356567382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02443999983370304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0886152744293213,
      "backward_entropy": 0.05428020159403483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.63652038574219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02449624612927437,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08849728107452393,
      "backward_entropy": 0.06011281410853068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.911808013916016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02455245703458786,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08838042020797729,
      "backward_entropy": 0.05978550513585409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.6971206665039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02460380271077156,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0882684826850891,
      "backward_entropy": 0.05943895710839166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.00932312011719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02465333789587021,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08816036581993103,
      "backward_entropy": 0.05908607112036811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.82540893554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02470146119594574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08805333971977233,
      "backward_entropy": 0.05872262848748101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.75267028808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02474832907319069,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08794732689857483,
      "backward_entropy": 0.05835268894831339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.81796264648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02479856088757515,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08783847093582153,
      "backward_entropy": 0.05157939592997233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.3011474609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024848060682415962,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08773147463798522,
      "backward_entropy": 0.05762562486860487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.9034194946289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024902286008000374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08762084245681763,
      "backward_entropy": 0.05727371904585096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.00242614746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024959638714790344,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08750873804092407,
      "backward_entropy": 0.07604717546039158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.615970611572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025022409856319427,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08739284276962281,
      "backward_entropy": 0.050033264689975314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.3584213256836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025078170001506805,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08728361129760742,
      "backward_entropy": 0.05623732672797309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.92724609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025136811658740044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08717304468154907,
      "backward_entropy": 0.04923222462336222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.69861602783203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025194982066750526,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08706377744674683,
      "backward_entropy": 0.055518918567233615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.46894836425781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02525274083018303,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08695573806762695,
      "backward_entropy": 0.07602343956629436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.65392303466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02531014196574688,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08684885501861572,
      "backward_entropy": 0.0479958520995246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.00764465332031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025365304201841354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08674639463424683,
      "backward_entropy": 0.0475804176595476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.1981201171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025415586307644844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08665047883987427,
      "backward_entropy": 0.053981890281041466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.5408935546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025465045124292374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08655595183372497,
      "backward_entropy": 0.04673831661542257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.10588836669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02551959455013275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08645813465118408,
      "backward_entropy": 0.046328607532713145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.60916137695312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025574298575520515,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08636094927787781,
      "backward_entropy": 0.05279206567340427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.91885375976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02562778815627098,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08626564145088196,
      "backward_entropy": 0.04549903008672926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.91748809814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02568339742720127,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0861701250076294,
      "backward_entropy": 0.04509151644176907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.95685577392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025738872587680817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08607512712478638,
      "backward_entropy": 0.04467619789971246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.76518249511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025794588029384613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08598098754882813,
      "backward_entropy": 0.04426109790802002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.1651153564453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025850996375083923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08588813543319702,
      "backward_entropy": 0.04385015699598524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.68820190429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025911549106240273,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0857927680015564,
      "backward_entropy": 0.043444163269466825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.7007064819336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025968335568904877,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08570019602775573,
      "backward_entropy": 0.043015748262405396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.70280456542969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026024039834737778,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08560984134674073,
      "backward_entropy": 0.04972042308913337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.2563705444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026074884459376335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08552278280258178,
      "backward_entropy": 0.04213588767581516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.55030059814453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026126183569431305,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08543592691421509,
      "backward_entropy": 0.04888454410764906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.052146911621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02617677114903927,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08535067439079284,
      "backward_entropy": 0.04122391011979845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.84020233154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026223642751574516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08526878952980041,
      "backward_entropy": 0.04075374868181017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.88864135742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026273133233189583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0851860761642456,
      "backward_entropy": 0.04029431607988146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.34652709960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0263203214854002,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08510534763336182,
      "backward_entropy": 0.03982467783821954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.28671264648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026370931416749954,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08502230644226075,
      "backward_entropy": 0.046749644809299044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.67622375488281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02641928754746914,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.084941565990448,
      "backward_entropy": 0.04631830586327447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.68768310546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02646973915398121,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08485981225967407,
      "backward_entropy": 0.03841563728120592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.482421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026518112048506737,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08478045463562012,
      "backward_entropy": 0.03794254197014703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.28093719482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026572005823254585,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08469903469085693,
      "backward_entropy": 0.045068687862820096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.90983581542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02662602998316288,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0846179485321045,
      "backward_entropy": 0.037037637498643666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.3508529663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02668323926627636,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08453574180603027,
      "backward_entropy": 0.044267624616622925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.4817123413086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026740510016679764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08445451259613038,
      "backward_entropy": 0.043870617945988975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.9561767578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02680056169629097,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08437243103981018,
      "backward_entropy": 0.07563450601365831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.15755462646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02686324156820774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08428999185562133,
      "backward_entropy": 0.03527325060632494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.28638458251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026925712823867798,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08420925736427307,
      "backward_entropy": 0.03484433889389038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.01360321044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026986662298440933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08413090705871581,
      "backward_entropy": 0.03441684775882297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.93624877929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02704739384353161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08405369520187378,
      "backward_entropy": 0.03398999240663317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.99081420898438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027107838541269302,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08397748470306396,
      "backward_entropy": 0.033561226394441396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.315673828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027169503271579742,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08390152454376221,
      "backward_entropy": 0.04117806752522787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.19420623779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027232499793171883,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08382619619369507,
      "backward_entropy": 0.0407986409134335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.47537231445312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027295585721731186,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08375265598297119,
      "backward_entropy": 0.0756207439634535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.433128356933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027358444407582283,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08368031978607178,
      "backward_entropy": 0.03190873397721185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.40846252441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02741423435509205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.083613121509552,
      "backward_entropy": 0.03148676951726278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.23511505126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027469323948025703,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08354747891426087,
      "backward_entropy": 0.03923833701345655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.9592514038086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027523156255483627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08348250389099121,
      "backward_entropy": 0.030639433198504977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.653968811035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02757653035223484,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0834189772605896,
      "backward_entropy": 0.03021656142340766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.35854721069336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02762344479560852,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08335899114608765,
      "backward_entropy": 0.02977831496132745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.13666915893555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02766771800816059,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08330109119415283,
      "backward_entropy": 0.029339548614290025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.67778015136719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027708174660801888,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08324574232101441,
      "backward_entropy": 0.03709000017907885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.74354553222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027752194553613663,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08318924903869629,
      "backward_entropy": 0.028470930125978258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.14557647705078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027796970680356026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08313358426094056,
      "backward_entropy": 0.03623778290218777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.84667205810547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02784637361764908,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08307660222053528,
      "backward_entropy": 0.03583928280406528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.4749984741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027899956330657005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08301857709884644,
      "backward_entropy": 0.027275434798664518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.35379409790039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027958746999502182,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08295907974243164,
      "backward_entropy": 0.026908010244369507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.462646484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028013810515403748,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0829019844532013,
      "backward_entropy": 0.026533881823221844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.5694808959961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028070788830518723,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08284431099891662,
      "backward_entropy": 0.03437032302220663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.48137664794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028128599748015404,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0827873170375824,
      "backward_entropy": 0.03401093350516425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.42052459716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02818574383854866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08273153901100158,
      "backward_entropy": 0.0336489114496443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.27082824707031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028244763612747192,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0826753854751587,
      "backward_entropy": 0.025093313720491197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.035892486572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028306107968091965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08261979818344116,
      "backward_entropy": 0.024752898348702326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.54170989990234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028365066275000572,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08256590366363525,
      "backward_entropy": 0.03259847561518351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.71402740478516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028423333540558815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08251299262046814,
      "backward_entropy": 0.03224643733766344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.69371795654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02848069556057453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0824607253074646,
      "backward_entropy": 0.02371669974591997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.83649444580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028543252497911453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08240797519683837,
      "backward_entropy": 0.02339269717534383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.2044906616211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02860063686966896,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08235787153244019,
      "backward_entropy": 0.023059033685260348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.69169616699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028658999130129814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08230828046798706,
      "backward_entropy": 0.022733999623192683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.27118682861328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02871846593916416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08225951194763184,
      "backward_entropy": 0.022420845097965665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.79212951660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02877722680568695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08221153020858765,
      "backward_entropy": 0.02210723360379537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.00545883178711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02883659116923809,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08216355443000793,
      "backward_entropy": 0.02179449134402805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.71880340576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028891338035464287,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0821179747581482,
      "backward_entropy": 0.021477147936820984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.3492202758789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028948547318577766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08207189440727233,
      "backward_entropy": 0.029240810208850436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.46360778808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029005371034145355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08202650547027587,
      "backward_entropy": 0.020861576000849407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.66643524169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02906157448887825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08198131322860717,
      "backward_entropy": 0.02055266996224721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.785400390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02911773882806301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08193694353103638,
      "backward_entropy": 0.02025071448749966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.013145446777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02917781099677086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08189207315444946,
      "backward_entropy": 0.01996174454689026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.3044204711914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029235998168587685,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08184831142425537,
      "backward_entropy": 0.027671585480372112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.13722229003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02929372526705265,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08180496096611023,
      "backward_entropy": 0.02736781371964349,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.66918182373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02934936247766018,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08176193237304688,
      "backward_entropy": 0.019088207019699946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.60002899169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029408223927021027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08171947598457337,
      "backward_entropy": 0.01881350576877594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.72187805175781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029468223452568054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08167746663093567,
      "backward_entropy": 0.018546392520268757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.618408203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029533522203564644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08163517713546753,
      "backward_entropy": 0.026225520504845515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.324161529541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029596755281090736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08159427642822266,
      "backward_entropy": 0.01804855465888977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.59727478027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02965824492275715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08155473470687866,
      "backward_entropy": 0.017802862657441035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.01299285888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029717711731791496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08151581287384033,
      "backward_entropy": 0.01755309436056349,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.12025451660156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029778633266687393,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08147753477096557,
      "backward_entropy": 0.02518017590045929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.08576583862305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02984050288796425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08143923282623292,
      "backward_entropy": 0.01708780394660102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.5399284362793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029900288209319115,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08140121698379517,
      "backward_entropy": 0.024685068262947932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.51849365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02995859645307064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08136416673660278,
      "backward_entropy": 0.02443603177865346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.37665557861328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03001669980585575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08132736682891846,
      "backward_entropy": 0.016386997368600633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.06890106201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030074551701545715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08129063844680787,
      "backward_entropy": 0.02393836776415507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.811641693115234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030132291838526726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08125416040420533,
      "backward_entropy": 0.01592392060491774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.06853485107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030186457559466362,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08121938109397889,
      "backward_entropy": 0.023442299829588994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.87709045410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030243858695030212,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08118445277214051,
      "backward_entropy": 0.023209345009591844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.084877014160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030297400429844856,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08115053176879883,
      "backward_entropy": 0.02296938333246443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.05963897705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03035043738782406,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08111726045608521,
      "backward_entropy": 0.01504779855410258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.80771255493164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030403871089220047,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.081083744764328,
      "backward_entropy": 0.022505286667082045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.56753540039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030456766486167908,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08105097413063049,
      "backward_entropy": 0.014629284540812174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.71392822265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03051021136343479,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08101798295974731,
      "backward_entropy": 0.022056546476152208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.213436126708984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03056589886546135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08098496198654175,
      "backward_entropy": 0.014231701691945394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.6088638305664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030620984733104706,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08095285892486573,
      "backward_entropy": 0.01404406295882331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.88459777832031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030676912516355515,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08092126846313477,
      "backward_entropy": 0.013864329291714562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.41819763183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03073858842253685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08088894486427307,
      "backward_entropy": 0.013695537216133542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.05598449707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030797408893704414,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08085697889328003,
      "backward_entropy": 0.021093448003133137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.985801696777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030859272927045822,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08082494735717774,
      "backward_entropy": 0.020922970440652635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.02501678466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030918480828404427,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08079353570938111,
      "backward_entropy": 0.013190090656280518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.66577911376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03097946010529995,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08076232671737671,
      "backward_entropy": 0.01303145537773768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.3463363647461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031042220070958138,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08073140382766723,
      "backward_entropy": 0.012882180511951447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.059898376464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031108923256397247,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08070033192634582,
      "backward_entropy": 0.020293205976486206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.06547546386719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.031175168231129646,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08066927194595337,
      "backward_entropy": 0.07575139734480116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.65631103515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031242435798048973,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08063847422599793,
      "backward_entropy": 0.012465931475162506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.26040267944336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03130912780761719,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08060778379440307,
      "backward_entropy": 0.01988931165801154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.76985168457031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031373895704746246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08057729005813599,
      "backward_entropy": 0.012189713617165884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.13471221923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03143744915723801,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08054758310317993,
      "backward_entropy": 0.012056168582704332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.68871307373047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031502172350883484,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08051765561103821,
      "backward_entropy": 0.019481539726257324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.4411735534668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031568288803100586,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08048819303512574,
      "backward_entropy": 0.019360085328420002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.114933013916016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03163417428731918,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08045897483825684,
      "backward_entropy": 0.019239308105574712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.003379821777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03169720619916916,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08043026924133301,
      "backward_entropy": 0.01911253896024492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.84893798828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03176034241914749,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.080401611328125,
      "backward_entropy": 0.01142858796649509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.61663818359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03182606026530266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08037280440330505,
      "backward_entropy": 0.01886925929122501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.15335083007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03188906982541084,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0803446352481842,
      "backward_entropy": 0.018747132685449388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.01876831054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03195492923259735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08031622171401978,
      "backward_entropy": 0.01107229706313875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.61528015136719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03202196583151817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08028788566589355,
      "backward_entropy": 0.010961169997851053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.822364807128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032091375440359116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08025944232940674,
      "backward_entropy": 0.018429933322800532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.56782531738281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032159075140953064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08023141622543335,
      "backward_entropy": 0.018329342206319172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.732120513916016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03222624585032463,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08020322322845459,
      "backward_entropy": 0.018226522538397048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.48648071289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032290566712617874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08017541170120239,
      "backward_entropy": 0.01053231375084983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.26473999023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03235747665166855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08014730215072632,
      "backward_entropy": 0.010428627332051596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.986873626708984,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032426562160253525,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08011895418167114,
      "backward_entropy": 0.07618516021304661,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.2832260131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032493796199560165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08009074926376343,
      "backward_entropy": 0.010223943326208327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.737451553344727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032564472407102585,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0800622582435608,
      "backward_entropy": 0.017739610539542303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.04590606689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03262955695390701,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08003456592559814,
      "backward_entropy": 0.010022481282552084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.18151092529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03269709646701813,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08000661134719848,
      "backward_entropy": 0.009923241204685636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.488502502441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0327657088637352,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07997871041297913,
      "backward_entropy": 0.009827933377689786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.82128143310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032828979194164276,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07995156049728394,
      "backward_entropy": 0.0173736313978831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.31414794921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03289339691400528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07992401123046874,
      "backward_entropy": 0.009629900256792704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.26962280273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032955072820186615,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07989633083343506,
      "backward_entropy": 0.017178244060940213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.68331909179688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03301822021603584,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07986836433410645,
      "backward_entropy": 0.009426076379087236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.066890716552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03308532014489174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07984021902084351,
      "backward_entropy": 0.009331827362378439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.705772399902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03315102681517601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981230020523071,
      "backward_entropy": 0.009238405360115899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.21825408935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03321392461657524,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978435754776,
      "backward_entropy": 0.016818576388888888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.21317291259766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03327330946922302,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797568142414093,
      "backward_entropy": 0.01672442422972785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.13768768310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03333577886223793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07972898483276367,
      "backward_entropy": 0.008950139085451761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.39397430419922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0334022156894207,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797006368637085,
      "backward_entropy": 0.016561066110928852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.091835021972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03347011283040047,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07967246770858764,
      "backward_entropy": 0.00878185530503591,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.73501968383789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033537860959768295,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0796442985534668,
      "backward_entropy": 0.016431811783048842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.71176528930664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033604420721530914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07961654067039489,
      "backward_entropy": 0.008628246684869131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.435543060302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03367077559232712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07958874106407166,
      "backward_entropy": 0.008552616669072045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.16322326660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033737149089574814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07956085205078126,
      "backward_entropy": 0.008479448656241098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.05574035644531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03380374237895012,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07953315377235412,
      "backward_entropy": 0.016206529405381944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.74024963378906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0338701456785202,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07950527667999267,
      "backward_entropy": 0.07648391193813747,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.40023422241211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03393763303756714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07947710156440735,
      "backward_entropy": 0.01609939005639818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.711273193359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03400641307234764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07944889068603515,
      "backward_entropy": 0.008202893866433037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.33021926879883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03407236561179161,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07942081689834594,
      "backward_entropy": 0.01600239508681827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.40290069580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03413715586066246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07939292788505554,
      "backward_entropy": 0.008066880206267038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.24757385253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03419959917664528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07936524152755738,
      "backward_entropy": 0.007999000449975332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.49765396118164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03426000103354454,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07933769226074219,
      "backward_entropy": 0.015845075249671936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.00156784057617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03432111814618111,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07931019067764282,
      "backward_entropy": 0.015797076953781977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.42837524414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034380219876766205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0792827546596527,
      "backward_entropy": 0.007801381250222524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.31815719604492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03444239869713783,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0792547583580017,
      "backward_entropy": 0.0077383799685372245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.040178298950195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03450377285480499,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07922674417495727,
      "backward_entropy": 0.0076763373282220625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.61966705322266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03456178307533264,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07919892072677612,
      "backward_entropy": 0.015603310532040067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.813507080078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03462304547429085,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07917054891586303,
      "backward_entropy": 0.015556759304470487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.16781997680664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03468361496925354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07914218306541443,
      "backward_entropy": 0.007489838533931308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.26215362548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034742340445518494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07911399602890015,
      "backward_entropy": 0.007429147760073344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.305538177490234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03480552136898041,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07908506989479065,
      "backward_entropy": 0.015426678789986504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.473854064941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03486783429980278,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0790562391281128,
      "backward_entropy": 0.015389055013656616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.33528900146484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03493081405758858,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07902733087539673,
      "backward_entropy": 0.007266871631145477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.69336700439453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034997858107089996,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0789976716041565,
      "backward_entropy": 0.015337692366706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.44185638427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035067394375801086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07896779775619507,
      "backward_entropy": 0.007177846299277412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.832332611083984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035138893872499466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07893729209899902,
      "backward_entropy": 0.007135109768973457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.422550201416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03521137312054634,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07890671491622925,
      "backward_entropy": 0.007096537285380893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.69712829589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0352831594645977,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07887604236602783,
      "backward_entropy": 0.01528587606218126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.559101104736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03535780310630798,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07884464263916016,
      "backward_entropy": 0.0070160503188769026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.7644157409668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03542808070778847,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07881388664245606,
      "backward_entropy": 0.006973923080497318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.45773696899414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03549942374229431,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07878300547599792,
      "backward_entropy": 0.00693595740530226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.33584594726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035568930208683014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07875211238861084,
      "backward_entropy": 0.006895449426439073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.081790924072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035644032061100006,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0787203550338745,
      "backward_entropy": 0.0068595024446646375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.846012115478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035715825855731964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07868893146514892,
      "backward_entropy": 0.006821581059032016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.064724922180176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0357859767973423,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.078657728433609,
      "backward_entropy": 0.00678434678249889,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.499210357666016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03584979102015495,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07862755060195922,
      "backward_entropy": 0.015178784728050232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.15261459350586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035912659019231796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0785974383354187,
      "backward_entropy": 0.015159022476938035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.959774017333984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035975947976112366,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0785670280456543,
      "backward_entropy": 0.015142692459954156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.432817459106445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03603952378034592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07853646278381347,
      "backward_entropy": 0.006628022425704532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.581130027770996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036099955439567566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07850643396377563,
      "backward_entropy": 0.006590899907880359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.38454055786133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036156512796878815,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07847720980644227,
      "backward_entropy": 0.0065550295015176134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.8780632019043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036213938146829605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07844755053520203,
      "backward_entropy": 0.015079976783858405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.406173706054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03627327084541321,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07841721773147584,
      "backward_entropy": 0.006481897913747364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.43670654296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03632867708802223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07838764190673828,
      "backward_entropy": 0.006444796091980404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.434629440307617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03638623282313347,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07835728526115418,
      "backward_entropy": 0.0064086152447594535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.327489852905273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03644252195954323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0783271074295044,
      "backward_entropy": 0.006374714275201161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.156026840209961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03649749979376793,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07829712629318238,
      "backward_entropy": 0.014996780289544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.60030746459961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03654896467924118,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07826777696609497,
      "backward_entropy": 0.014980220132403903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.976900100708008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03660064935684204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07823828458786011,
      "backward_entropy": 0.00627200636598799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.75102233886719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03665149211883545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07820872664451599,
      "backward_entropy": 0.006237834692001343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.577938079833984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03670386224985123,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07817867994308472,
      "backward_entropy": 0.014927723341517977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.02163314819336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036757610738277435,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0781480312347412,
      "backward_entropy": 0.006173274583286709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.805375099182129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03681136295199394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07811721563339233,
      "backward_entropy": 0.006141435354948044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.03759765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03686165064573288,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07808712720870972,
      "backward_entropy": 0.014878761437204149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.280717849731445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036917004734277725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07805559635162354,
      "backward_entropy": 0.006077388094531165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.693729400634766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03697105869650841,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0780242145061493,
      "backward_entropy": 0.014849546882841323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.276519775390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03702631592750549,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07799229025840759,
      "backward_entropy": 0.014835943778355917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.32898712158203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037081435322761536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07796037793159485,
      "backward_entropy": 0.014821689989831712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.419803619384766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037137649953365326,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07792792320251465,
      "backward_entropy": 0.01480986840195126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.968929290771484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037190407514572144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07789641618728638,
      "backward_entropy": 0.005931104222933452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.03219223022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037244491279125214,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07786436080932617,
      "backward_entropy": 0.01479170388645596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.4251651763916,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037302035838365555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07783111333847045,
      "backward_entropy": 0.005880104170905219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.53409194946289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037358079105615616,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07779812812805176,
      "backward_entropy": 0.005854821039570702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.14166831970215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03741740062832832,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07776401042938233,
      "backward_entropy": 0.005831686986817254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.036521911621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03747394308447838,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07773059606552124,
      "backward_entropy": 0.005807988345623016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.851844787597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03753141313791275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07769671082496643,
      "backward_entropy": 0.005785177565283245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.71973419189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03758961707353592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07766237258911132,
      "backward_entropy": 0.0057621850735611385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.46522521972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03764750808477402,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07762793302536011,
      "backward_entropy": 0.005740674005614387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.75609588623047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03770620375871658,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07759307622909546,
      "backward_entropy": 0.00571981527739101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.22941589355469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0377701073884964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07755621671676635,
      "backward_entropy": 0.00570166152384546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.256553649902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03783305361866951,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07751952409744263,
      "backward_entropy": 0.005683783027860854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.201576232910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037893980741500854,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0774833619594574,
      "backward_entropy": 0.014773352278603448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.978174209594727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03795759379863739,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07744601368904114,
      "backward_entropy": 0.014779759777916802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.849754333496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03801931068301201,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07740920782089233,
      "backward_entropy": 0.014789677328533597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.06203842163086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0380791500210762,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0773729145526886,
      "backward_entropy": 0.00561780979235967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.86452865600586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038139455020427704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07733619213104248,
      "backward_entropy": 0.005601877553595437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.05963134765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038200199604034424,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07729905843734741,
      "backward_entropy": 0.014810509151882596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.320192337036133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03826028108596802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07726199626922607,
      "backward_entropy": 0.005570451418558757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.6553955078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.038318753242492676,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07722535133361816,
      "backward_entropy": 0.07691656218634711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.048922538757324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03837454691529274,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07718960046768189,
      "backward_entropy": 0.005541135867436727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.960926055908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03842694312334061,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0771551251411438,
      "backward_entropy": 0.005526764525307549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.844575881958008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038478314876556396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07712080478668212,
      "backward_entropy": 0.005511695312129127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.45477294921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03852887824177742,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07708665132522582,
      "backward_entropy": 0.005497362464666367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.43646240234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038575321435928345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07705414295196533,
      "backward_entropy": 0.005481786612007353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.152320861816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03862353786826134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07702058553695679,
      "backward_entropy": 0.005466614332464006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.722094535827637,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038670022040605545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07698763608932495,
      "backward_entropy": 0.005450660569800271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.976806640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0387139655649662,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07695571184158326,
      "backward_entropy": 0.005434979167249467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.82242202758789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03875989839434624,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07692261934280395,
      "backward_entropy": 0.07693215211232503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.93790054321289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03880768269300461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07688848972320557,
      "backward_entropy": 0.005404686762226952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.49644470214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03885819390416145,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07685278654098511,
      "backward_entropy": 0.005390770319435332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.888769149780273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03891006484627724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0768163800239563,
      "backward_entropy": 0.005377831558386485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.966087341308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038961011916399,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07678011655807496,
      "backward_entropy": 0.014853421184751723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.30926513671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03901216387748718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0767435610294342,
      "backward_entropy": 0.005352271927727593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.068477630615234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03906668722629547,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07670507431030274,
      "backward_entropy": 0.014856595132086012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.61390686035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03912423178553581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07666485905647277,
      "backward_entropy": 0.00532719161775377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.150863647460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03918241336941719,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07662407159805298,
      "backward_entropy": 0.005315328223837746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.165306091308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03923682123422623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0765850841999054,
      "backward_entropy": 0.005302721427546607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.06553268432617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03929002210497856,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07654657363891601,
      "backward_entropy": 0.005290430453088548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.90637969970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039344217628240585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0765071988105774,
      "backward_entropy": 0.0052786047259966535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.90881633758545,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03939826786518097,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07646771669387817,
      "backward_entropy": 0.014859489268726774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.695741653442383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0394490510225296,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07642987966537476,
      "backward_entropy": 0.0052560203605228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.690452575683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039498910307884216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07639232873916627,
      "backward_entropy": 0.0052446578111913465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.476781845092773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03954694792628288,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07635561227798462,
      "backward_entropy": 0.005233703388108147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.371559143066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03959443047642708,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07631901502609253,
      "backward_entropy": 0.005223519686195586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.34160232543945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039641380310058594,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07628252506256103,
      "backward_entropy": 0.005213731692896949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.9393253326416,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039693016558885574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07624297142028809,
      "backward_entropy": 0.01486535370349884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.795738220214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03974468633532524,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07620319724082947,
      "backward_entropy": 0.005193899903032515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.11031723022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039796389639377594,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07616312503814697,
      "backward_entropy": 0.014864325523376465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.600013732910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039850179105997086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07612150311470031,
      "backward_entropy": 0.0051737746430767905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.677419662475586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03990692272782326,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.076077800989151,
      "backward_entropy": 0.014861058857705858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.6434407234191895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039962153881788254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07603484988212586,
      "backward_entropy": 0.0051548828681310015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.87898254394531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04001284018158913,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07599462270736694,
      "backward_entropy": 0.014856490823957656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.32204818725586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04006672650575638,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07595204114913941,
      "backward_entropy": 0.005135380145576265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.205154418945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04011940956115723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0759100079536438,
      "backward_entropy": 0.005126500295268165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.090639114379883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04017099365592003,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07586845159530639,
      "backward_entropy": 0.014851328399446275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.978452682495117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04022158309817314,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07582733035087585,
      "backward_entropy": 0.005110373513566123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.868330001831055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040271248668432236,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07578654289245605,
      "backward_entropy": 0.014852535393502977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.320619583129883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04032004252076149,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07574616670608521,
      "backward_entropy": 0.005095178054438697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.48086929321289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04036703705787659,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07570677995681763,
      "backward_entropy": 0.014851765500174629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.16092300415039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040415503084659576,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07566611766815186,
      "backward_entropy": 0.0050807081990771824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.80316162109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04046221822500229,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0756264567375183,
      "backward_entropy": 0.0050740597976578605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.00567626953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04050938040018082,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07558622360229492,
      "backward_entropy": 0.005067532675133811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.229589462280273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04055796191096306,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07554463744163513,
      "backward_entropy": 0.005061383876535628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.683734893798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04060579091310501,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07550337314605712,
      "backward_entropy": 0.005055493778652615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.519428253173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04065493121743202,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07546082139015198,
      "backward_entropy": 0.005049471639924579,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.126293182373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040705252438783646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07541708946228028,
      "backward_entropy": 0.005043494618601269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.182653427124023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040755610913038254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07537300586700439,
      "backward_entropy": 0.005037614040904575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.842632293701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04080703482031822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07532784342765808,
      "backward_entropy": 0.005032137036323547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.140769004821777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04085836932063103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07528244853019714,
      "backward_entropy": 0.005026664170953963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.454429626464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04090559482574463,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07524018287658692,
      "backward_entropy": 0.005021268294917213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.350549697875977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04095214232802391,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07519819736480712,
      "backward_entropy": 0.005016021844413545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.247913360595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04099806770682335,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07515645623207093,
      "backward_entropy": 0.01485214134057363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.182893753051758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04104343056678772,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07511496543884277,
      "backward_entropy": 0.005005833175447252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.033086776733398,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041089292615652084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07507272958755493,
      "backward_entropy": 0.0050010813607109916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.958670616149902,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041133586317300797,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07503159642219544,
      "backward_entropy": 0.004996414813730452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.69717025756836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04117647558450699,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07499146461486816,
      "backward_entropy": 0.014845990472369723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.936339855194092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04122306406497955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07494778633117676,
      "backward_entropy": 0.004987790352768368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.6484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04126601666212082,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07490719556808471,
      "backward_entropy": 0.004983772834142049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.658348083496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04130867123603821,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07486658096313477,
      "backward_entropy": 0.004979853000905778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.720437049865723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04135894402861595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07481892108917236,
      "backward_entropy": 0.004976169930564033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.667741775512695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04140620306134224,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07477378845214844,
      "backward_entropy": 0.004972672710816066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.413357186829671e-05,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04145073890686035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07473095655441284,
      "backward_entropy": 0.014836760030852424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.359862327575684,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04149084910750389,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07469218373298644,
      "backward_entropy": 0.004966128203603957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.05921173095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041529927402734756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07465416789054871,
      "backward_entropy": 0.00496308629711469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.20054626464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0415690541267395,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07461585998535156,
      "backward_entropy": 0.004960130486223433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.158255577087402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04161115735769272,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07457436323165893,
      "backward_entropy": 0.004957248767217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.875946044921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04165201634168625,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07453389763832093,
      "backward_entropy": 0.014824575848049588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.6722092628479,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04169565439224243,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0744903564453125,
      "backward_entropy": 0.004951756033632491,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.897869110107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0417359359562397,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07445003986358642,
      "backward_entropy": 0.004949147502581279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.878268241882324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04177805781364441,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07440755367279053,
      "backward_entropy": 0.004946601059701707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.206523895263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04181892052292824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0743661105632782,
      "backward_entropy": 0.004944188313351737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.3261661529541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04185766726732254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07432665228843689,
      "backward_entropy": 0.00494184179438485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.79900550842285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04189644381403923,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0742869257926941,
      "backward_entropy": 0.014802331725756327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.687650680541992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04193622246384621,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07424583435058593,
      "backward_entropy": 0.00493746002515157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.57362174987793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04197688400745392,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07420355677604676,
      "backward_entropy": 0.014793425798416138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.457456588745117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0420183390378952,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07416015863418579,
      "backward_entropy": 0.014788680606418185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.806921005249023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0420604944229126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07411575913429261,
      "backward_entropy": 0.004931467688745922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.216707229614258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04210424795746803,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07406925559043884,
      "backward_entropy": 0.004929743707180023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.09271240234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042148441076278687,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07402207255363465,
      "backward_entropy": 0.014773039354218377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.574237823486328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042193036526441574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07397412061691284,
      "backward_entropy": 0.014766934845182631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.475481033325195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04223702847957611,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07392662167549133,
      "backward_entropy": 0.0049243759777810835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.033655166625977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04228048026561737,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0738793969154358,
      "backward_entropy": 0.014754093355602689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.64235782623291,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04232248663902283,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0738335132598877,
      "backward_entropy": 0.01474784811337789,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.79173469543457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04236221686005592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07379010915756226,
      "backward_entropy": 0.004920120868417952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.6521053314209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04240370914340019,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07374427318572999,
      "backward_entropy": 0.004918927947680156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.754151344299316,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04244677349925041,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0736962914466858,
      "backward_entropy": 0.014727410342958238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.684161186218262,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04248838871717453,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07364981174468994,
      "backward_entropy": 0.014719795849588182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.616013526916504,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04252869635820389,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0736046314239502,
      "backward_entropy": 0.004915490332576964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.73340606689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04256785288453102,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07356052398681641,
      "backward_entropy": 0.004914670768711302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.323540687561035,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042606890201568604,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07351628541946412,
      "backward_entropy": 0.014695836438073052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.422094345092773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04264392331242561,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07347434759140015,
      "backward_entropy": 0.004913294066985448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.361713409423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04268011823296547,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0734331488609314,
      "backward_entropy": 0.004912942234012816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.501914978027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042715489864349365,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07339279651641846,
      "backward_entropy": 0.014669706424077352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.239908218383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04275202378630638,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07335073947906494,
      "backward_entropy": 0.004911659078465568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.477081298828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04278774932026863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07330937385559082,
      "backward_entropy": 0.004911322974496418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.150157928466797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04282735660672188,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07326276302337646,
      "backward_entropy": 0.014639794826507568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.031606674194336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04286674037575722,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07321617007255554,
      "backward_entropy": 0.0049100468556086225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.983478546142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042904067784547806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07317209839820862,
      "backward_entropy": 0.004909479783640968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.86846351623535,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04294046759605408,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07312898635864258,
      "backward_entropy": 0.07698555787404378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.858832359313965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04297790676355362,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07308409214019776,
      "backward_entropy": 0.00490895700123575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.797008514404297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04301439970731735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07304024696350098,
      "backward_entropy": 0.004908963210052914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.56041717529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04305005818605423,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07299723625183105,
      "backward_entropy": 0.004909281101491716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.347797393798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04308677092194557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0729525089263916,
      "backward_entropy": 0.0049095046189096235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8716232776641846,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04312536120414734,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0729048192501068,
      "backward_entropy": 0.014550843172603183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.097152709960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04316102713346481,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07286111712455749,
      "backward_entropy": 0.004910186760955387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.486099243164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043198637664318085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07281432151794434,
      "backward_entropy": 0.004910451256566578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.424189567565918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04323527216911316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07276862859725952,
      "backward_entropy": 0.004910945064491696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.725622177124023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0432710275053978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07272393107414246,
      "backward_entropy": 0.004911733998192681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.597858428955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043308697640895844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07267607450485229,
      "backward_entropy": 0.014493309789233737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.72161102294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04334813356399536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07262523174285888,
      "backward_entropy": 0.014484226703643799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.60926055908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04338820278644562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07257311344146729,
      "backward_entropy": 0.004915752344661289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.194133758544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043428823351860046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07251988649368286,
      "backward_entropy": 0.004917326072851817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.701913833618164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043470799922943115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0724643349647522,
      "backward_entropy": 0.004918463942077424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.306645393371582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043512266129255295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07240908145904541,
      "backward_entropy": 0.004920209033621682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.893256187438965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04355139657855034,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07235717177391052,
      "backward_entropy": 0.014435031347804599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.653764724731445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043589264154434204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0723070740699768,
      "backward_entropy": 0.0049225278198719025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.107236862182617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043628741055727005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07225409746170045,
      "backward_entropy": 0.004923241006003486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.254387855529785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04367052763700485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07219719886779785,
      "backward_entropy": 0.01439286106162601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.162311553955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043711744248867035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07214078903198243,
      "backward_entropy": 0.004924481941594018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.104766845703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04375241696834564,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07208484411239624,
      "backward_entropy": 0.004925246040026347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.963972091674805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04379436746239662,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07202649116516113,
      "backward_entropy": 0.004925987786716885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.880852699279785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04383743926882744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0719659686088562,
      "backward_entropy": 0.004926580521795485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.340316772460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043879758566617966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07190625667572022,
      "backward_entropy": 0.004927199747827318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.271814346313477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04392053559422493,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07184867858886719,
      "backward_entropy": 0.004928160044882033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.20626449584961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043959930539131165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07179293632507325,
      "backward_entropy": 0.0049295975930160945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.763268947601318,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043998051434755325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07173892259597778,
      "backward_entropy": 0.014276329014036391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.081807136535645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04403413459658623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07168806791305542,
      "backward_entropy": 0.004932790166801876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.70290184020996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044069282710552216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0716383695602417,
      "backward_entropy": 0.004934819208251106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.644110202789307,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04410529136657715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07158679366111756,
      "backward_entropy": 0.0049367596705754595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.605878829956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044139448553323746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07153817415237426,
      "backward_entropy": 0.004938594996929169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.8519287109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04417194426059723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07149217128753663,
      "backward_entropy": 0.004940442740917206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.328901290893555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04420381411910057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0714469313621521,
      "backward_entropy": 0.004942368716001511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.235733032226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04423678666353226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0713995099067688,
      "backward_entropy": 0.0049436357286241316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.459492206573486,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04427080228924751,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07134981155395508,
      "backward_entropy": 0.004945039335224364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.842203140258789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044303156435489655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07130278944969178,
      "backward_entropy": 0.004946524070368873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.57758903503418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04433569684624672,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07125518918037414,
      "backward_entropy": 0.004947582052813636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.871167182922363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04436759650707245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07120838165283203,
      "backward_entropy": 0.004948978208833271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.3146748542785645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044400572776794434,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07115925550460815,
      "backward_entropy": 0.004950059784783257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.415833473205566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044431980699300766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07111282348632812,
      "backward_entropy": 0.004951256016890208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.484784126281738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04446287080645561,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07106686234474183,
      "backward_entropy": 0.004953089273638195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.620107650756836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04449409618973732,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0710199534893036,
      "backward_entropy": 0.014033628834618462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.50872039794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04452730342745781,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07096893191337586,
      "backward_entropy": 0.004956703219148848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.263830184936523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04456229880452156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07091412544250489,
      "backward_entropy": 0.004958499636914995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.27501678466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04459715634584427,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07085931301116943,
      "backward_entropy": 0.0049598510894510485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.129027366638184,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044633615761995316,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07080103158950805,
      "backward_entropy": 0.013963338401582506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.029987335205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04467063769698143,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07074131965637206,
      "backward_entropy": 0.004962611529562209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.919569969177246,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044708989560604095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07067864537239074,
      "backward_entropy": 0.013925370242860582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.811728477478027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04474768042564392,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07061500549316406,
      "backward_entropy": 0.004964459273550246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.763415336608887,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04478668048977852,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07055035829544068,
      "backward_entropy": 0.013884248005019294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.596985816955566,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044825147837400436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07048633098602294,
      "backward_entropy": 0.013864386412832472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.593879699707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04486390948295593,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07042137384414673,
      "backward_entropy": 0.013843495812680986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.258222579956055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04490205645561218,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07035741806030274,
      "backward_entropy": 0.013820336924658881,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.274921417236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04494136571884155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07029058933258056,
      "backward_entropy": 0.0049676526751783155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.334301948547363,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04498085007071495,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07022299766540527,
      "backward_entropy": 0.07698450485865276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.247892379760742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04501964896917343,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07015657424926758,
      "backward_entropy": 0.013750859432750277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.372909545898438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04505785182118416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.070090913772583,
      "backward_entropy": 0.004968087292379803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.545681476593018,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04509477689862251,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07002732753753663,
      "backward_entropy": 0.004969382451640235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.256569862365723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04512961581349373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06996800899505615,
      "backward_entropy": 0.004970317085584004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.392515182495117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045163389295339584,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.069910728931427,
      "backward_entropy": 0.004971020337608125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7187681198120117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045198652893304825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06984965801239014,
      "backward_entropy": 0.013636969029903412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.782843589782715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04523124173283577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06979413032531738,
      "backward_entropy": 0.004973225295543671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.38577938079834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04526375234127045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06973852515220642,
      "backward_entropy": 0.004974061002333959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.292461395263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0452970489859581,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06968066096305847,
      "backward_entropy": 0.013567907942665948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.925971508026123,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045331135392189026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06962031126022339,
      "backward_entropy": 0.004977575192848842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.489764213562012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0453641302883625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06956237554550171,
      "backward_entropy": 0.004978879872295592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.8154706954956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04539705440402031,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06950408220291138,
      "backward_entropy": 0.004980666355954276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.517065048217773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045429106801748276,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06944733262062072,
      "backward_entropy": 0.013483986258506775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.841928482055664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04546276852488518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06938637495040893,
      "backward_entropy": 0.004985502196682824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.653322219848633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04549700394272804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06932367086410522,
      "backward_entropy": 0.004987885140710407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.539020299911499,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04553014785051346,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06926336288452148,
      "backward_entropy": 0.0049896736939748125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.034515857696533,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04556075483560562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06920883655548096,
      "backward_entropy": 0.004991324411498176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.998109817504883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045589908957481384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06915733218193054,
      "backward_entropy": 0.004993431684043672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.933843612670898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04561926797032356,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06910513639450074,
      "backward_entropy": 0.0049953220619095694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.801011085510254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04564882069826126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06905198097229004,
      "backward_entropy": 0.004997226513094372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.905782222747803,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045680031180381775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06899456977844239,
      "backward_entropy": 0.00499831885099411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.736603736877441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04570966213941574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06894086003303528,
      "backward_entropy": 0.004999275836679671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.087640762329102,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04573945328593254,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06888623833656311,
      "backward_entropy": 0.0769797232415941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.205597400665283,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04577014967799187,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06882894039154053,
      "backward_entropy": 0.005002114507887099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.7743096351623535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045800138264894485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06877297163009644,
      "backward_entropy": 0.0050042664839161765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.847528457641602,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04582870751619339,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06872016191482544,
      "backward_entropy": 0.013189951578776041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.415491104125977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04585818573832512,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0686648964881897,
      "backward_entropy": 0.005008524904648463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.01597785949707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04588774964213371,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06860921382904053,
      "backward_entropy": 0.005009848210546706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.925933837890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04591662809252739,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06855493783950806,
      "backward_entropy": 0.005010935167471568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.918754577636719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04594726860523224,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06849550008773804,
      "backward_entropy": 0.00501312729385164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.44364070892334,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045977141708135605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06843768954277038,
      "backward_entropy": 0.005015440699126985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.091894149780273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04600786790251732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06837720274925232,
      "backward_entropy": 0.005018342700269487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.040952682495117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04603853449225426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06831648349761962,
      "backward_entropy": 0.005020996762646569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2454166412353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04607219249010086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06824756264686585,
      "backward_entropy": 0.005024000588390563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.885592460632324,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04610326513648033,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06818506717681885,
      "backward_entropy": 0.012989449004332224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.6181321144104,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046134304255247116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06812217235565185,
      "backward_entropy": 0.0050313543114397265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.942731857299805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04616449028253555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06806120872497559,
      "backward_entropy": 0.005035312639342414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.201214790344238,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046195413917303085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06799781322479248,
      "backward_entropy": 0.005039443572362264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.164722204208374,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046228453516960144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06792823672294616,
      "backward_entropy": 0.005043545530902015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.148235559463501,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04625885933637619,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06786590814590454,
      "backward_entropy": 0.005046468642022874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.373736381530762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04628690704703331,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06780998706817627,
      "backward_entropy": 0.005048545284403695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.224189758300781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04631441831588745,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06775505542755127,
      "backward_entropy": 0.005051366157001919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.47479248046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04634065553545952,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06770334243774415,
      "backward_entropy": 0.005054219315449397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.245045185089111,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04636797681450844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06764798164367676,
      "backward_entropy": 0.012812222043673197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.203516006469727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04639481380581856,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06759352684020996,
      "backward_entropy": 0.005061497704850303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.21359634399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046421173959970474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0675400972366333,
      "backward_entropy": 0.005065489146444533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.156638145446777,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04644777625799179,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06748571395874023,
      "backward_entropy": 0.012755034698380364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.054889678955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046474628150463104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06743023991584778,
      "backward_entropy": 0.0050723374717765385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.055892944335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04650026559829712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06737791299819947,
      "backward_entropy": 0.005075910439093907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.990235805511475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04652692750096321,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0673223614692688,
      "backward_entropy": 0.005079081488980187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.932862758636475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04655380919575691,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06726579666137696,
      "backward_entropy": 0.005082231014966965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.844367980957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04658091813325882,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06720805168151855,
      "backward_entropy": 0.005085848685767915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.819030284881592,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046608876436948776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06714757680892944,
      "backward_entropy": 0.005089047882292006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.698963165283203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046636879444122314,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06708662509918213,
      "backward_entropy": 0.012612071302202012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.545361518859863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046665579080581665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06702349185943604,
      "backward_entropy": 0.0050939201480812496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.823009490966797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046695638447999954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0669560194015503,
      "backward_entropy": 0.005095556792285707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7953476905822754,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04672414809465408,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0668927252292633,
      "backward_entropy": 0.012540322211053636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.276314735412598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04675117880105972,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0668337881565094,
      "backward_entropy": 0.005099340859386656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.737539052963257,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046779733151197433,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06676976680755616,
      "backward_entropy": 0.005101152592235141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.558263301849365,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04680679365992546,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06671018600463867,
      "backward_entropy": 0.005102445267968708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.021858215332031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04683319851756096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06665240526199341,
      "backward_entropy": 0.005103211849927902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6529741287231445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04686109349131584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0665898323059082,
      "backward_entropy": 0.012408296267191568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.658212661743164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04688755422830582,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.066531503200531,
      "backward_entropy": 0.012378331687715318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.595339775085449,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04691619798541069,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06646595001220704,
      "backward_entropy": 0.012349324093924629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.343751430511475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04694334417581558,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06640494465827942,
      "backward_entropy": 0.005103832317723168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.068872451782227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046969879418611526,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06634526848793029,
      "backward_entropy": 0.005104773574405246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.762211799621582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04699645936489105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06628533601760864,
      "backward_entropy": 0.005105203638474147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.428374290466309,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047023843973875046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06622223258018493,
      "backward_entropy": 0.005106464442279603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.897534370422363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047052621841430664,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06615411639213561,
      "backward_entropy": 0.0051084040767616695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.252346992492676,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04708125442266464,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06608610153198242,
      "backward_entropy": 0.012189848555458916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.086996555328369,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04711112380027771,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0660136103630066,
      "backward_entropy": 0.0051133640938335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.041548252105713,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047140032052993774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06594394445419312,
      "backward_entropy": 0.005116126603550381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6755123138427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04716813191771507,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06587638854980468,
      "backward_entropy": 0.012125995424058702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.612672805786133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047194063663482666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06581587791442871,
      "backward_entropy": 0.005122742719120449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.192910194396973,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047220002859830856,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06575526595115662,
      "backward_entropy": 0.012079344027572207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.633386492729187,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04724676162004471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06569131016731262,
      "backward_entropy": 0.005128033459186554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.624649167060852,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04727158322930336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06563329100608825,
      "backward_entropy": 0.005132221513324314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.609566688537598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047294508665800095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06558171510696412,
      "backward_entropy": 0.00513520629869567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.943881034851074,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04731900617480278,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06552483439445496,
      "backward_entropy": 0.005136951804161072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.3080668449401855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047344449907541275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06546400189399719,
      "backward_entropy": 0.005140245374706056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.695515155792236,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047370001673698425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0654023289680481,
      "backward_entropy": 0.011948467956648933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.855071067810059,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.047394994646310806,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06534212827682495,
      "backward_entropy": 0.07697404755486383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5476124286651611,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047422051429748535,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06527450680732727,
      "backward_entropy": 0.011909592482778762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.05754017829895,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04744705930352211,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06521376967430115,
      "backward_entropy": 0.01188966714673572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.036304473876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04747089743614197,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06515671014785766,
      "backward_entropy": 0.00516004612048467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.514326095581055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047493595629930496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06510353088378906,
      "backward_entropy": 0.005163497394985623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5024234056472778,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04751589521765709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06505168676376342,
      "backward_entropy": 0.005166078607241313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.451120376586914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047536566853523254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06500557065010071,
      "backward_entropy": 0.005168055080705219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.357183933258057,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04755701124668121,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06496012806892396,
      "backward_entropy": 0.00516904890537262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.38804817199707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04757857322692871,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06491055488586425,
      "backward_entropy": 0.005169873436292012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.460588812828064,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04759983345866203,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06486191749572753,
      "backward_entropy": 0.0051701391736666364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.63890266418457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047619566321372986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06481855511665344,
      "backward_entropy": 0.005170121788978577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.148582935333252,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04764115437865257,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06476806402206421,
      "backward_entropy": 0.005170896649360657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8432936668395996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04766374081373215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06471356153488159,
      "backward_entropy": 0.005171933521827062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8261067867279053,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047685373574495316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06466202735900879,
      "backward_entropy": 0.005173748979965846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.410014033317566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04770597442984581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06461457014083863,
      "backward_entropy": 0.005173936486244202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.943970203399658,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047725025564432144,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06457287669181824,
      "backward_entropy": 0.011535939243104722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.140411853790283,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0477452352643013,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06452678442001343,
      "backward_entropy": 0.005171251793702443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.744676351547241,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04776529222726822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06448091268539428,
      "backward_entropy": 0.005170244309637282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.082005023956299,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047784578055143356,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0644376277923584,
      "backward_entropy": 0.0051695336070325636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.056222915649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04780381917953491,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06439403295516968,
      "backward_entropy": 0.0051696209443940055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.703070163726807,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04782289266586304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0643511414527893,
      "backward_entropy": 0.0051687782009442644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9969217777252197,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04784313589334488,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06430350542068482,
      "backward_entropy": 0.01134238557683097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6469829082489014,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0478631928563118,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06425629258155822,
      "backward_entropy": 0.005168399877018399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.629685401916504,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047882575541734695,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06421102285385132,
      "backward_entropy": 0.01128642674949434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.113365173339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04790126532316208,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0641680121421814,
      "backward_entropy": 0.005172446784045961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.462411403656006,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047922395169734955,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06411534547805786,
      "backward_entropy": 0.011240152021249136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8506453037261963,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04794447496533394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0640586793422699,
      "backward_entropy": 0.005179223087098863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.820406198501587,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047966159880161285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0640033483505249,
      "backward_entropy": 0.0051827629407246905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5291218757629395,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0479874424636364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.063949453830719,
      "backward_entropy": 0.005185645901494556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.508338689804077,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048007845878601074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06389856338500977,
      "backward_entropy": 0.005189177890618642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.971673011779785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048027534037828445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06384979486465454,
      "backward_entropy": 0.00519446987244818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.165400981903076,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048047710210084915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0637986719608307,
      "backward_entropy": 0.005200560722086165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.895472526550293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04806888848543167,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06374331116676331,
      "backward_entropy": 0.00520696159866121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.856706619262695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04809032753109932,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06368668675422669,
      "backward_entropy": 0.011097462640868293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6119370460510254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0481119304895401,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06362939476966858,
      "backward_entropy": 0.005218069586488936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5843284130096436,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048133254051208496,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06357263922691345,
      "backward_entropy": 0.011066188414891561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.917558193206787,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04815421253442764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06351703405380249,
      "backward_entropy": 0.005230407334036297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5254364013671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04817603528499603,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06345763802528381,
      "backward_entropy": 0.011035495334201388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.819168567657471,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04819740727543831,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06339995861053467,
      "backward_entropy": 0.011018413636419509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.616236209869385,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04821951687335968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06333916187286377,
      "backward_entropy": 0.005246625178390079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2938039302825928,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04824172705411911,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06327770948410034,
      "backward_entropy": 0.0052508873244126635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2715299129486084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0482628308236599,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0632206916809082,
      "backward_entropy": 0.005254352672232522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.499818801879883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048283059149980545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06316651105880737,
      "backward_entropy": 0.01093816426065233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1246918439865112,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048303525894880295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06311129331588745,
      "backward_entropy": 0.0052627383006943595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.635520935058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04832246154546738,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06306226253509521,
      "backward_entropy": 0.005265531854497062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.19952130317688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04834289848804474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0630069613456726,
      "backward_entropy": 0.005267918523814943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2661802768707275,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04836244508624077,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06295487880706788,
      "backward_entropy": 0.005270563893847995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.239332437515259,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04838181287050247,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06290292739868164,
      "backward_entropy": 0.005274251103401184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.285158157348633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04840103164315224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06285096406936645,
      "backward_entropy": 0.010806374251842499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.12911057472229,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04842052236199379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0627978801727295,
      "backward_entropy": 0.005282986909151077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0619218349456787,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04843920096755028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06274787187576295,
      "backward_entropy": 0.0052868566579288906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.096142292022705,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048456549644470215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06270323395729065,
      "backward_entropy": 0.005290225976043277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.224716663360596,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04847331345081329,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06266065835952758,
      "backward_entropy": 0.010723399619261423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0401246547698975,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048491764813661575,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06261066198349,
      "backward_entropy": 0.010703982578383552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.012220936827361584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04850882291793823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06256673336029053,
      "backward_entropy": 0.00530036124918196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0505731105804443,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048524074256420135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06253068447113037,
      "backward_entropy": 0.005301179985205333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.034398078918457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04853940010070801,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06249425411224365,
      "backward_entropy": 0.005301341828372743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.007556676864624,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048555415123701096,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.062454360723495486,
      "backward_entropy": 0.010596230626106262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9791078567504883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048570942133665085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06241620779037475,
      "backward_entropy": 0.005303214821550582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.963859796524048,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048587072640657425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06237537860870361,
      "backward_entropy": 0.0053039830591943525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9638733863830566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04860325902700424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06233384609222412,
      "backward_entropy": 0.0053054168820381165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9503246545791626,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04861897602677345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0622938334941864,
      "backward_entropy": 0.005307844115628136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.794468402862549,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04863424599170685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.062255334854125974,
      "backward_entropy": 0.0053108880917231245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.966937243938446,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048651233315467834,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06220922470092773,
      "backward_entropy": 0.010452585915724436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.757713794708252,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04866700991988182,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0621683657169342,
      "backward_entropy": 0.005315966904163361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9524378180503845,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04868392273783684,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.062122249603271486,
      "backward_entropy": 0.07696598105960423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9443458318710327,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04869963228702545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06208134889602661,
      "backward_entropy": 0.010383899013201395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.797412633895874,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04871431365609169,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.062044739723205566,
      "backward_entropy": 0.01036051246854994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6989877223968506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0487290658056736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06200770735740661,
      "backward_entropy": 0.00532309255666203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.587795734405518,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048744503408670425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06196736693382263,
      "backward_entropy": 0.005324737893210517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.369595050811768,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048761095851659775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0619215190410614,
      "backward_entropy": 0.0053272801968786455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.812428593635559,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04877976328134537,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061866194009780884,
      "backward_entropy": 0.010271360476811727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7939541339874268,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.048797495663166046,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06181524395942688,
      "backward_entropy": 0.07696402072906494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.665034055709839,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048814550042152405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06176689863204956,
      "backward_entropy": 0.0053332845369974775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6425282955169678,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0488315112888813,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061718666553497316,
      "backward_entropy": 0.005335804902844959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.978585720062256,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04884841665625572,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061670058965682985,
      "backward_entropy": 0.0053394801086849636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.187514781951904,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048867784440517426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06161036491394043,
      "backward_entropy": 0.0053429243465264635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.1338725090026855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04888826981186867,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06154574155807495,
      "backward_entropy": 0.005345423188474443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.544475793838501,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04890982434153557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061476010084152224,
      "backward_entropy": 0.005348223365015454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8448677659034729,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04893074557185173,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061409056186676025,
      "backward_entropy": 0.00535064438978831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.148715496063232,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04895009845495224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061348891258239745,
      "backward_entropy": 0.01009151256746716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.9264068603515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04897010698914528,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.061285400390625,
      "backward_entropy": 0.07696496115790473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2515153884887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048991214483976364,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061216425895690915,
      "backward_entropy": 0.010057904654079013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4162633419036865,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04901229590177536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06114714145660401,
      "backward_entropy": 0.005365233040518231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3881664276123047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049032751470804214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061080682277679446,
      "backward_entropy": 0.005369373493724399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5816277265548706,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04905277118086815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061015534400939944,
      "backward_entropy": 0.01001651171180937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.121706247329712,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049071721732616425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060955476760864255,
      "backward_entropy": 0.00537919294503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.319150447845459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04909070208668709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06089538335800171,
      "backward_entropy": 0.005382234437598122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5338183641433716,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04910926893353462,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060836958885192874,
      "backward_entropy": 0.005385234124130673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5180964469909668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04912697523832321,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06078224778175354,
      "backward_entropy": 0.005388264026906755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.253836154937744,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0491439588367939,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06073042154312134,
      "backward_entropy": 0.005392227735784318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2331337928771973,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04916072636842728,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.060679292678833006,
      "backward_entropy": 0.00991461674372355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.946977376937866,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04917731508612633,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06062866449356079,
      "backward_entropy": 0.009899745384852091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9237406253814697,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049194276332855225,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060575675964355466,
      "backward_entropy": 0.005406508843104045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.895965337753296,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049211401492357254,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.060522055625915526,
      "backward_entropy": 0.009872444801860385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4384976625442505,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04922868683934212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06046772003173828,
      "backward_entropy": 0.005413853873809178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7162442207336426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04924517869949341,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.060417115688323975,
      "backward_entropy": 0.00983617537551456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8162925243377686,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04926052317023277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060371649265289304,
      "backward_entropy": 0.00541945712433921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7943408489227295,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049276240170001984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0603240966796875,
      "backward_entropy": 0.0054223789936966365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.074655055999756,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04929216578602791,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.060276031494140625,
      "backward_entropy": 0.009777698251936171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7406485080718994,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04930800572037697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06022751331329346,
      "backward_entropy": 0.005425936232010524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0383431911468506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049324166029691696,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0601771354675293,
      "backward_entropy": 0.005428847753339344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3494009971618652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0493401475250721,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06012730002403259,
      "backward_entropy": 0.005432153741518657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0021989345550537,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04935548081994057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060080325603485106,
      "backward_entropy": 0.00543554210000568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3271284103393555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04937068372964859,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060033750534057614,
      "backward_entropy": 0.0054389188687006635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3157953023910522,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04938524588942528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05999038219451904,
      "backward_entropy": 0.005441388322247399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6575837731361389,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049399226903915405,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05994991064071655,
      "backward_entropy": 0.009652329815758599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.292341709136963,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049412209540605545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05991442203521728,
      "backward_entropy": 0.009629645281367831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9208800792694092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04942486062645912,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059880053997039794,
      "backward_entropy": 0.005445358239942127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.542310953140259,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04943764954805374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05984461307525635,
      "backward_entropy": 0.005447491175598568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5213358402252197,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049450866878032684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05980745553970337,
      "backward_entropy": 0.005447799960772197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.119243860244751,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04946449026465416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059768521785736085,
      "backward_entropy": 0.00544680075512992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4797706604003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04947907477617264,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059724283218383786,
      "backward_entropy": 0.005447118232647578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8417316675186157,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04949382320046425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.059679841995239256,
      "backward_entropy": 0.009489980836709341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0073808240704238415,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04950844496488571,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05963575839996338,
      "backward_entropy": 0.009462524619367387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.408676862716675,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049521639943122864,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05959839820861816,
      "backward_entropy": 0.005441671030388938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9843735694885254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04953537881374359,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05955776572227478,
      "backward_entropy": 0.005442233549224006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1870660781860352,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049549978226423264,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059512597322463986,
      "backward_entropy": 0.005443219509389665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5943988561630249,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04956403747200966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059469854831695555,
      "backward_entropy": 0.005444839182827208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1659425497055054,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04957703500986099,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.059432697296142575,
      "backward_entropy": 0.009351596236228943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3079352378845215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04958968237042427,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05939685106277466,
      "backward_entropy": 0.009331491258409288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.148363709449768,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049602802842855453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05935848355293274,
      "backward_entropy": 0.005447684476772944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1385403871536255,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049615465104579926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0593224048614502,
      "backward_entropy": 0.0054487188657124834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.251546621322632,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04962772876024246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05928811430931091,
      "backward_entropy": 0.009269732567999098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9006073474884033,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0496404729783535,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.059251463413238524,
      "backward_entropy": 0.009248448742760552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8628408908843994,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04965496063232422,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05920574069023132,
      "backward_entropy": 0.005451451573106978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6435562372207642,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049671027809381485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05915182828903198,
      "backward_entropy": 0.005452699545356963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.703425168991089,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049686696380376816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05910008549690247,
      "backward_entropy": 0.0054527877105606925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6072266101837158,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04970292001962662,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05904542207717896,
      "backward_entropy": 0.009164371424251132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6445207595825195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04971879720687866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.058992236852645874,
      "backward_entropy": 0.009143008953995176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.569973111152649,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049735210835933685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05893622636795044,
      "backward_entropy": 0.005453967385821872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5541094541549683,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04975137859582901,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05888068675994873,
      "backward_entropy": 0.005457044475608402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.067819118499756,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04976722598075867,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.058826553821563723,
      "backward_entropy": 0.009092421995268928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.024507761001587,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049783989787101746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05876759886741638,
      "backward_entropy": 0.005464190824164284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5029782056808472,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04980071261525154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05870882272720337,
      "backward_entropy": 0.005467129250367482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9766876697540283,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049816958606243134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058652639389038086,
      "backward_entropy": 0.0054692961275577545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.440617561340332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0498332716524601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05859565734863281,
      "backward_entropy": 0.0054722631143199075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9311668872833252,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049850039184093475,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0585361123085022,
      "backward_entropy": 0.005475838979085286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.48119500279426575,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04986676201224327,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05847656726837158,
      "backward_entropy": 0.005479377177026536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.47763100266456604,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04988231882452965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058422648906707765,
      "backward_entropy": 0.005484387692477968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9368848204612732,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0498967207968235,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05837472677230835,
      "backward_entropy": 0.008976004189915128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.4702642261981964,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049910515546798706,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058329707384109496,
      "backward_entropy": 0.005494017981820636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.289454221725464,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049923207610845566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05829112529754639,
      "backward_entropy": 0.005496604161130058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2633230686187744,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04993647336959839,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058249902725219724,
      "backward_entropy": 0.00549675938155916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.687516450881958,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04995042458176613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05820465087890625,
      "backward_entropy": 0.005497669180234273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.331134557723999,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04996529221534729,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0581546425819397,
      "backward_entropy": 0.005497872001594967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8804379105567932,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04997989535331726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05810556411743164,
      "backward_entropy": 0.005499069475465351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.735169529914856,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04999383166432381,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05805978178977966,
      "backward_entropy": 0.005500517785549164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8619289398193359,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050007909536361694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05801317095756531,
      "backward_entropy": 0.00882727735572391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6969283819198608,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050021346658468246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05796983242034912,
      "backward_entropy": 0.005502983927726746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8448371887207031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05003504082560539,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05792468786239624,
      "backward_entropy": 0.005505323823955324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8348113894462585,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050048068165779114,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05788331031799317,
      "backward_entropy": 0.005506580074628194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2362515926361084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05006057024002075,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05784450769424439,
      "backward_entropy": 0.008751293023427328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2233620882034302,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05007297173142433,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057805979251861574,
      "backward_entropy": 0.005509922901789348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2119134664535522,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050085313618183136,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05776746273040771,
      "backward_entropy": 0.005512553370661206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3933489322662354,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05009756237268448,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0577292263507843,
      "backward_entropy": 0.005515288561582565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7936578392982483,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05011080950498581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05768555402755737,
      "backward_entropy": 0.00551757092277209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9540444612503052,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05012349411845207,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05764468908309937,
      "backward_entropy": 0.005520083010196686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7759614586830139,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05013673007488251,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05760089159011841,
      "backward_entropy": 0.0055219804247220354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.15007483959198,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05014945566654205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05755938291549682,
      "backward_entropy": 0.005525089800357819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1368379592895508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05016200244426727,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057518744468688966,
      "backward_entropy": 0.005527939647436142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1253408193588257,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05017445981502533,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0574781060218811,
      "backward_entropy": 0.005531809396213955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4826462268829346,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05018680915236473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057437777519226074,
      "backward_entropy": 0.0055362627738051945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4685161113739014,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05019943416118622,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057395488023757935,
      "backward_entropy": 0.00554144423868921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.3688642382621765,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050212208181619644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057352519035339354,
      "backward_entropy": 0.005545673684941398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.080538034439087,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05022402107715607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057314860820770266,
      "backward_entropy": 0.0055489130318164825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7148929238319397,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050235673785209656,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.057278227806091306,
      "backward_entropy": 0.008529050482643975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.05915105342865,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050246864557266235,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05724401473999023,
      "backward_entropy": 0.008511450555589464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.3537605106830597,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05025795102119446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057210403680801394,
      "backward_entropy": 0.0055541106396251256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6944923400878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050268277525901794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05718086957931519,
      "backward_entropy": 0.005555131369166904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0286242961883545,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05027826875448227,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05715304613113403,
      "backward_entropy": 0.005556092494063907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.694522738456726,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05028833821415901,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057124412059783934,
      "backward_entropy": 0.005557576815287272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6756391525268555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05029920116066933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05709095597267151,
      "backward_entropy": 0.005560000737508138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9930294752120972,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05030965059995651,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05705971121788025,
      "backward_entropy": 0.005561974313524034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.005794017110019922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05032121762633324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05702173709869385,
      "backward_entropy": 0.00556539785530832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6548864245414734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05033165216445923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0569898247718811,
      "backward_entropy": 0.0055690039363172315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.3286108672618866,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05034177750349045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05695932507514954,
      "backward_entropy": 0.005573229657279121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.3257296681404114,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05035117641091347,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0569330096244812,
      "backward_entropy": 0.005576242175367143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2709436416625977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050359923392534256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05691041946411133,
      "backward_entropy": 0.005578155318895976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6329213976860046,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05036919191479683,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05688457489013672,
      "backward_entropy": 0.005580684791008632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9394224286079407,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0503782220184803,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05685985088348389,
      "backward_entropy": 0.005583314018117057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.546998143196106,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050387319177389145,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0568347692489624,
      "backward_entropy": 0.005585084358851115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5329418182373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05039722099900246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05680478811264038,
      "backward_entropy": 0.005587403145101335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9122565984725952,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050407808274030685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05677075982093811,
      "backward_entropy": 0.005589653634362751,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 1.6188671688176692,
    "avg_log_Z": -0.049776698350906375,
    "success_rate": 1.0,
    "avg_reward": 72.4,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.0,
      "1": 0.22,
      "2": 0.78
    },
    "avg_forward_entropy": 0.05875894725322724,
    "avg_backward_entropy": 0.006337776858773497,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}