{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07687303755018446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07684788439008924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.31260681152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948888063430787,
      "backward_entropy": 0.07685518264770508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.28466796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10949199199676514,
      "backward_entropy": 0.07685034142600165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.81443786621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00019999995129182935,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10949501991271973,
      "backward_entropy": 0.07685278521643744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.22894287109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0002994098176714033,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10949786901473998,
      "backward_entropy": 0.07688625653584798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.27476501464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00039913636283017695,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10950064659118652,
      "backward_entropy": 0.07685744762420654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.17320251464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0004994955961592495,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950322151184082,
      "backward_entropy": 0.07686432864930895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.18511962890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0005997008411213756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950561761856079,
      "backward_entropy": 0.07686614990234375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.6872100830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0006990508991293609,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10950785875320435,
      "backward_entropy": 0.07686422268549602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.12428283691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0007978698704391718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950999259948731,
      "backward_entropy": 0.07686951425340441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.9718780517578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0008950568735599518,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1095120906829834,
      "backward_entropy": 0.07691033681233723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.03042602539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.000993954367004335,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10951410531997681,
      "backward_entropy": 0.07687003082699245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.95497131347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00109211599919945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095160722732544,
      "backward_entropy": 0.07687403096093072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.03472900390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0011905195424333215,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10951797962188721,
      "backward_entropy": 0.07687364684210883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.01063537597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001288461615331471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10951976776123047,
      "backward_entropy": 0.076876786020067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.39901733398438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0013835490681231022,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10952152013778686,
      "backward_entropy": 0.07692695326275295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.96543884277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0014803773956373334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952316522598267,
      "backward_entropy": 0.07687917020585802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.3548126220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0015758335357531905,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10952471494674683,
      "backward_entropy": 0.07687993844350179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.97512817382812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0016710411291569471,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10952622890472412,
      "backward_entropy": 0.07688134246402317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.42568969726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001765335677191615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095276117324829,
      "backward_entropy": 0.07688230938381618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.20123291015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0018587594386190176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952887535095215,
      "backward_entropy": 0.07688321669896443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.36607360839844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0019542158115655184,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953006744384766,
      "backward_entropy": 0.07694521215226915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.59123229980469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0020498293451964855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095310926437378,
      "backward_entropy": 0.0768850909339057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.28269958496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0021391219925135374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095320701599121,
      "backward_entropy": 0.07688723670111762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.14617156982422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0022279168479144573,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953299999237061,
      "backward_entropy": 0.07688816388448079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.76747131347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0023114944342523813,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953379869461059,
      "backward_entropy": 0.07688886589474148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.16470336914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0023954068310558796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095345139503479,
      "backward_entropy": 0.07688710424635145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.97079467773438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0024793059565126896,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953525304794312,
      "backward_entropy": 0.07695982191297743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.12294006347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002559662563726306,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953590869903565,
      "backward_entropy": 0.07696183522542317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.32813262939453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0026404771488159895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953655242919921,
      "backward_entropy": 0.07689105139838324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.8165054321289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0027193583082407713,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953737497329712,
      "backward_entropy": 0.07696562343173557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.8760986328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0027946780901402235,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10953817367553711,
      "backward_entropy": 0.07688747511969672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.54754638671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0028690819162875414,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953885316848755,
      "backward_entropy": 0.07696895466910468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.88485717773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002948031760752201,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953937768936158,
      "backward_entropy": 0.07689183950424194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.44105529785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0030304112005978823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10953984260559083,
      "backward_entropy": 0.07688707775539821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.5980987548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003116125473752618,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954017639160156,
      "backward_entropy": 0.07689236932330662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.2503890991211,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0032006597612053156,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954045057296753,
      "backward_entropy": 0.07697562376658122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.03924560546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0032812110148370266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954073667526246,
      "backward_entropy": 0.0768926011191474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.60137939453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0033628777600824833,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954091548919678,
      "backward_entropy": 0.07697826623916626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.41192626953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00345020554959774,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954095125198364,
      "backward_entropy": 0.07697961065504286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 261.3223876953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0035374921280890703,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954093933105469,
      "backward_entropy": 0.076980902089013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.58659362792969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0036316101904958487,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954078435897827,
      "backward_entropy": 0.07689294550153944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.64266967773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0037222339306026697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10954059362411499,
      "backward_entropy": 0.07688531610700819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.429443359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0038164197467267513,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954018831253051,
      "backward_entropy": 0.07698481612735325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.5876922607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00391016760841012,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953965187072753,
      "backward_entropy": 0.07689309120178223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.20643615722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004005575552582741,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10953900814056397,
      "backward_entropy": 0.07688438892364502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.65196228027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00410139374434948,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095382809638977,
      "backward_entropy": 0.07688400480482313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.76832580566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004197466652840376,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10953744649887084,
      "backward_entropy": 0.07688356770409478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.65386962890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004298620391637087,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953638553619385,
      "backward_entropy": 0.07689299848344591,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.2274169921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004399577621370554,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953516960144043,
      "backward_entropy": 0.0769917435116238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.8058624267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004499038215726614,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953388214111329,
      "backward_entropy": 0.0768929123878479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.70599365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004595773294568062,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953253507614136,
      "backward_entropy": 0.0768926805920071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.7935791015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004691597539931536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953106880187988,
      "backward_entropy": 0.0768923627005683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.71316528320312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004787096753716469,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10952936410903931,
      "backward_entropy": 0.07699524031745063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.16078186035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004882073029875755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10952750444412232,
      "backward_entropy": 0.07689154148101807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.5215301513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004976468160748482,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10952551364898681,
      "backward_entropy": 0.07689103153016832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.20094299316406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005071727558970451,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109523344039917,
      "backward_entropy": 0.0769974258210924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.848876953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005164826288819313,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10952103137969971,
      "backward_entropy": 0.07699809471766154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.8574981689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005260292906314135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10951845645904541,
      "backward_entropy": 0.07688922352261013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.58241271972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0053561595268547535,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10951577425003052,
      "backward_entropy": 0.07699943913353814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.70477294921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00545502919703722,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1095129132270813,
      "backward_entropy": 0.07700010140736897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.5176239013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005555357318371534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950988531112671,
      "backward_entropy": 0.07687494489881727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.74729919433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0056581380777060986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950667858123779,
      "backward_entropy": 0.07687420315212673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.99278259277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005760458763688803,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10950338840484619,
      "backward_entropy": 0.07688603798548381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.3272247314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00586159760132432,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10949991941452027,
      "backward_entropy": 0.07687249448564318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.33003234863281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005961065646260977,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10949642658233642,
      "backward_entropy": 0.07687147458394368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.33250427246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006055108271539211,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10949294567108155,
      "backward_entropy": 0.0768833425309923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.25160217285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006148488260805607,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948935747146607,
      "backward_entropy": 0.07688217692905003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.6423797607422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006241029594093561,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948572158813477,
      "backward_entropy": 0.07700432671440972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.07925415039062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006338303908705711,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094820499420166,
      "backward_entropy": 0.07700473070144653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.98756408691406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006439595948904753,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10947836637496948,
      "backward_entropy": 0.07700520091586643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.24404907226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006544169969856739,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.109474515914917,
      "backward_entropy": 0.0768636663754781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.69775390625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0066509004682302475,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10947041511535645,
      "backward_entropy": 0.07700624068578084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.4359130859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006757527124136686,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109466290473938,
      "backward_entropy": 0.07700672414567736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 258.1003723144531,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006863177288323641,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10946208238601685,
      "backward_entropy": 0.07700718773735894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.328125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006974251940846443,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945749282836914,
      "backward_entropy": 0.07700769106547038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.73402404785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007087421137839556,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945272445678711,
      "backward_entropy": 0.0770081811481052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.3347625732422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0072003379464149475,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10944775342941285,
      "backward_entropy": 0.07700864473978679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.04995727539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007313504349440336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10944271087646484,
      "backward_entropy": 0.07686984539031982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.3623504638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007427869830280542,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10943737030029296,
      "backward_entropy": 0.07686869965659247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.8307800292969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007540709804743528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10943185091018677,
      "backward_entropy": 0.07685214943355984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.77015686035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007657897658646107,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10942596197128296,
      "backward_entropy": 0.07701040638817681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.67523193359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007773059885948896,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094199538230896,
      "backward_entropy": 0.07686491145028009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 292.67852783203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007888183929026127,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10941386222839355,
      "backward_entropy": 0.0768634941842821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.14364624023438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008008986711502075,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10940740108489991,
      "backward_entropy": 0.0770115852355957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.61424255371094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008127134293317795,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10940085649490357,
      "backward_entropy": 0.07686094443003337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.77162170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00824499037116766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10939416885375977,
      "backward_entropy": 0.07685950067308214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.81796264648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008359149098396301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1093874454498291,
      "backward_entropy": 0.07684161927964953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.66973876953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008472497574985027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938057899475098,
      "backward_entropy": 0.07683967219458686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.80224609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008584653958678246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10937365293502807,
      "backward_entropy": 0.07683749993642171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.59478759765625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0086964201182127,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10936652421951294,
      "backward_entropy": 0.07701338662041558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.43702697753906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008804836310446262,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935940742492675,
      "backward_entropy": 0.07701356543434991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.34194946289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00891419593244791,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935205221176147,
      "backward_entropy": 0.07701373100280762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.1630401611328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009024154394865036,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10934451818466187,
      "backward_entropy": 0.0770138767030504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.3407745361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009129293262958527,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10933712720870972,
      "backward_entropy": 0.07684323522779676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.30812072753906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009233441203832626,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10932954549789428,
      "backward_entropy": 0.07701404889424641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.99180603027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009336700662970543,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10932176113128662,
      "backward_entropy": 0.076818671491411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.94960021972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009442606940865517,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10931364297866822,
      "backward_entropy": 0.07683510250515407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.5413589477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00955083966255188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10930520296096802,
      "backward_entropy": 0.0768124262491862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.54092407226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009651794098317623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10929707288742066,
      "backward_entropy": 0.0768088632159763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.61354064941406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009754089638590813,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10928871631622314,
      "backward_entropy": 0.07701422770818074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.78074645996094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009855516254901886,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10928016901016235,
      "backward_entropy": 0.07701421446270412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.26605224609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009959743358194828,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109271240234375,
      "backward_entropy": 0.07701420783996582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.7270965576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01006654929369688,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10926189422607421,
      "backward_entropy": 0.07681478394402398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.22409057617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010170506313443184,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10925252437591552,
      "backward_entropy": 0.0768109228875902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.6645965576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010272028855979443,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10924310684204101,
      "backward_entropy": 0.07680687639448378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.3401336669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010371246375143528,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10923361778259277,
      "backward_entropy": 0.07680265108744304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.76473999023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01047486625611782,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10922359228134156,
      "backward_entropy": 0.07677753766377766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.93702697753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010582242161035538,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10921304225921631,
      "backward_entropy": 0.0767735309071011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.19505310058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010688862763345242,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1092022180557251,
      "backward_entropy": 0.07676930560006036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.20680236816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01079897303134203,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10919079780578614,
      "backward_entropy": 0.07678627967834473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.70680236816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010907038114964962,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10917916297912597,
      "backward_entropy": 0.07678205437130398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.35838317871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011010553687810898,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10916751623153687,
      "backward_entropy": 0.07675631841023763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.39657592773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011114038527011871,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10915553569793701,
      "backward_entropy": 0.07677347130245632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.16090393066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01121492963284254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10914345979690551,
      "backward_entropy": 0.07674668894873725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.07281494140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011315594427287579,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1091310739517212,
      "backward_entropy": 0.0767415099673801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.2920379638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011415433138608932,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10911834239959717,
      "backward_entropy": 0.07673626475863987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.15312194824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011513051576912403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10910549163818359,
      "backward_entropy": 0.07673082086775038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.48760986328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011604581028223038,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10909292697906495,
      "backward_entropy": 0.07672497961256239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.74732971191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011693221516907215,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10908036231994629,
      "backward_entropy": 0.07701369126637776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.50189208984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011784657835960388,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10906727313995361,
      "backward_entropy": 0.0767127341694302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.53587341308594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011877245269715786,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10905381441116332,
      "backward_entropy": 0.07701357205708821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.80860900878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011967993341386318,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10904030799865723,
      "backward_entropy": 0.07672887378268772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.7725067138672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012062652967870235,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10902637243270874,
      "backward_entropy": 0.07701346609327528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.0534210205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012157131917774677,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10901213884353637,
      "backward_entropy": 0.0767180389828152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.05148315429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01225390937179327,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089971661567688,
      "backward_entropy": 0.07671258846918742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.41566467285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012349671684205532,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10898196697235107,
      "backward_entropy": 0.07667519648869832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.43556213378906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012443515472114086,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10896661281585693,
      "backward_entropy": 0.07701337999767727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.15966796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012535891495645046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089510202407837,
      "backward_entropy": 0.07669505145814684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.1533203125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012629342265427113,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1089349627494812,
      "backward_entropy": 0.07701338662041558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.59770202636719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012727861292660236,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10891863107681274,
      "backward_entropy": 0.07664808962080213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.00611877441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012818470597267151,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10890295505523681,
      "backward_entropy": 0.07664072513580322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.14561462402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012906013987958431,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10888738632202148,
      "backward_entropy": 0.07663293679555257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.36974334716797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012991570867598057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10887155532836915,
      "backward_entropy": 0.07662504249148899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.96397399902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013073529116809368,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1088560700416565,
      "backward_entropy": 0.07661676406860352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.79190063476562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013153565116226673,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1088406801223755,
      "backward_entropy": 0.07701314820183648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.33489990234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013235751539468765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10882480144500732,
      "backward_entropy": 0.07659947872161865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.39231872558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01332024671137333,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10880832672119141,
      "backward_entropy": 0.07663483089870876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.24501037597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013402708806097507,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10879178047180176,
      "backward_entropy": 0.07662752601835462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.8644561767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013487471267580986,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10877432823181152,
      "backward_entropy": 0.07662032710181342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.7705078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013575800694525242,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10875608921051025,
      "backward_entropy": 0.07656372918023004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.17567443847656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0136679382994771,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10873713493347167,
      "backward_entropy": 0.07701277732849121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.77638244628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013762478716671467,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10871754884719849,
      "backward_entropy": 0.07660054498248631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.41824340820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013860630802810192,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10869717597961426,
      "backward_entropy": 0.07653717199961345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.08233642578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013958166353404522,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1086765170097351,
      "backward_entropy": 0.07658763726552327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.01564025878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014052090235054493,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10865602493286133,
      "backward_entropy": 0.07658050457636516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.64395141601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014149329625070095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10863476991653442,
      "backward_entropy": 0.07657354407840306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.27976989746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014244524762034416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10861345529556274,
      "backward_entropy": 0.07649946212768555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.08883666992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014340546913444996,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10859165191650391,
      "backward_entropy": 0.07655862967173259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.72341918945312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01443477999418974,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10856972932815552,
      "backward_entropy": 0.07701320118374294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.60025024414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014521905221045017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10854847431182861,
      "backward_entropy": 0.07654182116190593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.81373596191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014610584825277328,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10852664709091187,
      "backward_entropy": 0.07645987139807807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.4992218017578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014704753644764423,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10850365161895752,
      "backward_entropy": 0.07652429077360365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.83811950683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014799728989601135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10848015546798706,
      "backward_entropy": 0.07644034756554498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.7742919921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014893912710249424,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10845643281936646,
      "backward_entropy": 0.07701325416564941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.00294494628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014986236579716206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10843262672424317,
      "backward_entropy": 0.07641907533009847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.08251953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015081321820616722,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10840795040130616,
      "backward_entropy": 0.07640839285320705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.35667419433594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015171727165579796,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1083836555480957,
      "backward_entropy": 0.07701322767469618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.12777709960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01526384986937046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10835865736007691,
      "backward_entropy": 0.07638563050164117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.03530883789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015354455448687077,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10833349227905273,
      "backward_entropy": 0.0764565732744005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.5100555419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015441094525158405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10830861330032349,
      "backward_entropy": 0.07636155022515191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.60316467285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015526656061410904,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10828351974487305,
      "backward_entropy": 0.07643342018127441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.9556121826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015615483745932579,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10825746059417725,
      "backward_entropy": 0.07642179065280491,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.91450500488281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015706853941082954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1082306146621704,
      "backward_entropy": 0.07632305224736531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.432373046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01579299569129944,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1082042932510376,
      "backward_entropy": 0.07630940278371175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.34228515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015884840860962868,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10817652940750122,
      "backward_entropy": 0.07701326741112603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.80157470703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015975335612893105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10814857482910156,
      "backward_entropy": 0.07628207074271308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.65699768066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016063323244452477,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10812065601348878,
      "backward_entropy": 0.07636084821489122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.7307586669922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0161499734967947,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10809259414672852,
      "backward_entropy": 0.07701306872897679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.7241439819336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016235951334238052,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10806417465209961,
      "backward_entropy": 0.07633340358734131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.5807647705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016318704932928085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10803555250167847,
      "backward_entropy": 0.07622252570258246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.59002685546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016402369365096092,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10800566673278808,
      "backward_entropy": 0.07630281978183323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.87229919433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01648993417620659,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10797425508499145,
      "backward_entropy": 0.07628732257419163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.5732192993164,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01658034883439541,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1079416275024414,
      "backward_entropy": 0.07701219452752008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.42821502685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016665641218423843,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10790948867797852,
      "backward_entropy": 0.07625494400660197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.19261932373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01674429327249527,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10787832736968994,
      "backward_entropy": 0.0761434899436103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.16807556152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016816694289445877,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10784801244735717,
      "backward_entropy": 0.07701105541653103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.63189697265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016892263665795326,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10781519412994385,
      "backward_entropy": 0.07619739903344049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.80170440673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016967661678791046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10778176784515381,
      "backward_entropy": 0.07608948813544379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.51409912109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01703902706503868,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10774850845336914,
      "backward_entropy": 0.0770095255639818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.61720275878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017113836482167244,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10771361589431763,
      "backward_entropy": 0.07613799307081434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.13623046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017190827056765556,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1076773762702942,
      "backward_entropy": 0.07700868447621663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.01800537109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01726846769452095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10764021873474121,
      "backward_entropy": 0.07609800497690837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.78134155273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01734643243253231,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1076022744178772,
      "backward_entropy": 0.07607765992482503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.71043395996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01742732711136341,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10756285190582275,
      "backward_entropy": 0.07597694132063124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.24169921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01751253567636013,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10752158164978028,
      "backward_entropy": 0.0760381884045071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.95982360839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017598621547222137,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10747936964035035,
      "backward_entropy": 0.07594133085674709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017688089981675148,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10743554830551147,
      "backward_entropy": 0.07600002818637425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.23818969726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01777629368007183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10739140510559082,
      "backward_entropy": 0.07590523031022814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.25141143798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01786043308675289,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10734777450561524,
      "backward_entropy": 0.07595937119589911,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.3017578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01794118992984295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10730447769165039,
      "backward_entropy": 0.07586454020606147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.26219177246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018017413094639778,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1072619080543518,
      "backward_entropy": 0.07700705528259277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.6349868774414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018097056075930595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10721760988235474,
      "backward_entropy": 0.0758210817972819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.33770751953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018173912540078163,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10717350244522095,
      "backward_entropy": 0.07586786482069227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.3791961669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01825491338968277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10712742805480957,
      "backward_entropy": 0.07577668295966254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.20940399169922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018338188529014587,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10708026885986328,
      "backward_entropy": 0.07582436667548285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.49608612060547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018415186554193497,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10703451633453369,
      "backward_entropy": 0.07580086919996473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.23919677734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018489951267838478,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10698884725570679,
      "backward_entropy": 0.07570579979154798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.96998596191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018563924357295036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10694273710250854,
      "backward_entropy": 0.0756801962852478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.37474060058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018638404086232185,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10689630508422851,
      "backward_entropy": 0.07572570112016466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.541259765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0187132116407156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10684980154037475,
      "backward_entropy": 0.07562726073794895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.1062240600586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0187936220318079,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10680098533630371,
      "backward_entropy": 0.07560087574852838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.8849639892578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018868612125515938,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10675350427627564,
      "backward_entropy": 0.07700321409437391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.51390075683594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018942538648843765,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10670584440231323,
      "backward_entropy": 0.0756235917409261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.41625213623047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01902100443840027,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10665597915649414,
      "backward_entropy": 0.07700252532958984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.98052978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019096428528428078,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10660684108734131,
      "backward_entropy": 0.07548730240927802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.17787170410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01917211525142193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10655701160430908,
      "backward_entropy": 0.07545726829104954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.11705017089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01924682967364788,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10650684833526611,
      "backward_entropy": 0.07551702525880602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 234.19515991210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01932067796587944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1064563512802124,
      "backward_entropy": 0.07548852761586507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.67039489746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0194021537899971,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10640232563018799,
      "backward_entropy": 0.07546204990810818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.63670349121094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019485827535390854,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10634677410125733,
      "backward_entropy": 0.07700112130906847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.48919677734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019574003294110298,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1062887191772461,
      "backward_entropy": 0.07531201839447021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.57415771484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019660888239741325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10623042583465576,
      "backward_entropy": 0.07538359695010716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.6336898803711,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01974145509302616,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10617396831512452,
      "backward_entropy": 0.07700128025478786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.69558715820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019814224913716316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10612008571624756,
      "backward_entropy": 0.0752198961046007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.927001953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019889453426003456,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10606441497802735,
      "backward_entropy": 0.0752907329135471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 264.3463439941406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019962552934885025,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10600886344909669,
      "backward_entropy": 0.07699992921617296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.4656219482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02004636637866497,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1059482216835022,
      "backward_entropy": 0.07512279351552327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.11834716796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02012839913368225,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10588754415512085,
      "backward_entropy": 0.07509105735354954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.82879638671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020210323855280876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10582613945007324,
      "backward_entropy": 0.07516576184166802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.553466796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020293910056352615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10576319694519043,
      "backward_entropy": 0.07502683003743489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.5819549560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020374255254864693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10570094585418702,
      "backward_entropy": 0.07510002454121907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.96807098388672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020452769473195076,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10563874244689941,
      "backward_entropy": 0.07699929343329535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.64678955078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020528698340058327,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10557695627212524,
      "backward_entropy": 0.07492125034332275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.12643432617188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020608272403478622,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10551269054412842,
      "backward_entropy": 0.07699857817755805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.77699279785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020687883719801903,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10544762611389161,
      "backward_entropy": 0.0769983000225491,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.50579833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020770082250237465,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10538089275360107,
      "backward_entropy": 0.07491922378540039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.23294067382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02085220068693161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10531344413757324,
      "backward_entropy": 0.0747750202814738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.15985107421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020932616665959358,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10524603128433227,
      "backward_entropy": 0.07699785629908244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.80197143554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021011509001255035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1051785945892334,
      "backward_entropy": 0.07481013403998481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.62765502929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02109081670641899,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10511012077331543,
      "backward_entropy": 0.07477205329471165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.47772216796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0211691465228796,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10504132509231567,
      "backward_entropy": 0.07699702845679389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.0198974609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021246319636702538,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10497229099273682,
      "backward_entropy": 0.07457322544521755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.5745620727539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02132396586239338,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10490214824676514,
      "backward_entropy": 0.0746514532301161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.2393035888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02139734849333763,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10483342409133911,
      "backward_entropy": 0.07460802131228977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.3138427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02147117629647255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.104763662815094,
      "backward_entropy": 0.07443879710303412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.61212158203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021545980125665665,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10469249486923218,
      "backward_entropy": 0.07451969385147095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.83424377441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02162124216556549,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10462025403976441,
      "backward_entropy": 0.07447477181752522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.43873596191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021702876314520836,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10454370975494384,
      "backward_entropy": 0.07443216111924914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.88958740234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021784277632832527,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10446643829345703,
      "backward_entropy": 0.07438867621951634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.34022521972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021865619346499443,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10438827276229859,
      "backward_entropy": 0.07434433036380345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.17019653320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02194705419242382,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10430915355682373,
      "backward_entropy": 0.07415871487723456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.21092224121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022029653191566467,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10422848463058472,
      "backward_entropy": 0.07411006424162123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.57166290283203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022117875516414642,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10414372682571411,
      "backward_entropy": 0.07699349191453722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.021240234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02220124378800392,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1040609359741211,
      "backward_entropy": 0.0769933991962009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.37046813964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022281771525740623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10397894382476806,
      "backward_entropy": 0.07396242353651258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.8289794921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02236097864806652,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1038968324661255,
      "backward_entropy": 0.07390961382124159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.79885864257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022439146414399147,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10381443500518799,
      "backward_entropy": 0.07401145829094781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.24500274658203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022515008226037025,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10373256206512452,
      "backward_entropy": 0.07395699289109972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.8408203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022587263956665993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10365209579467774,
      "backward_entropy": 0.07374107837677002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.5773696899414,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02266480214893818,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10356737375259399,
      "backward_entropy": 0.07699024015002781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 213.05287170410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022740107029676437,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10348318815231324,
      "backward_entropy": 0.07362851831648085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.139404296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02282186783850193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10339399576187133,
      "backward_entropy": 0.07357390721638997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.58396911621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022903436794877052,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10330402851104736,
      "backward_entropy": 0.0736743344200982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.5458755493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022984983399510384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10321316719055176,
      "backward_entropy": 0.07345918814341228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.93048095703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02306317910552025,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1031257152557373,
      "backward_entropy": 0.07698793543709649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.55638122558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023141849786043167,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10303733348846436,
      "backward_entropy": 0.07698726654052734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.69932556152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023223677650094032,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10294598340988159,
      "backward_entropy": 0.07343125343322754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.01666259765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023303667083382607,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10285488367080689,
      "backward_entropy": 0.07336722479926215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.02969360351562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02338547445833683,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10276179313659668,
      "backward_entropy": 0.07330291801028782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.43226623535156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023463265970349312,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10267090797424316,
      "backward_entropy": 0.07698511415057713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.87049865722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02354389615356922,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10257716178894043,
      "backward_entropy": 0.07316729757520887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.71449279785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023622969165444374,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10248363018035889,
      "backward_entropy": 0.07698388894399007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.3882293701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023706825450062752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1023859977722168,
      "backward_entropy": 0.07286443975236681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.66432189941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023794395849108696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10228486061096191,
      "backward_entropy": 0.0729631847805447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.59677124023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023887045681476593,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10217938423156739,
      "backward_entropy": 0.07272840870751275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.12217712402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023977862671017647,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10207434892654418,
      "backward_entropy": 0.07283210754394531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.72862243652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02406746707856655,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10196914672851562,
      "backward_entropy": 0.07276342974768744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.98118591308594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02416207268834114,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10185960531234742,
      "backward_entropy": 0.07698476314544678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.21484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02425536699593067,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10175011157989503,
      "backward_entropy": 0.0726286702685886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 287.4165344238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024354852735996246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10163533687591553,
      "backward_entropy": 0.07237004571490818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.90835571289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02446506731212139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1015125036239624,
      "backward_entropy": 0.07230767938825819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.84104919433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024570995941758156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1013918399810791,
      "backward_entropy": 0.07224112749099731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.243896484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02467707172036171,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10127007961273193,
      "backward_entropy": 0.07237908575269911,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.10812377929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02478731796145439,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10114433765411376,
      "backward_entropy": 0.07210446728600396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.72764587402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024894313886761665,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10102012157440185,
      "backward_entropy": 0.07224875688552856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.39157104492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024999096989631653,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10089668035507202,
      "backward_entropy": 0.07217807239956325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.7077178955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025101322680711746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10077433586120606,
      "backward_entropy": 0.07187905576494005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.62750244140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02520110458135605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10065294504165649,
      "backward_entropy": 0.07202506065368652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.24407958984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025297779589891434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1005331039428711,
      "backward_entropy": 0.07194252146614923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.45010375976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025394482538104057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10041227340698242,
      "backward_entropy": 0.07162494129604763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.9373779296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025483721867203712,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10029677152633668,
      "backward_entropy": 0.0717664361000061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.74305725097656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025567609816789627,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10018491744995117,
      "backward_entropy": 0.07699010107252333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.05059814453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02564268186688423,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10007960796356201,
      "backward_entropy": 0.07132626242107815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.99179077148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02571834996342659,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09997273683547973,
      "backward_entropy": 0.07145559125476414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.51702880859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025797255337238312,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09986207485198975,
      "backward_entropy": 0.07134898503621419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.18267822265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025870319455862045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09975562691688537,
      "backward_entropy": 0.07123564349280463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.4170150756836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025942835956811905,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09964861869812011,
      "backward_entropy": 0.07112029525968763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.89479064941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02601168118417263,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0995438575744629,
      "backward_entropy": 0.07076755497190687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.71815490722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026081634685397148,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09943718314170838,
      "backward_entropy": 0.07064798805448744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.63136291503906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026155302301049232,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09932630658149719,
      "backward_entropy": 0.0769689679145813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.31008911132812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02622809261083603,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09921554327011109,
      "backward_entropy": 0.0706401268641154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.6253662109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026302345097064972,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09910283088684083,
      "backward_entropy": 0.07696362336476643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.93785095214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02637777104973793,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09898838996887208,
      "backward_entropy": 0.0703989929623074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.27833557128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02645411156117916,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09887242317199707,
      "backward_entropy": 0.07027795579698351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.52227020263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026529625058174133,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09875650405883789,
      "backward_entropy": 0.0701545344458686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.21589660644531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02660243585705757,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09864225387573242,
      "backward_entropy": 0.07002655665079753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.09063720703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026673562824726105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09852885007858277,
      "backward_entropy": 0.06968363788392809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.03833770751953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026750268414616585,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09840962886810303,
      "backward_entropy": 0.06976961427264744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.6501922607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026821551844477654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09829488396644592,
      "backward_entropy": 0.06943341096242268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.23971557617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026893019676208496,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09817880392074585,
      "backward_entropy": 0.06950079070197211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.86497497558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0269687008112669,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09805810451507568,
      "backward_entropy": 0.06936878628200954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.91127014160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027043506503105164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09793738126754761,
      "backward_entropy": 0.06903685463799371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.86506652832031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027120403945446014,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.097813880443573,
      "backward_entropy": 0.07694207297431098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.46009063720703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027196908369660378,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0976901650428772,
      "backward_entropy": 0.06896279255549113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.34788513183594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0272716972976923,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09756767153739929,
      "backward_entropy": 0.07693992720709907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.538246154785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027345722541213036,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09744528532028199,
      "backward_entropy": 0.06868040561676025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.75566101074219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0274122916162014,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09733022451400757,
      "backward_entropy": 0.06852763228946263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.39094543457031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0274764746427536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09721697568893432,
      "backward_entropy": 0.06837048795488146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.19700622558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027537908405065536,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09710594415664672,
      "backward_entropy": 0.07692978779474895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.8832244873047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02760162204504013,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09699178338050843,
      "backward_entropy": 0.07692688041263157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.66238403320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02766496315598488,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09687730073928832,
      "backward_entropy": 0.06775192419687907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.41242218017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027725812047719955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09676493406295776,
      "backward_entropy": 0.06759054793251885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.71829223632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027785181999206543,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09665367603302003,
      "backward_entropy": 0.06742904583613078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.36791229248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027839824557304382,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09654707312583924,
      "backward_entropy": 0.06736176543765598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.52175903320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027892857789993286,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09644162654876709,
      "backward_entropy": 0.06708978944354588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.99932861328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027950460091233253,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09633067846298218,
      "backward_entropy": 0.06699836254119873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.88961791992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028017917647957802,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09620900154113769,
      "backward_entropy": 0.06683116488986546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.3156280517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028089987114071846,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09608200788497925,
      "backward_entropy": 0.06661048200395372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.23843383789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02816874347627163,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09594786763191224,
      "backward_entropy": 0.06651492913564046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.68312072753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028251390904188156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09580950736999512,
      "backward_entropy": 0.06632016764746772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.77635192871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028335588052868843,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09566935300827026,
      "backward_entropy": 0.06617662641737196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.8734893798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028425488620996475,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09552325010299682,
      "backward_entropy": 0.06607210636138916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.3521728515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028516646474599838,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09537568092346191,
      "backward_entropy": 0.06592714124255711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.52777862548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028602592647075653,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0952336311340332,
      "backward_entropy": 0.06577154000600179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.62928009033203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028685826808214188,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09509435892105103,
      "backward_entropy": 0.06560974650912815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.54310607910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02876753732562065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0949563980102539,
      "backward_entropy": 0.06544280052185059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.61383056640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028850896283984184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09481639862060547,
      "backward_entropy": 0.06525354915195042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.15814208984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02893529087305069,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09467505216598511,
      "backward_entropy": 0.06510559717814128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.99299621582031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029021063819527626,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09453204870224,
      "backward_entropy": 0.06493504842122395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.87960815429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02910507097840309,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09439078569412232,
      "backward_entropy": 0.0647586186726888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.05088806152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029189154505729675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09424929022789001,
      "backward_entropy": 0.06457632780075073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.63128662109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029277779161930084,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09410245418548584,
      "backward_entropy": 0.06440532869762844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.58738708496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02936854586005211,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09395246505737305,
      "backward_entropy": 0.06423827674653795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.119140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029451733455061913,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.093810772895813,
      "backward_entropy": 0.06405889987945557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.95634460449219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0295308455824852,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09367324113845825,
      "backward_entropy": 0.0638731320699056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.73167419433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029607968404889107,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09353755116462707,
      "backward_entropy": 0.06368447674645318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.9089126586914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02968752570450306,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09339873790740967,
      "backward_entropy": 0.0634453296661377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.503662109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029764601960778236,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09326262474060058,
      "backward_entropy": 0.06323934263653225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.86163330078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02984081394970417,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09312751293182372,
      "backward_entropy": 0.06311021910773383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02992091327905655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09298791885375976,
      "backward_entropy": 0.06291776233249241,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.98938751220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030006416141986847,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0928423285484314,
      "backward_entropy": 0.06272546450297038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.73462677001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03009021282196045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09269871711730956,
      "backward_entropy": 0.06252650419871013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.76141357421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03017127700150013,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09255829453468323,
      "backward_entropy": 0.06219192345937093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.16146850585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030255431309342384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09241453409194947,
      "backward_entropy": 0.06211776203579373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.650634765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030334096401929855,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09227749705314636,
      "backward_entropy": 0.06175283590952555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.32792663574219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030412372201681137,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09214084148406983,
      "backward_entropy": 0.0615280999077691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.50364685058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0304880253970623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09200717210769653,
      "backward_entropy": 0.06148076057434082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.64706420898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03056434541940689,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0918726921081543,
      "backward_entropy": 0.0610647996266683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.5486068725586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030643075704574585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09173566102981567,
      "backward_entropy": 0.06104589833153619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.2078857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03071766346693039,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09160366058349609,
      "backward_entropy": 0.060829467243618436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.5218276977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030791379511356354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0914726734161377,
      "backward_entropy": 0.06060473124186198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.33270263671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03086373209953308,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09134338498115539,
      "backward_entropy": 0.06011316511366102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.177001953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03093680739402771,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09121330976486205,
      "backward_entropy": 0.07689395878050062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.58934020996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031002802774310112,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0910920262336731,
      "backward_entropy": 0.05960609515508016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.9293212890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031072603538632393,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09096648693084716,
      "backward_entropy": 0.05935282839669122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.35230255126953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03114219941198826,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09084131717681884,
      "backward_entropy": 0.07688044177161323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.21336364746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031209519132971764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09071911573410034,
      "backward_entropy": 0.0588323540157742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.67040252685547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.031272485852241516,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09060229659080506,
      "backward_entropy": 0.07686970631281535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.60523223876953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03133322671055794,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09048817157745362,
      "backward_entropy": 0.07686281204223633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.8310089111328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03139122948050499,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09037739038467407,
      "backward_entropy": 0.05799504121144613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.17774963378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0314522460103035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09026304483413697,
      "backward_entropy": 0.0577168862024943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.3619384765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0315176397562027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09014366865158081,
      "backward_entropy": 0.05790515740712484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.30988311767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031586822122335434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09002025723457337,
      "backward_entropy": 0.05718708700603909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.23095703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03165535256266594,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08989814519882203,
      "backward_entropy": 0.056923627853393555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.7950439453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03172973170876503,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08977013826370239,
      "backward_entropy": 0.0768405728869968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.36299133300781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031809814274311066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08963642120361329,
      "backward_entropy": 0.05643021398120456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.60913848876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031888581812381744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08950499892234802,
      "backward_entropy": 0.05618036455578274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.44141387939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031963933259248734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08937821984291076,
      "backward_entropy": 0.05649295118119982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.8515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03203437849879265,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08925766944885254,
      "backward_entropy": 0.05564649899800619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.77301788330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03210345655679703,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08913906812667846,
      "backward_entropy": 0.05597559611002604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.02354431152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03216414526104927,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0890308141708374,
      "backward_entropy": 0.055070188310411244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.61647033691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032223254442214966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08892465233802796,
      "backward_entropy": 0.0554170376724667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.88185119628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03228802978992462,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08881232142448425,
      "backward_entropy": 0.05514536301294962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.595458984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03235035762190819,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08870338201522827,
      "backward_entropy": 0.05418248971303304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.34947204589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03241096809506416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08859681487083435,
      "backward_entropy": 0.054580377207862005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.1261749267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032476406544446945,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08848499059677124,
      "backward_entropy": 0.05359641710917155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.0317840576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03254358470439911,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08837157487869263,
      "backward_entropy": 0.053313884470197886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.35824584960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0326131246984005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08825606107711792,
      "backward_entropy": 0.053783026006486684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.67818832397461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03268146514892578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08814252614974975,
      "backward_entropy": 0.05276140570640564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.46392059326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03274449333548546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08803592920303345,
      "backward_entropy": 0.052466770013173424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.0210189819336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03280898183584213,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08792824745178222,
      "backward_entropy": 0.05217487282223172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.713863372802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03287447616457939,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08782004117965699,
      "backward_entropy": 0.05188287629021539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.62372589111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03293481096625328,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08771839737892151,
      "backward_entropy": 0.05239944325553046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.82998657226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032993558794260025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08761907815933227,
      "backward_entropy": 0.05210206574863858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.75003051757812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03305375948548317,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08751882314682007,
      "backward_entropy": 0.05180890692604913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.63516235351562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03311695158481598,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08741565346717835,
      "backward_entropy": 0.07678859763675266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.38885498046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033177658915519714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08731589317321778,
      "backward_entropy": 0.05122699009047614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.52781677246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033243365585803986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08721120357513427,
      "backward_entropy": 0.0509332021077474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.97482299804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03331303223967552,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08710294961929321,
      "backward_entropy": 0.05065192778905233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.87421989440918,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03338056802749634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08699790835380554,
      "backward_entropy": 0.04945439762539334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.88929748535156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03344075381755829,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08690186738967895,
      "backward_entropy": 0.07678405443827312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.9428482055664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03350333869457245,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08680387139320374,
      "backward_entropy": 0.04883630077044169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.45829772949219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03356388211250305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08670880794525146,
      "backward_entropy": 0.04947493473688761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.5684585571289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03362395614385605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08661499619483948,
      "backward_entropy": 0.04819914698600769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.80431365966797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03368394076824188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08652186989784241,
      "backward_entropy": 0.048857139216529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.39484405517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03374321013689041,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0864301085472107,
      "backward_entropy": 0.04853796296649509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.0347671508789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03379913419485092,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08634258508682251,
      "backward_entropy": 0.07676351070404053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.52528381347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03385542333126068,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08625515699386596,
      "backward_entropy": 0.04690188831753201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.7728500366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03391657769680023,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08616308569908142,
      "backward_entropy": 0.04756487078136868,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.61135864257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03398028761148453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08606905341148377,
      "backward_entropy": 0.047257688310411244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.18142700195312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03404243662953377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08597767353057861,
      "backward_entropy": 0.04694681035147773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.82746124267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03410506993532181,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08588690757751465,
      "backward_entropy": 0.045656184355417885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.70453643798828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0341644324362278,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0858004331588745,
      "backward_entropy": 0.07675083478291829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.61748504638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03422556445002556,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0857128381729126,
      "backward_entropy": 0.04501019583808051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.2202377319336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03428931161761284,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08562330007553101,
      "backward_entropy": 0.044695900546179876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.73910522460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034352414309978485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08553520441055298,
      "backward_entropy": 0.04437991645601061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.8613510131836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03441711142659187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08544632196426391,
      "backward_entropy": 0.04407117101881239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.2964859008789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034483086317777634,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08535687923431397,
      "backward_entropy": 0.04471138119697571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.76083374023438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034548889845609665,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08526842594146729,
      "backward_entropy": 0.04346840249167548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.26727294921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03461770713329315,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08517781496047974,
      "backward_entropy": 0.043178684181637235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.25769805908203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03468441590666771,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08509045243263244,
      "backward_entropy": 0.04287955164909363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.86373901367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034753087908029556,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08500235080718994,
      "backward_entropy": 0.04257863097720676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.63116455078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03482168912887573,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08491522073745728,
      "backward_entropy": 0.043214920494291514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.4564666748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03488588333129883,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08483327627182007,
      "backward_entropy": 0.041971292760637074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.6243896484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03495479375123978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08474749326705933,
      "backward_entropy": 0.04262861278322008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.541259765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03502580523490906,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0846606969833374,
      "backward_entropy": 0.04139039251539442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.0437240600586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03509439527988434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08457714319229126,
      "backward_entropy": 0.04109316733148363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.67405700683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035160984843969345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08449630737304688,
      "backward_entropy": 0.04174740115801493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.26727294921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03523142263293266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08441274762153625,
      "backward_entropy": 0.04145398404863146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.60430908203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03530777618288994,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08432475328445435,
      "backward_entropy": 0.04022739330927531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.80370330810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03539072722196579,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0842321276664734,
      "backward_entropy": 0.04092721144358317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.1864242553711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03547373786568642,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0841407299041748,
      "backward_entropy": 0.039712117777930364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.12090301513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035556480288505554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08405084609985351,
      "backward_entropy": 0.04041715131865607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.78138732910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03563782945275307,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08396379947662354,
      "backward_entropy": 0.040141175190607704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.38304901123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03571464121341705,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08388208150863648,
      "backward_entropy": 0.038895083798302546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.13087463378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03579133376479149,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08380159139633178,
      "backward_entropy": 0.03860613703727722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.20150756835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03587167710065842,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08371933102607727,
      "backward_entropy": 0.0383228063583374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.89119720458984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03594711422920227,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08364245891571045,
      "backward_entropy": 0.0389631986618042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.49352264404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03602178394794464,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08356699347496033,
      "backward_entropy": 0.037731955448786415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.1291961669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03609707951545715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0834917664527893,
      "backward_entropy": 0.03744183315171136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.44365692138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03617538511753082,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08341494798660279,
      "backward_entropy": 0.03715802894698249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.59770965576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03625199571251869,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.083340585231781,
      "backward_entropy": 0.037773973411983915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.7078628540039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03632628917694092,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0832689106464386,
      "backward_entropy": 0.03657999965879652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.91075134277344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03640061616897583,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08319817781448365,
      "backward_entropy": 0.07684388425615099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.8655242919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036479201167821884,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08312486410140991,
      "backward_entropy": 0.03601525558365716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.07462310791016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03656144440174103,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08304991722106933,
      "backward_entropy": 0.03574391868379381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.19066619873047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03663904219865799,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08298002481460572,
      "backward_entropy": 0.0362954404619005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.57549285888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03671230003237724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0829148530960083,
      "backward_entropy": 0.03598336047596402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.53536224365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03678623586893082,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08285012245178222,
      "backward_entropy": 0.03487162788709005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.06311798095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03685803338885307,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08278795480728149,
      "backward_entropy": 0.03457513782713148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.36563110351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03692809119820595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08272801041603088,
      "backward_entropy": 0.035057498349083796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.31879425048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03699789196252823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08266894817352295,
      "backward_entropy": 0.034757375717163086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.33258056640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03706797957420349,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08261008262634277,
      "backward_entropy": 0.03446960780355665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.47012329101562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0371369943022728,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08255296349525451,
      "backward_entropy": 0.07683759265475804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.14928436279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03720296546816826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08249898552894593,
      "backward_entropy": 0.03312526808844672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.88522338867188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03726784884929657,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08244625329971314,
      "backward_entropy": 0.03283791078461541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.33985137939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037330303341150284,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08239599466323852,
      "backward_entropy": 0.033275090985827975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.11421203613281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03739558905363083,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08234425187110901,
      "backward_entropy": 0.032982942130830556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.877017974853516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03745809942483902,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0822953462600708,
      "backward_entropy": 0.03199369708697001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.5387954711914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03751733899116516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08224939703941345,
      "backward_entropy": 0.03171271416876051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.895912170410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03757607191801071,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08220452070236206,
      "backward_entropy": 0.03210269080268012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.516178131103516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037631772458553314,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08216251730918885,
      "backward_entropy": 0.031151413917541504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.43276596069336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037684936076402664,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08212280869483948,
      "backward_entropy": 0.030870858165952895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.71125793457031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037735145539045334,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08208605051040649,
      "backward_entropy": 0.030586050616370306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.401758193969727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037788745015859604,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08204717040061951,
      "backward_entropy": 0.03031591574350993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.937564849853516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03783643618226051,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08201332092285156,
      "backward_entropy": 0.030629121594958834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.7686767578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0378839336335659,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08197994828224182,
      "backward_entropy": 0.029753370417488947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.09339141845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0379333421587944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0819457471370697,
      "backward_entropy": 0.02948378523190816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.606061935424805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03798483684659004,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08191033601760864,
      "backward_entropy": 0.029224693775177002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.86563110351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03803175315260887,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08187865018844605,
      "backward_entropy": 0.029493014017740887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.78483581542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038084886968135834,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0818430244922638,
      "backward_entropy": 0.029227601157294378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.57546997070312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03813832253217697,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08180782794952393,
      "backward_entropy": 0.02847456932067871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.9263687133789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03819264471530914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08177354335784912,
      "backward_entropy": 0.02869157327546014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.34002685546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038250092417001724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08173727393150329,
      "backward_entropy": 0.027992202175988093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.272804260253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03830617666244507,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08170251250267029,
      "backward_entropy": 0.0277602838145362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.427734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038359761238098145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08166990280151368,
      "backward_entropy": 0.027528239621056452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.72398376464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038417693227529526,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08163453340530395,
      "backward_entropy": 0.027308214041921828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.95110321044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03847246244549751,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08160245418548584,
      "backward_entropy": 0.027079406711790297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.72724151611328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038531482219696045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08156751394271851,
      "backward_entropy": 0.026863061719470553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.23262023925781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03859127685427666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08153284788131714,
      "backward_entropy": 0.026913351482815213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.42080688476562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038655005395412445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08149569630622863,
      "backward_entropy": 0.02644444339805179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.43313598632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03872455656528473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0814560055732727,
      "backward_entropy": 0.026469671063952975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.78215789794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038794372230768204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08141688108444214,
      "backward_entropy": 0.026264548301696777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.3143539428711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038864776492118835,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08137798309326172,
      "backward_entropy": 0.02593213154209985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.22410583496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03893411159515381,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08134042024612427,
      "backward_entropy": 0.02576661937766605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.37971496582031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039003416895866394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08130371570587158,
      "backward_entropy": 0.025598234600490995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.15988159179688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039069656282663345,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0812699019908905,
      "backward_entropy": 0.025417500072055392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.16222381591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03913670778274536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08123616576194763,
      "backward_entropy": 0.02523538635836707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.47296905517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03920433297753334,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08120260834693908,
      "backward_entropy": 0.025073723660575017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.8149642944336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03927465155720711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08116810917854309,
      "backward_entropy": 0.0248162266280916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.93210220336914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039346884936094284,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08113335371017456,
      "backward_entropy": 0.024730563163757324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.29232788085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03941633924841881,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08110106587409974,
      "backward_entropy": 0.024552911520004272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.51699829101562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03948616236448288,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0810690701007843,
      "backward_entropy": 0.07686101065741645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.44429779052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03955672308802605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08103728294372559,
      "backward_entropy": 0.023945937554041546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.08940124511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03962630778551102,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08100653886795044,
      "backward_entropy": 0.023730193575223286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.2657470703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03969200327992439,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08097877502441406,
      "backward_entropy": 0.023858333627382915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.4361343383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03975706174969673,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08095183968544006,
      "backward_entropy": 0.02329027818308936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.92170715332031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03982366994023323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08092412948608399,
      "backward_entropy": 0.023089034689797297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.404523849487305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03988741710782051,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08089841008186341,
      "backward_entropy": 0.023380479878849454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.059268951416016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03994504734873772,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08087707161903382,
      "backward_entropy": 0.02321609854698181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.9446907043457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04000095650553703,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08085712790489197,
      "backward_entropy": 0.023044496774673462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.77306365966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040054768323898315,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08083869218826294,
      "backward_entropy": 0.022876991166008845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.20421600341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04010869935154915,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08082070350646972,
      "backward_entropy": 0.022708972295125324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.02865600585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04016733914613724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08079994916915893,
      "backward_entropy": 0.02255450851387448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.172725677490234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04022740200161934,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08077881336212159,
      "backward_entropy": 0.02240359452035692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.5413589477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040283940732479095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08076006770133973,
      "backward_entropy": 0.022254872653219435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.33265686035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04034089669585228,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08074148893356323,
      "backward_entropy": 0.021226170990202162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.54450225830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040407370775938034,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08071814775466919,
      "backward_entropy": 0.021979739268620808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.02936553955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04046779125928879,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08069870471954346,
      "backward_entropy": 0.02184107568528917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.12898254394531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04052950441837311,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08067926168441772,
      "backward_entropy": 0.020667327774895564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.49022674560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04059573635458946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08065808415412903,
      "backward_entropy": 0.020480697353680927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.17549133300781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0406663715839386,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08063477873802186,
      "backward_entropy": 0.021447224749459162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.38001251220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0407409593462944,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08061013221740723,
      "backward_entropy": 0.020126120911704168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.53771209716797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040816325694322586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08058571815490723,
      "backward_entropy": 0.019948590132925246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.19746398925781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04088978469371796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08056296706199646,
      "backward_entropy": 0.021073194013701543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.0196304321289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040964823216199875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08053969144821167,
      "backward_entropy": 0.02095463540818956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.07906723022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041043203324079514,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08051532506942749,
      "backward_entropy": 0.020841646525594924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.53821563720703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04111716151237488,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0804943323135376,
      "backward_entropy": 0.020708016223377652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.60647964477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04118955507874489,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08047457933425903,
      "backward_entropy": 0.020575079652998183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.73929595947266,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.041258446872234344,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08045705556869506,
      "backward_entropy": 0.07691097921795315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.83499145507812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04132562875747681,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08044131994247436,
      "backward_entropy": 0.020309262805514865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.29265594482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04139401391148567,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0804245948791504,
      "backward_entropy": 0.020188182592391968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.48361587524414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04146194085478783,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08040807247161866,
      "backward_entropy": 0.020075913932588365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.74945831298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04152753949165344,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0803926169872284,
      "backward_entropy": 0.01997809608777364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.58783721923828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0415964350104332,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08037559986114502,
      "backward_entropy": 0.01988227168718974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.33287811279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041663844138383865,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08036033511161804,
      "backward_entropy": 0.019773181941774156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.54208755493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041734207421541214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0803439736366272,
      "backward_entropy": 0.017649259832170274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.90860748291016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041801370680332184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08032934069633484,
      "backward_entropy": 0.01748710208468967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.828067779541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04187082126736641,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08031381368637085,
      "backward_entropy": 0.019464964667956035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.68478393554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04193822667002678,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08029953241348267,
      "backward_entropy": 0.019369001189867657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.18585968017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04200756922364235,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08028457164764405,
      "backward_entropy": 0.017030003998014662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.71712493896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04207935929298401,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08026891350746154,
      "backward_entropy": 0.01917945345242818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.04039764404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04215162619948387,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08025317192077637,
      "backward_entropy": 0.019088856048054166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.73176574707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0422251895070076,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08023715019226074,
      "backward_entropy": 0.01899907820754581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.446266174316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04229514300823212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08022313714027404,
      "backward_entropy": 0.016435336735513475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.289146423339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04236355051398277,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08020991086959839,
      "backward_entropy": 0.018827569153573778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.77397918701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0424305722117424,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08019731044769288,
      "backward_entropy": 0.01875357660982344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.345481872558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042499762028455734,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08018421530723571,
      "backward_entropy": 0.018685077627499897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.17869567871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04256700724363327,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08017227649688721,
      "backward_entropy": 0.015902997718916997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.7656021118164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042633868753910065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0801605224609375,
      "backward_entropy": 0.018545349438985188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.878318786621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042704589664936066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08014631867408753,
      "backward_entropy": 0.01565794481171502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.52642059326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042770687490701675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08013520240783692,
      "backward_entropy": 0.018428183264202543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.43838119506836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042837873101234436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08012369871139527,
      "backward_entropy": 0.018370507491959467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.5156478881836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042903508991003036,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0801131010055542,
      "backward_entropy": 0.018313455912801955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.57674789428711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04297563061118126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08009951114654541,
      "backward_entropy": 0.01827066805627611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.08405303955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04304506629705429,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08008769750595093,
      "backward_entropy": 0.015082750055525038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.9544906616211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04312125965952873,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08007304668426514,
      "backward_entropy": 0.018167684475580852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.2158203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04320017993450165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08005724549293518,
      "backward_entropy": 0.018127095368173387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.456148147583008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043274566531181335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08004404306411743,
      "backward_entropy": 0.014766979548666213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.509490966796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04334251582622528,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08003422617912292,
      "backward_entropy": 0.01801305181450314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.49118423461914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043407682329416275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0800258219242096,
      "backward_entropy": 0.017952639195654128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.302268981933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04347018897533417,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0800188958644867,
      "backward_entropy": 0.01789213716983795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.736976623535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04353116452693939,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08001294136047363,
      "backward_entropy": 0.014291197061538696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.92409896850586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04359141364693642,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08000732660293579,
      "backward_entropy": 0.0177665286593967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.80414962768555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04364984482526779,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08000283241271973,
      "backward_entropy": 0.017711901002460055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.64651489257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043707866221666336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07999910116195678,
      "backward_entropy": 0.017643584145439997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.33238983154297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04376765713095665,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07999424338340759,
      "backward_entropy": 0.017579767439100478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.33431243896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04383184388279915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.079986572265625,
      "backward_entropy": 0.013714127242565155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.67230224609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043897029012441635,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07997889518737793,
      "backward_entropy": 0.017476047078768413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.421354293823242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04396142438054085,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07997184991836548,
      "backward_entropy": 0.01743420461813609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.37406539916992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04402294382452965,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07996598482131959,
      "backward_entropy": 0.017404054601987202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.59651184082031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04408209025859833,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07996141910552979,
      "backward_entropy": 0.017368510365486145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.06871032714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044144537299871445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07995527982711792,
      "backward_entropy": 0.013239334026972452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.345149993896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044210635125637054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07994768619537354,
      "backward_entropy": 0.013153827024830712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.53033447265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044277023524045944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07993990182876587,
      "backward_entropy": 0.01729155249065823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.58007049560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044346652925014496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07993086576461791,
      "backward_entropy": 0.01299415197637346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.798072814941406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.044416896998882294,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07992170453071594,
      "backward_entropy": 0.0769831207063463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.85484313964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04448361322283745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07991448640823365,
      "backward_entropy": 0.01721260282728407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.703369140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04454846680164337,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07990834712982178,
      "backward_entropy": 0.017171922657224867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.465576171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044608648866415024,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07990437746047974,
      "backward_entropy": 0.017133399844169617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.17129516601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04466778412461281,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07990095615386963,
      "backward_entropy": 0.017091365324126348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.65357208251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04473042115569115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07989639639854432,
      "backward_entropy": 0.012440386745664809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.9964828491211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0447981134057045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07988947629928589,
      "backward_entropy": 0.017004123992390104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.678531646728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04486821964383125,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07988138794898987,
      "backward_entropy": 0.012264177203178406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.43886947631836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044936612248420715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07987417578697205,
      "backward_entropy": 0.016946269406212702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.4454345703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04500362277030945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07986754179000854,
      "backward_entropy": 0.01210133069091373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.16809844970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04507151246070862,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07986061573028565,
      "backward_entropy": 0.012022181517548032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.88382339477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04513448476791382,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0798559308052063,
      "backward_entropy": 0.011943133340941535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.96334457397461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04519791156053543,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07985097169876099,
      "backward_entropy": 0.01186881297164493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.828128814697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04525702819228172,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07984794974327088,
      "backward_entropy": 0.01684475276205275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.80442810058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04531241953372955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07984654903411866,
      "backward_entropy": 0.011727924976083968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.46161651611328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04536628723144531,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07984601259231568,
      "backward_entropy": 0.016823978887663946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.36370849609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04542664438486099,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07984219193458557,
      "backward_entropy": 0.016827255487442017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.335758209228516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045487940311431885,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07983837127685547,
      "backward_entropy": 0.011529852946599325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.276275634765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045548949390649796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983497381210328,
      "backward_entropy": 0.01679690182209015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.2800521850586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0456080362200737,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07983245849609374,
      "backward_entropy": 0.011381136874357859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.521644592285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04567387327551842,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982689142227173,
      "backward_entropy": 0.011315569281578064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.26931381225586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04573956876993179,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982146143913268,
      "backward_entropy": 0.01124977899922265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.403480529785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04580528289079666,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981597185134888,
      "backward_entropy": 0.01674760381380717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.38883972167969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04586988314986229,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798110008239746,
      "backward_entropy": 0.016741269164615206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.13410568237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04593318700790405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980670928955078,
      "backward_entropy": 0.011059208048714532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.350276947021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04599585756659508,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980302572250367,
      "backward_entropy": 0.010985400941636827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.7064437866211,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04605952277779579,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07979902625083923,
      "backward_entropy": 0.07700230677922566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.95671272277832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046125199645757675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979427576065064,
      "backward_entropy": 0.016648285918765597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.604248046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046186644583940506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797917127609253,
      "backward_entropy": 0.016606623927752178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.446449279785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04624487832188606,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979063391685486,
      "backward_entropy": 0.016566283173031278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.12294387817383,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04630470275878906,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978875637054443,
      "backward_entropy": 0.016526740458276536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.49003219604492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046362027525901794,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978829145431518,
      "backward_entropy": 0.01647851202223036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.985477447509766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04641808196902275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797892689704895,
      "backward_entropy": 0.016420950492223103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.719858169555664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046473875641822815,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979020476341248,
      "backward_entropy": 0.01031677590476142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.6549072265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04652608931064606,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979280948638916,
      "backward_entropy": 0.01632556484805213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.52357482910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04657858610153198,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979490756988525,
      "backward_entropy": 0.016289114952087402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.792055130004883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0466313436627388,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979661226272583,
      "backward_entropy": 0.010081312722629972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.433902740478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04668211191892624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979919910430908,
      "backward_entropy": 0.01001260346836514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.748046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04673471674323082,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980036735534668,
      "backward_entropy": 0.00995127773947186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.935874938964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046789512038230896,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980031371116639,
      "backward_entropy": 0.00989004886812634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.12571716308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04684280976653099,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980111837387086,
      "backward_entropy": 0.009827084839344025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.3690185546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046897269785404205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980118989944458,
      "backward_entropy": 0.016185431016816035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.84881591796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04695483669638634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979952096939087,
      "backward_entropy": 0.016173594527774386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.163238525390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047012995928525925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979800701141357,
      "backward_entropy": 0.009647130138344236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.45984649658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047068797051906586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979791164398194,
      "backward_entropy": 0.009592934615082212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.208011627197266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04712826386094093,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979596257209778,
      "backward_entropy": 0.009546488523483276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.956083297729492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04718592390418053,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979488968849183,
      "backward_entropy": 0.01617839104599423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.17942810058594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04724113643169403,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07979488372802734,
      "backward_entropy": 0.07700081004036798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.778892517089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047297846525907516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979439496994019,
      "backward_entropy": 0.016192199455367193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.69252014160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04735330492258072,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07979439496994019,
      "backward_entropy": 0.07700325383080377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.023118019104004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047409553080797195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979407310485839,
      "backward_entropy": 0.009303701420625051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.45737075805664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04746045544743538,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979645133018494,
      "backward_entropy": 0.01619637840323978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.073482513427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04751068353652954,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979902625083923,
      "backward_entropy": 0.016196131706237793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.340110778808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04756345599889755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980042099952697,
      "backward_entropy": 0.0161971648534139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.94572448730469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04761578142642975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980214953422546,
      "backward_entropy": 0.00908912718296051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.84322738647461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04767519235610962,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980058193206788,
      "backward_entropy": 0.01619156863954332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.171592712402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04773417487740517,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979912757873535,
      "backward_entropy": 0.016197764211230807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.08279800415039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04779044911265373,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797988772392273,
      "backward_entropy": 0.01620625125037299,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.16729736328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04784877970814705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979766130447388,
      "backward_entropy": 0.008904376791583167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.502374649047852,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04790392890572548,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979809641838073,
      "backward_entropy": 0.008855748507711623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.203704833984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04795457050204277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980060577392578,
      "backward_entropy": 0.00880939265092214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.03749084472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04800551384687424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980287075042725,
      "backward_entropy": 0.008764370448059507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.696571350097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048056863248348236,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980477809906006,
      "backward_entropy": 0.016247878472010296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.049863815307617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04811222106218338,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798046350479126,
      "backward_entropy": 0.016269226868947346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.596824645996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048166852444410324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980473637580872,
      "backward_entropy": 0.008654202024141947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.801703453063965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04821894317865372,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980610728263855,
      "backward_entropy": 0.01632669733630286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.09280776977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048268191516399384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980868220329285,
      "backward_entropy": 0.008588488731119368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.46739196777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04832097142934799,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980968952178955,
      "backward_entropy": 0.016382250520918105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.995086669921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04837329313158989,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981103658676147,
      "backward_entropy": 0.01639515823788113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.170249938964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04842637851834297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981178760528565,
      "backward_entropy": 0.008480984303686354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.92828369140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0484791025519371,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981281280517578,
      "backward_entropy": 0.00844342095984353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.5921630859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04853189364075661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981377840042114,
      "backward_entropy": 0.008405541380246481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.218293190002441,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048587020486593246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981361150741577,
      "backward_entropy": 0.01646928654776679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.979408264160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048637259751558304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981581687927246,
      "backward_entropy": 0.008329028884569803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.5811653137207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0486886203289032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0798175573348999,
      "backward_entropy": 0.00828703824016783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.34978485107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048742879182100296,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981809377670288,
      "backward_entropy": 0.008239923251999749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.66950798034668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04879678413271904,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981881499290466,
      "backward_entropy": 0.01647050182024638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.07590103149414,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.048848096281290054,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07982074022293091,
      "backward_entropy": 0.0770160092247857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.91329956054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04890143871307373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982152700424194,
      "backward_entropy": 0.00810047321849399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.694639205932617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04896073043346405,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981936931610108,
      "backward_entropy": 0.016480788588523865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.26317596435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049017712473869324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981843948364258,
      "backward_entropy": 0.008026643759674497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.27447509765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049072057008743286,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981864213943482,
      "backward_entropy": 0.007986140747865042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.933001518249512,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049125347286462784,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798193633556366,
      "backward_entropy": 0.0164971434407764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.84085083007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04917542263865471,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982155084609985,
      "backward_entropy": 0.016508498125606112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.28093719482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04923228174448013,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982058525085449,
      "backward_entropy": 0.007875190840827094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.92489242553711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049293484538793564,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981746792793273,
      "backward_entropy": 0.01653760506047143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.691240310668945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04935386776924133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981473207473755,
      "backward_entropy": 0.00781456877787908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.60306549072266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04941270127892494,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981259822845459,
      "backward_entropy": 0.016578584909439087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.574079513549805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04947840794920921,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980734705924988,
      "backward_entropy": 0.01659794317351447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.662895679473877,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04954059422016144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980375289916992,
      "backward_entropy": 0.007728945877816942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.10608673095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04959724843502045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980283498764038,
      "backward_entropy": 0.007695911659134759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.230573654174805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04965387284755707,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980177402496338,
      "backward_entropy": 0.007666846944226159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.18055534362793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04970880597829819,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980160713195801,
      "backward_entropy": 0.01665904786851671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.987382888793945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04976196959614754,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980257272720337,
      "backward_entropy": 0.016663740078608196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.883512496948242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049813903868198395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980404496192932,
      "backward_entropy": 0.016667816374037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.43971252441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04986473172903061,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980599403381347,
      "backward_entropy": 0.016670919126934476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.804916381835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04991629347205162,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980749607086182,
      "backward_entropy": 0.016663660605748493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.315433502197266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049966223537921906,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980952858924865,
      "backward_entropy": 0.007447015080187056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.228294372558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05001629516482353,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981138229370117,
      "backward_entropy": 0.01667074031300015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.00579833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05007008835673332,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981075048446655,
      "backward_entropy": 0.016681187682681613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.158536911010742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05012387037277222,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981066703796387,
      "backward_entropy": 0.016696823967827693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.08100128173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05017683655023575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981087565422058,
      "backward_entropy": 0.007323209610250261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.186910629272461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050228942185640335,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981139421463013,
      "backward_entropy": 0.01675396164258321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.52827835083008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05027690902352333,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798138439655304,
      "backward_entropy": 0.016787777344385784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.891233444213867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05032530426979065,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981606721878051,
      "backward_entropy": 0.007252114514509837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.36236572265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050372809171676636,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0798187494277954,
      "backward_entropy": 0.007225593758953942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.12059783935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05042296648025513,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982008457183838,
      "backward_entropy": 0.01687732007768419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.571250915527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05047355592250824,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982110381126403,
      "backward_entropy": 0.016910182105170354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.494444847106934,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05052521079778671,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982156872749328,
      "backward_entropy": 0.016938515835338168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.896299362182617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05057379975914955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982354164123535,
      "backward_entropy": 0.007126084632343716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.255413055419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050620656460523605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982627153396607,
      "backward_entropy": 0.007101334631443024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.547481536865234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050666987895965576,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982920408248902,
      "backward_entropy": 0.0170282158586714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.47930908203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05071398615837097,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983171343803405,
      "backward_entropy": 0.01705905795097351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.587787628173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050761353224515915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07983405590057373,
      "backward_entropy": 0.007026165723800659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.61591720581055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05080721527338028,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983702421188354,
      "backward_entropy": 0.017114655839072332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.37238311767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050857655704021454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07983779907226562,
      "backward_entropy": 0.006974624262915717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.725709915161133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05090947076678276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07983781099319458,
      "backward_entropy": 0.006949845287534926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.34133529663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05096001923084259,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983841896057128,
      "backward_entropy": 0.01718087659941779,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.93962097167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05101294070482254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07983789443969727,
      "backward_entropy": 0.006897459427515666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.80385971069336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05106685683131218,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983680367469788,
      "backward_entropy": 0.017229311996036105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.19330406188965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0511215515434742,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798353374004364,
      "backward_entropy": 0.017252718408902485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.240467071533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051175229251384735,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983418703079223,
      "backward_entropy": 0.0172836830218633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.106201171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05122721567749977,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983397245407105,
      "backward_entropy": 0.01730325652493371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.16233444213867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05128488317131996,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983115911483765,
      "backward_entropy": 0.017308930555979412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.92784881591797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0513429194688797,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982813119888306,
      "backward_entropy": 0.01731444564130571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.665647506713867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05140161141753197,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982455492019654,
      "backward_entropy": 0.017327810327212017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.59232711791992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05145741626620293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07982237339019775,
      "backward_entropy": 0.006682609518369039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.50777816772461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05151408165693283,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981961965560913,
      "backward_entropy": 0.017361236943138972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.493732452392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0515710823237896,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981668710708618,
      "backward_entropy": 0.006631695148017671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.14358901977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05162402242422104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981582880020141,
      "backward_entropy": 0.006602261629369523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.806884765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0516793392598629,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981386184692382,
      "backward_entropy": 0.006569116065899531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.005456924438477,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051737427711486816,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07981041669845582,
      "backward_entropy": 0.01738757888476054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.087934494018555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05179436504840851,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798075795173645,
      "backward_entropy": 0.01738168133629693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.45596694946289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05184847488999367,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980610132217407,
      "backward_entropy": 0.006470800273948246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.01253890991211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05190345644950867,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07980404496192932,
      "backward_entropy": 0.01737683183617062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.583589553833008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05196068063378334,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0798008918762207,
      "backward_entropy": 0.017370790243148804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.97404479980469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052016254514455795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979846000671387,
      "backward_entropy": 0.006374910473823547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.677570343017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0520724318921566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07979559302330017,
      "backward_entropy": 0.0063453055918216705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.007946014404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052125755697488785,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979416847229004,
      "backward_entropy": 0.01737270752588908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.226049423217773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05217849090695381,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797930359840393,
      "backward_entropy": 0.0173671907848782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.382450103759766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05222949758172035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0797927975654602,
      "backward_entropy": 0.006244367195500268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.83865737915039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05228123441338539,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979211807250977,
      "backward_entropy": 0.017338338825437758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.17596435546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05233471468091011,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979050874710084,
      "backward_entropy": 0.017326101660728455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.9061393737793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05238623917102814,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978968620300293,
      "backward_entropy": 0.01732880539364285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.06195831298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05243843421339989,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978851795196533,
      "backward_entropy": 0.017331350180837844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.94188690185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05248849093914032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07978833317756653,
      "backward_entropy": 0.006087723291582531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.9088020324707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05253692716360092,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978882789611816,
      "backward_entropy": 0.017362104521857366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.351802825927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05258965119719505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07978701591491699,
      "backward_entropy": 0.006045058369636536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.14267349243164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052640851587057114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978599071502686,
      "backward_entropy": 0.017409066359202068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.558502197265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0526927225291729,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797845482826233,
      "backward_entropy": 0.017426013946533203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.949832916259766,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052744217216968536,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0797832727432251,
      "backward_entropy": 0.0770163271162245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.14822769165039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05279994010925293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977964878082275,
      "backward_entropy": 0.005954802450206544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.425228118896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05285991355776787,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977379560470581,
      "backward_entropy": 0.005935036059882905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.38385772705078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0529201403260231,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976765632629394,
      "backward_entropy": 0.017521166139178805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.511945724487305,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052976999431848526,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07976327538490295,
      "backward_entropy": 0.07701606220669216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.860093116760254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05303237587213516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975951433181763,
      "backward_entropy": 0.005881624503268136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.020206451416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053084585815668106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975727319717407,
      "backward_entropy": 0.005868000288804372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.69952964782715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05313863232731819,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975392937660217,
      "backward_entropy": 0.017666026949882507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.08745765686035,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053192783147096634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797505497932434,
      "backward_entropy": 0.017699129051632352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.071800231933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05324568599462509,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974768877029419,
      "backward_entropy": 0.005824466546376546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.22162437438965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0532967783510685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974581718444824,
      "backward_entropy": 0.005805656727817323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.18156433105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05334833636879921,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974365949630738,
      "backward_entropy": 0.0057845765517817605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.785552978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05340125784277916,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974073886871338,
      "backward_entropy": 0.0057612839672300555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.65473175048828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05345378443598747,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07973790764808655,
      "backward_entropy": 0.0770130025015937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.499937057495117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05350595340132713,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797351598739624,
      "backward_entropy": 0.017825825346840754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.409049034118652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05355674773454666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973307371139526,
      "backward_entropy": 0.0056958554519547355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.334451675415039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05360516533255577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973225116729736,
      "backward_entropy": 0.0056726982196172076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.273776054382324,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053651440888643265,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973253726959229,
      "backward_entropy": 0.017865104807747736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.148061752319336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053695693612098694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973393201828002,
      "backward_entropy": 0.017869147989485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.092818260192871,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053739070892333984,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973580360412598,
      "backward_entropy": 0.017867639660835266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.633670806884766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053781088441610336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973825931549072,
      "backward_entropy": 0.005568935639328427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.336097717285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05382564291357994,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973925471305847,
      "backward_entropy": 0.0055444153646628065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.73113250732422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053873591125011444,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797382116317749,
      "backward_entropy": 0.017890984813372295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.695215225219727,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05392075330018997,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07973748445510864,
      "backward_entropy": 0.07701445288128322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.912643432617188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05396665260195732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973746061325074,
      "backward_entropy": 0.0054795994526810115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.60317611694336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05401018261909485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973861694335938,
      "backward_entropy": 0.017941688497861225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.081193923950195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05405716598033905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973771095275879,
      "backward_entropy": 0.005442861053678725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.0015869140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054104823619127274,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973623275756836,
      "backward_entropy": 0.018008626169628568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.931041717529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054152712225914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973451018333436,
      "backward_entropy": 0.005420659979184468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.050783157348633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05420035123825073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973288297653199,
      "backward_entropy": 0.0054074906640582615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.662853240966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05424745753407478,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973138093948365,
      "backward_entropy": 0.018141690227720473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.571333885192871,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05429168790578842,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973147630691528,
      "backward_entropy": 0.018186615573035345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.825273513793945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05433376878499985,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797326147556305,
      "backward_entropy": 0.018240700165430706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.128479957580566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05437548831105232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973394393920899,
      "backward_entropy": 0.005369252628750271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.06799030303955,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05441594868898392,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973583936691284,
      "backward_entropy": 0.01834700008233388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.04636001586914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05445526912808418,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973830699920655,
      "backward_entropy": 0.018399811453289457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.29606246948242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05449320748448372,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974159121513366,
      "backward_entropy": 0.0053393033643563586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.426025390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05453583970665932,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0797420859336853,
      "backward_entropy": 0.0053266435861587524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.333252906799316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054578013718128204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974274158477783,
      "backward_entropy": 0.00531386997964647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.743999481201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05461767688393593,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07974482774734497,
      "backward_entropy": 0.018550912539164226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.1822452545166,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054658256471157074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974622249603272,
      "backward_entropy": 0.005285918712615967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.00471305847168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05469856411218643,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974767684936523,
      "backward_entropy": 0.005272892200284534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.156235694885254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05474061891436577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974799871444702,
      "backward_entropy": 0.005260953058799108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.211809158325195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054780248552560806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974967360496521,
      "backward_entropy": 0.005247559812333848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.446334838867188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05482232943177223,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07974987626075744,
      "backward_entropy": 0.01872464186615414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.029753684997559,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05486313998699188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975068688392639,
      "backward_entropy": 0.005218785256147385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.352164268493652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054901640862226486,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975279688835143,
      "backward_entropy": 0.005204040971067216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.32813262939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054938968271017075,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975552678108215,
      "backward_entropy": 0.005188416275713179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.251800537109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05497795715928078,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975726127624512,
      "backward_entropy": 0.01883295178413391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.412569046020508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05501555651426315,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975975275039673,
      "backward_entropy": 0.01884531643655565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.385772705078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05505559965968132,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976076602935792,
      "backward_entropy": 0.018848407599661086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.289247512817383,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055095378309488297,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976194024085999,
      "backward_entropy": 0.018854540255334642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.232301712036133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055135078728199005,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976292371749878,
      "backward_entropy": 0.018867986069785223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.14501190185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055174462497234344,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976402640342713,
      "backward_entropy": 0.005073572198549907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.856470108032227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05521364137530327,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976517081260681,
      "backward_entropy": 0.018897290031115215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.58366584777832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05525173246860504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976682782173157,
      "backward_entropy": 0.018916582067807514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.07844352722168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05529218539595604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976707220077514,
      "backward_entropy": 0.0050214119255542755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.372077941894531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0553332082927227,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976692914962769,
      "backward_entropy": 0.01893972357114156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.643340110778809,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05537116155028343,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0797684669494629,
      "backward_entropy": 0.0049875229597091675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.66712188720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055407825857400894,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977073192596436,
      "backward_entropy": 0.004969616731007894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.60614585876465,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05544460937380791,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977290153503418,
      "backward_entropy": 0.004953553693162071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.740930557250977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05548134446144104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977486848831176,
      "backward_entropy": 0.004937324259016249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.49866485595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055521003901958466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977503538131714,
      "backward_entropy": 0.0049232496983475154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.41138458251953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.055561356246471405,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07977471947669983,
      "backward_entropy": 0.0770093666182624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.250131130218506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05560217797756195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977404594421386,
      "backward_entropy": 0.004896652781301075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.223987579345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05563964322209358,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977535724639892,
      "backward_entropy": 0.004881564113828871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.07695770263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055678654462099075,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977569699287415,
      "backward_entropy": 0.019123019443617925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.080655097961426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05571943148970604,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977485060691833,
      "backward_entropy": 0.019147776895099215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.90793800354004,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055758997797966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977460622787476,
      "backward_entropy": 0.019177751408682928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.83810806274414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055799227207899094,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977386713027954,
      "backward_entropy": 0.019209893213378057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.833145141601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055839672684669495,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977298498153687,
      "backward_entropy": 0.019232019782066345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.879443168640137,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05587942898273468,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977247834205628,
      "backward_entropy": 0.01924708154466417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.21814727783203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055917538702487946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977293729782105,
      "backward_entropy": 0.019253335065311857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.20343513786792755,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055958542972803116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.079771488904953,
      "backward_entropy": 0.004767265170812607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.148496627807617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05599537864327431,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977250218391418,
      "backward_entropy": 0.00475290459063318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.02888298034668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05603397637605667,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977238297462463,
      "backward_entropy": 0.004739682293600506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.561367988586426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05607422813773155,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977111339569092,
      "backward_entropy": 0.019348955816692777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.8214168548584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056113068014383316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977063655853271,
      "backward_entropy": 0.004715716673268212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.18752069771289825,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056153155863285065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976936101913452,
      "backward_entropy": 0.019395695792304143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.58493423461914,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.056189242750406265,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07977049350738526,
      "backward_entropy": 0.07701002226935492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.619662761688232,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056226979941129684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977048754692077,
      "backward_entropy": 0.004673987627029419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.290675163269043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056262608617544174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977172136306762,
      "backward_entropy": 0.004659347236156464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.21876335144043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05629706010222435,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797736406326294,
      "backward_entropy": 0.019467671712239582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.827668190002441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05633072927594185,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977593541145325,
      "backward_entropy": 0.004627534498771031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.434768676757812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056364621967077255,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977798581123352,
      "backward_entropy": 0.019499401251475017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.7112455368042,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0563991516828537,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977962493896484,
      "backward_entropy": 0.019506227638986375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.848445892333984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05643346905708313,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978134751319885,
      "backward_entropy": 0.019506762425104778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.57073974609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056469645351171494,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978185415267944,
      "backward_entropy": 0.01951114336649577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.06016731262207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05650537833571434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978256344795227,
      "backward_entropy": 0.019508381684621174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.418249130249023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05654183402657509,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978277206420899,
      "backward_entropy": 0.019506497515572443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.411226272583008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056577883660793304,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978317141532898,
      "backward_entropy": 0.01949983172946506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.814434051513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05661560222506523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07978243827819824,
      "backward_entropy": 0.004483513947990205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.175674438476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05665579438209534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07978009581565856,
      "backward_entropy": 0.004467427730560303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.543724060058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05669540911912918,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977799773216247,
      "backward_entropy": 0.019511820541487798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.022204399108887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056737158447504044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07977452874183655,
      "backward_entropy": 0.004437285164992015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.492918014526367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056777969002723694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977154850959778,
      "backward_entropy": 0.019526996546321444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.430347442626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05681715905666351,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976949214935303,
      "backward_entropy": 0.004404938055409325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.770916938781738,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05685495212674141,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976821660995484,
      "backward_entropy": 0.019536265068584018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.10004997253418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0568925216794014,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976697683334351,
      "backward_entropy": 0.019548738996187847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.738794326782227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05693032592535019,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976552844047546,
      "backward_entropy": 0.004358152962393231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.892839431762695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056970130652189255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0797627329826355,
      "backward_entropy": 0.004341323342588212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.139725685119629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05701026692986488,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975964546203614,
      "backward_entropy": 0.004325698233313031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.637420654296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057048846036195755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975744009017945,
      "backward_entropy": 0.019562820593516033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.594301223754883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0570901483297348,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975345253944396,
      "backward_entropy": 0.01956333054436578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.952265739440918,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05713161453604698,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07974926233291627,
      "backward_entropy": 0.01956885556379954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.896346092224121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05717151239514351,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797459602355957,
      "backward_entropy": 0.019578609201643202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.063817977905273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05720987170934677,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07974352836608886,
      "backward_entropy": 0.004249723421202766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.571069240570068,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057247914373874664,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07974119186401367,
      "backward_entropy": 0.019603888193766277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.916190147399902,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05728365480899811,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07974023818969726,
      "backward_entropy": 0.019615872038735285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.557605743408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05731939151883125,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973920702934265,
      "backward_entropy": 0.019637063145637512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.774551391601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05735861137509346,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07973580360412598,
      "backward_entropy": 0.01967144012451172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.558574676513672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05739728361368179,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0797324001789093,
      "backward_entropy": 0.0770149827003479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.625299453735352,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057434581220149994,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07972973585128784,
      "backward_entropy": 0.019739828175968595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.438371658325195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057471565902233124,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07972704768180847,
      "backward_entropy": 0.004178785615497165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.39719009399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05750775337219238,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07972473502159119,
      "backward_entropy": 0.004174076020717621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.636402130126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057542797178030014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.079723060131073,
      "backward_entropy": 0.004168791489468681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.288833618164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05758019536733627,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07971970438957214,
      "backward_entropy": 0.019920931922064886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.357669830322266,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05761643126606941,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07971701622009278,
      "backward_entropy": 0.07700955867767334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.248838424682617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057653896510601044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07971336841583251,
      "backward_entropy": 0.004152488377359178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.134905815124512,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0576925203204155,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07970882654190063,
      "backward_entropy": 0.020049363374710083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.0123233795166,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057731371372938156,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07970397472381592,
      "backward_entropy": 0.020085336433516607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.87333869934082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05777201056480408,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07969785928726196,
      "backward_entropy": 0.004131120526128345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.891071319580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05781436339020729,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07969045042991638,
      "backward_entropy": 0.020149267382091947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.743691444396973,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05785598233342171,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07968341112136841,
      "backward_entropy": 0.020188818375269573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.736522674560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05789754539728165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07967624664306641,
      "backward_entropy": 0.020227750142415363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.321725845336914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05793831869959831,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07966951131820679,
      "backward_entropy": 0.020268877347310383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.445703506469727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05798077955842018,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07966147661209107,
      "backward_entropy": 0.004101847608884175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.6471529006958,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058023180812597275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07965334057807923,
      "backward_entropy": 0.020354418290985957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.417450904846191,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05806400254368782,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07964614629745484,
      "backward_entropy": 0.020404560698403254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.725389003753662,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05810422822833061,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07963918447494507,
      "backward_entropy": 0.0040930890374713475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.075949668884277,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058142177760601044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07963367700576782,
      "backward_entropy": 0.02051874001820882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.985614776611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05818029120564461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07962794303894043,
      "backward_entropy": 0.0040885839197370745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8622019290924072,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05821859464049339,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07962193489074706,
      "backward_entropy": 0.020616489979955886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.332976341247559,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058253902941942215,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07961794137954711,
      "backward_entropy": 0.020662109057108562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.735442161560059,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05828800052404404,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07961468696594239,
      "backward_entropy": 0.020701963040563796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.941158294677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05832265317440033,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07961091995239258,
      "backward_entropy": 0.004069036493698756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.489769458770752,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058357078582048416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07960714101791382,
      "backward_entropy": 0.020776838064193726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.139408111572266,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05838964879512787,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07960457801818847,
      "backward_entropy": 0.07698779635959202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.428452014923096,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0584213063120842,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07960257530212403,
      "backward_entropy": 0.020848777559068467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.348250389099121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058451373130083084,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07960160374641419,
      "backward_entropy": 0.020883040295706853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.641913414001465,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05848251283168793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07959970235824584,
      "backward_entropy": 0.004037929077943166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.348574638366699,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058513738214969635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07959755659103393,
      "backward_entropy": 0.004031976477967368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.915890216827393,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058543190360069275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07959667444229127,
      "backward_entropy": 0.020983732408947416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.677779197692871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05857217311859131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0795960009098053,
      "backward_entropy": 0.00401656577984492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.581506729125977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05860269069671631,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.079594087600708,
      "backward_entropy": 0.02103185488118066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.365214347839355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05863484740257263,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07959085702896118,
      "backward_entropy": 0.021048272649447124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.289336204528809,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05866669863462448,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07958775162696838,
      "backward_entropy": 0.021056926912731595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.167956352233887,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058698683977127075,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.079584401845932,
      "backward_entropy": 0.02107445564534929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.28109359741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058728866279125214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07958230376243591,
      "backward_entropy": 0.003966641508870655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.62692642211914,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05876198410987854,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07957795858383179,
      "backward_entropy": 0.07699422703848945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.00469398498535,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058795589953660965,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07957311868667602,
      "backward_entropy": 0.021092535720931158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.408424377441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05883355811238289,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07956503629684449,
      "backward_entropy": 0.021095648407936096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.27303123474121,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0588730052113533,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07955573797225952,
      "backward_entropy": 0.00392000749707222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.955486297607422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05891397222876549,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07954516410827636,
      "backward_entropy": 0.0770023266474406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.02100944519043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058952443301677704,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07953633069992065,
      "backward_entropy": 0.021109768086009555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.285109043121338,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058992303907871246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07952632308006287,
      "backward_entropy": 0.021115379201041326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.999985694885254,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05903045833110809,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07951750755310058,
      "backward_entropy": 0.021122905943128798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4466989040374756,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05906854197382927,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07950859069824219,
      "backward_entropy": 0.0038700906766785514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.780370235443115,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059103578329086304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07950189709663391,
      "backward_entropy": 0.003860357320970959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.087316513061523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05913659930229187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07949665784835816,
      "backward_entropy": 0.021152940061357286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.07408416271209717,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059170860797166824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07949031591415405,
      "backward_entropy": 0.003841895196172926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.911157608032227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05920187383890152,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07948641777038574,
      "backward_entropy": 0.021190775765313044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.527844429016113,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05923416092991829,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07948141098022461,
      "backward_entropy": 0.0038262133797009787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.179825782775879,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05926700681447983,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07947583198547363,
      "backward_entropy": 0.021230823463863797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.859704494476318,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05929948762059212,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07947041988372802,
      "backward_entropy": 0.021250021126535203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.569912910461426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059330862015485764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07946577072143554,
      "backward_entropy": 0.021266573005252414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.704720497131348,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05936047434806824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07946237325668334,
      "backward_entropy": 0.0037927449577384526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.164254188537598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05939235910773277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0794570803642273,
      "backward_entropy": 0.0037846925357977548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.888528823852539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05942485108971596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07945117950439454,
      "backward_entropy": 0.003777753147814009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.828810691833496,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05945703014731407,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07944538593292236,
      "backward_entropy": 0.021348489655388728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.94266128540039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05948896333575249,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07943966388702392,
      "backward_entropy": 0.02137551208337148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.87203311920166,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05952156335115433,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07943326830863953,
      "backward_entropy": 0.021410710281795926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.79482650756836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05955461412668228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07942637205123901,
      "backward_entropy": 0.021447206536928814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.990148544311523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059588197618722916,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07941888570785523,
      "backward_entropy": 0.021490333808792964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.643921852111816,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05962333083152771,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07941004037857055,
      "backward_entropy": 0.021524103151427373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.267721652984619,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05965862423181534,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07940092086791992,
      "backward_entropy": 0.07700713475545247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.2304911613464355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05969177559018135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07939341068267822,
      "backward_entropy": 0.021590244438913133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.207142353057861,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059723302721977234,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07938710451126099,
      "backward_entropy": 0.0216313898563385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.23900032043457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059753138571977615,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07938210964202881,
      "backward_entropy": 0.021672177645895217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.155051231384277,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05978213995695114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07937765121459961,
      "backward_entropy": 0.02171120544274648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.1290602684021,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059809643775224686,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07937433123588562,
      "backward_entropy": 0.021747963296042547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.15364933013916,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05983584001660347,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07937201261520385,
      "backward_entropy": 0.02178462180826399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.091230869293213,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059862103313207626,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07936950922012329,
      "backward_entropy": 0.0037115878529018825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.058175802230835,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059887904673814774,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07936725616455079,
      "backward_entropy": 0.021842948264545865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.0208234786987305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05991186946630478,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0793664813041687,
      "backward_entropy": 0.0036998718149132198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.92812442779541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05993572250008583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07936559319496155,
      "backward_entropy": 0.0036952408651510873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.9218363761901855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05996139347553253,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07936305999755859,
      "backward_entropy": 0.0036903959181573656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.882095813751221,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05998745560646057,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07936002016067505,
      "backward_entropy": 0.021981962853007846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.830296039581299,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060013607144355774,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07935681343078613,
      "backward_entropy": 0.022012554936938815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.855618000030518,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06004004180431366,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0793532133102417,
      "backward_entropy": 0.022045812673038907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.817865371704102,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06006583571434021,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07935007214546204,
      "backward_entropy": 0.022073619895511203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.605629920959473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06009117141366005,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07934715747833251,
      "backward_entropy": 0.022101658913824294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.547518730163574,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0601174458861351,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07934336662292481,
      "backward_entropy": 0.022127423021528456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.594982624053955,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06014455482363701,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07933868169784546,
      "backward_entropy": 0.0036532974077595603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.173659324645996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060171939432621,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0793336033821106,
      "backward_entropy": 0.003648791876104143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.501249313354492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060201581567525864,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07932641506195068,
      "backward_entropy": 0.003645284722248713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.446746349334717,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060230985283851624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07931928634643555,
      "backward_entropy": 0.003641183591551251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7270407676696777,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060260288417339325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0793121337890625,
      "backward_entropy": 0.022282684842745464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.65042781829834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06028791144490242,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07930634021759034,
      "backward_entropy": 0.003632496628496382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8582686185836792,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06031821668148041,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07929807901382446,
      "backward_entropy": 0.022332774268256292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.648499488830566,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060346271842718124,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07929168939590454,
      "backward_entropy": 0.022359836432668898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.772184371948242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060376230627298355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07928348779678344,
      "backward_entropy": 0.0036168984240955776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.139409065246582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06040715426206589,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07927432060241699,
      "backward_entropy": 0.022397417161199782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.84399127960205,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060437776148319244,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07926528453826905,
      "backward_entropy": 0.003604772604174084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.022342681884766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060468889772892,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07925558090209961,
      "backward_entropy": 0.022442337539460924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.713233947753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06050227954983711,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07924376130104065,
      "backward_entropy": 0.02246592938899994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.35746955871582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060535553842782974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07923191785812378,
      "backward_entropy": 0.0035905933214558493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.861237049102783,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06056947633624077,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07921933531761169,
      "backward_entropy": 0.02249661087989807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.494307518005371,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06060267612338066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0792073130607605,
      "backward_entropy": 0.022511040170987446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.74622917175293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06063586100935936,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07919511198997498,
      "backward_entropy": 0.0035728303094704947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.690511703491211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06066841632127762,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07918336391448974,
      "backward_entropy": 0.022539978226025898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.285433769226074,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060700397938489914,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07917195558547974,
      "backward_entropy": 0.022557535105281405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.04317702353000641,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06073245406150818,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07916035652160644,
      "backward_entropy": 0.0035558686488204533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6619776487350464,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06076134368777275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07915163636207581,
      "backward_entropy": 0.022592667076322768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.710103988647461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060788169503211975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07914466857910156,
      "backward_entropy": 0.0035458534128136104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6417484283447266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06081622466444969,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07913639545440673,
      "backward_entropy": 0.003541327185100979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.984097480773926,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06084214150905609,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0791301190853119,
      "backward_entropy": 0.022670621673266094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.9265031814575195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0608687624335289,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07912297248840332,
      "backward_entropy": 0.022697826226552326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.741510391235352,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06089607998728752,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0791150152683258,
      "backward_entropy": 0.02272955576578776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.149740695953369,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060922540724277496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07910773754119874,
      "backward_entropy": 0.0035246474047501883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.585847020149231,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060947708785533905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07910159826278687,
      "backward_entropy": 0.003520732952488793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.7160444259643555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0609709732234478,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0790972352027893,
      "backward_entropy": 0.022816278868251376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.6693315505981445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06099512055516243,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.079091876745224,
      "backward_entropy": 0.02284399668375651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.5828728675842285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061019908636808395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07908576726913452,
      "backward_entropy": 0.0228648218843672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.059521675109863,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06104417145252228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07908003330230713,
      "backward_entropy": 0.022887279589970905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.533462405204773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06106850504875183,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07907413840293884,
      "backward_entropy": 0.02290725542439355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.49325704574585,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06109100580215454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07906993627548217,
      "backward_entropy": 0.0034898954133192697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.89276123046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06111319735646248,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07906590104103088,
      "backward_entropy": 0.003484609226385752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.837749481201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061137083917856216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0790600061416626,
      "backward_entropy": 0.003480441454384062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9481170177459717,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0611623115837574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07905265092849731,
      "backward_entropy": 0.02299828827381134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.60721492767334,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0611862950026989,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07904642224311828,
      "backward_entropy": 0.023022118541929457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.343624114990234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06121290847659111,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07903741598129273,
      "backward_entropy": 0.02304570045736101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.15934944152832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061238668859004974,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07902913093566895,
      "backward_entropy": 0.023065755764643352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8634681701660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0612650029361248,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07902016639709472,
      "backward_entropy": 0.0034576704104741416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.650333881378174,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06128993257880211,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07901246547698974,
      "backward_entropy": 0.023108185993300542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8213489055633545,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061314865946769714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0790046513080597,
      "backward_entropy": 0.023130012883080378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4129605293273926,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06133855879306793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07899796366691589,
      "backward_entropy": 0.0034439416809214484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.913115978240967,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06136070564389229,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07899270057678223,
      "backward_entropy": 0.023185571034749348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.864234924316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061383605003356934,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07898656725883484,
      "backward_entropy": 0.0034369869778553643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.8170318603515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06140727549791336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07897949814796448,
      "backward_entropy": 0.023236201869116888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.423434734344482,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06143159419298172,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07897167205810547,
      "backward_entropy": 0.02325782510969374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.705535888671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06145576760172844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07896389365196228,
      "backward_entropy": 0.023271469606293574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.9992756843566895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06147875264286995,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07895721793174744,
      "backward_entropy": 0.0034172474924061033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.623127460479736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061503056436777115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07894903421401978,
      "backward_entropy": 0.0034118669314516913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.569053649902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061527881771326065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07894023656845092,
      "backward_entropy": 0.023314177989959717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.221546173095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06155333295464516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0789305329322815,
      "backward_entropy": 0.02333166367477841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6093811988830566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0615786612033844,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07892083525657653,
      "backward_entropy": 0.003396628424525261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5862786769866943,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061602476984262466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07891267538070679,
      "backward_entropy": 0.0033905464741918775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.452579498291016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06162508949637413,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07890571355819702,
      "backward_entropy": 0.02337228258450826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.064494609832764,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061650726944208145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07889537811279297,
      "backward_entropy": 0.023380256361431546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7751424312591553,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06167616322636604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07888514995574951,
      "backward_entropy": 0.003372007980942726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7440271377563477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06170080602169037,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0788756251335144,
      "backward_entropy": 0.003365877394874891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.715097188949585,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061724789440631866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07886666059494019,
      "backward_entropy": 0.023408591747283936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.910236358642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06174817681312561,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07885822057723998,
      "backward_entropy": 0.0234229928917355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.8711042404174805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06177143752574921,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07884987592697143,
      "backward_entropy": 0.023431451784239873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.236240863800049,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061794668436050415,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07884137034416198,
      "backward_entropy": 0.02343943218390147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4102182388305664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061819061636924744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07883150577545166,
      "backward_entropy": 0.02344824042585161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.306424140930176,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06184220686554909,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07882287502288818,
      "backward_entropy": 0.02345998419655694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5465948581695557,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061867035925388336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0788122296333313,
      "backward_entropy": 0.023470295800103083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.681119918823242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06189103424549103,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07880241870880127,
      "backward_entropy": 0.023478143744998507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3380496501922607,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061914872378110886,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0787926435470581,
      "backward_entropy": 0.0234845197863049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.606576442718506,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06193723529577255,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07878453731536865,
      "backward_entropy": 0.02348108424080743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1611981391906738,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0619596503674984,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0787761926651001,
      "backward_entropy": 0.02348075310389201,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 6.640951477438211,
    "avg_log_Z": -0.06068534217774868,
    "success_rate": 1.0,
    "avg_reward": 36.3,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.01,
      "1": 0.66,
      "2": 0.33
    },
    "avg_forward_entropy": 0.07916145539283753,
    "avg_backward_entropy": 0.016833905236174663,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}