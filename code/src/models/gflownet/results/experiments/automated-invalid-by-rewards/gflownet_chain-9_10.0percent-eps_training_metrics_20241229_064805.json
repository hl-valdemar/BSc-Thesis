{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07694748375150892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07695004675123426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.44354248046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967621803283692,
      "backward_entropy": 0.07680640618006389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.40916442871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1096808910369873,
      "backward_entropy": 0.07681251896752252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.37510681152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00019999989308416843,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10968548059463501,
      "backward_entropy": 0.07695166932211982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.81166076660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00029999963589943945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969001054763794,
      "backward_entropy": 0.07695453034506904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.85804748535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00039947754703462124,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969444513320922,
      "backward_entropy": 0.076955689324273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.2991180419922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0004992222529835999,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969878435134887,
      "backward_entropy": 0.0768359899520874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 229.16348266601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0005986165488138795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10970301628112793,
      "backward_entropy": 0.07695868280198839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.71502685546875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0006976753938943148,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970717668533325,
      "backward_entropy": 0.07684661282433404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 229.5478057861328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0007959199137985706,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971121788024903,
      "backward_entropy": 0.0769611464606391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.68765258789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0008940925472415984,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10971513986587525,
      "backward_entropy": 0.07696486843956842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.07460021972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0009929443476721644,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10971895456314087,
      "backward_entropy": 0.07686156034469604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 278.0912780761719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0010909994598478079,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097226858139038,
      "backward_entropy": 0.07696819305419922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.58636474609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0011906421277672052,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10972630977630615,
      "backward_entropy": 0.07687106397416857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 261.50830078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0012906317133456469,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10972973108291625,
      "backward_entropy": 0.07687571313646105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.01425170898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013913913862779737,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10973306894302368,
      "backward_entropy": 0.07697294818030463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.9888000488281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0014916540822014213,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10973632335662842,
      "backward_entropy": 0.0768846140967475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.00421142578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0015926925698295236,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10973948240280151,
      "backward_entropy": 0.07697591516706678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.47190856933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0016938477056100965,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974254608154296,
      "backward_entropy": 0.07697737216949463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.49586486816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0017944562714546919,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974551439285278,
      "backward_entropy": 0.07697217994266087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.40916442871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018939453875645995,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097483515739441,
      "backward_entropy": 0.07698007424672444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.39381408691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001993137411773205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10975109338760376,
      "backward_entropy": 0.07697408729129368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.75347900390625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00209055095911026,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975375175476074,
      "backward_entropy": 0.07690866125954522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.34075927734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0021870937198400497,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.109756338596344,
      "backward_entropy": 0.07698365714814928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.6303253173828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0022822031751275063,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10975883007049561,
      "backward_entropy": 0.07698465055889553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.0834503173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002377481199800968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976126194000244,
      "backward_entropy": 0.07697731256484985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.88328552246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0024736602790653706,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10976369380950927,
      "backward_entropy": 0.07692228423224555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.67303466796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0025677222292870283,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976603031158447,
      "backward_entropy": 0.07698733276791042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.08782958984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002663060324266553,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976828336715698,
      "backward_entropy": 0.07698820696936713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.606201171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0027594503480941057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977046489715576,
      "backward_entropy": 0.07698041862911648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.9966278076172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002856820123270154,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097724437713623,
      "backward_entropy": 0.07693484094407824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.62213134765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0029541561380028725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977429151535034,
      "backward_entropy": 0.07698199484083387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.26902770996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00305166351608932,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977582931518555,
      "backward_entropy": 0.07698276307847765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.27285766601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0031459250021725893,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977721214294434,
      "backward_entropy": 0.07694347699483235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.4159698486328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0032412223517894745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977858304977417,
      "backward_entropy": 0.07699302832285564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.49290466308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003335921559482813,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977988243103028,
      "backward_entropy": 0.07699373033311632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.01806640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0034302440471947193,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978111028671264,
      "backward_entropy": 0.07695137129889594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.25445556640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0035243150778114796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978223085403442,
      "backward_entropy": 0.07699502838982476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.05902099609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0036196457222104073,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978330373764038,
      "backward_entropy": 0.07699566417270237,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.58570861816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0037138015031814575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978426933288574,
      "backward_entropy": 0.07698717382219103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.26519775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0038069961592555046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978512763977051,
      "backward_entropy": 0.07698775662316217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.07980346679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003901745891198516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978590250015259,
      "backward_entropy": 0.07699730661180285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.85679626464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003992410376667976,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097866177558899,
      "backward_entropy": 0.07699777020348443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.38388061523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004082274157553911,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978726148605347,
      "backward_entropy": 0.07699819405873616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.591796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00417151628062129,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978784561157226,
      "backward_entropy": 0.07696886857350667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.56674194335938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0042628031224012375,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978834629058838,
      "backward_entropy": 0.07697068320380317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.65576171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0043549444526433945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978878736495971,
      "backward_entropy": 0.07699077659183079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.73353576660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004445991944521666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978919267654419,
      "backward_entropy": 0.07699120044708252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.25994873046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004536234773695469,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978952646255494,
      "backward_entropy": 0.0770001941257053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.46751403808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004625851754099131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978976488113404,
      "backward_entropy": 0.0769919819302029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 292.70599365234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004713090136647224,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097899079322815,
      "backward_entropy": 0.07697885566287571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.37547302246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004805342294275761,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097899317741394,
      "backward_entropy": 0.07698036564721002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.93470764160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004899387247860432,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978987216949462,
      "backward_entropy": 0.07699314090940687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.0889129638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004993102513253689,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978976488113404,
      "backward_entropy": 0.07700188954671223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.2257843017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0050878566689789295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978950262069702,
      "backward_entropy": 0.07699392239252727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.20925903320312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005181973334401846,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978922843933106,
      "backward_entropy": 0.07700254519780476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.79339599609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00527413422241807,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978884696960449,
      "backward_entropy": 0.07698727978600396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.5788116455078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005367235746234655,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978838205337524,
      "backward_entropy": 0.07698852486080593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.9682159423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005459492094814777,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097878098487854,
      "backward_entropy": 0.07699525356292725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.28677368164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0055519831366837025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978710651397705,
      "backward_entropy": 0.07699557145436604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.98512268066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005644448567181826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978634357452392,
      "backward_entropy": 0.0770039161046346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.26866149902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005734976846724749,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978550910949707,
      "backward_entropy": 0.07699614100986057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.38531494140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00582463014870882,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978462696075439,
      "backward_entropy": 0.07699637942843968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.21539306640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005912566091865301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097836971282959,
      "backward_entropy": 0.07699659135606554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.933349609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005999923683702946,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097827434539795,
      "backward_entropy": 0.07699590259128147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.66448974609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006084755528718233,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978174209594727,
      "backward_entropy": 0.07699677679273817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 276.578857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006170568522065878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978060960769653,
      "backward_entropy": 0.07699708806143866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.01971435546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006260939873754978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097793459892273,
      "backward_entropy": 0.07699728674358791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.23736572265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006347482092678547,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977805852890014,
      "backward_entropy": 0.07700537972980076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.5291748046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0064326561987400055,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977672338485718,
      "backward_entropy": 0.077005492316352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.47715759277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006514617241919041,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097753405570984,
      "backward_entropy": 0.07700557178921169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.22508239746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006596600171178579,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977392196655274,
      "backward_entropy": 0.07700137297312419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.0756378173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006676533725112677,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977246761322021,
      "backward_entropy": 0.07699771059883966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.0488739013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006757006980478764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977089405059814,
      "backward_entropy": 0.07699767748514812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.37037658691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006837980821728706,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976922512054443,
      "backward_entropy": 0.07699765099419488,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.27320861816406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006919148378074169,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10976749658584595,
      "backward_entropy": 0.07700376378165351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.3157196044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007001672405749559,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976563692092896,
      "backward_entropy": 0.07700581020779079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.56201171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007084256503731012,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976370573043823,
      "backward_entropy": 0.07699753178490533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.10699462890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007167811039835215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976169109344483,
      "backward_entropy": 0.07699748542573717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.50261688232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007253667805343866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10975942611694336,
      "backward_entropy": 0.07699745231204563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.58917236328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007335249334573746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10975710153579712,
      "backward_entropy": 0.0770058896806505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.3641357421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007416391279548407,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975464582443237,
      "backward_entropy": 0.07700681024127537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.70132446289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007493531797081232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10975223779678345,
      "backward_entropy": 0.07699720064798991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.31272888183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007571463473141193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974974632263183,
      "backward_entropy": 0.07700577709409925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.41071319580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007648995611816645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974719524383544,
      "backward_entropy": 0.07700571748945448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.8134307861328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007723092567175627,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097446322441101,
      "backward_entropy": 0.07700563801659478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.78817749023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0077973101288080215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974197387695313,
      "backward_entropy": 0.07699662446975708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.66143798828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007871653884649277,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10973923206329346,
      "backward_entropy": 0.07700902223587036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 276.6405029296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007945933379232883,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097364068031311,
      "backward_entropy": 0.07700933350457086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.7101593017578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008026658557355404,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10973330736160278,
      "backward_entropy": 0.07700966464148627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.92520141601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008108287118375301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10973000526428223,
      "backward_entropy": 0.0769961675008138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.34161376953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008191668428480625,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10972648859024048,
      "backward_entropy": 0.07701028055614895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 259.2309875488281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00827653519809246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097227931022644,
      "backward_entropy": 0.07700497574276394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.93472290039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008365817368030548,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971883535385132,
      "backward_entropy": 0.07699604829152425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.44456481933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00845484621822834,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10971487760543823,
      "backward_entropy": 0.07701114813486735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.85617065429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008540306240320206,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10971099138259888,
      "backward_entropy": 0.07700468434227838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.17205810546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008623926900327206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10970699787139893,
      "backward_entropy": 0.07699580325020684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.35997009277344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008709050714969635,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970282554626465,
      "backward_entropy": 0.07701185014512804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 276.2060852050781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008794493041932583,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109698486328125,
      "backward_entropy": 0.0770120620727539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.218505859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008885267190635204,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969383716583252,
      "backward_entropy": 0.07700406180487739,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.39288330078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008974142372608185,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10968919992446899,
      "backward_entropy": 0.07699539264043172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.06029510498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009061708115041256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10968446731567383,
      "backward_entropy": 0.07699526680840386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.01983642578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009144873358309269,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967978239059448,
      "backward_entropy": 0.07701283031039768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.85910034179688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009226158261299133,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967504978179932,
      "backward_entropy": 0.07700339953104655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.69985961914062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00930703990161419,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967018604278564,
      "backward_entropy": 0.07701314820183648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.3860321044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009389404207468033,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1096651792526245,
      "backward_entropy": 0.07700297567579481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.03985595703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00947556085884571,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10965986251831054,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.48420715332031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009563980624079704,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10965433120727539,
      "backward_entropy": 0.07700259155697292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.16290283203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009648026898503304,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10964879989624024,
      "backward_entropy": 0.07700236638387044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.53594970703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009731315076351166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10964314937591553,
      "backward_entropy": 0.07699365086025661,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.80816650390625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009811545722186565,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10963754653930664,
      "backward_entropy": 0.07701394293043348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.17762756347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009890236891806126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10963201522827148,
      "backward_entropy": 0.07700163788265652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.913330078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0099689532071352,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10962631702423095,
      "backward_entropy": 0.07701415485805935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.59600830078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010044985450804234,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10962071418762206,
      "backward_entropy": 0.07701424095365736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.58502197265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010123510845005512,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10961514711380005,
      "backward_entropy": 0.07700078354941474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.5460968017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010206492617726326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10960922241210938,
      "backward_entropy": 0.07699191570281982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.77549743652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010293446481227875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10960296392440796,
      "backward_entropy": 0.07700041929880778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.07115173339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010382900014519691,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10959641933441162,
      "backward_entropy": 0.07699152496125963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.87173461914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010469811037182808,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958991050720215,
      "backward_entropy": 0.07699127991994222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.76283264160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0105579923838377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958312749862671,
      "backward_entropy": 0.07699104150136311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.4626922607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010648760944604874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10957601070404052,
      "backward_entropy": 0.07699083619647556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.77760314941406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01073845662176609,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10956873893737792,
      "backward_entropy": 0.07701487673653497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.13514709472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010826894082129002,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10956140756607055,
      "backward_entropy": 0.07699035273657905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.16424560546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01091331709176302,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095539927482605,
      "backward_entropy": 0.07699909475114611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.65296936035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010998730547726154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10954649448394775,
      "backward_entropy": 0.07698978318108453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.35874938964844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01108562108129263,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953876972198487,
      "backward_entropy": 0.07701510190963745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.64641571044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011172660626471043,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10953088998794555,
      "backward_entropy": 0.07698918051189846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.19740295410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011251418851315975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952339172363282,
      "backward_entropy": 0.07698876327938503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.45721435546875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011332670226693153,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1095155954360962,
      "backward_entropy": 0.0770152277416653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.30987548828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011413359083235264,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10950781106948852,
      "backward_entropy": 0.07698791556888157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.74267578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011498112231492996,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10949966907501221,
      "backward_entropy": 0.0770153005917867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.84625244140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011583361774682999,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10949128866195679,
      "backward_entropy": 0.07699665096071032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.53858947753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011670053005218506,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948269367218018,
      "backward_entropy": 0.0769865910212199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.90296936035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011756057851016521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10947390794754028,
      "backward_entropy": 0.07698612742953831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.5254364013672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011844422668218613,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10946484804153442,
      "backward_entropy": 0.07701542642381456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.01507568359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011933518573641777,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945563316345215,
      "backward_entropy": 0.07698512077331543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.40151977539062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012022590264678001,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10944619178771972,
      "backward_entropy": 0.07701547940572102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.47796630859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01210820209234953,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10943682193756103,
      "backward_entropy": 0.07701549927393596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.2509765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01219091471284628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10942736864089966,
      "backward_entropy": 0.07698337899314033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.4224090576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012273978441953659,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10941778421401978,
      "backward_entropy": 0.07699330647786458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.61859130859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01235777884721756,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10940790176391602,
      "backward_entropy": 0.0769927634133233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.45762634277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012440926395356655,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10939791202545165,
      "backward_entropy": 0.076992220348782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.20669555664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012525823898613453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10938757658004761,
      "backward_entropy": 0.07698056432935926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.5158233642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012610283680260181,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10937701463699341,
      "backward_entropy": 0.0769797960917155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.01620483398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012694044038653374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10936630964279175,
      "backward_entropy": 0.07697898811764187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.35165405273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012770992703735828,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935603380203247,
      "backward_entropy": 0.07701554563310412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.87339782714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012846985831856728,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10934557914733886,
      "backward_entropy": 0.0770155323876275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.84226989746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012922236695885658,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10933490991592407,
      "backward_entropy": 0.0769760012626648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.6190185546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012996829114854336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10932400226593017,
      "backward_entropy": 0.07697486877441406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.5969543457031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013069577515125275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1093130350112915,
      "backward_entropy": 0.0769860479566786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.1130828857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013148700818419456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10930125713348389,
      "backward_entropy": 0.07697244485219319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.7179183959961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013225638307631016,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10928939580917359,
      "backward_entropy": 0.0769839816623264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.9015655517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013298057951033115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10927768945693969,
      "backward_entropy": 0.07696977588865492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.78668212890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013373254798352718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1092654824256897,
      "backward_entropy": 0.07696832550896539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.50985717773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01344992034137249,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10925288200378418,
      "backward_entropy": 0.07696681552463108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.91529846191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013528446666896343,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10923974514007569,
      "backward_entropy": 0.07701499594582452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.32562255859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013605712912976742,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10922648906707763,
      "backward_entropy": 0.07701489660474989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.00538635253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01368174608796835,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10921310186386109,
      "backward_entropy": 0.07697628604041205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.98721313476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013758056797087193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10919939279556275,
      "backward_entropy": 0.07696025901370579,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.01006317138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013829820789396763,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10918589830398559,
      "backward_entropy": 0.0769583781560262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.083984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013898568227887154,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10917246341705322,
      "backward_entropy": 0.07697167661454943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.07762145996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013970683328807354,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10915838479995728,
      "backward_entropy": 0.07697006728914049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.7276611328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014046863652765751,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10914366245269776,
      "backward_entropy": 0.07696851094563802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.8465118408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014121227897703648,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10912895202636719,
      "backward_entropy": 0.07696686850653754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.35986328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014194262214004993,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10911417007446289,
      "backward_entropy": 0.07696513334910075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.23248291015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014261091127991676,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10909987688064575,
      "backward_entropy": 0.07696323924594456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.03643798828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014329316094517708,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10908509492874145,
      "backward_entropy": 0.07696131202909681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.55731201171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014396438375115395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10907011032104492,
      "backward_entropy": 0.07695929871665107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.19129943847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014462689869105816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10905487537384033,
      "backward_entropy": 0.07693779468536377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.6326141357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01452749315649271,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1090395450592041,
      "backward_entropy": 0.07695502705044216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.704345703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014595135115087032,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10902353525161743,
      "backward_entropy": 0.07695293426513672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.19131469726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014666617847979069,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10900669097900391,
      "backward_entropy": 0.0769508679707845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.60116577148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014735284261405468,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10898985862731933,
      "backward_entropy": 0.07692664199405247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.45733642578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01480771042406559,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10897217988967896,
      "backward_entropy": 0.07692380746205647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.63841247558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01487900409847498,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10895426273345947,
      "backward_entropy": 0.07694400681389703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.55572509765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014948723837733269,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10893622636795045,
      "backward_entropy": 0.07694151666429308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.7895050048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015022354200482368,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10891727209091187,
      "backward_entropy": 0.07691494623819987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.19468688964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015094640664756298,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10889811515808105,
      "backward_entropy": 0.07691192626953125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.89336395263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01516561210155487,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10887876749038697,
      "backward_entropy": 0.07690875397788154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.5626220703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015231400728225708,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1088598608970642,
      "backward_entropy": 0.07701153887642755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.64031982421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015296426601707935,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1088407039642334,
      "backward_entropy": 0.07690178023444282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.04408264160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015361002646386623,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1088212251663208,
      "backward_entropy": 0.0770111083984375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.78984069824219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015428585931658745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10880043506622314,
      "backward_entropy": 0.07692118485768636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.43124389648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01549132727086544,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1087796688079834,
      "backward_entropy": 0.07691770129733616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.39524841308594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01555607933551073,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10875800848007203,
      "backward_entropy": 0.07701045274734497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.77427673339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015625055879354477,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1087350845336914,
      "backward_entropy": 0.07688285244835748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.78970336914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01569531485438347,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10871140956878662,
      "backward_entropy": 0.07690734333462185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.16395568847656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015763189643621445,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1086876392364502,
      "backward_entropy": 0.07700985007815891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.8997039794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015833688899874687,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10866290330886841,
      "backward_entropy": 0.07689983314938015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.60597229003906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01590711995959282,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10863707065582276,
      "backward_entropy": 0.07700945271386041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.06724548339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01598273776471615,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10861032009124756,
      "backward_entropy": 0.07689240905973646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.52674865722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016059119254350662,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10858291387557983,
      "backward_entropy": 0.07688854800330268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.36920166015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016129815950989723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10855612754821778,
      "backward_entropy": 0.07685177193747626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.1382598876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016200343146920204,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10852893590927123,
      "backward_entropy": 0.07687985897064209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.26727294921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016272637993097305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10850110054016113,
      "backward_entropy": 0.0768412086698744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.90774536132812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016344619914889336,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10847299098968506,
      "backward_entropy": 0.07700819439358181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.29071044921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016418011859059334,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10844383239746094,
      "backward_entropy": 0.07700799571143256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.66879272460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016492566093802452,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1084139108657837,
      "backward_entropy": 0.07686127556694879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.05360412597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016566717997193336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10838372707366943,
      "backward_entropy": 0.07681887679629856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.25386047363281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016640400514006615,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10835322141647338,
      "backward_entropy": 0.07700739966498481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.49049377441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01671142317354679,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10832278728485108,
      "backward_entropy": 0.07684522867202759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.27182006835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01678377203643322,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10829148292541504,
      "backward_entropy": 0.07683948675791423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.41845703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016859762370586395,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10825877189636231,
      "backward_entropy": 0.07679375012715657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.3551788330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016940787434577942,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1082240104675293,
      "backward_entropy": 0.07678773668077257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.5464630126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017019441351294518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10818930864334106,
      "backward_entropy": 0.07678123315175374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.01017761230469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01709832064807415,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10815401077270508,
      "backward_entropy": 0.076817168129815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.83091735839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017172671854496002,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10811922550201417,
      "backward_entropy": 0.07681043942769368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.85548400878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017247045412659645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10808377265930176,
      "backward_entropy": 0.07680337958865696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.146728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017323561012744904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10804766416549683,
      "backward_entropy": 0.07675127188364665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.58570861816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017399776726961136,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10801124572753906,
      "backward_entropy": 0.07674326499303182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 251.92147827148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01747719757258892,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10797381401062012,
      "backward_entropy": 0.07673539055718316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.75396728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017561858519911766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1079338550567627,
      "backward_entropy": 0.07672819826338026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.842041015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017646513879299164,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1078932523727417,
      "backward_entropy": 0.07700497574276394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.2167510986328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017728466540575027,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10785268545150757,
      "backward_entropy": 0.07676041126251221,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.25973510742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01780787669122219,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10781210660934448,
      "backward_entropy": 0.07675227853986952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.52430725097656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01788146048784256,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10777251720428467,
      "backward_entropy": 0.0770041545232137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.9719467163086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017949433997273445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10773390531539917,
      "backward_entropy": 0.07668350802527533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.7900619506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018015487119555473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10769516229629517,
      "backward_entropy": 0.07667250103420681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.85580444335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01807958073914051,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10765631198883056,
      "backward_entropy": 0.07666097746955024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.20254516601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01814052276313305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10761774778366089,
      "backward_entropy": 0.07664880487653944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.63805389404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018205538392066956,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10757722854614257,
      "backward_entropy": 0.07668896516164143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.15390014648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018268713727593422,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10753659009933472,
      "backward_entropy": 0.07667704423268636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.97767639160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01833035796880722,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10749573707580566,
      "backward_entropy": 0.07661196258332995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.6136016845703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0183960422873497,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1074528694152832,
      "backward_entropy": 0.0769989358054267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.66883850097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018464108929038048,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10740849971771241,
      "backward_entropy": 0.07658731937408447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.59996032714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01853693090379238,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10736185312271118,
      "backward_entropy": 0.0765756368637085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.2481231689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01861243136227131,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10731353759765624,
      "backward_entropy": 0.07661787668863933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.0288848876953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018689097836613655,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10726401805877686,
      "backward_entropy": 0.07699757152133518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.44387817382812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018766582012176514,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10721336603164673,
      "backward_entropy": 0.07659494876861572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.70263671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018842291086912155,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1071624994277954,
      "backward_entropy": 0.07699690924750434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.5463104248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01891525462269783,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10711183547973632,
      "backward_entropy": 0.07656912671195136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.47140502929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018988175317645073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10706032514572143,
      "backward_entropy": 0.07649885283576117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.2214584350586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01906106434762478,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10700793266296386,
      "backward_entropy": 0.07648379935158624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.53823852539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019128326326608658,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1069568395614624,
      "backward_entropy": 0.07652458879682753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.65957641601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019200041890144348,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10690319538116455,
      "backward_entropy": 0.07699366410573323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.02896118164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019274387508630753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10684759616851806,
      "backward_entropy": 0.07649390565024482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.02882385253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019349172711372375,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10679104328155517,
      "backward_entropy": 0.07647838195164998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.72654724121094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01942189410328865,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10673424005508422,
      "backward_entropy": 0.07699180311626858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.63023376464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01949499547481537,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10667576789855956,
      "backward_entropy": 0.0763833655251397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.73216247558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019573118537664413,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10661451816558838,
      "backward_entropy": 0.07699067062801784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.34132385253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019652346149086952,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10655161142349243,
      "backward_entropy": 0.076413094997406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.52003479003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019732097163796425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10648800134658813,
      "backward_entropy": 0.07639638582865398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.59153747558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019810941070318222,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10642423629760742,
      "backward_entropy": 0.07698899507522583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.20628356933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019888177514076233,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10636012554168701,
      "backward_entropy": 0.07636014620463054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.85948181152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019966397434473038,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10629453659057617,
      "backward_entropy": 0.07627215650346544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.6636734008789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020042961463332176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10622872114181518,
      "backward_entropy": 0.07625081141789754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.54351806640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02011485956609249,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10616450309753418,
      "backward_entropy": 0.07698513401879205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.20323181152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020187389105558395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1060987114906311,
      "backward_entropy": 0.07627669970194499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.32638549804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02025897055864334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10603233575820922,
      "backward_entropy": 0.07618051105075413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.13760375976562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020330388098955154,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10596516132354736,
      "backward_entropy": 0.07698143190807766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.85006713867188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020404577255249023,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1058955192565918,
      "backward_entropy": 0.07620693577660455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.43785095214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020473167300224304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10582796335220337,
      "backward_entropy": 0.07610400517781575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.75065612792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020543675869703293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10575827360153198,
      "backward_entropy": 0.07607698440551758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.8248291015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020612170919775963,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10568860769271851,
      "backward_entropy": 0.0761290258831448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.5695571899414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020679548382759094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10561857223510743,
      "backward_entropy": 0.07601981692843968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.71685791015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02074393257498741,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.105549156665802,
      "backward_entropy": 0.07697233888838026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.88164520263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020814644172787666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10547511577606201,
      "backward_entropy": 0.07596073547999065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.542724609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020883219316601753,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10540122985839843,
      "backward_entropy": 0.07593076758914524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.98413848876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02094893530011177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1053279995918274,
      "backward_entropy": 0.07589964071909587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.9762420654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021011345088481903,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10525572299957275,
      "backward_entropy": 0.07586634159088135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.74020385742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02107684500515461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10518057346343994,
      "backward_entropy": 0.07583423455556233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.0244140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021142296493053436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10510443449020386,
      "backward_entropy": 0.07588578595055474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.5380859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021210184320807457,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10502575635910034,
      "backward_entropy": 0.07576931185192531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.43218994140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021279096603393555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10494543313980102,
      "backward_entropy": 0.0757368008295695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.9587860107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021350912749767303,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1048621654510498,
      "backward_entropy": 0.07570334275563557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.23828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02142825350165367,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10477440357208252,
      "backward_entropy": 0.07567073901494344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.7414093017578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021505674347281456,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10468558073043824,
      "backward_entropy": 0.07572419775856866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.09542846679688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0215860977768898,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10459396839141846,
      "backward_entropy": 0.0769538746939765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.93688201904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02166569046676159,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10450166463851929,
      "backward_entropy": 0.07565963268280029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.09004211425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021742308512330055,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10441020727157593,
      "backward_entropy": 0.07562397585974799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.52969360351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021815882995724678,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10431957244873047,
      "backward_entropy": 0.07549149460262722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.09056854248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02188969776034355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10422763824462891,
      "backward_entropy": 0.07545002301534016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.14306640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02196100354194641,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10413628816604614,
      "backward_entropy": 0.07550519042544895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.3582000732422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022028738632798195,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10404632091522217,
      "backward_entropy": 0.07694257630242242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.6556625366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022096015512943268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10395556688308716,
      "backward_entropy": 0.07531244887246026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.66432189941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022161200642585754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10386511087417602,
      "backward_entropy": 0.07526222864786784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.65886688232422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02223164401948452,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10376988649368286,
      "backward_entropy": 0.07532342274983723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.4268798828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022299936041235924,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10367510318756104,
      "backward_entropy": 0.07693168852064344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.50251007080078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022366993129253387,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10357989072799682,
      "backward_entropy": 0.07522784339057074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.75025939941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022430675104260445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10348608493804931,
      "backward_entropy": 0.07517608669069079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.97693634033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022496838122606277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10338923931121827,
      "backward_entropy": 0.0749984582265218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.74093627929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022558264434337616,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10329495668411255,
      "backward_entropy": 0.07506971226798163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.99900817871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02262110449373722,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10319844484329224,
      "backward_entropy": 0.07501431306203206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.10707092285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022685715928673744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10309967994689942,
      "backward_entropy": 0.0748177965482076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.5846405029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02275155670940876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10299888849258423,
      "backward_entropy": 0.07490385903252496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.99876403808594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022818636149168015,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10289614200592041,
      "backward_entropy": 0.07690417766571045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.3527069091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02288140542805195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10279597043991089,
      "backward_entropy": 0.07463337977727254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.59268188476562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022945735603570938,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10269355773925781,
      "backward_entropy": 0.07472480667961968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02300393581390381,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10259507894515991,
      "backward_entropy": 0.07689123021231757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.734130859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023069288581609726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10248968601226807,
      "backward_entropy": 0.07443403535419041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.68861389160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0231345072388649,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10238345861434936,
      "backward_entropy": 0.07452405823601617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.65179443359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023200811818242073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10227522850036622,
      "backward_entropy": 0.07429975933498806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.37246704101562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023271093145012856,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10216290950775146,
      "backward_entropy": 0.07687883244620429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.52879333496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023338936269283295,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1020514965057373,
      "backward_entropy": 0.07432352172003852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.36221313476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023409055545926094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10193718671798706,
      "backward_entropy": 0.07409437497456868,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.85159301757812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023478839546442032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1018223762512207,
      "backward_entropy": 0.0740233063697815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.51419067382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023546624928712845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10170830488204956,
      "backward_entropy": 0.07394845618142022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.66914367675781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0236153956502676,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10159238576889038,
      "backward_entropy": 0.07686196433173285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.20941162109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023681068792939186,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10147836208343505,
      "backward_entropy": 0.07395103904936048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.225341796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023749148473143578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10136104822158813,
      "backward_entropy": 0.07387102974785699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.84376525878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02381516806781292,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10124444961547852,
      "backward_entropy": 0.07363122039371067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.36811065673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02388029359281063,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10112721920013427,
      "backward_entropy": 0.07370021608140734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.8374481201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02394084818661213,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10101319551467895,
      "backward_entropy": 0.07344993617799547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.5802001953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024007244035601616,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10089292526245117,
      "backward_entropy": 0.07351884577009413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.11282348632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02407480590045452,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10077066421508789,
      "backward_entropy": 0.07327323489718968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.37164306640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02413756586611271,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10065193176269531,
      "backward_entropy": 0.07317711247338189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.8712615966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02420332096517086,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10052955150604248,
      "backward_entropy": 0.07324178351296319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.51219177246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024267984554171562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1004066824913025,
      "backward_entropy": 0.07298462920718723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.44920349121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02433760277926922,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10027799606323243,
      "backward_entropy": 0.07305037975311279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.90109252929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024406759068369865,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10014903545379639,
      "backward_entropy": 0.07295352882809109,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.57190704345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024476459249854088,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10001853704452515,
      "backward_entropy": 0.07285459836324056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.49395751953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024542640894651413,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09989064335823059,
      "backward_entropy": 0.07274866766399807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.18843078613281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024602698162198067,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0997680127620697,
      "backward_entropy": 0.07263296842575073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.57582092285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024660583585500717,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09964698553085327,
      "backward_entropy": 0.07234259446461995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.60198974609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024724410846829414,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09951910972595215,
      "backward_entropy": 0.07676839828491211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.97002410888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024793818593025208,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09938507080078125,
      "backward_entropy": 0.07229383786519368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.22862243652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024861227720975876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09925228357315063,
      "backward_entropy": 0.07218301296234131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.71574401855469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024929262697696686,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0991178035736084,
      "backward_entropy": 0.07207090987099542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.47940063476562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02499544434249401,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09898453950881958,
      "backward_entropy": 0.07195378674401177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.92047119140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025055009871721268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09885698556900024,
      "backward_entropy": 0.07164231936136882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.67974853515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025118350982666016,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09872535467147828,
      "backward_entropy": 0.07170053985383776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.03125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02518445998430252,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09859012365341187,
      "backward_entropy": 0.07157871458265516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.72315979003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025256330147385597,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09844887256622314,
      "backward_entropy": 0.07146328025394016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.4053955078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025328906252980232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09830636978149414,
      "backward_entropy": 0.07116060786777073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.00128936767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025405267253518105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0981598973274231,
      "backward_entropy": 0.0710483127170139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.27332305908203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025477498769760132,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0980171263217926,
      "backward_entropy": 0.07111010286543104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.03883361816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025547614321112633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09787607192993164,
      "backward_entropy": 0.07079941034317017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.52938842773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025618629530072212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09773354530334473,
      "backward_entropy": 0.07067256503634983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.8647232055664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0256829671561718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09759745597839356,
      "backward_entropy": 0.07053270604875353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.83381652832031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02574598789215088,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09746224880218506,
      "backward_entropy": 0.07055580615997314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.46509552001953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025803642347455025,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09733261466026306,
      "backward_entropy": 0.07667079236772326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.65640258789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025861041620373726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09720306396484375,
      "backward_entropy": 0.07008446587456597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.29583740234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025921862572431564,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09706900119781495,
      "backward_entropy": 0.0766492552227444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.60671997070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025984810665249825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09693236351013183,
      "backward_entropy": 0.06978786653942531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.65194702148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026047827675938606,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09679498076438904,
      "backward_entropy": 0.06976575321621364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.06010437011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026105759665369987,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09666330218315125,
      "backward_entropy": 0.06947926017973158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.04434204101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02616131491959095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09653351306915284,
      "backward_entropy": 0.06931123468610975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.91399383544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026215706020593643,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0964038074016571,
      "backward_entropy": 0.06913437445958455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.11636352539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02627008594572544,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09627394676208496,
      "backward_entropy": 0.06905335187911987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.89710235595703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02632799558341503,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0961392879486084,
      "backward_entropy": 0.0765705837143792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.16387176513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026382455602288246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09600837230682373,
      "backward_entropy": 0.06868293550279406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.43559265136719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026438014581799507,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09587582349777221,
      "backward_entropy": 0.06841388675901625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.96122741699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026491083204746246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0957451581954956,
      "backward_entropy": 0.06821937031216091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.41921997070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026545021682977676,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09561281204223633,
      "backward_entropy": 0.06802356243133545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.84886932373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02659488655626774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09548403024673462,
      "backward_entropy": 0.0678146481513977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.34230041503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026645634323358536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09535489082336426,
      "backward_entropy": 0.06766906049516466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.925048828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02670341357588768,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09521756172180176,
      "backward_entropy": 0.06741624408298069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.01930236816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026766296476125717,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09507461190223694,
      "backward_entropy": 0.06723182731204563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.58700561523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02683616615831852,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09492381811141967,
      "backward_entropy": 0.06710869736141628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.00949096679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02690538391470909,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09477370977401733,
      "backward_entropy": 0.06692555215623644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.29424285888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026972541585564613,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09462584257125854,
      "backward_entropy": 0.06673248608907063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.6808853149414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02703714556992054,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09448156356811524,
      "backward_entropy": 0.06652993626064724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.3769760131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02710016258060932,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09433884620666504,
      "backward_entropy": 0.06626918580796984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.9445343017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027160920202732086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09419920444488525,
      "backward_entropy": 0.06605792045593262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.45087432861328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02723061479628086,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09404997825622559,
      "backward_entropy": 0.06590790881050958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.23017883300781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027297262102365494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09390465021133423,
      "backward_entropy": 0.06566919220818414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.4127960205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027361560612916946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09376279711723327,
      "backward_entropy": 0.06549131207995945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.16265869140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027433110401034355,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09361328482627869,
      "backward_entropy": 0.06529737843407525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.22142028808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027509963139891624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09345865249633789,
      "backward_entropy": 0.06510444482167561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.64161682128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027587220072746277,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09330343008041382,
      "backward_entropy": 0.06493098205990261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.0030746459961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027664972469210625,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09314759969711303,
      "backward_entropy": 0.06474230686823527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.08617401123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027740629389882088,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09299469590187073,
      "backward_entropy": 0.06454414129257202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.93911743164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0278124138712883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09284614324569702,
      "backward_entropy": 0.06434936655892266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.92863464355469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02788791060447693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09269376993179321,
      "backward_entropy": 0.06412238544887966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.28522491455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027962887659668922,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09254269599914551,
      "backward_entropy": 0.06390951077143352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.02429962158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02803606353700161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09239439368247986,
      "backward_entropy": 0.06374301513036092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.7671661376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028104281052947044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09225189685821533,
      "backward_entropy": 0.0635210739241706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.37615203857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028173673897981644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09210816621780396,
      "backward_entropy": 0.06329731146494548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.62079620361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02823713980615139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09197151064872741,
      "backward_entropy": 0.06305589940812853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.44507598876953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028299588710069656,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09183620810508727,
      "backward_entropy": 0.07632091310289171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.34345245361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028361128643155098,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09170215129852295,
      "backward_entropy": 0.062427904870775014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.37476348876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028419986367225647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0915709137916565,
      "backward_entropy": 0.06229544348186917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.81150817871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02847086638212204,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09144899249076843,
      "backward_entropy": 0.06184758080376519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.3777084350586,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028524575755000114,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09132375717163085,
      "backward_entropy": 0.07624165217081706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.40745544433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028579106554389,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09119726419448852,
      "backward_entropy": 0.06125347481833564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.69938659667969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028635729104280472,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09106818437576295,
      "backward_entropy": 0.060958802700042725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.30410385131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028692729771137238,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09094006419181824,
      "backward_entropy": 0.06087061431672838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.39266204833984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028742246329784393,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09082109928131103,
      "backward_entropy": 0.060346643129984535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.745849609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02878924086689949,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09070523381233216,
      "backward_entropy": 0.06025840176476373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.34099578857422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028835199773311615,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09059053063392639,
      "backward_entropy": 0.07609628306494819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.6636505126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028877122327685356,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09048018455505372,
      "backward_entropy": 0.05934193399217394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.85818481445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028925413265824318,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09036271572113037,
      "backward_entropy": 0.05901503562927246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.01214599609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028971347957849503,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09024838805198669,
      "backward_entropy": 0.05896410014894274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.74219512939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029020974412560463,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09013066291809083,
      "backward_entropy": 0.05864702330695258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.5101318359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029072588309645653,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0900115728378296,
      "backward_entropy": 0.0583388606707255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.96746063232422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029124652966856956,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0898929238319397,
      "backward_entropy": 0.05771781338585748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.46923828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029174165800213814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08977780342102051,
      "backward_entropy": 0.05772114462322659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.77557373046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029222479090094566,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08966432809829712,
      "backward_entropy": 0.05705675813886854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.21192932128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029266314581036568,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08955715298652649,
      "backward_entropy": 0.05707179837756687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.4084701538086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029318271204829216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08944200277328491,
      "backward_entropy": 0.05676842398113675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.5887680053711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029367728158831596,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08933030366897583,
      "backward_entropy": 0.05605683061811659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.9061279296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029414812102913857,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08922169208526612,
      "backward_entropy": 0.05613076686859131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.98355102539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029466908425092697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08910855650901794,
      "backward_entropy": 0.055822551250457764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.10804748535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029518699273467064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08899577856063842,
      "backward_entropy": 0.05550541149245368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.06196594238281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029564935714006424,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08888953328132629,
      "backward_entropy": 0.05471810367372301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.3893585205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029612533748149872,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08878313899040222,
      "backward_entropy": 0.05437432395087348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.74359893798828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029666602611541748,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08867110013961792,
      "backward_entropy": 0.07567564646402995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.18543243408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029718726873397827,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08856117725372314,
      "backward_entropy": 0.053719129827287465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.60100555419922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029768025502562523,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.088454669713974,
      "backward_entropy": 0.05337379044956631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.0221710205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029819106683135033,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08834699988365173,
      "backward_entropy": 0.053543163670433894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.5894012451172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029876867309212685,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08823456168174744,
      "backward_entropy": 0.052721119589275785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.560302734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029938645660877228,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08811941146850585,
      "backward_entropy": 0.07558569643232557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.6517333984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02999950759112835,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08800599575042725,
      "backward_entropy": 0.052109446790483266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.60968017578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030059421434998512,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08789417743682862,
      "backward_entropy": 0.05178952548238966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.23348999023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030116068199276924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08778665065765381,
      "backward_entropy": 0.05201384756300184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.80394744873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030170317739248276,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08768303990364075,
      "backward_entropy": 0.05111134714550442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.66658782958984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030224842950701714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08758023977279664,
      "backward_entropy": 0.05076951781908671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.9079360961914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03027801774442196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08747947216033936,
      "backward_entropy": 0.05102721850077311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.27310180664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030332939699292183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08737806081771851,
      "backward_entropy": 0.050701687733332314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.51701354980469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030389178544282913,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0872761845588684,
      "backward_entropy": 0.049751063187917076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.36074829101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030444201081991196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08717650771141053,
      "backward_entropy": 0.05004924866888258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.43980407714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030499214306473732,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08707759380340577,
      "backward_entropy": 0.04907779561148749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.38570785522461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030550869181752205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08698348999023438,
      "backward_entropy": 0.0493858257929484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.69903564453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030597485601902008,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08689488172531128,
      "backward_entropy": 0.048361427254147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.924949645996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030652031302452087,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08680012226104736,
      "backward_entropy": 0.04802603522936503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.29180145263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030702900141477585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08670983314514161,
      "backward_entropy": 0.04837731520334879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.24677276611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03075275383889675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08662098050117492,
      "backward_entropy": 0.04803503221935696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.08180236816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030802112072706223,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08653355240821839,
      "backward_entropy": 0.04696200291315714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.03623962402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030849561095237732,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08644866943359375,
      "backward_entropy": 0.04659593436453077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.380126953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030890852212905884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08636977672576904,
      "backward_entropy": 0.04696786403656006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.7945556640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030934154987335205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08629020452499389,
      "backward_entropy": 0.046607408258650035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.38540649414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0309799462556839,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08620879650115967,
      "backward_entropy": 0.04545848899417453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.01941680908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031026655808091164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.086127108335495,
      "backward_entropy": 0.04589293400446574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.54780578613281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031072868034243584,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08604642152786254,
      "backward_entropy": 0.04553211728731791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.08196258544922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.031121591106057167,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08596453666687012,
      "backward_entropy": 0.0751034418741862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.90853881835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031168034300208092,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08588488101959228,
      "backward_entropy": 0.044814623064464994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.811546325683594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03121432475745678,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08580634593963624,
      "backward_entropy": 0.04362305005391439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.19857406616211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03125632926821709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08573224544525146,
      "backward_entropy": 0.044070919354756675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.18128967285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031296104192733765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08566100001335145,
      "backward_entropy": 0.043692327207989164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.40138244628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031343236565589905,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08558481931686401,
      "backward_entropy": 0.04249072074890137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.50306701660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03138794004917145,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08551200628280639,
      "backward_entropy": 0.04298464126057095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.29390716552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03143167868256569,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08544109463691711,
      "backward_entropy": 0.042632063229878746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.07929992675781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03147590160369873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08537085056304931,
      "backward_entropy": 0.04228350188997057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.090576171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03152136504650116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08530007600784302,
      "backward_entropy": 0.041933274931377836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.62528991699219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03156837075948715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08522907495498658,
      "backward_entropy": 0.0406985878944397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.55919647216797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03161943703889847,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0851560354232788,
      "backward_entropy": 0.04036708010567559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.13036346435547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03167032077908516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08508434891700745,
      "backward_entropy": 0.040035396814346313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.102237701416016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03172064572572708,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08501381278038025,
      "backward_entropy": 0.03969826963212755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.953025817871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031767405569553375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08494622111320496,
      "backward_entropy": 0.040248602628707886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.30754089355469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03181152045726776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08488143682479858,
      "backward_entropy": 0.039891719818115234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.9574432373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03185593709349632,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08481720685958863,
      "backward_entropy": 0.0395382477177514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.879261016845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03190883994102478,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08474830389022828,
      "backward_entropy": 0.038302196396721735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.70516967773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03195545822381973,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08468432426452636,
      "backward_entropy": 0.03795923458205329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.06950378417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03200162574648857,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08462090492248535,
      "backward_entropy": 0.038525011804368764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.58663558959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03205046430230141,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08455647230148315,
      "backward_entropy": 0.03818504015604655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.33576965332031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03209628164768219,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08449473977088928,
      "backward_entropy": 0.03693585925632053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.89835357666016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03214496746659279,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08443204164505005,
      "backward_entropy": 0.03660300042894152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.06950378417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03219306468963623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08437010049819946,
      "backward_entropy": 0.03715165787272983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.55233764648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032243698835372925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08430745601654052,
      "backward_entropy": 0.036816394991344877,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.58085632324219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032294128090143204,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08424589633941651,
      "backward_entropy": 0.07456884119245741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.37731170654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03234425187110901,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08418536186218262,
      "backward_entropy": 0.03614849845568339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.25939178466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03239571303129196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08412517309188842,
      "backward_entropy": 0.03582108020782471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.71609497070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03244929388165474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08406444191932679,
      "backward_entropy": 0.035494201713138156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.37055206298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032506514340639114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08400277495384216,
      "backward_entropy": 0.03435091177622477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.524169921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03256611526012421,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08394137620925904,
      "backward_entropy": 0.034062743186950684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.23102569580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03262457624077797,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08388155698776245,
      "backward_entropy": 0.03377043207486471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.86261749267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03268804773688316,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08382068872451783,
      "backward_entropy": 0.033502691321902804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.8584976196289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03275015205144882,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08376165628433227,
      "backward_entropy": 0.03322847021950616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.20836639404297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03281090036034584,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08370418548583984,
      "backward_entropy": 0.03375507394472758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.21504211425781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03287215158343315,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08364765644073487,
      "backward_entropy": 0.033477200402153864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.8067626953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03293449059128761,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08359112739562988,
      "backward_entropy": 0.03240297900305854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.02621459960938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03300067409873009,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08353357315063477,
      "backward_entropy": 0.03214228484365675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.72625732421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033067118376493454,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08347748517990113,
      "backward_entropy": 0.0318869948387146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.3875846862793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033131834119558334,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08342311382293702,
      "backward_entropy": 0.03162477413813273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.27507781982422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03319239988923073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08337157964706421,
      "backward_entropy": 0.032113485866122775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.85572052001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033251844346523285,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08332129716873168,
      "backward_entropy": 0.0310680005285475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.7059211730957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033312343060970306,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08327080011367798,
      "backward_entropy": 0.031541761424806386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.28280258178711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03336929529905319,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08322277665138245,
      "backward_entropy": 0.031247529718610976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.39863586425781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03342382609844208,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08317605257034302,
      "backward_entropy": 0.030941466490427654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.46477508544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03348034620285034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08312907218933105,
      "backward_entropy": 0.030641257762908936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.30371856689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03354150801897049,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08308098912239074,
      "backward_entropy": 0.02964174747467041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.873619079589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03360671177506447,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08303211927413941,
      "backward_entropy": 0.02938651376300388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.67183685302734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03366846218705177,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0829848051071167,
      "backward_entropy": 0.029112928443484835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.2035026550293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0337289534509182,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.082938551902771,
      "backward_entropy": 0.029483470651838515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.31927490234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03378629311919212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08289480209350586,
      "backward_entropy": 0.02918348378605313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.17996978759766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03384169563651085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08285253047943116,
      "backward_entropy": 0.02888011932373047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.19630432128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03389638289809227,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08281092643737793,
      "backward_entropy": 0.027971575657526653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.29519653320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0339464470744133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08277133703231812,
      "backward_entropy": 0.02825386987792121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.94735717773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03399745002388954,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0827327847480774,
      "backward_entropy": 0.027381244632932875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.2819938659668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03405233845114708,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08269338607788086,
      "backward_entropy": 0.02766619126001994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.44304656982422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03410495072603226,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08265471458435059,
      "backward_entropy": 0.02682736681567298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.8735580444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03415694460272789,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08261632919311523,
      "backward_entropy": 0.027061002122031316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.12652587890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03420867770910263,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08257842063903809,
      "backward_entropy": 0.02626056306891971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.380409240722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034261468797922134,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08254050016403199,
      "backward_entropy": 0.025983229279518127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.88341522216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03431153669953346,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08250399827957153,
      "backward_entropy": 0.02615596354007721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.27393341064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034361861646175385,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08246799707412719,
      "backward_entropy": 0.02542607651816474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.80801391601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0344151146709919,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08243187069892884,
      "backward_entropy": 0.02516443861855401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.752994537353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0344696119427681,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08239582777023316,
      "backward_entropy": 0.02529917988512251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.754451751708984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03452284634113312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08236077427864075,
      "backward_entropy": 0.025025480323367648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.44244384765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034573283046483994,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08232678174972534,
      "backward_entropy": 0.024746042158868577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.14852905273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034621357917785645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0822938323020935,
      "backward_entropy": 0.024140016900168523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.9097900390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03467100113630295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08226062059402466,
      "backward_entropy": 0.02418564756711324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.52297973632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03472638130187988,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08222651481628418,
      "backward_entropy": 0.02392765548494127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.7784423828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03478414937853813,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08219236135482788,
      "backward_entropy": 0.023679029610421922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.68010711669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03484586626291275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08215799927711487,
      "backward_entropy": 0.023449495434761047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.77875518798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0349082387983799,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08212400674819946,
      "backward_entropy": 0.023033834165996976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.075538635253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03496956080198288,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08209051489830017,
      "backward_entropy": 0.02299331956439548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.45390319824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0350290946662426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08205808401107788,
      "backward_entropy": 0.022763489021195307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.06121063232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03508618101477623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08202718496322632,
      "backward_entropy": 0.022540201743443806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.92706298828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035146214067935944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08199655413627624,
      "backward_entropy": 0.022255685594346788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.87017822265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03520423173904419,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08196645975112915,
      "backward_entropy": 0.022121798661020067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.2856216430664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035263609141111374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08193672299385071,
      "backward_entropy": 0.02189364367061191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.934932708740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03532245010137558,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08190728425979614,
      "backward_entropy": 0.02171538273493449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.34270477294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03537667170166969,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08187861442565918,
      "backward_entropy": 0.021522273619969685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.80117797851562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035433512181043625,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08184969425201416,
      "backward_entropy": 0.021337931354840595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.291038513183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03549298271536827,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08182092308998108,
      "backward_entropy": 0.021078652805752225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.793880462646484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03554711118340492,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08179328441619874,
      "backward_entropy": 0.020985984139972262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.43130111694336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035597823560237885,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08176643848419189,
      "backward_entropy": 0.02065878775384691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.04196166992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03564468398690224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08174012899398804,
      "backward_entropy": 0.02060621976852417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.86474609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03569209203124046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08171350955963134,
      "backward_entropy": 0.02021234730879466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.83940124511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035741906613111496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08168677687644958,
      "backward_entropy": 0.020000952813360427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.63339233398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035796359181404114,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08165960311889649,
      "backward_entropy": 0.019806037346522015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.89051818847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03585227206349373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08163247108459473,
      "backward_entropy": 0.019615795877244737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.5394515991211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035905685275793076,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08160578012466431,
      "backward_entropy": 0.019736692309379578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.08734893798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035962045192718506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08157879114151001,
      "backward_entropy": 0.019581254985597398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.43767547607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03601887822151184,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08155235052108764,
      "backward_entropy": 0.019433887468443975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.700355529785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036073166877031326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08152633309364318,
      "backward_entropy": 0.018866481052504644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.09114074707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03612641245126724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08150044679641724,
      "backward_entropy": 0.019129037857055664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.2137680053711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036179251968860626,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08147507309913635,
      "backward_entropy": 0.018500155872768827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.4305305480957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03623674809932709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08144964575767517,
      "backward_entropy": 0.018337006370226543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.18263244628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03629382699728012,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08142485618591308,
      "backward_entropy": 0.018186103966501024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.40211486816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036349061876535416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08140053153038025,
      "backward_entropy": 0.018629940019713506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.91120910644531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03640373796224594,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08137671947479248,
      "backward_entropy": 0.017899463574091595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.27836990356445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03645682707428932,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08135346174240113,
      "backward_entropy": 0.017761783467398748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.306884765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03650785610079765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08133023381233215,
      "backward_entropy": 0.017615904410680134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.79239273071289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03655825927853584,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08130685687065124,
      "backward_entropy": 0.07478529877132839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.795047760009766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03660997003316879,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08128361701965332,
      "backward_entropy": 0.017325417862998113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.69583511352539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03666260838508606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08126035332679749,
      "backward_entropy": 0.017187439733081393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.87527847290039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03671596571803093,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08123695850372314,
      "backward_entropy": 0.01789658268292745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.581363677978516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036766912788152695,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08121329545974731,
      "backward_entropy": 0.017785441544320848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.21512222290039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03681838884949684,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08118938207626343,
      "backward_entropy": 0.017673174540201824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.781253814697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036869391798973083,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08116520643234253,
      "backward_entropy": 0.01660783092180888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.78199005126953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03692036122083664,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08114137649536132,
      "backward_entropy": 0.07489746146731907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.99790573120117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036969974637031555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08111777305603027,
      "backward_entropy": 0.016327430804570515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.25993347167969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03701787814497948,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08109400272369385,
      "backward_entropy": 0.017250150442123413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.7497787475586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037067171186208725,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0810701847076416,
      "backward_entropy": 0.017148580816056993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.51795196533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03712012618780136,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08104678988456726,
      "backward_entropy": 0.01705525153213077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.92098236083984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03717543184757233,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0810229480266571,
      "backward_entropy": 0.015786101420720417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.378902435302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03723512217402458,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08099850416183471,
      "backward_entropy": 0.01567328307363722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.95502471923828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03729517385363579,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08097407817840577,
      "backward_entropy": 0.016835790541436937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.498592376708984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03735662251710892,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08094938993453979,
      "backward_entropy": 0.015458068913883634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.01453399658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03741740062832832,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0809245765209198,
      "backward_entropy": 0.01534158984820048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.16117095947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03748423978686333,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08089947700500488,
      "backward_entropy": 0.01663226882616679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.88422393798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03755452111363411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.080874502658844,
      "backward_entropy": 0.015139450629552206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.84135437011719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03762776777148247,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08084943294525146,
      "backward_entropy": 0.016532820132043626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.12093734741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03770071640610695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08082422018051147,
      "backward_entropy": 0.014956552121374343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.106401443481445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037767473608255386,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08079907894134522,
      "backward_entropy": 0.01485141118367513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.25555419921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037828538566827774,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08077428340911866,
      "backward_entropy": 0.016346183088090684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.11210632324219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03788994625210762,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08074966073036194,
      "backward_entropy": 0.01628111137284173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.31379699707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037951547652482986,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08072512149810791,
      "backward_entropy": 0.016221559709972806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.884883880615234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038014378398656845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08070061206817628,
      "backward_entropy": 0.01442682577504052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.28716278076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03807690367102623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806756854057312,
      "backward_entropy": 0.014326602220535278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.195838928222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03814138099551201,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08065005540847778,
      "backward_entropy": 0.01604617304272122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.80123519897461,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03820422291755676,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0806243896484375,
      "backward_entropy": 0.07555371522903442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.999080657958984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038264427334070206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08059887886047364,
      "backward_entropy": 0.0140072719918357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.833927154541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03832460194826126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08057323694229127,
      "backward_entropy": 0.013896589477856955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.17092514038086,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03838323801755905,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08054715394973755,
      "backward_entropy": 0.0756072203318278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.301231384277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0384400300681591,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08052129149436951,
      "backward_entropy": 0.01569875909222497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.2552490234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03849726915359497,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0804953157901764,
      "backward_entropy": 0.013560930060015785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.80023765563965,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.038555845618247986,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08046900033950806,
      "backward_entropy": 0.07565514908896552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.5255126953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03861132264137268,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08044297695159912,
      "backward_entropy": 0.015497182806332907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.63508605957031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038672130554914474,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08041642904281616,
      "backward_entropy": 0.015437576505872939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.1348876953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038732659071683884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08038952350616455,
      "backward_entropy": 0.013139829867415957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.229183197021484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03879350796341896,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0803628146648407,
      "backward_entropy": 0.015312896834479438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.21766662597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03885222226381302,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08033605813980102,
      "backward_entropy": 0.015255129999584623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.08555603027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03892182186245918,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0803088366985321,
      "backward_entropy": 0.012855471008353762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.17084884643555,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.038988083600997925,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08028156757354736,
      "backward_entropy": 0.07578690184487237,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.81959915161133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03905489295721054,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08025418519973755,
      "backward_entropy": 0.01510595613055759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.9713191986084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03911853954195976,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08022651672363282,
      "backward_entropy": 0.015046897861692641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.90491485595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03917819634079933,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08019853234291077,
      "backward_entropy": 0.01498139070139991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.09077453613281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03924053534865379,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08017037510871887,
      "backward_entropy": 0.014924628867043389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.89420700073242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03930393233895302,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08014211654663086,
      "backward_entropy": 0.014872012866867913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.47110366821289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03936545550823212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08011341094970703,
      "backward_entropy": 0.01218190954791175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.95370864868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03942584618926048,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08008460998535157,
      "backward_entropy": 0.012085128989484575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.20463562011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039483651518821716,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08005548119544983,
      "backward_entropy": 0.011983537011676364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.26168823242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03954048827290535,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08002609014511108,
      "backward_entropy": 0.014630228281021118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.091968536376953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039600178599357605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07999646663665771,
      "backward_entropy": 0.01457442839940389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.423751831054688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039656247943639755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07996665239334107,
      "backward_entropy": 0.014512813753551908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.78736877441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03970835730433464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07993693351745605,
      "backward_entropy": 0.011587143772178225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.225215911865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039761438965797424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07990671396255493,
      "backward_entropy": 0.011491989096005758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.6981201171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03981437906622887,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07987642288208008,
      "backward_entropy": 0.01434778008196089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.24107360839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039869341999292374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07984586954116821,
      "backward_entropy": 0.011313829157087538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.00239562988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039924900978803635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981500029563904,
      "backward_entropy": 0.011226729386382632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.800758361816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03998110070824623,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797838807106018,
      "backward_entropy": 0.01421192122830285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.464149475097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040037885308265686,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975249290466309,
      "backward_entropy": 0.014171207944552103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.018932342529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04009397327899933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07972084283828736,
      "backward_entropy": 0.010977129969331954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.10040283203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040147192776203156,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07968909740447998,
      "backward_entropy": 0.01409031781885359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.96774673461914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04020160436630249,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07965718507766724,
      "backward_entropy": 0.010819104810555777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.713640213012695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04025556892156601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07962496876716614,
      "backward_entropy": 0.010743861397107443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.64550018310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040306899696588516,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07959263324737549,
      "backward_entropy": 0.013995097743140327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.60504150390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04036505147814751,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0795599341392517,
      "backward_entropy": 0.013970797260602316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.29548263549805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04041986167430878,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07952664494514465,
      "backward_entropy": 0.013939052820205688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.99174880981445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04047327861189842,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07949316501617432,
      "backward_entropy": 0.013911371429761251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.263147354125977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04052857682108879,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07945937514305115,
      "backward_entropy": 0.013884696695539687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.73563766479492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04058093577623367,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07942543029785157,
      "backward_entropy": 0.010310419731669955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.204444885253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04063336178660393,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07939125895500183,
      "backward_entropy": 0.013828050759103563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.167016983032227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04068213328719139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07935705184936523,
      "backward_entropy": 0.010170961419741312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.65435791015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04072752967476845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07932273149490357,
      "backward_entropy": 0.010097570717334747,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.62515640258789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04077210649847984,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07928804159164429,
      "backward_entropy": 0.01373251113626692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.11410903930664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04081983491778374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07925291657447815,
      "backward_entropy": 0.013708168433772193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.375303268432617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040867727249860764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07921737432479858,
      "backward_entropy": 0.013683060805002848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.485904693603516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04091429337859154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07918157577514648,
      "backward_entropy": 0.009816268252001869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.62137222290039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04096224531531334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07914500236511231,
      "backward_entropy": 0.009744420647621155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.300966262817383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0410105399787426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0791080892086029,
      "backward_entropy": 0.009674914181232452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.423973083496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041056763380765915,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07907134294509888,
      "backward_entropy": 0.013552319672372606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.608163833618164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0411059707403183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07903378009796143,
      "backward_entropy": 0.009541670481363932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.112770080566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0411517471075058,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07899634838104248,
      "backward_entropy": 0.009475237793392606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.9100456237793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041200190782547,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07895826101303101,
      "backward_entropy": 0.009411580032772489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.802555084228516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041248854249715805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07891981601715088,
      "backward_entropy": 0.00934894879659017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.51437759399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041297540068626404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0788809597492218,
      "backward_entropy": 0.00928532580534617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.102771759033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04134851321578026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07884168028831481,
      "backward_entropy": 0.009223461151123047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.411983489990234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04139811173081398,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07880197167396545,
      "backward_entropy": 0.009159107175138261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.84523010253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04145125299692154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07876121401786804,
      "backward_entropy": 0.00909972521993849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.54241943359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0415063202381134,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07872007489204406,
      "backward_entropy": 0.013327951232592264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.3814582824707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04156346246600151,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07867865562438965,
      "backward_entropy": 0.008992255561881594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.86935806274414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04162212461233139,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07863686680793762,
      "backward_entropy": 0.008941442602210574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.61042785644531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04168115556240082,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07859476804733276,
      "backward_entropy": 0.013285674982600741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.454341888427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04174076393246651,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07855243086814881,
      "backward_entropy": 0.008846319384045072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.125545501708984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041800640523433685,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07850967645645142,
      "backward_entropy": 0.013272422055403391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.91999816894531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04185951128602028,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07846680879592896,
      "backward_entropy": 0.013262882828712463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.01102066040039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04191763699054718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0784238338470459,
      "backward_entropy": 0.008707097007168664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.610450744628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041978295892477036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07838008403778077,
      "backward_entropy": 0.008662489553292593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.33552551269531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0420391708612442,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07833607196807861,
      "backward_entropy": 0.008619548545943366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.267086029052734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04209940880537033,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07829195261001587,
      "backward_entropy": 0.013247243232197232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.83259582519531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04215851053595543,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07824803590774536,
      "backward_entropy": 0.008539368708928427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.63901138305664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04222039133310318,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07820299863815308,
      "backward_entropy": 0.01324980871544944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.57154083251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042284294962882996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0781568467617035,
      "backward_entropy": 0.008466392755508423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.32598876953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042347971349954605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07811074256896973,
      "backward_entropy": 0.013254182206259834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.75828552246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04241158440709114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.078064227104187,
      "backward_entropy": 0.013258869449297586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.19302749633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0424770824611187,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07801675796508789,
      "backward_entropy": 0.008360264201958975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.68234634399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042540811002254486,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07796907424926758,
      "backward_entropy": 0.008321029444535574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.12251091003418,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.042604364454746246,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07792094945907593,
      "backward_entropy": 0.07643755276997884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.2678337097168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042665284126996994,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07787286043167115,
      "backward_entropy": 0.013252012431621552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.410362243652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042726218700408936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0778243064880371,
      "backward_entropy": 0.013246032926771376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.0467586517334,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042786188423633575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07777581810951233,
      "backward_entropy": 0.008162572979927063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.084815979003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04284311830997467,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07772747278213502,
      "backward_entropy": 0.008124241398440467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.40691375732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04289939999580383,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0776787519454956,
      "backward_entropy": 0.00808664576874839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.264568328857422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04295390844345093,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07763053178787231,
      "backward_entropy": 0.013230405747890472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.624454498291016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043004728853702545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07758289575576782,
      "backward_entropy": 0.013222051991356744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.138553619384766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04305543005466461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07753485441207886,
      "backward_entropy": 0.007966049015522003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.099838256835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04310282692313194,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07748768329620362,
      "backward_entropy": 0.007924021946059333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.646491527557373,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043147116899490356,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07744114398956299,
      "backward_entropy": 0.013186300794283548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.0677604675293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04318757355213165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07739535570144654,
      "backward_entropy": 0.007833054496182336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.29997634887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043229006230831146,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07734870910644531,
      "backward_entropy": 0.013151521484057108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.7747745513916,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043272290378808975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07730093002319335,
      "backward_entropy": 0.00774529410733117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.95888900756836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04331653192639351,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0772524356842041,
      "backward_entropy": 0.013129411472214593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.57792854309082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043362535536289215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07720304727554321,
      "backward_entropy": 0.007671423256397247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.723488807678223,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043408703058958054,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0771531879901886,
      "backward_entropy": 0.013117089867591858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.097206115722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04345179349184036,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07710420489311218,
      "backward_entropy": 0.013100442787011465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.34819030761719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04349425062537193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07705512642860413,
      "backward_entropy": 0.0075453221797943115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.043132781982422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04353844374418259,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07700480818748474,
      "backward_entropy": 0.007502409319082896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.989986419677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043582990765571594,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07695381045341491,
      "backward_entropy": 0.007458432681030697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.0693359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043629374355077744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07690194845199586,
      "backward_entropy": 0.013027046289708879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.618375778198242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04367932304739952,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07684783935546875,
      "backward_entropy": 0.007383108139038086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.41620635986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04372905194759369,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07679342031478882,
      "backward_entropy": 0.007345070441563924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.35674285888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04377906024456024,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07673877477645874,
      "backward_entropy": 0.007311289509137471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.12666130065918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04382979869842529,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07668346166610718,
      "backward_entropy": 0.0072747742136319475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.04945182800293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043880645185709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07662777304649353,
      "backward_entropy": 0.0072418732775582206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.91625213623047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043929435312747955,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07657274603843689,
      "backward_entropy": 0.01298315574725469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.46173095703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043977271765470505,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07651784420013427,
      "backward_entropy": 0.012980447047286563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.659833908081055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04402828961610794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07646104097366332,
      "backward_entropy": 0.007147615982426537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.437904357910156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04407834634184837,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07640440464019775,
      "backward_entropy": 0.07657376925150554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.09317398071289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044128160923719406,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0763474702835083,
      "backward_entropy": 0.012974878152211508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.740966796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044179029762744904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07628964185714722,
      "backward_entropy": 0.007059767014450497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.27104187011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04423157498240471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07623081207275391,
      "backward_entropy": 0.007029264337486691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.07443618774414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04428688436746597,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07616953849792481,
      "backward_entropy": 0.00700061602724923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.684446334838867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04434056580066681,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0761086881160736,
      "backward_entropy": 0.006971982618172963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.492002487182617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04439346864819527,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07604773044586181,
      "backward_entropy": 0.012954023977120718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.3563232421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044446077197790146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07598658800125122,
      "backward_entropy": 0.0069102731843789416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.411781311035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04449821263551712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07592532634735108,
      "backward_entropy": 0.0068797870642609065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.038103103637695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04455219954252243,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0758625864982605,
      "backward_entropy": 0.012934323814180162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.54998016357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044605664908885956,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0758001148700714,
      "backward_entropy": 0.012930424677001106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.189125061035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.044661618769168854,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07573502659797668,
      "backward_entropy": 0.07663703627056545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.530941009521484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044715944677591324,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0756708800792694,
      "backward_entropy": 0.012923947638935514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.487369537353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04477189853787422,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07560527324676514,
      "backward_entropy": 0.006749890744686127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.818256378173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04482503980398178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07554108500480652,
      "backward_entropy": 0.00672567304637697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.107271194458008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04487672075629234,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07547764778137207,
      "backward_entropy": 0.006701267013947169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.073875427246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04492797330021858,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0754137635231018,
      "backward_entropy": 0.012920492225223117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.098731994628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04498181492090225,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07534756660461425,
      "backward_entropy": 0.012914817366335127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.702249526977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04503330960869789,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07528274059295655,
      "backward_entropy": 0.006626520719793107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.666757583618164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04508161544799805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07521986961364746,
      "backward_entropy": 0.006604227340883679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.628110885620117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04512675106525421,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07515887022018433,
      "backward_entropy": 0.012910371025403341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.00027084350586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045173268765211105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07509643435478211,
      "backward_entropy": 0.012908564673529731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.905309677124023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04521907493472099,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07503408193588257,
      "backward_entropy": 0.006534450583987766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.53288269042969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045264072716236115,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0749719500541687,
      "backward_entropy": 0.012908561362160577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.842866897583008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04531213268637657,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07490691542625427,
      "backward_entropy": 0.012903920478290982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.56907081604004,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04536011815071106,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07484152913093567,
      "backward_entropy": 0.012898157040278116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.677263259887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04540718346834183,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07477647066116333,
      "backward_entropy": 0.01289438870218065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.42808723449707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045455075800418854,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07471026182174682,
      "backward_entropy": 0.012885223660204146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.150687217712402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04550289735198021,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07464363574981689,
      "backward_entropy": 0.006394017073843215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.2215461730957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04554779827594757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07457916736602783,
      "backward_entropy": 0.0063700564205646515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.02466583251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045595716685056686,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07451162338256836,
      "backward_entropy": 0.006345562636852264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.87458038330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04564262554049492,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07444454431533813,
      "backward_entropy": 0.006321977410051558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.678913116455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04568968340754509,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07437690496444702,
      "backward_entropy": 0.012840650147861905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.96687126159668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04573778435587883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0743078351020813,
      "backward_entropy": 0.006279961516459783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.363483428955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04578187316656113,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07424240112304688,
      "backward_entropy": 0.006258569657802582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.621362686157227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045827217400074005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07417500019073486,
      "backward_entropy": 0.006237407111459308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.72496032714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045870933681726456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07410868406295776,
      "backward_entropy": 0.006217963579628203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.28624725341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0459178164601326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07403901815414429,
      "backward_entropy": 0.006198614835739136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.613715171813965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04596356675028801,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07397003173828125,
      "backward_entropy": 0.012807442082299126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.36746597290039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04600643366575241,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07390358448028564,
      "backward_entropy": 0.00615433148211903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.958675384521484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04605545848608017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07383056879043579,
      "backward_entropy": 0.012786692215336693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.552064895629883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046103283762931824,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07375833988189698,
      "backward_entropy": 0.012778217593828836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.391304969787598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04615093767642975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07368589639663696,
      "backward_entropy": 0.0060935045282046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.58201217651367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04619555547833443,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0736161708831787,
      "backward_entropy": 0.006072647869586945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.778173446655273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046242259442806244,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07354388236999512,
      "backward_entropy": 0.006052882307105594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.008073806762695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04628973454236984,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07347016930580139,
      "backward_entropy": 0.012736811406082578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.602297782897949,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04633720964193344,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07339605093002319,
      "backward_entropy": 0.006013786213265525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.121532440185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04638076573610306,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07332599759101868,
      "backward_entropy": 0.012719592286480797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.633766174316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04642169550061226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07325856685638428,
      "backward_entropy": 0.005975189722246594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.021035194396973,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04646312817931175,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07319012880325318,
      "backward_entropy": 0.005956008202499813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.979426383972168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0465022511780262,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07312403917312622,
      "backward_entropy": 0.005937679774231381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.29587173461914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04653916880488396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07306029796600341,
      "backward_entropy": 0.005918827321794298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.320354461669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04657692462205887,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07299507856369018,
      "backward_entropy": 0.012665988670455085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.487451553344727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04661363363265991,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07293080091476441,
      "backward_entropy": 0.005881853401660919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.800398826599121,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046652235090732574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07286383509635926,
      "backward_entropy": 0.0058650535841782885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.862730026245117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046688757836818695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07279902696609497,
      "backward_entropy": 0.005848635401990678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.7546443939209,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04672614857554436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07273290157318116,
      "backward_entropy": 0.012630730867385864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.31714630126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0467643141746521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07266522645950317,
      "backward_entropy": 0.005815770890977647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.928847312927246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0468023456633091,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07259742617607116,
      "backward_entropy": 0.005800512929757436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.70918083190918,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04683922976255417,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07253088355064392,
      "backward_entropy": 0.012606167131000094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.094303131103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04687788337469101,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07246155738830566,
      "backward_entropy": 0.0057700615790155195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.43041229248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04691999778151512,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0723872184753418,
      "backward_entropy": 0.012592241168022156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.07325553894043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04696337878704071,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07231073379516602,
      "backward_entropy": 0.0057432883315616185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.759737014770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04700687155127525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0722338616847992,
      "backward_entropy": 0.005729627692037159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.989093780517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047049663960933685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07215734720230102,
      "backward_entropy": 0.0057167841328514945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.69965171813965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04709353670477867,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07207903265953064,
      "backward_entropy": 0.0057035622497399645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.23537540435791,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047137558460235596,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07200009822845459,
      "backward_entropy": 0.01256283786561754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.456520080566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04717903211712837,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07192452549934387,
      "backward_entropy": 0.005679802762137519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.3389892578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04722088947892189,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07184802293777466,
      "backward_entropy": 0.012557032207647959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.34725570678711,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04726303741335869,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07177059650421143,
      "backward_entropy": 0.07679821385277642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.092134475708008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0473080575466156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07168847322463989,
      "backward_entropy": 0.005648017343547609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.973967552185059,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047353047877550125,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0716059923171997,
      "backward_entropy": 0.005637500021192763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.973621368408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04739709571003914,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07152456045150757,
      "backward_entropy": 0.0125421028998163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.558469772338867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0474376417696476,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07144834995269775,
      "backward_entropy": 0.012538618511623807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.684889793395996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0474812313914299,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07136687040328979,
      "backward_entropy": 0.005607801179091136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.483415603637695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04752398282289505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07128636837005616,
      "backward_entropy": 0.005598335216442744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.491793632507324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04756689444184303,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07120513916015625,
      "backward_entropy": 0.005589246335956786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.70173454284668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047609031200408936,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0711248278617859,
      "backward_entropy": 0.005580224924617344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.481563568115234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04764866456389427,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07104825973510742,
      "backward_entropy": 0.005570828914642334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.809854745864868,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047686975449323654,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07097363471984863,
      "backward_entropy": 0.012515074676937528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.925464630126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04772225022315979,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0709040880203247,
      "backward_entropy": 0.012508596811029647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.8247013092041,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04775840789079666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07083237171173096,
      "backward_entropy": 0.005543446375264062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.977771759033203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04779530689120293,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07075906991958618,
      "backward_entropy": 0.07683246665530735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.895188331604004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04783197119832039,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07068581581115722,
      "backward_entropy": 0.005525970624552833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.51555824279785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04786842316389084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07061257362365722,
      "backward_entropy": 0.005517183078659905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.092477798461914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04790554940700531,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07053768634796143,
      "backward_entropy": 0.01247728533214993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.64236831665039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04794415831565857,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07045975923538209,
      "backward_entropy": 0.005499930845366584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.834136962890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047982357442379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07038220167160034,
      "backward_entropy": 0.0054915472865104675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.084259033203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048021890223026276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07030179500579833,
      "backward_entropy": 0.005483102467324998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.56647491455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04806177318096161,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07022032141685486,
      "backward_entropy": 0.012446677519215478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.286672592163086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048102833330631256,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07013643980026245,
      "backward_entropy": 0.01243888172838423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.196239471435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04814319685101509,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07005317807197571,
      "backward_entropy": 0.005459396789471309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.107131958007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048182934522628784,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06997085809707641,
      "backward_entropy": 0.012421786785125732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.010197639465332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04822211340069771,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06988931894302368,
      "backward_entropy": 0.005443972431951099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.937180519104004,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04825907200574875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06981198787689209,
      "backward_entropy": 0.01240388552347819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.855269432067871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048295725136995316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06973491907119751,
      "backward_entropy": 0.005429192549652523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.660778045654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0483320988714695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06965811252593994,
      "backward_entropy": 0.005421995288795895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.689889907836914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048369910567998886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06957787275314331,
      "backward_entropy": 0.005414810859494739,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.810712814331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048407308757305145,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06949809789657593,
      "backward_entropy": 0.0054076653387811445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.89798355102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04844687506556511,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06941349506378174,
      "backward_entropy": 0.005400792178180482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.502033233642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04848668351769447,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.069327974319458,
      "backward_entropy": 0.012346105443106757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.337615013122559,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048528388142585754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06923807263374329,
      "backward_entropy": 0.005387525591585372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.558677673339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04856926202774048,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06914958953857422,
      "backward_entropy": 0.0053810543484157985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.734333038330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04861021786928177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06906044483184814,
      "backward_entropy": 0.005374674167897966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.329858779907227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04865207150578499,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06896895170211792,
      "backward_entropy": 0.005368473215235604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.45716667175293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048693884164094925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06887719631195069,
      "backward_entropy": 0.00536233310898145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.877788543701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04873648285865784,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06878324151039124,
      "backward_entropy": 0.0053563035196728176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.982611656188965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048778120428323746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06869103908538818,
      "backward_entropy": 0.005350304560528861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.521125793457031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04881969839334488,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0685987114906311,
      "backward_entropy": 0.005344424810674455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.6072359085083,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048859573900699615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06850968599319458,
      "backward_entropy": 0.005338586866855621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.520904541015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04889871925115585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06842196583747864,
      "backward_entropy": 0.0053326938715245985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.7629451751709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04893720895051956,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06833539009094239,
      "backward_entropy": 0.005326844751834869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.347888946533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04897754639387131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06824409365653991,
      "backward_entropy": 0.00532112932867474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.391613006591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04901709407567978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0681541919708252,
      "backward_entropy": 0.005315492550532023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.087817192077637,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049057524651288986,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06806186437606812,
      "backward_entropy": 0.01217473049958547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.112038612365723,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049095481634140015,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06797537803649903,
      "backward_entropy": 0.012158253126674227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.004307746887207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04913364723324776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06788808703422547,
      "backward_entropy": 0.00529753209816085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.927599906921387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049169570207595825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.067806077003479,
      "backward_entropy": 0.005291355566846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.77315902709961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04920509085059166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06772461533546448,
      "backward_entropy": 0.005285252713494831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.827852249145508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049241866916418076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06763945817947388,
      "backward_entropy": 0.005279586133029725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.614575386047363,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04927733540534973,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06755727529525757,
      "backward_entropy": 0.012069693870014615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.419099807739258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04931321367621422,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06747368574142457,
      "backward_entropy": 0.005268086575799518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.650264739990234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04935025796294212,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0673866868019104,
      "backward_entropy": 0.012032280365626017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.729413032531738,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04938596487045288,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06730276942253113,
      "backward_entropy": 0.012013525598578982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.225953102111816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049419645220041275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06722394227981568,
      "backward_entropy": 0.005251385271549225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.307256698608398,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04945390671491623,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06714296936988831,
      "backward_entropy": 0.011974997818470001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.041051864624023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049487870186567307,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.067062509059906,
      "backward_entropy": 0.0052409908837742275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.524261474609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04952232912182808,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06698035597801208,
      "backward_entropy": 0.0052357084221310085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.385242462158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049558818340301514,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06689224243164063,
      "backward_entropy": 0.005230627540085051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.996155738830566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04959714040160179,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06679871678352356,
      "backward_entropy": 0.005226024736960729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.37000274658203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049634747207164764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06670662760734558,
      "backward_entropy": 0.011878449883725908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.246604919433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04967327043414116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06661152243614196,
      "backward_entropy": 0.011860664519998763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.74807357788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049712538719177246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06651406288146973,
      "backward_entropy": 0.0052135830952061545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.000946998596191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04975093901157379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06641876697540283,
      "backward_entropy": 0.0052092232637935216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.294956684112549,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04978775233030319,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06632816195487976,
      "backward_entropy": 0.005204486350218455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.885425090789795,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049822453409433365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06624279022216797,
      "backward_entropy": 0.00520043240653144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.438369750976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049855951219797134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06616009473800659,
      "backward_entropy": 0.005196174813641442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.549750328063965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049889206886291504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06607779264450073,
      "backward_entropy": 0.011741060349676345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5789105892181396,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049923673272132874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06599165797233582,
      "backward_entropy": 0.005189214315679338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.55687141418457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04995542764663696,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06591338515281678,
      "backward_entropy": 0.0051856496267848546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.6150617599487305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04999154806137085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06582189798355102,
      "backward_entropy": 0.005182002981503804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.14311981201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050026264041662216,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06573449969291686,
      "backward_entropy": 0.01165929271115197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.494097709655762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05006357282400131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06563893556594849,
      "backward_entropy": 0.005174590481652154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.439210891723633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05010087415575981,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06554269194602966,
      "backward_entropy": 0.011618319484922621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.841841697692871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050136733800172806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0654504656791687,
      "backward_entropy": 0.005168531503942277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.08411979675293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05017196759581566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0653598427772522,
      "backward_entropy": 0.005165737122297287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.848114490509033,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050208963453769684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06526309251785278,
      "backward_entropy": 0.005164118276702033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4105443954467773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0502437949180603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06517263650894164,
      "backward_entropy": 0.005163158393568463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.701622009277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050275903195142746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0650903820991516,
      "backward_entropy": 0.005162516815794839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.743002414703369,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05031002312898636,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0650011956691742,
      "backward_entropy": 0.005162647200955285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.709558486938477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05034223198890686,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06491775512695312,
      "backward_entropy": 0.005163089268737369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.34937286376953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050372712314128876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0648394525051117,
      "backward_entropy": 0.011486186749405332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.913097381591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050405241549015045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06475432515144348,
      "backward_entropy": 0.005164097994565964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.209473609924316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05043889209628105,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06466519832611084,
      "backward_entropy": 0.005164321925905015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.857004165649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050472062081098557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06457740068435669,
      "backward_entropy": 0.005164200647009743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.604504585266113,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05050412565469742,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06449284553527831,
      "backward_entropy": 0.005164513985315959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.755221366882324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050537317991256714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06440420150756836,
      "backward_entropy": 0.00516473005215327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.937243461608887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050569407641887665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0643186867237091,
      "backward_entropy": 0.00516557859049903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.659179210662842,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050601206719875336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06423364877700806,
      "backward_entropy": 0.005166865057415432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.007582664489746,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05063191428780556,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06415239572525025,
      "backward_entropy": 0.01136003765794966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.564599990844727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05066315829753876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06406874656677246,
      "backward_entropy": 0.005168035212490294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1833906173706055,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05069331079721451,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06398956775665283,
      "backward_entropy": 0.07692313194274902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.085180282592773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05072104185819626,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0639189600944519,
      "backward_entropy": 0.00516557859049903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.2875165939331055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05075092241168022,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06383918523788452,
      "backward_entropy": 0.011276019944085015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.626765251159668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05077926814556122,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06376505494117737,
      "backward_entropy": 0.011253153284390768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.446029663085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050808344036340714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06368812918663025,
      "backward_entropy": 0.005160452591048347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.75904655456543,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050837285816669464,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06361124515533448,
      "backward_entropy": 0.011209408442179361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.322062492370605,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05086885392665863,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06352522373199462,
      "backward_entropy": 0.011184889409277175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1978912353515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05090000852942467,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06344056129455566,
      "backward_entropy": 0.005154326972034242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.196113586425781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05093006789684296,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06335967779159546,
      "backward_entropy": 0.005151194830735524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.131759643554688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05095984786748886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06327970623970032,
      "backward_entropy": 0.005147778325610691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.056586742401123,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05098949745297432,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06319948434829711,
      "backward_entropy": 0.005145719481839074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.014627456665039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05101833865046501,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06312154531478882,
      "backward_entropy": 0.005145031130976147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.955240249633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05104636028409004,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06304640769958496,
      "backward_entropy": 0.005144495103094313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.784619331359863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05107440799474716,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06297045350074768,
      "backward_entropy": 0.0051450613472196795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.838403224945068,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05110510438680649,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06288458108901977,
      "backward_entropy": 0.005145299765798781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.775968551635742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05113545432686806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06279984712600709,
      "backward_entropy": 0.005145463678571913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.64144229888916,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05116552487015724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06271567344665527,
      "backward_entropy": 0.0051460882855786216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.382726669311523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051195960491895676,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06262997388839722,
      "backward_entropy": 0.005146453777949016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.482253074645996,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05122809484601021,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06253758072853088,
      "backward_entropy": 0.010910511016845703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.403697967529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051260434091091156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06244397759437561,
      "backward_entropy": 0.005147897120979097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.318818092346191,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05129285156726837,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06234996318817139,
      "backward_entropy": 0.01087013218137953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.083922386169434,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051325470209121704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.062254559993743894,
      "backward_entropy": 0.0051498545540703666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.329381942749023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051358841359615326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06215620636940002,
      "backward_entropy": 0.005151459326346715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.943925857543945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051391538232564926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06205983161926269,
      "backward_entropy": 0.005152946131096946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.77805233001709,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05142822861671448,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061947965621948244,
      "backward_entropy": 0.010795517100228203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.892104148864746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05146521329879761,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061834913492202756,
      "backward_entropy": 0.0051545264820257826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.806106567382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05150185152888298,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06172252893447876,
      "backward_entropy": 0.005156199137369792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.933116912841797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05153803154826164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06161203384399414,
      "backward_entropy": 0.005156991796361076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.188485622406006,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051575757563114166,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0614953875541687,
      "backward_entropy": 0.01071721977657742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7252249717712402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05161147564649582,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061387133598327634,
      "backward_entropy": 0.005155214005046421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.458540916442871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05164426937699318,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061290276050567624,
      "backward_entropy": 0.005153585639264848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3672590255737305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05167689919471741,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061194074153900144,
      "backward_entropy": 0.005151047474808163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.297090530395508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05170745402574539,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061106300354003905,
      "backward_entropy": 0.005147765080134074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.228367805480957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05173826590180397,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06101653575897217,
      "backward_entropy": 0.005146535320414437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.776622772216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05176910385489464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06092643737792969,
      "backward_entropy": 0.0051447492506768965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.4685282707214355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05180065706372261,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.060833019018173215,
      "backward_entropy": 0.010519790152708689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.02633853442966938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05183146521449089,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06074274182319641,
      "backward_entropy": 0.01048944228225284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.769833564758301,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05185914784669876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06066537499427795,
      "backward_entropy": 0.005138338439994388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.732823371887207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051885951310396194,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06059125065803528,
      "backward_entropy": 0.0051364344027307295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5840580463409424,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05191190168261528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060520517826080325,
      "backward_entropy": 0.005134393357568317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.203143119812012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05193579941987991,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06045839786529541,
      "backward_entropy": 0.0051318200098143685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.691185474395752,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051959820091724396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060395312309265134,
      "backward_entropy": 0.005130205303430557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.070629835128784,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0519845224916935,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060328996181488036,
      "backward_entropy": 0.005128948638836543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.086901664733887,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052007902413606644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06026825904846191,
      "backward_entropy": 0.010274589889579348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.021300792694092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05203258618712425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060201555490493774,
      "backward_entropy": 0.005124951402346293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.486699104309082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05205714702606201,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060135507583618165,
      "backward_entropy": 0.005122040294938617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.824456214904785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052081022411584854,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06007225513458252,
      "backward_entropy": 0.005118924710485671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.34037971496582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05210740491747856,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.059997725486755374,
      "backward_entropy": 0.010144535038206313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.374325275421143,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05213407427072525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05992195606231689,
      "backward_entropy": 0.005113715098963844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.337550640106201,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052159927785396576,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059849333763122556,
      "backward_entropy": 0.005111691852410634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.728916645050049,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05218503996729851,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059779423475265506,
      "backward_entropy": 0.005110612346066369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.922489166259766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0522100068628788,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059710025787353516,
      "backward_entropy": 0.005109484824869368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4266812801361084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05223662406206131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0596331536769867,
      "backward_entropy": 0.0051082103616661495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.193930625915527,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052261244505643845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0595646858215332,
      "backward_entropy": 0.005108207050296996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.16169548034668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05228525772690773,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059498369693756104,
      "backward_entropy": 0.005109417355722851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.231949806213379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05230865627527237,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05943474769592285,
      "backward_entropy": 0.005110935618480046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.453088760375977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05233323201537132,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05936539769172668,
      "backward_entropy": 0.005112082593970829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.403961658477783,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052357614040374756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05929690599441528,
      "backward_entropy": 0.005112136817640728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.027315139770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05238199234008789,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059227931499481204,
      "backward_entropy": 0.0051136430766847395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.960090160369873,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0524057000875473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05916174650192261,
      "backward_entropy": 0.005115441150135464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9631147384643555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05243062600493431,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05908947587013245,
      "backward_entropy": 0.005118568738301595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.529207229614258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052454691380262375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059021347761154176,
      "backward_entropy": 0.005120247188541625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.046456336975098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05247917026281357,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058951270580291745,
      "backward_entropy": 0.005121041089296341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8568146228790283,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05250534787774086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05887283086776733,
      "backward_entropy": 0.005123535792032878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.354221820831299,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05253070220351219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0587978184223175,
      "backward_entropy": 0.005127008590433333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7848520278930664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052556395530700684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058720874786376956,
      "backward_entropy": 0.005130595631069607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.237392902374268,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05258142203092575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05864628553390503,
      "backward_entropy": 0.005136584242184957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7202069759368896,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052606917917728424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05856884717941284,
      "backward_entropy": 0.00514381296104855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.907886028289795,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052631624042987823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058494722843170165,
      "backward_entropy": 0.005151375300354428,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.2812724113464355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052656158804893494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05842111110687256,
      "backward_entropy": 0.005158993519014782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.230928897857666,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05268172174692154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05834227204322815,
      "backward_entropy": 0.00516749628716045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.593223810195923,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05270514264702797,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0582741379737854,
      "backward_entropy": 0.0769382052951389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5632882118225098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052727844566106796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05820920467376709,
      "backward_entropy": 0.005178403523233201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.539900779724121,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05274989455938339,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058147269487380984,
      "backward_entropy": 0.005182377994060516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.667460918426514,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05277115851640701,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058089935779571535,
      "backward_entropy": 0.005182947135633892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.62578821182251,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052792344242334366,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058033323287963866,
      "backward_entropy": 0.005181531939241622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.864100933074951,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0528135821223259,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057976162433624266,
      "backward_entropy": 0.005180097702476714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.548993110656738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05283605679869652,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0579124927520752,
      "backward_entropy": 0.005179985115925471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.269273519515991,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05285841226577759,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05784925222396851,
      "backward_entropy": 0.0051793596810764736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.252277135848999,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052879586815834045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057791602611541745,
      "backward_entropy": 0.005178663465711806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.435583114624023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05289963632822037,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05773946046829224,
      "backward_entropy": 0.005177072766754363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1202062368392944,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052919819951057434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.057686352729797365,
      "backward_entropy": 0.00935096542040507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.283017635345459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05293857306241989,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057639789581298825,
      "backward_entropy": 0.005175263103511598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1037030220031738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05295702442526817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05759455561637879,
      "backward_entropy": 0.005174763914611604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.306819438934326,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05297422036528587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057554996013641356,
      "backward_entropy": 0.0051753587192959255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.271640777587891,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05299172177910805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05751439332962036,
      "backward_entropy": 0.005174780057536231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.510382652282715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05300968512892723,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.057470643520355226,
      "backward_entropy": 0.009217527177598741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.2056803703308105,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05303061753511429,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05741174221038818,
      "backward_entropy": 0.005176292525397407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0987608432769775,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.053051598370075226,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05735267996788025,
      "backward_entropy": 0.07693321175045437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1116368770599365,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05307155102491379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05729842185974121,
      "backward_entropy": 0.005179279380374485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0787949562072754,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05309096723794937,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05724700689315796,
      "backward_entropy": 0.07693302631378174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0349628925323486,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053110234439373016,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.057195305824279785,
      "backward_entropy": 0.009103893405861326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.0402421951293945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05312825366854668,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057149237394332884,
      "backward_entropy": 0.005188611232572132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.019278049468994,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05314651131629944,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05710182785987854,
      "backward_entropy": 0.005193355596727795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.000920295715332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053163930773735046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05705866813659668,
      "backward_entropy": 0.009053488572438559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.892762660980225,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0531807504594326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05701802968978882,
      "backward_entropy": 0.005202529331048329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.811357021331787,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053199540823698044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05696720480918884,
      "backward_entropy": 0.009022422962718539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9934312701225281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05322059616446495,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.056904882192611694,
      "backward_entropy": 0.07693840397728814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9403831958770752,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053239986300468445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056851142644882204,
      "backward_entropy": 0.005217887875106599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.824401617050171,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05325843393802643,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05680200457572937,
      "backward_entropy": 0.07693956295649211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.848374843597412,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05327695980668068,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05675272941589356,
      "backward_entropy": 0.07693965567482842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8935120105743408,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053295210003852844,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056704354286193845,
      "backward_entropy": 0.005226516475280126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7303671836853027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05331265181303024,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05665993690490723,
      "backward_entropy": 0.005229312512609694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7003049850463867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05333040654659271,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05661355257034302,
      "backward_entropy": 0.005232597804731793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.494212627410889,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053348470479249954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056565070152282716,
      "backward_entropy": 0.005236739499701394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.542215347290039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05336777865886688,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05650996565818787,
      "backward_entropy": 0.005241272764073478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.02349918521940708,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05338774248957634,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05645108222961426,
      "backward_entropy": 0.0052467456294430625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.580047369003296,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053405825048685074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056401395797729494,
      "backward_entropy": 0.005253524416022831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.431929588317871,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053424056619405746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.056350702047348024,
      "backward_entropy": 0.008819907903671265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.268511772155762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05344289168715477,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05629711151123047,
      "backward_entropy": 0.005264265255795585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7536152601242065,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053462713956832886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056238627433776854,
      "backward_entropy": 0.005267534818914201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5973284244537354,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05348164960741997,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.056184124946594236,
      "backward_entropy": 0.00877104616827435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.277312278747559,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0535002276301384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05613120794296265,
      "backward_entropy": 0.005277682509687211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7109955549240112,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053519394248723984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05607467293739319,
      "backward_entropy": 0.005282803542084164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.368168592453003,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053537629544734955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05602295398712158,
      "backward_entropy": 0.005287870350811217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.338179111480713,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053555965423583984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0559706449508667,
      "backward_entropy": 0.005292479776673847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.491560220718384,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053574416786432266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05591751337051391,
      "backward_entropy": 0.005297088788615333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6589069366455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053592365235090256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05586725473403931,
      "backward_entropy": 0.0053000979953342015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2535219192504883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053609296679496765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055822896957397464,
      "backward_entropy": 0.005300545444091161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4265077114105225,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05362645909190178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0557769775390625,
      "backward_entropy": 0.005301379909118016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4054958820343018,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05364333093166351,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05573259592056275,
      "backward_entropy": 0.005302321165800095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.534584999084473,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053659968078136444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05568884611129761,
      "backward_entropy": 0.0053037549886438586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9256112575531006,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05367825925350189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05563588142395019,
      "backward_entropy": 0.005305210335387124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5719661712646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053697049617767334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05558043718338013,
      "backward_entropy": 0.005306388768884871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5588756799697876,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053714849054813385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055530256032943724,
      "backward_entropy": 0.005306969500250286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.061490058898926,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05373172461986542,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.055485081672668454,
      "backward_entropy": 0.008506240944067637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.034822702407837,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05374876409769058,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055438876152038574,
      "backward_entropy": 0.005306340754032135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0063178539276123,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05376594513654709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05539171695709229,
      "backward_entropy": 0.0053062281674808925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2435576915740967,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05378330871462822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05534309148788452,
      "backward_entropy": 0.005307150383790334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.221315622329712,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05380026251077652,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055296725034713744,
      "backward_entropy": 0.005307443026039336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.479862093925476,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05381693318486214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055251479148864746,
      "backward_entropy": 0.005308508459064696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4650837182998657,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05383274704217911,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05521109104156494,
      "backward_entropy": 0.005308257208930122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8822062015533447,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05384788289666176,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05517417788505554,
      "backward_entropy": 0.008349111510647668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1507062911987305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0538632869720459,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.055135679244995114,
      "backward_entropy": 0.00832599401473999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.244385242462158,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053878460079431534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05509843230247498,
      "backward_entropy": 0.005307200054327647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.112149238586426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05389461666345596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05505646467208862,
      "backward_entropy": 0.005303858882851071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4047240018844604,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05391055345535278,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055015116930007935,
      "backward_entropy": 0.005301801280842887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.076961040496826,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05392579734325409,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05497739315032959,
      "backward_entropy": 0.0053002362449963885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.095906734466553,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05394087731838226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05494005680084228,
      "backward_entropy": 0.005299427443080478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.4060282707214355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05395713821053505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05489618182182312,
      "backward_entropy": 0.005299244489934709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.353435754776001,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05397525429725647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05484238862991333,
      "backward_entropy": 0.005298511021667057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.316955804824829,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05399373918771744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05478633642196655,
      "backward_entropy": 0.0052976662086115945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.284214973449707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054012641310691833,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05472766160964966,
      "backward_entropy": 0.008109678824742636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9570902585983276,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05403178557753563,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05466766953468323,
      "backward_entropy": 0.005298136009110345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.489931106567383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05405038595199585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05461032390594482,
      "backward_entropy": 0.005299279673231972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2882615327835083,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054070111364126205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05454684495925903,
      "backward_entropy": 0.008048630423016019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.14182448387146,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054088592529296875,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054490625858306885,
      "backward_entropy": 0.005298709703816308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.728616237640381,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05410744249820709,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05443187952041626,
      "backward_entropy": 0.07693667544258966,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 3.7197407221235337,
    "avg_log_Z": -0.05319565642625093,
    "success_rate": 1.0,
    "avg_reward": 74.1,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.07,
      "1": 0.12,
      "2": 0.81
    },
    "avg_forward_entropy": 0.056965083479881294,
    "avg_backward_entropy": 0.010657621485491596,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}