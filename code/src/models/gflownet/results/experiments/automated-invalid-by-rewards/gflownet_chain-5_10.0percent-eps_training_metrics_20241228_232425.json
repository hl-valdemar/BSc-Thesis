{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 0.45454545454545453
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 0.45454545454545453
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1384374737739563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.13812487125396727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.58676147460938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18269010384877524,
      "backward_entropy": 0.1381657600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.5598907470703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18270405133565268,
      "backward_entropy": 0.1384389281272888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.1064453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00020000015501864254,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18271774053573608,
      "backward_entropy": 0.13818709850311278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.50657653808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00030022600549273193,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18273097276687622,
      "backward_entropy": 0.13813294172286988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.9127655029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0003999866312369704,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18274370829264322,
      "backward_entropy": 0.1381352424621582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.8867950439453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0004993847687728703,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18275614579518637,
      "backward_entropy": 0.13813722133636475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.28829956054688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005985616007819772,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18276830514272055,
      "backward_entropy": 0.13844587802886962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.3959503173828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0006984727224335074,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1827803055445353,
      "backward_entropy": 0.13844741582870485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.37721252441406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0007990424637682736,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.182792067527771,
      "backward_entropy": 0.13824176788330078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.90748596191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0008990881615318358,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18280359109242758,
      "backward_entropy": 0.13825032711029053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.8490447998047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0009998533641919494,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18281495571136475,
      "backward_entropy": 0.1382588505744934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.41897583007812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0011021726531907916,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18282626072565714,
      "backward_entropy": 0.13826743364334107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.93882751464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.001204619649797678,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18283732732137045,
      "backward_entropy": 0.13845467567443848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.22348022460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013080146163702011,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18284821510314941,
      "backward_entropy": 0.1384563684463501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.6767120361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0014110897900536656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18285886446634927,
      "backward_entropy": 0.13815534114837646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.76341247558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0015111195389181376,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18286913633346558,
      "backward_entropy": 0.13815767765045167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.10635375976562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001610276522114873,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287914991378784,
      "backward_entropy": 0.13830794095993043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.93594360351562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0017108508618548512,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18288898468017578,
      "backward_entropy": 0.1383155107498169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.1471710205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018124297494068742,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18289842208226523,
      "backward_entropy": 0.13846333026885987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.9006805419922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0019159787334501743,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18290776014328003,
      "backward_entropy": 0.138464617729187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.98171997070312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0020195620600134134,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18291683991750082,
      "backward_entropy": 0.138338041305542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.33639526367188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002123755170032382,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18292572100957236,
      "backward_entropy": 0.13817096948623658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.49009704589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0022292982321232557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293424447377524,
      "backward_entropy": 0.13817334175109863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.2196044921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0023351821582764387,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18294256925582886,
      "backward_entropy": 0.13836004734039306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.3252716064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0024405443109571934,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1829506754875183,
      "backward_entropy": 0.13847099542617797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.91415405273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0025480506010353565,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295864264170328,
      "backward_entropy": 0.1384723663330078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.1360321044922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002656562253832817,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18296645085016885,
      "backward_entropy": 0.13838133811950684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.90711975097656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0027640315238386393,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18297400077184042,
      "backward_entropy": 0.1383882522583008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.9387969970703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0028714940417557955,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1829813321431478,
      "backward_entropy": 0.13847615718841552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.21060180664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002977802651003003,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18298824628194174,
      "backward_entropy": 0.13819016218185426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.74220275878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0030851890332996845,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18299496173858643,
      "backward_entropy": 0.13847849369049073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.21389770507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0031911630649119616,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18300145864486694,
      "backward_entropy": 0.1381939172744751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.6075439453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0032944739796221256,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18300771713256836,
      "backward_entropy": 0.13848010301589966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.801025390625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0034021777100861073,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18301395575205484,
      "backward_entropy": 0.13842642307281494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.78629302978516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0035084758419543505,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18301995595296225,
      "backward_entropy": 0.13848164081573486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.6646728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0036106326151639223,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1830256183942159,
      "backward_entropy": 0.13819785118103028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.71749877929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00371324154548347,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18303112188975015,
      "backward_entropy": 0.13848237991333007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.7755889892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0038150157779455185,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18303638696670532,
      "backward_entropy": 0.13848263025283813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.71755981445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003914728760719299,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1830414136250814,
      "backward_entropy": 0.1384826898574829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.062744140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0040154121816158295,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1830462614695231,
      "backward_entropy": 0.13848273754119872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.2373504638672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00411536218598485,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18305091063181558,
      "backward_entropy": 0.138460910320282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.9585723876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004219975788146257,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18305546045303345,
      "backward_entropy": 0.13819876909255982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.16746520996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004325929097831249,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1830599308013916,
      "backward_entropy": 0.13848304748535156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.72406005859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004432078450918198,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18306410312652588,
      "backward_entropy": 0.1381995677947998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.49301147460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004534129984676838,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1830679178237915,
      "backward_entropy": 0.13819963932037355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.85417938232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004635245073586702,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18307149410247803,
      "backward_entropy": 0.1381995439529419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.91199493408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004727853462100029,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1830747922261556,
      "backward_entropy": 0.1384821891784668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.89322662353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004816465079784393,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18307781219482422,
      "backward_entropy": 0.138197660446167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.55095672607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0049014766700565815,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18308061361312866,
      "backward_entropy": 0.13848066329956055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.50686645507812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004980808589607477,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18308315674463907,
      "backward_entropy": 0.1384796380996704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.97792053222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005062003154307604,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18308556079864502,
      "backward_entropy": 0.13847875595092773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.0550994873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005148689262568951,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18308774630228677,
      "backward_entropy": 0.13847823143005372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.26046752929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005238137673586607,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18308979272842407,
      "backward_entropy": 0.13847782611846923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.96324157714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005326386541128159,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309166034062704,
      "backward_entropy": 0.13850576877593995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.17271423339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005414784420281649,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18309334913889566,
      "backward_entropy": 0.13847703933715821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.59989929199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005497555714100599,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309477965037027,
      "backward_entropy": 0.1381821632385254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.1529541015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00557700265198946,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309601147969565,
      "backward_entropy": 0.13851261138916016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.43580627441406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005654511973261833,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309706449508667,
      "backward_entropy": 0.1385144591331482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.2716522216797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005733538419008255,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18309797843297324,
      "backward_entropy": 0.13847315311431885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.3854217529297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005812810268253088,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309871355692545,
      "backward_entropy": 0.13851802349090575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.65167236328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00589345907792449,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1830992897351583,
      "backward_entropy": 0.13847146034240723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.0585479736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005974086467176676,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309970696767172,
      "backward_entropy": 0.1381589412689209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.17198181152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0060563827864825726,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18309994538625082,
      "backward_entropy": 0.13846957683563232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.03118133544922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006132894195616245,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1831000049908956,
      "backward_entropy": 0.13852425813674926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.83200073242188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006204906851053238,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309990564982095,
      "backward_entropy": 0.13814332485198974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.53890991210938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006281120702624321,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309966723124185,
      "backward_entropy": 0.1385265588760376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.63865661621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006357844918966293,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309923013051352,
      "backward_entropy": 0.138132905960083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.07118225097656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006434354465454817,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309865395228067,
      "backward_entropy": 0.13852866888046264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.5582275390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006511526647955179,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309793869654337,
      "backward_entropy": 0.13812222480773925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.8634490966797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006595639046281576,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309704462687174,
      "backward_entropy": 0.13853116035461427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.08340454101562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006681170780211687,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18309597174326578,
      "backward_entropy": 0.13853251934051514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.70970153808594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006771092303097248,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1830947200457255,
      "backward_entropy": 0.13853399753570556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.9735107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0068606603890657425,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309330940246582,
      "backward_entropy": 0.1381026029586792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.60655212402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006947638932615519,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18309171994527182,
      "backward_entropy": 0.1380974292755127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.75552368164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007035491988062859,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18308991193771362,
      "backward_entropy": 0.13809190988540648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.9765625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007122560404241085,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18308794498443604,
      "backward_entropy": 0.13853905200958253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.82052612304688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007210387382656336,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18308575948079428,
      "backward_entropy": 0.1385401725769043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.07774353027344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007296756841242313,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18308337529500326,
      "backward_entropy": 0.13854126930236815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.7022247314453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007384259253740311,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18308071295420328,
      "backward_entropy": 0.13854222297668456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.44430541992188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007480188272893429,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18307777245839438,
      "backward_entropy": 0.13854334354400635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.53116607666016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007576028350740671,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1830745736757914,
      "backward_entropy": 0.13854440450668334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.31248474121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007669042330235243,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.183071235815684,
      "backward_entropy": 0.13804659843444825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.54397583007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007770079653710127,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18306758006413779,
      "backward_entropy": 0.13804047107696532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.6246795654297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00787349697202444,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18306368589401245,
      "backward_entropy": 0.13843104839324952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.007568359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007980876602232456,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18305951356887817,
      "backward_entropy": 0.13802835941314698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.79844665527344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00809159129858017,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18305504322052002,
      "backward_entropy": 0.1385505199432373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.23463439941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008202320896089077,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18305031458536783,
      "backward_entropy": 0.13842592239379883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.36219787597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008311216719448566,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18304534753163657,
      "backward_entropy": 0.13842400312423705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.7518768310547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008418722078204155,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18304014205932617,
      "backward_entropy": 0.1385543704032898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.65185546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008524852804839611,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18303469816843668,
      "backward_entropy": 0.13799834251403809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.3511505126953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008634178899228573,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18302891651789346,
      "backward_entropy": 0.1385565996170044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.49703979492188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008746643550693989,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1830227772394816,
      "backward_entropy": 0.13855782747268677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.17440795898438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008856883272528648,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18301637967427573,
      "backward_entropy": 0.13855892419815063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.61143493652344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008968264795839787,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18300968408584595,
      "backward_entropy": 0.13855986595153807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.2134246826172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009084871038794518,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18300247192382812,
      "backward_entropy": 0.1385608434677124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.56272888183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009204600006341934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1829948623975118,
      "backward_entropy": 0.13795847892761232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.98587036132812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009325950406491756,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18298697471618652,
      "backward_entropy": 0.13840391635894775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.81642150878906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009442387148737907,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18297886848449707,
      "backward_entropy": 0.13856394290924073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.49238586425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009562186896800995,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18297028541564941,
      "backward_entropy": 0.13839919567108155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.8929443359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00967620499432087,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18296164274215698,
      "backward_entropy": 0.1383964776992798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.2676239013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009790676645934582,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18295284112294516,
      "backward_entropy": 0.13792273998260499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.06784057617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009904380887746811,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18294374148050943,
      "backward_entropy": 0.13839114904403688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.09912109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010017146356403828,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1829343636830648,
      "backward_entropy": 0.13790665864944457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 259.6924743652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010131939314305782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1829245686531067,
      "backward_entropy": 0.13838553428649902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.9123992919922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010252241045236588,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18291417757670084,
      "backward_entropy": 0.1385695219039917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.23245239257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010375121608376503,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18290332953135172,
      "backward_entropy": 0.13788254261016847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.81504821777344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010497399605810642,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18289212385813394,
      "backward_entropy": 0.13857139348983766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.1822967529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010621948167681694,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18288048108418783,
      "backward_entropy": 0.1378665804862976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.2976837158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010748343542218208,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1828683614730835,
      "backward_entropy": 0.13785871267318725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.0380859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010872871614992619,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.182856023311615,
      "backward_entropy": 0.13837001323699952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.4991455078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010993637144565582,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18284348646799722,
      "backward_entropy": 0.13836687803268433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.22926330566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011118165217339993,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18283039331436157,
      "backward_entropy": 0.13836393356323243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.51119995117188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011244063265621662,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.182816743850708,
      "backward_entropy": 0.13782625198364257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.06415557861328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011366080492734909,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18280293544133505,
      "backward_entropy": 0.13857707977294922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.15187072753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011481783352792263,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18278912703196207,
      "backward_entropy": 0.13780651092529297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.5722885131836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011593113653361797,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18277527888615927,
      "backward_entropy": 0.13779525756835936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.5135498046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011694184504449368,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18276168902715048,
      "backward_entropy": 0.13834404945373535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.68435668945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011797063983976841,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18274768193562826,
      "backward_entropy": 0.13776943683624268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.4017791748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011899334378540516,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18273321787516275,
      "backward_entropy": 0.1383334517478943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.1752471923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012001419439911842,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1827183961868286,
      "backward_entropy": 0.13774116039276124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.64169311523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012107758782804012,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18270285924275717,
      "backward_entropy": 0.13832244873046876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.57077026367188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012212053872644901,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18268712361653647,
      "backward_entropy": 0.13771321773529052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.15489196777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012319229543209076,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18267075220743814,
      "backward_entropy": 0.13769998550415039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.10490417480469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012424376793205738,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18265414237976074,
      "backward_entropy": 0.138305401802063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.57337951660156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012523363344371319,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18263769149780273,
      "backward_entropy": 0.13857417106628417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.6217498779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01261802390217781,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18262116114298502,
      "backward_entropy": 0.13829197883605956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.7469482421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012710136361420155,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18260439236958823,
      "backward_entropy": 0.1385727882385254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.39662170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012804505415260792,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1825869878133138,
      "backward_entropy": 0.13827731609344482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.2483367919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012899967841804028,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18256906668345133,
      "backward_entropy": 0.13826994895935057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.24873352050781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01299744751304388,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18255003293355307,
      "backward_entropy": 0.13758503198623656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.4524383544922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013090495951473713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18253080050150552,
      "backward_entropy": 0.13825455904006959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.23648071289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01318116020411253,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1825112501780192,
      "backward_entropy": 0.1382461428642273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.9669952392578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013273321092128754,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18249112367630005,
      "backward_entropy": 0.13823773860931396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.34011840820312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013365272432565689,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1824705203374227,
      "backward_entropy": 0.1385669231414795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.4903564453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013456917367875576,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18244953950246176,
      "backward_entropy": 0.13748641014099122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.34182739257812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013546585105359554,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18242812156677246,
      "backward_entropy": 0.13856500387191772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.9742202758789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013638816773891449,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1824058691660563,
      "backward_entropy": 0.1385641098022461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.38333129882812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013727789744734764,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1823835770289103,
      "backward_entropy": 0.13856310844421388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.1897430419922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013815067708492279,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1823609471321106,
      "backward_entropy": 0.13856196403503418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.72189331054688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013910169713199139,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18233782052993774,
      "backward_entropy": 0.13856117725372313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.10061645507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014006068930029869,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18231417735417685,
      "backward_entropy": 0.13735928535461425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.2669219970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014106965623795986,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1822894016901652,
      "backward_entropy": 0.13733854293823242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.39559936523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014207547530531883,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18226399024327597,
      "backward_entropy": 0.1373163342475891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.78543853759766,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01431246753782034,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.182237446308136,
      "backward_entropy": 0.13855860233306885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.87135314941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014412187971174717,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.182210902372996,
      "backward_entropy": 0.1381199598312378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.85275268554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014516432769596577,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18218322594960532,
      "backward_entropy": 0.13810961246490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.166015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014621606096625328,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18215477466583252,
      "backward_entropy": 0.1385566473007202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.6904754638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01472932193428278,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18212544918060303,
      "backward_entropy": 0.1380887508392334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.28515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014840624295175076,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18209501107533774,
      "backward_entropy": 0.13717628717422486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.12840270996094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01495217252522707,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18206389745076498,
      "backward_entropy": 0.13855557441711425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.23362731933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015058551914989948,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1820330818494161,
      "backward_entropy": 0.13855504989624023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.49325561523438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015171344392001629,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1820006767908732,
      "backward_entropy": 0.13855490684509278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.29026794433594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015285284258425236,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18196729818979898,
      "backward_entropy": 0.13855469226837158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.6429901123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015398100018501282,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1819335619608561,
      "backward_entropy": 0.13802506923675537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.7565460205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01550978422164917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18189942836761475,
      "backward_entropy": 0.1370295286178589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.26614379882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015621352009475231,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18186448017756143,
      "backward_entropy": 0.13700296878814697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.7507781982422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01573851890861988,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18182788292566934,
      "backward_entropy": 0.13697752952575684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.4317626953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015852436423301697,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18179112672805786,
      "backward_entropy": 0.13797963857650758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.10464477539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015965044498443604,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1817539930343628,
      "backward_entropy": 0.1379675030708313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.8313751220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016078058630228043,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18171622355779013,
      "backward_entropy": 0.1379552364349365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.1920166015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016195034608244896,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18167686462402344,
      "backward_entropy": 0.13855326175689697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.24606323242188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016311567276716232,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18163686990737915,
      "backward_entropy": 0.13855333328247071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.62757873535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016429129987955093,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1815959413846334,
      "backward_entropy": 0.13791885375976562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.37366485595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01654018461704254,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18155535062154135,
      "backward_entropy": 0.13790516853332518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.85568237304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016642259433865547,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18151569366455078,
      "backward_entropy": 0.13675763607025146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 286.2716369628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016749301925301552,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1814741293589274,
      "backward_entropy": 0.13787469863891602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.11717224121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016865044832229614,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18143010139465332,
      "backward_entropy": 0.13786154985427856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.8770294189453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01698143035173416,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18138500054677328,
      "backward_entropy": 0.13784830570220946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.13430786132812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017096059396862984,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18133914470672607,
      "backward_entropy": 0.13663283586502076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.37289428710938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017207525670528412,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18129277229309082,
      "backward_entropy": 0.13781976699829102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.24197387695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017320819199085236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18124447266260782,
      "backward_entropy": 0.13656275272369384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.77511596679688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01743541657924652,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18119498093922934,
      "backward_entropy": 0.13652703762054444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.289794921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01754716783761978,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18114540974299112,
      "backward_entropy": 0.13854894638061524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.1585693359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017660032957792282,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1810945471127828,
      "backward_entropy": 0.13775877952575682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.9532470703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01777244731783867,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18104257186253866,
      "backward_entropy": 0.13641843795776368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.7378387451172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017886212095618248,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18098944425582886,
      "backward_entropy": 0.1363825798034668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.4283905029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017996840178966522,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1809361775716146,
      "backward_entropy": 0.13771003484725952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.48570251464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01810293272137642,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18088293075561523,
      "backward_entropy": 0.13854751586914063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.20181274414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018209369853138924,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18082855145136514,
      "backward_entropy": 0.13767207860946656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.1879425048828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018314994871616364,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18077357610066733,
      "backward_entropy": 0.13854577541351318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.31814575195312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018423642963171005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18071659406026205,
      "backward_entropy": 0.13617308139801027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.810302734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01853518933057785,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18065794308980307,
      "backward_entropy": 0.1376159071922302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.52920532226562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01864657737314701,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18059837818145752,
      "backward_entropy": 0.13759709596633912,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.0629425048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01875378005206585,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18053913116455078,
      "backward_entropy": 0.13757674694061278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.02847290039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018862590193748474,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18047835429509482,
      "backward_entropy": 0.13599808216094972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.7511444091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018967581912875175,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1804179549217224,
      "backward_entropy": 0.13595116138458252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.87509155273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019070392474532127,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.180357297261556,
      "backward_entropy": 0.1385411024093628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.98876953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019169412553310394,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18029659986495972,
      "backward_entropy": 0.13748652935028077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.9394989013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019269680604338646,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1802345315615336,
      "backward_entropy": 0.13746166229248047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.98098754882812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01936960592865944,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1801715890566508,
      "backward_entropy": 0.13743623495101928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.5191650390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019466841593384743,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18010892470677695,
      "backward_entropy": 0.1374096155166626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.7012939453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01956673339009285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18004419406255087,
      "backward_entropy": 0.135636043548584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.40402221679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019667774438858032,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1799785296122233,
      "backward_entropy": 0.13735653162002565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.6537628173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019769618287682533,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17991232872009277,
      "backward_entropy": 0.13552217483520507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.09800720214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019869476556777954,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1798457702000936,
      "backward_entropy": 0.1354610800743103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.3034210205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019966527819633484,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1797795295715332,
      "backward_entropy": 0.13726729154586792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.85446166992188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020061857998371124,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1797125538190206,
      "backward_entropy": 0.13533444404602052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.09207153320312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020155759528279305,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1796449621518453,
      "backward_entropy": 0.1371945858001709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.9613494873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020247116684913635,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17957733074824014,
      "backward_entropy": 0.137153697013855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.5840606689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02034030295908451,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17950781186421713,
      "backward_entropy": 0.13512612581253053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.07493591308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02043377049267292,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1794370412826538,
      "backward_entropy": 0.13706905841827394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.97115325927734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020524393767118454,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17936607201894125,
      "backward_entropy": 0.13702367544174193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.71194458007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020609181374311447,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17929579814275107,
      "backward_entropy": 0.13489155769348143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.8983917236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02069762721657753,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1792227029800415,
      "backward_entropy": 0.13480558395385742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.4563751220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020783741027116776,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.179149329662323,
      "backward_entropy": 0.13471574783325196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.40248107910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020867880433797836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17907567818959555,
      "backward_entropy": 0.13681952953338622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.43629455566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02095348760485649,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1790001392364502,
      "backward_entropy": 0.13453210592269899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.53053283691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021034477278590202,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17892436186472574,
      "backward_entropy": 0.134437096118927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.96844482421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021115420386195183,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1788469155629476,
      "backward_entropy": 0.13848934173583985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.29359436035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021193519234657288,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17876931031545004,
      "backward_entropy": 0.13848459720611572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.61653137207031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02127012424170971,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17869057257970175,
      "backward_entropy": 0.1384793758392334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.70987701416016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02134409174323082,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1786115566889445,
      "backward_entropy": 0.13645429611206056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.61427307128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02141414023935795,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17853285868962607,
      "backward_entropy": 0.1363837242126465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.36620330810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021482812240719795,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17845404148101807,
      "backward_entropy": 0.1337653875350952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.38587951660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021542388945817947,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17837893962860107,
      "backward_entropy": 0.13363273143768312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.6233673095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021596811711788177,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17830546696980795,
      "backward_entropy": 0.1361476421356201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 234.9303741455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021654067561030388,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17822865645090738,
      "backward_entropy": 0.13335599899291992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.28499603271484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021720796823501587,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17814234892527261,
      "backward_entropy": 0.13598214387893676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.85336303710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0217847041785717,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17805582284927368,
      "backward_entropy": 0.13308312892913818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.2051544189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021852152422070503,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17796562115351358,
      "backward_entropy": 0.1329483985900879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.21287536621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021924423053860664,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1778713862101237,
      "backward_entropy": 0.13840749263763427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.7679901123047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021995972841978073,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17777552207310995,
      "backward_entropy": 0.1326815366744995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.52267456054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022066999226808548,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17767820755640665,
      "backward_entropy": 0.13253965377807617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.5501708984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022140873596072197,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17757793267567953,
      "backward_entropy": 0.13838465213775636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.7181396484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02221696265041828,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1774746576944987,
      "backward_entropy": 0.1383778214454651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.97300720214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022295281291007996,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17736870050430298,
      "backward_entropy": 0.13211960792541505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.01242065429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022373728454113007,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17726077636082968,
      "backward_entropy": 0.13520877361297606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.17784118652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022453712299466133,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17714997132619223,
      "backward_entropy": 0.13511970043182372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.11846923828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02253531664609909,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17703664302825928,
      "backward_entropy": 0.13835444450378417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.554931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022613996639847755,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1769237518310547,
      "backward_entropy": 0.131514573097229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.87743377685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02269713021814823,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17680623133977255,
      "backward_entropy": 0.13484616279602052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.70079040527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02277575246989727,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17669043938318887,
      "backward_entropy": 0.13119269609451295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.1486358642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022854739800095558,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1765729784965515,
      "backward_entropy": 0.13465288877487183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.7213897705078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022934529930353165,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17645418643951416,
      "backward_entropy": 0.13832442760467528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.7266845703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023013388738036156,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1763349970181783,
      "backward_entropy": 0.13069427013397217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.91986083984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023096011951565742,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17621235052744547,
      "backward_entropy": 0.13053433895111083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 231.00076293945312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023177308961749077,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1760894258817037,
      "backward_entropy": 0.13425499200820923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 231.58364868164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023266157135367393,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.175959845383962,
      "backward_entropy": 0.13021652698516845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.679931640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02336149848997593,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1758241057395935,
      "backward_entropy": 0.13407468795776367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.8846435546875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023455629125237465,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17568777004877725,
      "backward_entropy": 0.13830838203430176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.11988830566406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023551687598228455,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17554875214894614,
      "backward_entropy": 0.13830819129943847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 246.25827026367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0236479751765728,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17540812492370605,
      "backward_entropy": 0.13379522562026977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.6992950439453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02375148795545101,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1752607226371765,
      "backward_entropy": 0.1383095383644104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.4146728515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023852486163377762,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17511296272277832,
      "backward_entropy": 0.13361308574676514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.58059692382812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02395329438149929,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17496412992477417,
      "backward_entropy": 0.1335143566131592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.55638122558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024056561291217804,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17481174071629843,
      "backward_entropy": 0.1334131956100464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.8272247314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024160323664546013,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17465686798095703,
      "backward_entropy": 0.1333092212677002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.49073791503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024261992424726486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17450213432312012,
      "backward_entropy": 0.12856345176696776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.0381317138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02436121366918087,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17434694369633993,
      "backward_entropy": 0.12836570739746095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 230.0431365966797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0244572926312685,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17419294516245523,
      "backward_entropy": 0.1281569480895996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.73129272460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024558909237384796,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1740326484044393,
      "backward_entropy": 0.1279508113861084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.07962036132812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024658530950546265,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17387243111928305,
      "backward_entropy": 0.1277385473251343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.5702667236328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024754852056503296,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17371330658594766,
      "backward_entropy": 0.13829206228256224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.98587036132812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024848662316799164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17355545361836752,
      "backward_entropy": 0.13241162300109863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.82969665527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024941731244325638,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1733973821004232,
      "backward_entropy": 0.12706358432769777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.91758728027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02503485232591629,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17323696613311768,
      "backward_entropy": 0.1321085810661316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.18333435058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025125907734036446,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17307788133621216,
      "backward_entropy": 0.13194942474365234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.02700424194336,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025217678397893906,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17291617393493652,
      "backward_entropy": 0.13825809955596924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.00491333007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025299569591879845,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17276239395141602,
      "backward_entropy": 0.13161096572875977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.7083740234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025380074977874756,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17260825634002686,
      "backward_entropy": 0.1314268469810486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.49374389648438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025460755452513695,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1724521517753601,
      "backward_entropy": 0.13822102546691895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.6986846923828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02554462105035782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1722914179166158,
      "backward_entropy": 0.1310550093650818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.02870178222656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02562692016363144,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1721306045850118,
      "backward_entropy": 0.13819829225540162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.7436981201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025713296607136726,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17196369171142578,
      "backward_entropy": 0.1306696653366089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.28407287597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02580239251255989,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17179294427235922,
      "backward_entropy": 0.1244422435760498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.22607421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025888103991746902,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17162438233693442,
      "backward_entropy": 0.1241611123085022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.01863098144531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02597765438258648,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1714499592781067,
      "backward_entropy": 0.12387983798980713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.1968231201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02606247551739216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17127970854441324,
      "backward_entropy": 0.12987711429595947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.6226348876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026149850338697433,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17110512653986612,
      "backward_entropy": 0.123289954662323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.16246032714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02623707242310047,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17092937231063843,
      "backward_entropy": 0.12945387363433838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.7794952392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026324274018406868,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1707531213760376,
      "backward_entropy": 0.12269196510314942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.94473266601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026418553665280342,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1705686648686727,
      "backward_entropy": 0.12241129875183106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.40956115722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026513967663049698,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17037999629974365,
      "backward_entropy": 0.12882118225097655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.38286590576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026615232229232788,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.170183797677358,
      "backward_entropy": 0.1286210298538208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.11695861816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026710614562034607,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16999328136444092,
      "backward_entropy": 0.12839939594268798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.09774780273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026807773858308792,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1697996457417806,
      "backward_entropy": 0.12122316360473633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.6248321533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02690238319337368,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16960767904917398,
      "backward_entropy": 0.12091381549835205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.55287170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027003226801753044,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16940629482269287,
      "backward_entropy": 0.12773189544677735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.8740692138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027106186375021935,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16920006275177002,
      "backward_entropy": 0.1203351616859436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.27772521972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02721424028277397,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16898677746454874,
      "backward_entropy": 0.12731202840805053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.774169921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027322394773364067,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16877126693725586,
      "backward_entropy": 0.1271024227142334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.79795837402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027422014623880386,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1685640017191569,
      "backward_entropy": 0.12686214447021485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.3804168701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02751230262219906,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16836525996526083,
      "backward_entropy": 0.11905605792999267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.37974548339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027601873502135277,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16816582282384238,
      "backward_entropy": 0.11868464946746826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.42692565917969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027689196169376373,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16796747843424478,
      "backward_entropy": 0.12603068351745605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.44871520996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027770401909947395,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16777571042378744,
      "backward_entropy": 0.12572406530380248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.11917114257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02785010077059269,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16758362452189127,
      "backward_entropy": 0.12540888786315918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.86984252929688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02792973630130291,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16738911469777426,
      "backward_entropy": 0.13804687261581422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.77818298339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028008149936795235,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16719460487365723,
      "backward_entropy": 0.11662203073501587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.965087890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02808580920100212,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16700029373168945,
      "backward_entropy": 0.11618525981903076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.09917449951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028166962787508965,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16680039962132773,
      "backward_entropy": 0.11575530767440796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.85643005371094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028245698660612106,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16660377383232117,
      "backward_entropy": 0.12375630140304565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.69802856445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028326230123639107,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16640194257100424,
      "backward_entropy": 0.1234208106994629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.08230590820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028408382087945938,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16619712114334106,
      "backward_entropy": 0.11450202465057373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.74478149414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02848910354077816,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16599287589391074,
      "backward_entropy": 0.12274880409240722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.81089782714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028570495545864105,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1657875974973043,
      "backward_entropy": 0.11365145444869995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.69965362548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028650548309087753,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1655825674533844,
      "backward_entropy": 0.12205264568328858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.0009765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02872551418840885,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16538439194361368,
      "backward_entropy": 0.11276403665542603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.01737976074219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028806010261178017,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16517850756645203,
      "backward_entropy": 0.12132428884506226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.65540313720703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028883541002869606,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16497443119684854,
      "backward_entropy": 0.12095246315002442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.7910919189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02895733341574669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16477473576863608,
      "backward_entropy": 0.11140486001968383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.74219512939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029033711180090904,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16457001368204752,
      "backward_entropy": 0.11093826293945312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.92161560058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029106371104717255,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16436946392059326,
      "backward_entropy": 0.11045066118240357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.5977783203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029187457635998726,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16415661573410034,
      "backward_entropy": 0.10999218225479127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.39875030517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029264703392982483,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1639487942059835,
      "backward_entropy": 0.1095192551612854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.1969451904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029338164255023003,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16374504566192627,
      "backward_entropy": 0.10902413129806518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.7745361328125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029415655881166458,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1635348598162333,
      "backward_entropy": 0.1377251386642456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.23780822753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029490984976291656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1633269190788269,
      "backward_entropy": 0.1080438733100891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.29766845703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029568569734692574,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1631144881248474,
      "backward_entropy": 0.11734635829925537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.26747131347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029645182192325592,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1629019876321157,
      "backward_entropy": 0.11692333221435547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.65415954589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029725968837738037,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16268416245778403,
      "backward_entropy": 0.10655426979064941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.38386535644531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029804518446326256,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.162470410267512,
      "backward_entropy": 0.11609251499176025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.33892822265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029879657551646233,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16226098934809366,
      "backward_entropy": 0.13762803077697755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.78740692138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02995498664677143,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1620513399442037,
      "backward_entropy": 0.11521462202072144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.40377044677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030030276626348495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16184250513712564,
      "backward_entropy": 0.10459176301956177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.20631408691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030104879289865494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16163299481074014,
      "backward_entropy": 0.10409101247787475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.06041717529297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030186332762241364,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16141261657079062,
      "backward_entropy": 0.11390001773834228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.10367965698242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030261827632784843,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1611988047758738,
      "backward_entropy": 0.10309298038482666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.6429214477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030329788103699684,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.160996675491333,
      "backward_entropy": 0.11295461654663086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.78609466552734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03039826638996601,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16079323490460715,
      "backward_entropy": 0.11245957612991334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.476318359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030467111617326736,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16058868169784546,
      "backward_entropy": 0.10141243934631347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.50041198730469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03053315170109272,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16038723786671957,
      "backward_entropy": 0.11145049333572388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.18246459960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030597010627388954,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16018899281819662,
      "backward_entropy": 0.10023307800292969,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.95592498779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030664978548884392,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1599848965803782,
      "backward_entropy": 0.11041353940963745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.79480743408203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030729930847883224,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15978332360585532,
      "backward_entropy": 0.09905942678451538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.23448181152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030789781361818314,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15958942969640097,
      "backward_entropy": 0.0984333336353302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.11157989501953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.030853966251015663,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15938870112101236,
      "backward_entropy": 0.13730549812316895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.99354553222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030917519703507423,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15918864806493124,
      "backward_entropy": 0.09721275568008422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.70663452148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03098462149500847,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15898142258326212,
      "backward_entropy": 0.09660733938217163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.30531883239746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03105631098151207,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15876622994740805,
      "backward_entropy": 0.09601451754570008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.40962219238281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031117327511310577,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15856693188349405,
      "backward_entropy": 0.10664882659912109,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.95848083496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031176766380667686,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15836993853251138,
      "backward_entropy": 0.10607807636260987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.81462860107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031239794567227364,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15816943844159445,
      "backward_entropy": 0.10552682876586914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.88978576660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031302157789468765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15796929597854614,
      "backward_entropy": 0.093470698595047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.55693054199219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031366001814603806,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15776779254277548,
      "backward_entropy": 0.10441250801086426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.7901611328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03142509236931801,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15757413705190024,
      "backward_entropy": 0.10382918119430543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.8037872314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03148111328482628,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1573846141497294,
      "backward_entropy": 0.10322693586349488,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.31555938720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03154350072145462,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15718541542689005,
      "backward_entropy": 0.09093186855316163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.92455291748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031605686992406845,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15698665380477905,
      "backward_entropy": 0.10210137367248535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.33162689208984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031664762645959854,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15679305791854858,
      "backward_entropy": 0.08965309858322143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.0338897705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031720828264951706,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15660464763641357,
      "backward_entropy": 0.08898293972015381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.26822662353516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031783316284418106,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1564053495724996,
      "backward_entropy": 0.10033793449401855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.35551452636719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03184543177485466,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15620575348536173,
      "backward_entropy": 0.09976719617843628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.4481658935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03190755471587181,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1560073991616567,
      "backward_entropy": 0.0991921067237854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.73804473876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03197508677840233,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1557997465133667,
      "backward_entropy": 0.08647592663764954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.61309814453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032043080776929855,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15559062361717224,
      "backward_entropy": 0.08584799766540527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.62884521484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03210770711302757,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15538830558458963,
      "backward_entropy": 0.09752088785171509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.86687469482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03217051550745964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15518917640050253,
      "backward_entropy": 0.08456089496612548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.2812957763672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032229840755462646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15499464670817056,
      "backward_entropy": 0.0838827908039093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.59437561035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03229375556111336,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.154793381690979,
      "backward_entropy": 0.13669734001159667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.89691925048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032362017780542374,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15458636482556662,
      "backward_entropy": 0.09517550468444824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.87218475341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03242945671081543,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15438024202982584,
      "backward_entropy": 0.08197329044342042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.39872741699219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032493237406015396,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1541796326637268,
      "backward_entropy": 0.09402450323104858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.14049530029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03255840763449669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1539780100186666,
      "backward_entropy": 0.08067553043365479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.59567260742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0326215885579586,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15377938747406006,
      "backward_entropy": 0.0928417682647705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.08678436279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03268926218152046,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15357518196105957,
      "backward_entropy": 0.0922658085823059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.46846771240234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032754622399806976,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1533737579981486,
      "backward_entropy": 0.07871781587600708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.22940063476562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03281649947166443,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15317783753077188,
      "backward_entropy": 0.09104348421096801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.06314086914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03288132697343826,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15297770500183105,
      "backward_entropy": 0.09043487310409545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.61297607421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032951079308986664,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15276851256688437,
      "backward_entropy": 0.07669473886489868,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.079158782958984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033017177134752274,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15256611506144205,
      "backward_entropy": 0.08925068378448486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.96451568603516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03307858109474182,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15237224102020264,
      "backward_entropy": 0.13649444580078124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.85796356201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03313850238919258,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15218075116475424,
      "backward_entropy": 0.08798234462738037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.56346130371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03319820016622543,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15198881427447,
      "backward_entropy": 0.13643178939819336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.52436065673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03325715661048889,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15179995695749918,
      "backward_entropy": 0.07320694327354431,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.9424819946289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03331493213772774,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15161331494649252,
      "backward_entropy": 0.08605135679244995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.71490478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033371880650520325,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1514288286368052,
      "backward_entropy": 0.07179369926452636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.58770751953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03343232721090317,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15123925606409708,
      "backward_entropy": 0.08477469682693481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.83619689941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03349485248327255,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1510480841000875,
      "backward_entropy": 0.08416236639022827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.31660461425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033559054136276245,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1508552630742391,
      "backward_entropy": 0.08356085419654846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.18535614013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03362033888697624,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1506680448849996,
      "backward_entropy": 0.06911656260490417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.44483947753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033679015934467316,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15048566460609436,
      "backward_entropy": 0.08230259418487548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.9161148071289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03373720124363899,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15030578772226968,
      "backward_entropy": 0.06777656674385071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.7399444580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03379767760634422,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15012356638908386,
      "backward_entropy": 0.08104675412178039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.53245544433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03386121615767479,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14993615945180258,
      "backward_entropy": 0.06648698449134827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.81910705566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03392752259969711,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14974440137545267,
      "backward_entropy": 0.07986228466033936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.57913970947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033994160592556,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14955471952756247,
      "backward_entropy": 0.07928204536437988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.27503967285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03406209871172905,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1493634581565857,
      "backward_entropy": 0.07870310544967651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.3083724975586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03412981703877449,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14917384584744772,
      "backward_entropy": 0.06397932767868042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.3326416015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034194331616163254,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.148989737033844,
      "backward_entropy": 0.0633380115032196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.29383850097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03425534814596176,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1488093932469686,
      "backward_entropy": 0.06265985369682311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.7439422607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03432001173496246,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14862475792566934,
      "backward_entropy": 0.07626447677612305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.01273345947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03438955917954445,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14843565225601196,
      "backward_entropy": 0.07570117712020874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.42192840576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03445762023329735,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14825069904327393,
      "backward_entropy": 0.060813868045806886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.07787322998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0345226414501667,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14807130893071493,
      "backward_entropy": 0.07453421354293824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.37410736083984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034586455672979355,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.147894948720932,
      "backward_entropy": 0.05957927703857422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.55709075927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03464951738715172,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14772247274716696,
      "backward_entropy": 0.05896878242492676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.90400695800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034709785133600235,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14755315581957498,
      "backward_entropy": 0.07271567583084107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.0023422241211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03477058187127113,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14738216996192932,
      "backward_entropy": 0.07210973501205445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.78182220458984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03483036532998085,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14721360802650452,
      "backward_entropy": 0.05707244277000427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.2118148803711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034887950867414474,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14704889059066772,
      "backward_entropy": 0.056426900625228885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.0616683959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03494458273053169,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14688495794932047,
      "backward_entropy": 0.055766725540161134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.3584976196289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03500359505414963,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14671794573465982,
      "backward_entropy": 0.05511922240257263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.51531219482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03506636992096901,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14654609560966492,
      "backward_entropy": 0.054497772455215455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.456668853759766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03512975573539734,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14637491106987,
      "backward_entropy": 0.05388760566711426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.383697509765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0351891852915287,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14621078968048096,
      "backward_entropy": 0.06786568760871887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.560977935791016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03524314612150192,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1460540990034739,
      "backward_entropy": 0.06723253726959229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.2760009765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0352945402264595,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14590283234914145,
      "backward_entropy": 0.05194917321205139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.94627380371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035353049635887146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14574084679285684,
      "backward_entropy": 0.05132592916488647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.17371368408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035419829189777374,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14556931455930075,
      "backward_entropy": 0.06546232700347901,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.93711471557617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035488322377204895,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1453970472017924,
      "backward_entropy": 0.0649359405040741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.90766143798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035552993416786194,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14523329337437949,
      "backward_entropy": 0.06438429355621338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.00847244262695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035616543143987656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14507164557774863,
      "backward_entropy": 0.0490324079990387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.591407775878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03567660227417946,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14491673310597739,
      "backward_entropy": 0.04845456480979919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.0890655517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03573374077677727,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1447682579358419,
      "backward_entropy": 0.06265700459480286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.96940612792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0357966311275959,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14461304744084677,
      "backward_entropy": 0.04734136462211609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.5429573059082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03585886210203171,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14446000258127847,
      "backward_entropy": 0.04680030047893524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.34198760986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03591782599687576,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14431305726369223,
      "backward_entropy": 0.046253892779350284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.30597686767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035977862775325775,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1441653569539388,
      "backward_entropy": 0.045709085464477536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.64390563964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03603878617286682,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1440164049466451,
      "backward_entropy": 0.04516679346561432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.66637420654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03609982877969742,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1438698967297872,
      "backward_entropy": 0.04464604258537293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.37628173828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036158107221126556,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14372978607813516,
      "backward_entropy": 0.04413149356842041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.01774597167969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036213330924510956,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14359410603841147,
      "backward_entropy": 0.05824577212333679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.76197814941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03626871854066849,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1434584061304728,
      "backward_entropy": 0.04308077096939087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.45013427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03632606565952301,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14332187175750732,
      "backward_entropy": 0.04257493019104004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.78117370605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03638360649347305,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14318613211313883,
      "backward_entropy": 0.05661833882331848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.42416763305664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03644005209207535,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14305329322814941,
      "backward_entropy": 0.056087386608123777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.58573913574219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03649234399199486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1429266333580017,
      "backward_entropy": 0.041071963310241696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.15250396728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03654828295111656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14279655615488687,
      "backward_entropy": 0.04058560132980347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.7147216796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03660771623253822,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14266337951024374,
      "backward_entropy": 0.04012750685214996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.52458953857422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036672960966825485,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14252410332361856,
      "backward_entropy": 0.05409272909164429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.65619659423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0367409773170948,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14238412181536356,
      "backward_entropy": 0.03928896188735962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.8275089263916,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03680964931845665,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14224441846211752,
      "backward_entropy": 0.038881736993789676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.48460388183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03687150403857231,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14211362600326538,
      "backward_entropy": 0.052793610095977786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.92639923095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03693376109004021,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1419849991798401,
      "backward_entropy": 0.05234435200691223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.39458084106445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03699363023042679,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14186187585194907,
      "backward_entropy": 0.03761162459850311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.3986358642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037050891667604446,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1417426864306132,
      "backward_entropy": 0.051434326171875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.46573638916016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03711400926113129,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1416163444519043,
      "backward_entropy": 0.036795240640640256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.90379333496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03717700392007828,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14149120450019836,
      "backward_entropy": 0.03639419078826904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.35523986816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03724384307861328,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14136163393656412,
      "backward_entropy": 0.035999351739883424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.95938873291016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03731296956539154,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1412300169467926,
      "backward_entropy": 0.13595421314239503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.98368072509766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03738289326429367,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1410991350809733,
      "backward_entropy": 0.04936972856521606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.93000793457031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037453364580869675,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14096850156784058,
      "backward_entropy": 0.04898466467857361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.50540924072266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037525616586208344,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14083647727966309,
      "backward_entropy": 0.04860915243625641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.76578903198242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03759966790676117,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14070377747217813,
      "backward_entropy": 0.048243510723114016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.69956970214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037668123841285706,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1405781110127767,
      "backward_entropy": 0.03372668027877808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.11393737792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037741776555776596,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1404485801855723,
      "backward_entropy": 0.03335841298103333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.70208740234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03781558573246002,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1403197447458903,
      "backward_entropy": 0.032986956834793094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.24315643310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03788718953728676,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14019559820493063,
      "backward_entropy": 0.046736717224121094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.25068664550781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0379549004137516,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1400760809580485,
      "backward_entropy": 0.046346423029899594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.85205078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0380236878991127,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13995659351348877,
      "backward_entropy": 0.0318780243396759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.33282470703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.038092464208602905,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.139839768409729,
      "backward_entropy": 0.1363227605819702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.6635627746582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03816061094403267,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13972437381744385,
      "backward_entropy": 0.03118388056755066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.15546417236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038225386291742325,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13961301247278848,
      "backward_entropy": 0.044870099425315856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.9093246459961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.038294222205877304,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13949796557426453,
      "backward_entropy": 0.13639136552810668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.94349670410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03836563602089882,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13938221335411072,
      "backward_entropy": 0.030149960517883302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.39290237426758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038434866815805435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1392695109049479,
      "backward_entropy": 0.029814043641090394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.77710723876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03849942237138748,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13916263977686563,
      "backward_entropy": 0.043464171886444095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.04389190673828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0385655015707016,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13905568917592367,
      "backward_entropy": 0.029133626818656923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.45897674560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03862743824720383,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1389542818069458,
      "backward_entropy": 0.04275224208831787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.94498825073242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03869416564702988,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13884918888409933,
      "backward_entropy": 0.028487658500671385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.70250701904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03875663876533508,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13874985774358115,
      "backward_entropy": 0.028172174096107484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.27533721923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03881547600030899,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.138655553261439,
      "backward_entropy": 0.02785613536834717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.38201904296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03887893632054329,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13855666915575662,
      "backward_entropy": 0.027545350790023803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.9264144897461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038939714431762695,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1384607752164205,
      "backward_entropy": 0.041066744923591615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.09576416015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039002083241939545,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1383632024129232,
      "backward_entropy": 0.026905557513236998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.00946044921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03906472772359848,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13826610644658408,
      "backward_entropy": 0.040382829308509824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.96470642089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03912755101919174,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13816910982131958,
      "backward_entropy": 0.02626810371875763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.541500091552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03919233754277229,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13807167609532675,
      "backward_entropy": 0.02596547603607178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.1986312866211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03925476595759392,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13797802726427713,
      "backward_entropy": 0.039408785104751584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.392940521240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039317887276411057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13788535197575888,
      "backward_entropy": 0.03910652101039887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.04384231567383,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03937877342104912,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13779574632644653,
      "backward_entropy": 0.038804903626441956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.75477600097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039438508450984955,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13770670692125955,
      "backward_entropy": 0.038495239615440366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.46540069580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03949925675988197,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13761824369430542,
      "backward_entropy": 0.03819797933101654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.15291213989258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03955933824181557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13753111163775125,
      "backward_entropy": 0.02424927055835724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.05357360839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03961731493473053,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1374461849530538,
      "backward_entropy": 0.023967279493808745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.57484436035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03968072682619095,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1373579998811086,
      "backward_entropy": 0.03732799887657166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.11625289916992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0397477000951767,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13726828495661417,
      "backward_entropy": 0.02346122860908508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.19335174560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03981320559978485,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13718015948931375,
      "backward_entropy": 0.03681970536708832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.05778503417969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03987875208258629,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13709184527397156,
      "backward_entropy": 0.03655999004840851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.373111724853516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03994593769311905,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13700315356254578,
      "backward_entropy": 0.022708986699581147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.02950096130371,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040010593831539154,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13691763083140054,
      "backward_entropy": 0.036051779985427856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.36922454833984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04007020965218544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13683714469273886,
      "backward_entropy": 0.022208169102668762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.003028869628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04013235121965408,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1367557942867279,
      "backward_entropy": 0.03553706109523773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.02641677856445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040192604064941406,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13667685786883035,
      "backward_entropy": 0.035289925336837766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.96244812011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040252435952425,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13659894466400146,
      "backward_entropy": 0.021495795249938963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.74279022216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04031465947628021,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13651984930038452,
      "backward_entropy": 0.02127070724964142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.801387786865234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040379129350185394,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13643957177797952,
      "backward_entropy": 0.03459866940975189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.621456146240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040438462048769,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13636316855748495,
      "backward_entropy": 0.0343677431344986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.474876403808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040496036410331726,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13628832499186197,
      "backward_entropy": 0.034134122729301455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.06266784667969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04055212438106537,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13621501127878824,
      "backward_entropy": 0.03389990925788879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.0285415649414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04061238840222359,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1361391246318817,
      "backward_entropy": 0.0336796760559082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.40913391113281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040673766285181046,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13606293002764383,
      "backward_entropy": 0.03347131609916687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.26165008544922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04074044153094292,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13598408301671347,
      "backward_entropy": 0.033288431167602536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.1377182006836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040811724960803986,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13590278228123984,
      "backward_entropy": 0.019577783346176148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.15099334716797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04088692367076874,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13582001129786173,
      "backward_entropy": 0.01939949095249176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.15971374511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040957339107990265,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13574079672495523,
      "backward_entropy": 0.019208940863609313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.31481170654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04102891683578491,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1356611947218577,
      "backward_entropy": 0.019015060365200044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.50252914428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04109913855791092,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13558353980382284,
      "backward_entropy": 0.018825033307075502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.00045394897461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041166678071022034,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1355084478855133,
      "backward_entropy": 0.018633818626403807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.96669387817383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0412333682179451,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13543490568796793,
      "backward_entropy": 0.018446791172027587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.79669952392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041299086064100266,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1353625456492106,
      "backward_entropy": 0.018258626759052276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.87279510498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041367966681718826,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1352884570757548,
      "backward_entropy": 0.018075203895568846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.61542510986328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04143853858113289,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13521415988604227,
      "backward_entropy": 0.031526029109954834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.81038284301758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04151078313589096,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13513994216918945,
      "backward_entropy": 0.01773059070110321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.35078430175781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041580237448215485,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13506810863812765,
      "backward_entropy": 0.031217628717422487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.57373809814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04165409877896309,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13499440749486288,
      "backward_entropy": 0.017400115728378296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.93496322631836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0417250357568264,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13492316007614136,
      "backward_entropy": 0.017237339913845063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.17984771728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04179473593831062,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13485336303710938,
      "backward_entropy": 0.017075061798095703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.741310119628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04186739772558212,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13478237390518188,
      "backward_entropy": 0.016917297244071962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.91936492919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04193604737520218,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13471460342407227,
      "backward_entropy": 0.016759179532527924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.0627670288086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04200729727745056,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.134645144144694,
      "backward_entropy": 0.03034287691116333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.1679573059082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04208020865917206,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13457574446996054,
      "backward_entropy": 0.01644517183303833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.41813659667969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042151883244514465,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.134507954120636,
      "backward_entropy": 0.016297273337841034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.54020309448242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042223621159791946,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1344406803448995,
      "backward_entropy": 0.016150301694869994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.11128997802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04229301959276199,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13437588016192117,
      "backward_entropy": 0.029811084270477295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.870256423950195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04236830398440361,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13430883487065634,
      "backward_entropy": 0.01587377190589905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.5866928100586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04243802651762962,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.134245495001475,
      "backward_entropy": 0.02957673966884613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.32780456542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042508505284786224,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13418289025624594,
      "backward_entropy": 0.015611593425273896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.24973678588867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0425846204161644,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13411781191825867,
      "backward_entropy": 0.029370981454849242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.81334686279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0426589734852314,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13405394554138184,
      "backward_entropy": 0.015363983809947968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.7945556640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04272908344864845,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13399261236190796,
      "backward_entropy": 0.029149967432022094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.72970199584961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04279688000679016,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13393298784891763,
      "backward_entropy": 0.02903563976287842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.59574508666992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04286409541964531,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13387431701024374,
      "backward_entropy": 0.014978978037834167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.53235626220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04293079674243927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1338165601094564,
      "backward_entropy": 0.014859005808830261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.39816665649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04299679398536682,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1337593992551168,
      "backward_entropy": 0.014737953245639802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.2801284790039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04306216537952423,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1337027351061503,
      "backward_entropy": 0.014616307616233826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.15010070800781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04312821477651596,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1336458126703898,
      "backward_entropy": 0.014491963386535644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.84181213378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043194763362407684,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13358857234319052,
      "backward_entropy": 0.01436411440372467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.87261962890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043263424187898636,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.133530855178833,
      "backward_entropy": 0.01424039900302887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.84885025024414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04333018884062767,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13347469766934714,
      "backward_entropy": 0.014123150706291198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.829410552978516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043394893407821655,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13341973225275675,
      "backward_entropy": 0.014004927873611451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.13031005859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04345664381980896,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13336668411890665,
      "backward_entropy": 0.027971071004867554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.04241943359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04352090135216713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13331262270609537,
      "backward_entropy": 0.027872332930564882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.161258697509766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043579623103141785,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13326168060302734,
      "backward_entropy": 0.013660238683223724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.96107482910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043638717383146286,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13321093718210855,
      "backward_entropy": 0.027682173252105712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.34236907958984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04370426759123802,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13315707445144653,
      "backward_entropy": 0.01343916803598404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.756805419921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0437723733484745,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1331027944882711,
      "backward_entropy": 0.01333482414484024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.630855560302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043839991092681885,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1330492893854777,
      "backward_entropy": 0.01323482096195221,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.1520004272461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04390712454915047,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1329964598019918,
      "backward_entropy": 0.027355128526687623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.69807815551758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04397515952587128,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13294344147046408,
      "backward_entropy": 0.013042786717414856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.023189544677734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044041506946086884,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13289223114649454,
      "backward_entropy": 0.02722766399383545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.711219787597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04410478472709656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1328424314657847,
      "backward_entropy": 0.012860210239887237,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.87953758239746,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04416915029287338,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13279223442077637,
      "backward_entropy": 0.02710043787956238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.775999069213867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04423055052757263,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1327434778213501,
      "backward_entropy": 0.027031850814819337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.22442626953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04428945481777191,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13269609212875366,
      "backward_entropy": 0.012582516670227051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.61138153076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04434734955430031,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13264928261439005,
      "backward_entropy": 0.012491412460803986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.48970413208008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044405680149793625,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13260241349538168,
      "backward_entropy": 0.01240125447511673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.367122650146484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04446440935134888,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13255556424458823,
      "backward_entropy": 0.02677209973335266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.664791107177734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04452350735664368,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13250873486200967,
      "backward_entropy": 0.1381819248199463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.708473205566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04458421841263771,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13246134916941324,
      "backward_entropy": 0.012137454748153687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.230623245239258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044643938541412354,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1324146787325541,
      "backward_entropy": 0.012054982781410217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.55308532714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04470154643058777,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13236929972966513,
      "backward_entropy": 0.1382163643836975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.449650764465332,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04476235806941986,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1323227882385254,
      "backward_entropy": 0.1382291078567505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.60921859741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04481825232505798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13227863113085428,
      "backward_entropy": 0.011829598993062972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.77061080932617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04487503319978714,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13223435481389365,
      "backward_entropy": 0.026435986161231995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.64945602416992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04493388533592224,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.132189412911733,
      "backward_entropy": 0.01169167533516884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.95982360839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04499436542391777,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13214385509490967,
      "backward_entropy": 0.011622683703899383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.91862869262695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04505884647369385,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13209667801856995,
      "backward_entropy": 0.011551789939403534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.481103897094727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04512200132012367,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13205032547314963,
      "backward_entropy": 0.02629745304584503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.56064796447754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045181144028902054,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13200571139653525,
      "backward_entropy": 0.011412262916564941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.75775909423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045238230377435684,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13196221987406412,
      "backward_entropy": 0.011344797164201736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.1742935180664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045295923948287964,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13191852966944376,
      "backward_entropy": 0.026186531782150267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.509429931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045360468327999115,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13187207778294882,
      "backward_entropy": 0.011211325228214265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.36302947998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04542479291558266,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13182584444681802,
      "backward_entropy": 0.026098769903182984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.25811004638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045489098876714706,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13177982966105142,
      "backward_entropy": 0.01107628047466278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.105743408203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04555464908480644,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13173346718152365,
      "backward_entropy": 0.026017123460769655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.99666213989258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045617278665304184,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13168847560882568,
      "backward_entropy": 0.010943646728992461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.8301887512207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04567863792181015,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1316441297531128,
      "backward_entropy": 0.010876300185918808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.62689971923828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04574037343263626,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13159982363382974,
      "backward_entropy": 0.0258915513753891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.4767951965332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04580371081829071,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13155500094095865,
      "backward_entropy": 0.01075432226061821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.435768127441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045868322253227234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.131509800752004,
      "backward_entropy": 0.025827813148498534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.153900146484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045932989567518234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1314647893110911,
      "backward_entropy": 0.02580324113368988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.8099136352539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04599880427122116,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13141938050587973,
      "backward_entropy": 0.010581597685813904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.030853271484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04606686159968376,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1313732067743937,
      "backward_entropy": 0.010523713380098342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.646514892578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04613439738750458,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13132737080256143,
      "backward_entropy": 0.010465111583471298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.007781982421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04620290920138359,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13128129641215006,
      "backward_entropy": 0.010408521443605424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.89902877807617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046269722282886505,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1312360167503357,
      "backward_entropy": 0.02565775215625763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.79019546508789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04633485525846481,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13119155168533325,
      "backward_entropy": 0.010299714654684067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.33527755737305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046398408710956573,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13114781181017557,
      "backward_entropy": 0.010243858397006988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.197593688964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0464617982506752,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13110427061716715,
      "backward_entropy": 0.02556280493736267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.66240692138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046525225043296814,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13106074929237366,
      "backward_entropy": 0.025528335571289064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.49795913696289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04659000784158707,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13101678093274435,
      "backward_entropy": 0.010078541934490204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.24026107788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046656012535095215,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13097238540649414,
      "backward_entropy": 0.010026482492685318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.64832305908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046720486134290695,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13092870513598123,
      "backward_entropy": 0.009974947571754456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.49329376220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046784862875938416,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13088510433832803,
      "backward_entropy": 0.009924116730690002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.83272933959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046851638704538345,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13084065914154053,
      "backward_entropy": 0.009872454404830932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.39745330810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04691953584551811,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13079577684402466,
      "backward_entropy": 0.009824354946613312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.8752212524414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046993356198072433,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13074851036071777,
      "backward_entropy": 0.00977681577205658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.02357482910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04706864804029465,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13070102532704672,
      "backward_entropy": 0.025310719013214113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.084232330322266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04714641720056534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1306525468826294,
      "backward_entropy": 0.009677815437316894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.88617706298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047224052250385284,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13060426712036133,
      "backward_entropy": 0.009628218412399293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.690635681152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04730150103569031,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13055618604024252,
      "backward_entropy": 0.00957934558391571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.01158905029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047378845512866974,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13050824403762817,
      "backward_entropy": 0.025193145871162413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.881771087646484,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.047453634440898895,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13046139478683472,
      "backward_entropy": 0.13852205276489257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.93965530395508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0475260354578495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13041559855143228,
      "backward_entropy": 0.009444788843393326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.487730026245117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047597598284482956,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13037015000979105,
      "backward_entropy": 0.025181853771209718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.522254943847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04766589775681496,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13032609224319458,
      "backward_entropy": 0.009364244341850281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.316486358642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04773256927728653,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13028263052304587,
      "backward_entropy": 0.009327767044305801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.36150360107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047796547412872314,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1302402118841807,
      "backward_entropy": 0.009294435381889343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.21930694580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0478605218231678,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1301977535088857,
      "backward_entropy": 0.009261872619390488,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.098411560058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047924358397722244,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13015548388163248,
      "backward_entropy": 0.00922764241695404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.98921012878418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047990430146455765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13011245926221213,
      "backward_entropy": 0.009189372509717941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.7578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0480523519217968,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13007145126660666,
      "backward_entropy": 0.009151309728622437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.74456024169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04811553284525871,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13003001610438028,
      "backward_entropy": 0.009114258736371995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.137451171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0481775626540184,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12998896837234497,
      "backward_entropy": 0.009083038568496704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.38837432861328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048244599252939224,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12994573513666788,
      "backward_entropy": 0.009049824625253677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.086490631103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0483112633228302,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12990264097849527,
      "backward_entropy": 0.009017585217952729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.276329040527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048378828912973404,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12985912958780924,
      "backward_entropy": 0.008986441045999527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.5210075378418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048444632440805435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12981651226679483,
      "backward_entropy": 0.008954951167106628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.049861907958984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048512622714042664,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12977289160092673,
      "backward_entropy": 0.008923405408859253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.65654754638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04857897013425827,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1297298769156138,
      "backward_entropy": 0.025185465812683105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.908809661865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048645004630088806,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12968693176905313,
      "backward_entropy": 0.008867111057043076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.025272369384766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04871322959661484,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1296428640683492,
      "backward_entropy": 0.008839922398328781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.21681594848633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048778485506772995,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12960014740626016,
      "backward_entropy": 0.008814385533332825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.68581771850586,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04884350299835205,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12955756982167563,
      "backward_entropy": 0.13857536315917968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.08919906616211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04890955239534378,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12951440612475076,
      "backward_entropy": 0.008765070885419845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.32779312133789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04897768422961235,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12947035829226175,
      "backward_entropy": 0.02519238889217377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.670406341552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049046482890844345,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12942599256833395,
      "backward_entropy": 0.008715389668941498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.469444274902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04911714792251587,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1293805738290151,
      "backward_entropy": 0.008690843731164933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.15111541748047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04918703809380531,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1293354630470276,
      "backward_entropy": 0.008667320013046265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.012950669042766094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04926104471087456,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12928829590479532,
      "backward_entropy": 0.00864262953400612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.813030242919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0493277832865715,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12924488385518393,
      "backward_entropy": 0.008620719611644744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.22768783569336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049390509724617004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1292032798131307,
      "backward_entropy": 0.008601948618888855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.065555572509766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04945437237620354,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1291611889998118,
      "backward_entropy": 0.025207221508026123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.95516586303711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0495193712413311,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1291182041168213,
      "backward_entropy": 0.02521619200706482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.583568572998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04958175867795944,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12907626231511435,
      "backward_entropy": 0.02523185610771179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.792654037475586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049640532582998276,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12903616825739542,
      "backward_entropy": 0.008534006029367446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.87529754638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049697309732437134,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1289968192577362,
      "backward_entropy": 0.008522412180900574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.24454116821289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04975821450352669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12895538409550986,
      "backward_entropy": 0.008508463948965072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.369152069091797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04982040822505951,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12891329328219095,
      "backward_entropy": 0.02530725598335266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.316287994384766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04987900331616402,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12887308994928995,
      "backward_entropy": 0.008480297774076462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.52027893066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049934353679418564,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12883448600769043,
      "backward_entropy": 0.008469322323799133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.52176284790039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04998918995261192,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1287960410118103,
      "backward_entropy": 0.025368744134902955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.70853424072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05004473403096199,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12875714898109436,
      "backward_entropy": 0.008448249101638794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.311946868896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050105731934309006,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12871513764063516,
      "backward_entropy": 0.008434199541807175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.097496032714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05016805976629257,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12867209315299988,
      "backward_entropy": 0.025410956144332884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.97957229614258,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050229161977767944,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12862963477770487,
      "backward_entropy": 0.02542397379875183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.909122467041016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050290338695049286,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12858704725901285,
      "backward_entropy": 0.025436383485794068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.889252662658691,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05034920573234558,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12854566176732382,
      "backward_entropy": 0.0083840511739254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.493560791015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05040479078888893,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12850610415140787,
      "backward_entropy": 0.025472947955131532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.562076568603516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05046216398477554,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12846543391545615,
      "backward_entropy": 0.13861043453216554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.04352569580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050518788397312164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1284250815709432,
      "backward_entropy": 0.02550655007362366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.184391021728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05057826638221741,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1283829708894094,
      "backward_entropy": 0.008345680683851242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.625249862670898,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05063796415925026,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1283405820528666,
      "backward_entropy": 0.025529932975769044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.352962493896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0506943017244339,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12830003102620444,
      "backward_entropy": 0.008327006548643111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.282093048095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05074876919388771,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12826059261957803,
      "backward_entropy": 0.02556595504283905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.475079536437988,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050801608711481094,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12822175025939941,
      "backward_entropy": 0.008314658701419831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.1369571685791,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05085177347064018,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1281843880812327,
      "backward_entropy": 0.008311440795660019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.757022857666016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050900693982839584,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12814762194951376,
      "backward_entropy": 0.00830903947353363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.99556350708008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05094968155026436,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1281106968720754,
      "backward_entropy": 0.008306773006916046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.57241439819336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051001086831092834,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12807199358940125,
      "backward_entropy": 0.008303142338991164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.47609519958496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05105232074856758,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12803314129511514,
      "backward_entropy": 0.008300009369850158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.381681442260742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051103375852108,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1279942492643992,
      "backward_entropy": 0.008297738432884217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.71550178527832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05115427449345589,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1279552678267161,
      "backward_entropy": 0.02578449249267578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.831295013427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05120382457971573,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1279171109199524,
      "backward_entropy": 0.008294990658760071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.13679885864258,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05125683173537254,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1278767983118693,
      "backward_entropy": 0.025828927755355835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.486419677734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051311783492565155,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12783518433570862,
      "backward_entropy": 0.008286365121603013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.888507843017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051367320120334625,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12779311339060465,
      "backward_entropy": 0.008280926942825317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.784082412719727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05142224580049515,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12775099277496338,
      "backward_entropy": 0.008276431262493134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.842668533325195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051476605236530304,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12770897150039673,
      "backward_entropy": 0.008272609114646912,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.583160400390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05152807757258415,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12766886750857034,
      "backward_entropy": 0.008270642161369324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.967132568359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05157929286360741,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12762882312138876,
      "backward_entropy": 0.02591060698032379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.07054901123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051634933799505234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1275856097539266,
      "backward_entropy": 0.025918373465538026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.955461502075195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05169225111603737,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12754082679748535,
      "backward_entropy": 0.008258911967277526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.03845977783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05174754932522774,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12749735514322916,
      "backward_entropy": 0.00825512856245041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.58242416381836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05180566757917404,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12745176752408346,
      "backward_entropy": 0.025939327478408814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.41008758544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05186516419053078,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12740498781204224,
      "backward_entropy": 0.008242742717266082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.442989349365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05192585289478302,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1273575226465861,
      "backward_entropy": 0.025935041904449462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.88190841674805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05198878049850464,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1273083488146464,
      "backward_entropy": 0.008225661516189576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.586597442626953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05205141752958298,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1272592544555664,
      "backward_entropy": 0.025914451479911803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.585941314697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0521126314997673,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12721101442972818,
      "backward_entropy": 0.008208788186311721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.265884399414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05217371881008148,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12716254591941833,
      "backward_entropy": 0.025898462533950804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.29848861694336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05223238468170166,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12711570660273233,
      "backward_entropy": 0.008195685595273972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.15708541870117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05229116231203079,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12706845998764038,
      "backward_entropy": 0.025891968607902528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.02434539794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05235002562403679,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12702097495396933,
      "backward_entropy": 0.008184919506311417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.74908447265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05241352319717407,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1269697149594625,
      "backward_entropy": 0.008176665008068084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.69990158081055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05248117074370384,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12691497802734375,
      "backward_entropy": 0.025858506560325623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.819005012512207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05254799500107765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12686060865720114,
      "backward_entropy": 0.00815604031085968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.635467529296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05261066183447838,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12680918971697488,
      "backward_entropy": 0.008149517327547073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.93896484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05267072841525078,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12675952911376953,
      "backward_entropy": 0.008145149052143096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.545013427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05273297429084778,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12670757373174033,
      "backward_entropy": 0.00813957378268242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.721439361572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05279827490448952,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1266532043615977,
      "backward_entropy": 0.008132240921258926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.7763786315918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05286407470703125,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12659801046053568,
      "backward_entropy": 0.008124920725822448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.451336860656738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05292918533086777,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12654311458269754,
      "backward_entropy": 0.008118421584367753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.1644172668457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052990298718214035,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1264910101890564,
      "backward_entropy": 0.0081152081489563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.994964599609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05305229872465134,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12643766403198242,
      "backward_entropy": 0.025790756940841673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.90835189819336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0531117208302021,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12638614575068155,
      "backward_entropy": 0.008109980076551438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.825815200805664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05316881462931633,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12633653481801352,
      "backward_entropy": 0.008110486716032029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.74553108215332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05322381108999252,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12628812591234842,
      "backward_entropy": 0.008112438023090363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.6683292388916,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0532769076526165,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1262411673863729,
      "backward_entropy": 0.02582782506942749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.65228271484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05332828685641289,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12619540095329285,
      "backward_entropy": 0.02584558427333832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.021577835083008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05338034778833389,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12614870071411133,
      "backward_entropy": 0.008125096559524536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.442096710205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05343189835548401,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1261022686958313,
      "backward_entropy": 0.02587752640247345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.18780517578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05348186567425728,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12605677048365274,
      "backward_entropy": 0.008135306090116501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.291154861450195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053534846752882004,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12600835164388022,
      "backward_entropy": 0.02590794563293457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.427215576171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053586091846227646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12596116463343301,
      "backward_entropy": 0.008142738044261933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.893402099609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053639091551303864,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12591222922007242,
      "backward_entropy": 0.008145623654127122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.058944702148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05369255691766739,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12586276729901633,
      "backward_entropy": 0.00814860239624977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.96092987060547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05374422296881676,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12581480542818704,
      "backward_entropy": 0.13862353563308716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.5034122467041,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05379756912589073,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12576494614283243,
      "backward_entropy": 0.008155208081007004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.823999404907227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05385132133960724,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12571456034978232,
      "backward_entropy": 0.008157317340373994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.48965835571289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05390322208404541,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12566561500231424,
      "backward_entropy": 0.008160495012998582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.9943962097168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05395674705505371,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12561468283335367,
      "backward_entropy": 0.025968384742736817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.965425491333008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05401500687003136,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12555898229281107,
      "backward_entropy": 0.008159507066011429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.98453140258789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05407314747571945,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1255033016204834,
      "backward_entropy": 0.025950214266777037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.406030654907227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054132264107465744,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1254465381304423,
      "backward_entropy": 0.008154743164777756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.6386604309082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05418897047638893,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12539196014404297,
      "backward_entropy": 0.008153825253248214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.46635818481445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05424676835536957,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12533591190973917,
      "backward_entropy": 0.008152022212743758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.099472045898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0543055459856987,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12527868151664734,
      "backward_entropy": 0.025905367732048035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.084426879882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05436083674430847,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12522464990615845,
      "backward_entropy": 0.008149797469377518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.94820022583008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054415151476860046,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1251711050669352,
      "backward_entropy": 0.008150193095207214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.860654830932617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054472897201776505,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12511348724365234,
      "backward_entropy": 0.025878128409385682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.74845314025879,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05452940613031387,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12505662441253662,
      "backward_entropy": 0.02586595416069031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.546707153320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0545848049223423,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12500062584877014,
      "backward_entropy": 0.00814414992928505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.40765380859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05464029312133789,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12494450807571411,
      "backward_entropy": 0.008143719285726547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.267929077148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054695844650268555,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12488818168640137,
      "backward_entropy": 0.008143734931945801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.128692626953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054751429706811905,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12483123938242595,
      "backward_entropy": 0.0081436388194561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.394731521606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054807037115097046,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12477412819862366,
      "backward_entropy": 0.008142880350351333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.085250854492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05486049875617027,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12471870581309001,
      "backward_entropy": 0.008143313974142075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.21109390258789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05491311475634575,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12466389934221904,
      "backward_entropy": 0.008144630491733551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.868925094604492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05496816337108612,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12460617224375407,
      "backward_entropy": 0.008144574612379074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.825714111328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055022191256284714,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12454927961031596,
      "backward_entropy": 0.025775012373924256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.665457725524902,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055078454315662384,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12448922793070476,
      "backward_entropy": 0.00814414992928505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.811893463134766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05513036996126175,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12443393468856812,
      "backward_entropy": 0.025753390789031983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.434856414794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055183686316013336,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12437671422958374,
      "backward_entropy": 0.008148298412561417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.168028831481934,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05523611977696419,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12432042757670085,
      "backward_entropy": 0.02573777437210083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.673858642578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055285654962062836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12426727016766866,
      "backward_entropy": 0.008156348764896394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.132793426513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05533362179994583,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12421576182047527,
      "backward_entropy": 0.008163276314735412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.037826538085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055381208658218384,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12416418393452962,
      "backward_entropy": 0.008170107007026672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.006133407820016146,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05542846769094467,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12411274512608846,
      "backward_entropy": 0.008177444338798523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.93010425567627,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055471230298280716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12406692902247111,
      "backward_entropy": 0.00818924531340599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.332386016845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05551200360059738,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12402315934499104,
      "backward_entropy": 0.02577482759952545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.96143341064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055552054196596146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12398004531860352,
      "backward_entropy": 0.008215457201004028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.006574630737305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055595628917217255,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12393221259117126,
      "backward_entropy": 0.025802138447761535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.137968063354492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05564027279615402,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12388257185618083,
      "backward_entropy": 0.00823458433151245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.715252876281738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05568378046154976,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12383416295051575,
      "backward_entropy": 0.008244088292121888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.338787078857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05572519078850746,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12378799915313721,
      "backward_entropy": 0.00825425609946251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.896121978759766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05576367303729057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1237453023592631,
      "backward_entropy": 0.025837147235870363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.771875381469727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05580470710992813,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12369877099990845,
      "backward_entropy": 0.025841766595840455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.36734390258789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05584804713726044,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12364890178044637,
      "backward_entropy": 0.008280745893716811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.002452850341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05589243769645691,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12359738349914551,
      "backward_entropy": 0.008285726606845855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.686274528503418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055936675518751144,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.123545507589976,
      "backward_entropy": 0.008289230614900589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.82412338256836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055979788303375244,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12349500258763631,
      "backward_entropy": 0.008293859660625458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.91917610168457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056022871285676956,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12344398101170857,
      "backward_entropy": 0.00829763412475586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.645627975463867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05606696009635925,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12339127063751221,
      "backward_entropy": 0.008299585431814194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.417839050292969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0561109222471714,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12333834171295166,
      "backward_entropy": 0.02578415870666504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.58323097229004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05615375563502312,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1232866644859314,
      "backward_entropy": 0.008303344249725342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.6570930480957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05619760975241661,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12323322892189026,
      "backward_entropy": 0.025755593180656434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.48810958862305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05624442920088768,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12317532300949097,
      "backward_entropy": 0.00830269679427147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.22185707092285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05629391223192215,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12311345338821411,
      "backward_entropy": 0.008298765867948532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.092485427856445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05634373798966408,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12305072943369548,
      "backward_entropy": 0.008294519782066346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.987259864807129,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05639383941888809,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12298715114593506,
      "backward_entropy": 0.02564116716384888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.907743453979492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05644111707806587,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12292725841204326,
      "backward_entropy": 0.025613421201705934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.891427040100098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056486934423446655,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12286921342213948,
      "backward_entropy": 0.008285468816757202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.847573280334473,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05653035640716553,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12281429767608643,
      "backward_entropy": 0.00828642025589943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.604740142822266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05657165125012398,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12276242176691692,
      "backward_entropy": 0.025560063123703004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.518795013427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05661303550004959,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12270996967951457,
      "backward_entropy": 0.00829242318868637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.00571823120117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056654468178749084,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12265700101852417,
      "backward_entropy": 0.008294585347175597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.507940292358398,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056698981672525406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12259894609451294,
      "backward_entropy": 0.008293895423412323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.628480911254883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05674225464463234,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12254252036412557,
      "backward_entropy": 0.008294861018657684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.16800308227539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05678338184952736,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12248908480008443,
      "backward_entropy": 0.008297738432884217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.081693649291992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05682460591197014,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1224352518717448,
      "backward_entropy": 0.008301294595003127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.493427276611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0568658709526062,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1223809023698171,
      "backward_entropy": 0.008304255455732346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.180963516235352,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05690921097993851,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12232297658920288,
      "backward_entropy": 0.0083061084151268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.227209091186523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0569513700902462,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12226657072703044,
      "backward_entropy": 0.02541124224662781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.366706848144531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05699547752737999,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12220676740010579,
      "backward_entropy": 0.008309291303157806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.641551971435547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057037342339754105,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12215022246042888,
      "backward_entropy": 0.025375014543533324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.279312133789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05707917734980583,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12209334969520569,
      "backward_entropy": 0.008315642923116684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.856498718261719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.057118963450193405,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12203943729400635,
      "backward_entropy": 0.1386234402656555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.988412857055664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05715795233845711,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12198664744695027,
      "backward_entropy": 0.008326634764671326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.155879020690918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057198166847229004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12193157275517781,
      "backward_entropy": 0.008331412076950073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.672188758850098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05723647400736809,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12187919020652771,
      "backward_entropy": 0.008336954563856126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.68763542175293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057274043560028076,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12182754278182983,
      "backward_entropy": 0.025304067134857177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.586362838745117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05731295794248581,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12177342176437378,
      "backward_entropy": 0.025289851427078246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.978307723999023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05735306069254875,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12171689669291179,
      "backward_entropy": 0.02527259588241577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.372182846069336,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05739524960517883,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12165649731953938,
      "backward_entropy": 0.1386237382888794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.71103286743164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057438287883996964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12159432967503865,
      "backward_entropy": 0.008353079110383988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.715591430664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05748305842280388,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1215288241704305,
      "backward_entropy": 0.008352611958980561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.028564453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05752742290496826,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.121463676293691,
      "backward_entropy": 0.008352378010749817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.767206192016602,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057572443038225174,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12139703830083211,
      "backward_entropy": 0.025153252482414245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.440616607666016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057615067809820175,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12133429447809856,
      "backward_entropy": 0.0251334547996521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.35228729248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05765749141573906,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12127150098482768,
      "backward_entropy": 0.025114983320236206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.577960968017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05769972503185272,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12120850880940755,
      "backward_entropy": 0.008362136036157607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.590394973754883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05774269625544548,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12114409605662028,
      "backward_entropy": 0.025073444843292235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.359966278076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05778348445892334,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1210830807685852,
      "backward_entropy": 0.008367355167865752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.501068115234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05782514810562134,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12101999918619792,
      "backward_entropy": 0.025033456087112427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.005578417796641588,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05786857008934021,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12095331152280171,
      "backward_entropy": 0.008369389176368713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.830158233642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05790780484676361,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12089412411053975,
      "backward_entropy": 0.024988168478012086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.190700531005859,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057947102934122086,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12083439032236735,
      "backward_entropy": 0.008376853913068772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.672794342041016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05798354744911194,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12077967325846355,
      "backward_entropy": 0.024953384697437287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.596895217895508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05802033096551895,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12072389324506123,
      "backward_entropy": 0.008387178182601929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.6478214263916,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05805744230747223,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12066722909609477,
      "backward_entropy": 0.13862235546112062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.439998626708984,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0580957867205143,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12060781319936116,
      "backward_entropy": 0.1386224150657654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.44696617126465,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0581342987716198,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1205478310585022,
      "backward_entropy": 0.02489059418439865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.411800384521484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05817384272813797,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12048535545667012,
      "backward_entropy": 0.008407836407423019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.328855514526367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05821526423096657,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1204188068707784,
      "backward_entropy": 0.008409614861011504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.073835372924805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05825935676693916,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12034672498703003,
      "backward_entropy": 0.024817347526550293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.009000778198242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05830197036266327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12027703722318013,
      "backward_entropy": 0.008409376442432403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.87813377380371,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05834425985813141,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12020766735076904,
      "backward_entropy": 0.024758140742778777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9613077640533447,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05838814005255699,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12013468146324158,
      "backward_entropy": 0.024727100133895875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.874112606048584,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058428701013326645,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12006806333859761,
      "backward_entropy": 0.02470138669013977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.834677696228027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05846717581152916,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12000525991121928,
      "backward_entropy": 0.00841320976614952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.378433227539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05850379914045334,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11994584401448567,
      "backward_entropy": 0.008416705578565598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.506234169006348,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05854250118136406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11988164981206258,
      "backward_entropy": 0.008418162167072297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.5702543258667,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058581169694662094,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1198171575864156,
      "backward_entropy": 0.008418551832437515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.512285232543945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058618854731321335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11975428462028503,
      "backward_entropy": 0.00841856524348259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.27044677734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05865570902824402,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11969271302223206,
      "backward_entropy": 0.13862032890319825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.599920749664307,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058692727237939835,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11963043610254924,
      "backward_entropy": 0.008421263098716736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.89809799194336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05872804671525955,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11957147717475891,
      "backward_entropy": 0.008424735814332961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.044187545776367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05876460298895836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11950947841008504,
      "backward_entropy": 0.008427472412586212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.707412719726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05880138278007507,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.119446595509847,
      "backward_entropy": 0.024435694515705108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7264771461486816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05883926525712013,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.119381050268809,
      "backward_entropy": 0.008435291051864625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.814491271972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05887439846992493,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11932109793027242,
      "backward_entropy": 0.024394161999225616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.741000175476074,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058909814804792404,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11926015218098958,
      "backward_entropy": 0.024373573064804078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.666741371154785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058945488184690475,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11919828255971272,
      "backward_entropy": 0.008451086282730103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.884952545166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058981385082006454,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11913553873697917,
      "backward_entropy": 0.008455460518598556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.886841773986816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059019315987825394,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11906768878300984,
      "backward_entropy": 0.008458064496517181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.435532569885254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0590563602745533,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11900164683659871,
      "backward_entropy": 0.008462433516979218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.947053909301758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05909345671534538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1189349889755249,
      "backward_entropy": 0.00846608802676201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.278338432312012,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059131551533937454,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11886575818061829,
      "backward_entropy": 0.024230223894119263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.652569770812988,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05916954204440117,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11879624923070271,
      "backward_entropy": 0.00847153589129448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.181060791015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05920664221048355,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11872851848602295,
      "backward_entropy": 0.02417791485786438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5155813694000244,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05924558639526367,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1186559796333313,
      "backward_entropy": 0.008478657901287079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.966994285583496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05928168073296547,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11858980854352315,
      "backward_entropy": 0.00848415344953537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.892216682434082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059317901730537415,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11852294206619263,
      "backward_entropy": 0.008489549905061722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4584908485412598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059354253113269806,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11845546960830688,
      "backward_entropy": 0.00849505066871643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.8776535987854,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05938795208930969,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11839398741722107,
      "backward_entropy": 0.0085016168653965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.353286743164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05942026898264885,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11833542585372925,
      "backward_entropy": 0.00851142629981041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.003908157348633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05945662036538124,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11826688051223755,
      "backward_entropy": 0.008517079800367356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.146954536437988,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05949389562010765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11819563309351604,
      "backward_entropy": 0.008520865440368652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.729660987854004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059530217200517654,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1181262731552124,
      "backward_entropy": 0.0085246704518795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.067995071411133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059564825147390366,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11806070804595947,
      "backward_entropy": 0.00853048861026764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.95416259765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05960145592689514,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11798985799153645,
      "backward_entropy": 0.008535351604223251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.921355247497559,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05963987857103348,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11791414022445679,
      "backward_entropy": 0.13862130641937256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.147850036621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05967728793621063,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1178405483563741,
      "backward_entropy": 0.008545088022947312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.070846557617188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059714578092098236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11776672800381978,
      "backward_entropy": 0.008550053834915161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.488201141357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05975174903869629,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11769280831019084,
      "backward_entropy": 0.008553978800773621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2337756156921387,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05979059636592865,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1176142692565918,
      "backward_entropy": 0.02380729913711548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.840397834777832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05982656031847,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11754276355107625,
      "backward_entropy": 0.008561355620622635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.576143264770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05986258387565613,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11747064193089803,
      "backward_entropy": 0.008567191660404205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.5216646194458,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05989773944020271,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11740036805470784,
      "backward_entropy": 0.008573517203330994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.77680492401123,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059932079166173935,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11733172337214152,
      "backward_entropy": 0.02371489107608795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.279258728027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05996742472052574,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11726008852322896,
      "backward_entropy": 0.008583629876375199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.07394790649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060001153498888016,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11719234784444173,
      "backward_entropy": 0.008591251075267791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.402627944946289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06003944203257561,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11711205045382182,
      "backward_entropy": 0.008595432341098785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.324360847473145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0600774921476841,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11703211069107056,
      "backward_entropy": 0.00860009789466858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.187166213989258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06011529266834259,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.116952250401179,
      "backward_entropy": 0.008604339510202407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.130572319030762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06015202775597572,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11687488357226054,
      "backward_entropy": 0.008609931170940398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.144329071044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060187771916389465,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11679957310358684,
      "backward_entropy": 0.008615871518850326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.029678344726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060225170105695724,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11671968301137288,
      "backward_entropy": 0.008619530498981476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.942270278930664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06026407331228256,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1166350245475769,
      "backward_entropy": 0.00862189158797264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.864091873168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06030256673693657,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11655100186665852,
      "backward_entropy": 0.00862327367067337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.785378456115723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060340769588947296,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11646726727485657,
      "backward_entropy": 0.008626236021518708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.783086776733398,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06037859991192818,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11638426780700684,
      "backward_entropy": 0.008627734333276748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.82047176361084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060415275394916534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11630388100941975,
      "backward_entropy": 0.008629602938890457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.673800468444824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06045006215572357,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11622852087020874,
      "backward_entropy": 0.00863274112343788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.365341186523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060483984649181366,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1161552866299947,
      "backward_entropy": 0.00863613784313202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.276751518249512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06051885336637497,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11607871452967326,
      "backward_entropy": 0.00863974094390869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.514764785766602,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06055457517504692,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11599918206532796,
      "backward_entropy": 0.008643986284732818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.096908569335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06058938056230545,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11592177549997966,
      "backward_entropy": 0.008650098741054536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.207254409790039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060624875128269196,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11584205428759257,
      "backward_entropy": 0.008652669936418533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.353761672973633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0606602318584919,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.115762193997701,
      "backward_entropy": 0.008654670417308807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.771883010864258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06069467216730118,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11568456888198853,
      "backward_entropy": 0.008658360689878464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.252618789672852,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060726530849933624,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11561441421508789,
      "backward_entropy": 0.023066474497318266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.205353736877441,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060757748782634735,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11554581920305888,
      "backward_entropy": 0.023033179342746735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.724438190460205,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060788415372371674,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11547840634981792,
      "backward_entropy": 0.023002758622169495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.926090240478516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060816869139671326,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11541747053464253,
      "backward_entropy": 0.02297210246324539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.440935134887695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060848332941532135,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11534688870112102,
      "backward_entropy": 0.008675722032785415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.029165267944336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06088080257177353,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11527270078659058,
      "backward_entropy": 0.00867629498243332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.660867214202881,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06091504544019699,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11519270141919453,
      "backward_entropy": 0.00867629200220108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2812604904174805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060946788638830185,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1151199738184611,
      "backward_entropy": 0.022827206552028655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.115702629089355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0609770268201828,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11505160729090373,
      "backward_entropy": 0.008682069182395936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.431082725524902,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06100839003920555,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11497931679089864,
      "backward_entropy": 0.008684176951646805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.187798976898193,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06103998422622681,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11490579446156819,
      "backward_entropy": 0.022725556790828706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.881478309631348,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06107015535235405,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11483643452326457,
      "backward_entropy": 0.008692830055952071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.684898853302002,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061101462692022324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11476292212804158,
      "backward_entropy": 0.022669634222984313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.0952301025390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06113215163350105,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11469103892644246,
      "backward_entropy": 0.008704324066638947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.066056728363037,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061161480844020844,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11462310949961345,
      "backward_entropy": 0.022619453072547913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.067620277404785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06118961051106453,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11455865701039632,
      "backward_entropy": 0.008721615374088287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.012154579162598,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06121821328997612,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11449227730433147,
      "backward_entropy": 0.022580140829086305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.434890747070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06124888360500336,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11441858609517415,
      "backward_entropy": 0.008739174157381058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.886367797851562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06128056347370148,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11434125900268555,
      "backward_entropy": 0.022536931931972502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.276715278625488,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06131235137581825,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11426297823588054,
      "backward_entropy": 0.022513562440872194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.758003234863281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06134498491883278,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1141816775004069,
      "backward_entropy": 0.008759070932865144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.535698890686035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06137761473655701,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11409986019134521,
      "backward_entropy": 0.008764313161373138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.624977111816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06141186133027077,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1140121618906657,
      "backward_entropy": 0.02243029177188873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.944788932800293,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06144591048359871,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11392457286516826,
      "backward_entropy": 0.13861898183822632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.488686561584473,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06148059666156769,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11383440097173055,
      "backward_entropy": 0.00877966284751892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.773544311523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06151503697037697,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11374453703562419,
      "backward_entropy": 0.008784616738557816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.679757118225098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061549969017505646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11365268627802531,
      "backward_entropy": 0.008787248283624649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.605437278747559,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061583004891872406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.113567054271698,
      "backward_entropy": 0.008789800852537156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.522195816040039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061616700142621994,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11347856124242146,
      "backward_entropy": 0.008791765570640564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.865927696228027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061650995165109634,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11338761448860168,
      "backward_entropy": 0.008793732523918152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.355705261230469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0616842545568943,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11329974730809529,
      "backward_entropy": 0.008796417713165283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.272392272949219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061718136072158813,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11320918798446655,
      "backward_entropy": 0.008799056708812713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.952012062072754,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0617525652050972,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11311626434326172,
      "backward_entropy": 0.022102390229701997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.323301315307617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06178667023777962,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11302388707796733,
      "backward_entropy": 0.008803668618202209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.815841674804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06182199716567993,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11292670170466106,
      "backward_entropy": 0.00880390927195549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.193549394607544,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06185690686106682,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11283066868782043,
      "backward_entropy": 0.02198740839958191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.51569128036499,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06188914552330971,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11274406313896179,
      "backward_entropy": 0.008806323260068893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.3162126541137695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061920542269945145,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11265985171000163,
      "backward_entropy": 0.008810236304998397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.287267208099365,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06195037439465523,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11258103450139363,
      "backward_entropy": 0.008815088123083115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.51032543182373,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06197882443666458,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11250696579615276,
      "backward_entropy": 0.008821818232536315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.343360424041748,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.062007445842027664,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1124318540096283,
      "backward_entropy": 0.021827445924282075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.400423049926758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06203550845384598,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1123585303624471,
      "backward_entropy": 0.008831074833869934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.345556259155273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062063854187726974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11228350798288982,
      "backward_entropy": 0.008836405724287033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.430989265441895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062092408537864685,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11220726370811462,
      "backward_entropy": 0.00884140208363533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.287567138671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06212266907095909,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11212408542633057,
      "backward_entropy": 0.008846447616815568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.253477096557617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06215370073914528,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11203744014104207,
      "backward_entropy": 0.008851379901170731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.084403991699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06218612939119339,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1119449536005656,
      "backward_entropy": 0.008854847401380539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.049001693725586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0622175969183445,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11185578505198161,
      "backward_entropy": 0.00885864645242691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.993851184844971,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062248848378658295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11176717281341553,
      "backward_entropy": 0.008860033750534058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.969818592071533,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062279295176267624,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11168110370635986,
      "backward_entropy": 0.008863887935876846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.841451644897461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06230821833014488,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11160067717234294,
      "backward_entropy": 0.008868535608053207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.770363807678223,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0623379610478878,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11151647567749023,
      "backward_entropy": 0.008872645348310471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.759893894195557,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062368396669626236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11142918467521667,
      "backward_entropy": 0.008875163644552231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.625102043151855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06239878386259079,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11134148637453715,
      "backward_entropy": 0.008878729492425918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.7344651222229,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06242981180548668,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11125081777572632,
      "backward_entropy": 0.02135661244392395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.58554744720459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062460001558065414,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11116294066111247,
      "backward_entropy": 0.008886870741844178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.408203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.062490105628967285,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1110750138759613,
      "backward_entropy": 0.021293087303638457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.604995250701904,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06252084672451019,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11098413666089375,
      "backward_entropy": 0.008895633369684219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.413687229156494,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06255077570676804,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11089591185251872,
      "backward_entropy": 0.1386149525642395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.520495414733887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06258068233728409,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11080706119537354,
      "backward_entropy": 0.008910048753023148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6560940742492676,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06260982900857925,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11072099208831787,
      "backward_entropy": 0.008919358253479004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.061474800109863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06263759732246399,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11064017812410991,
      "backward_entropy": 0.008931118994951248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.19739294052124,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06266619265079498,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11055529117584229,
      "backward_entropy": 0.008941981196403503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.711912155151367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06269489973783493,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11046926180521648,
      "backward_entropy": 0.00895504206418991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.858506202697754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0627250000834465,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11037694414456685,
      "backward_entropy": 0.008966048061847687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.787125587463379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06275559961795807,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11028217275937398,
      "backward_entropy": 0.008974237740039826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2325873374938965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06278667598962784,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11018506685892741,
      "backward_entropy": 0.008980987221002578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.190962791442871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0628168061375618,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11009150743484497,
      "backward_entropy": 0.00898861363530159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.577130317687988,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06284604221582413,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11000126600265503,
      "backward_entropy": 0.00899580717086792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7092732191085815,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0628758892416954,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10990790526072185,
      "backward_entropy": 0.009002664685249328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.444894790649414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0629035085439682,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10982388257980347,
      "backward_entropy": 0.020953145623207093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.032437324523926,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0629318356513977,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10973629355430603,
      "backward_entropy": 0.009018088132143021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.977460861206055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06295934319496155,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10965228080749512,
      "backward_entropy": 0.00902208238840103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.249685287475586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06298823654651642,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10956170161565144,
      "backward_entropy": 0.00902460739016533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.644551157951355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06301767379045486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10946818192799886,
      "backward_entropy": 0.009026034921407699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.119278907775879,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06304486095905304,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10938458641370137,
      "backward_entropy": 0.009027861058712006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.227806329727173,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06307276338338852,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10929726560910542,
      "backward_entropy": 0.009029289335012436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.994823455810547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06309930235147476,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10921551783879598,
      "backward_entropy": 0.009032876789569854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.51616096496582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0631265789270401,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10913000504175822,
      "backward_entropy": 0.020657235383987428,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.43946647644043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06315528601408005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1090371310710907,
      "backward_entropy": 0.009039632976055145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1256558895111084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06318507343530655,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1089392900466919,
      "backward_entropy": 0.02058063745498657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.190333366394043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0632132738828659,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10884831349054973,
      "backward_entropy": 0.009041159600019454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.138693332672119,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06324135512113571,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10875753561655681,
      "backward_entropy": 0.009042833745479584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.049896717071533,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06326941400766373,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10866592327753703,
      "backward_entropy": 0.009047596901655196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.549228668212891,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06329598277807236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10858104626337688,
      "backward_entropy": 0.009051842242479324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.479270935058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06332317739725113,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10849302013715108,
      "backward_entropy": 0.009053897112607956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9752442836761475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06335218250751495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10839649041493733,
      "backward_entropy": 0.009051813930273055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.830706596374512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06337962299585342,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10830697417259216,
      "backward_entropy": 0.009050555527210236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4672622680664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06340824067592621,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1082115372021993,
      "backward_entropy": 0.009048189967870712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.128158569335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06343474239110947,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1081253985563914,
      "backward_entropy": 0.009049557149410248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.308703899383545,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06346310675144196,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10803025960922241,
      "backward_entropy": 0.020165519416332246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.532708168029785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06349058449268341,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10793904463450114,
      "backward_entropy": 0.020120051503181458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.826368808746338,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06351926922798157,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10784127314885457,
      "backward_entropy": 0.009048206359148025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.5932159423828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06354635953903198,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10775116086006165,
      "backward_entropy": 0.02003553360700607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.313286781311035,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06357337534427643,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10766067107518514,
      "backward_entropy": 0.019997216761112213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3820703029632568,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06360156834125519,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10756391286849976,
      "backward_entropy": 0.009054111689329148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.730250120162964,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06362763792276382,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10747726758321126,
      "backward_entropy": 0.009059218317270279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.0581159591674805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06365249305963516,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10739574829737346,
      "backward_entropy": 0.019899578392505647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.688013792037964,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06367681920528412,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10731669267018636,
      "backward_entropy": 0.009077922254800797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.010187456384301186,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06370001286268234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10724262396494548,
      "backward_entropy": 0.01985195428133011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.9279866218566895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06372098624706268,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10717926422754924,
      "backward_entropy": 0.009102223813533783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9402682781219482,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0637437179684639,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10710648695627849,
      "backward_entropy": 0.009116826206445694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2124457359313965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06376608461141586,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1070350706577301,
      "backward_entropy": 0.009131047129631042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.883701801300049,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06378868222236633,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1069623629252116,
      "backward_entropy": 0.009142822027206421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.854776620864868,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0638107880949974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1068921685218811,
      "backward_entropy": 0.009150497615337372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.369972229003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06383249163627625,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10682411988576253,
      "backward_entropy": 0.009155721962451934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2726765871047974,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0638551115989685,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10675094525019328,
      "backward_entropy": 0.009159797430038452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.022543430328369,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06387607753276825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10668595631917317,
      "backward_entropy": 0.009163925796747208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4981720447540283,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06389745324850082,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10661824544270833,
      "backward_entropy": 0.009169015288352966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.71527099609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06391793489456177,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10655475656191508,
      "backward_entropy": 0.019573667645454408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6906607151031494,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06393828243017197,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1064912478129069,
      "backward_entropy": 0.009182316809892654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.882213592529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06395836919546127,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10642898082733154,
      "backward_entropy": 0.009187592566013337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.84767484664917,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0639789029955864,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1063639223575592,
      "backward_entropy": 0.009193123877048492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.213091850280762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06399980187416077,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10629669825236003,
      "backward_entropy": 0.009197565913200378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.774250507354736,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0640222355723381,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10622094074885051,
      "backward_entropy": 0.019403617084026336,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 7.11553820559755,
    "avg_log_Z": -0.06270213887095451,
    "success_rate": 1.0,
    "avg_reward": 72.2,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.02,
      "1": 0.2,
      "2": 0.78
    },
    "avg_forward_entropy": 0.11031103442112603,
    "avg_backward_entropy": 0.01395704947412014,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}