{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.0770072340965271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.07700278361638387,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.5813446044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094366192817688,
      "backward_entropy": 0.077015929751926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.54615783691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10944511890411376,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.04457092285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00019999990763608366,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945359468460084,
      "backward_entropy": 0.07700799571143256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.46058654785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0003000289434567094,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10946192741394042,
      "backward_entropy": 0.07700563801659478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.42782592773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00039956881664693356,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10947012901306152,
      "backward_entropy": 0.07701525423261854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.39520263671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.000498843495734036,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10947822332382202,
      "backward_entropy": 0.07700725396474202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.35391235351562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005979593261145055,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948619842529297,
      "backward_entropy": 0.07701480388641357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.3363800048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0006963631021790206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1094940185546875,
      "backward_entropy": 0.07700974411434597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.754150390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0007953651947900653,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095017671585083,
      "backward_entropy": 0.0770142740673489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.72787475585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0008929042960517108,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10950934886932373,
      "backward_entropy": 0.07701399591233996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.76829528808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0009900485165417194,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10951683521270753,
      "backward_entropy": 0.07701369788911608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.27407836914062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001087020500563085,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10952415466308593,
      "backward_entropy": 0.07701132694880168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.6371612548828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0011839296203106642,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953134298324585,
      "backward_entropy": 0.07701185014512804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.6069793701172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00128061359282583,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953842401504517,
      "backward_entropy": 0.07701234022776286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 277.6851806640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00137712515424937,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10954543352127075,
      "backward_entropy": 0.07701234685050116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 262.0484619140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001475854660384357,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10955239534378051,
      "backward_entropy": 0.0770118302769131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.9922332763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0015758121153339744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10955933332443238,
      "backward_entropy": 0.0770115786128574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.02157592773438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0016741676954552531,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10956612825393677,
      "backward_entropy": 0.07701396279864842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 295.4376220703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0017720202449709177,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10957280397415162,
      "backward_entropy": 0.07701428069008721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.9604034423828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018722915556281805,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095794677734375,
      "backward_entropy": 0.07701037989722358,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.92970275878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001971703954041004,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958600044250488,
      "backward_entropy": 0.07701281706492107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.84927368164062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002070378279313445,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10959240198135375,
      "backward_entropy": 0.0770150555504693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.33021545410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002169744810089469,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10959875583648682,
      "backward_entropy": 0.07700914806789821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.83763122558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0022683029528707266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10960503816604614,
      "backward_entropy": 0.0770087440808614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 277.75775146484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0023662284947931767,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1096110463142395,
      "backward_entropy": 0.07701559861501057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 278.640869140625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002466288162395358,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10961699485778809,
      "backward_entropy": 0.07701573106977674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.24855041503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0025680484250187874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10962291955947875,
      "backward_entropy": 0.07701366477542454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.71620178222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0026694468688219786,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10962872505187989,
      "backward_entropy": 0.07701378398471409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.68643188476562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0027712523005902767,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10963441133499145,
      "backward_entropy": 0.07701603571573894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.6466827392578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002870214870199561,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10963987112045288,
      "backward_entropy": 0.07700626055399577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.1855926513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002969909692183137,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10964508056640625,
      "backward_entropy": 0.07701408863067627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.041015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0030651737470179796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965012311935425,
      "backward_entropy": 0.07701418134901258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.6165008544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0031615858897566795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965510606765747,
      "backward_entropy": 0.0770142674446106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.53082275390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003256680443882942,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965995788574219,
      "backward_entropy": 0.07701434029473199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.91326904296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003348602680489421,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966466665267945,
      "backward_entropy": 0.07701629400253296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.9110870361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003438489744439721,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10966930389404297,
      "backward_entropy": 0.0770144330130683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.97879028320312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0035302808973938227,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967386960983276,
      "backward_entropy": 0.07700317435794407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 276.8094482421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003618233371526003,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10967831611633301,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.83351135253906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00370996561832726,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10968271493911744,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 278.5691223144531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003800684818997979,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10968695878982544,
      "backward_entropy": 0.0770020220014784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.23753356933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0038945963606238365,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969116687774658,
      "backward_entropy": 0.0770017041100396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 261.56304931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003987164702266455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969531536102295,
      "backward_entropy": 0.07701452573140462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.26771545410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004081984981894493,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969942808151245,
      "backward_entropy": 0.07700118091371325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.41192626953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00417551351711154,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10970343351364135,
      "backward_entropy": 0.07700092262691921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 261.4565124511719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0042682127095758915,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970726013183593,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.73875427246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004363202024251223,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097110629081726,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.0945281982422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004457963164895773,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10971473455429077,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.05523681640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004554223269224167,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10971832275390625,
      "backward_entropy": 0.0769997239112854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.9396514892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004650862887501717,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10972180366516113,
      "backward_entropy": 0.07701438003116184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.90518188476562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004748646169900894,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097252368927002,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.50315856933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004847439471632242,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10972858667373657,
      "backward_entropy": 0.07699881659613715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.3816375732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004946395754814148,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10973182916641236,
      "backward_entropy": 0.07701430055830213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.99267578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005045347847044468,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10973498821258545,
      "backward_entropy": 0.0770142674446106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.8662109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005141482688486576,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097380518913269,
      "backward_entropy": 0.07701422108544244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.18650817871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0052370172925293446,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10974104404449463,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.25079345703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005333781708031893,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974396467208862,
      "backward_entropy": 0.0770141151216295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.20729064941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005430807825177908,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974678993225098,
      "backward_entropy": 0.07699704170227051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.95086669921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0055290511809289455,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974953174591065,
      "backward_entropy": 0.07699673705630833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.3642578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005624693352729082,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10975213050842285,
      "backward_entropy": 0.07699641916486952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 259.9940185546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005718989763408899,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10975461006164551,
      "backward_entropy": 0.07699607478247748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.08518981933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005815668031573296,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975703001022338,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 279.07135009765625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005912629887461662,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975935459136962,
      "backward_entropy": 0.07701633373896281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.37611389160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0060121058486402035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976166725158691,
      "backward_entropy": 0.077013717757331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 261.1992492675781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006111308466643095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976390838623047,
      "backward_entropy": 0.07699484957589044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.33465576171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006212098523974419,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976607799530029,
      "backward_entropy": 0.07701357205708821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.5832977294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0063089728355407715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976810455322265,
      "backward_entropy": 0.07699436611599392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.328369140625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006405299063771963,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977004766464234,
      "backward_entropy": 0.07701631387074788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.83470153808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006501855794340372,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977190732955933,
      "backward_entropy": 0.07699375682406956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.22816467285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006593881640583277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977363586425781,
      "backward_entropy": 0.07701325416564941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.86456298828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006682869978249073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097752332687378,
      "backward_entropy": 0.07701317469278972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.41575622558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006773161701858044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097766637802124,
      "backward_entropy": 0.07701310846540663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.6586456298828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0068636443465948105,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977792739868164,
      "backward_entropy": 0.07701631387074788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.01205444335938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006952460389584303,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977904796600342,
      "backward_entropy": 0.07701631387074788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.50315856933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007042226381599903,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978009700775146,
      "backward_entropy": 0.07699104150136311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.5200958251953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00713023915886879,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978102684020996,
      "backward_entropy": 0.07699050505956014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.04371643066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007215549703687429,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978183746337891,
      "backward_entropy": 0.07698996199501885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.11831665039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00729852681979537,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097825288772583,
      "backward_entropy": 0.07698939243952434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.4801940917969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007379593793302774,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978310108184815,
      "backward_entropy": 0.07698876327938503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.55355834960938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007465111557394266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097834825515747,
      "backward_entropy": 0.07698804802364773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.80320739746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007550048641860485,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097838044166565,
      "backward_entropy": 0.0769873923725552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.18580627441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007637712638825178,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978400707244873,
      "backward_entropy": 0.0769867632124159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.8821258544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0077261351980268955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978405475616455,
      "backward_entropy": 0.0770124461915758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.91578674316406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007811930496245623,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109783935546875,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.20843505859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007894190959632397,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097836971282959,
      "backward_entropy": 0.07701234022776286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.7681121826172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007975541055202484,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978344678878785,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.72811889648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008059162646532059,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978305339813232,
      "backward_entropy": 0.07701219452752008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.957763671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008144821040332317,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097825050354004,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.00430297851562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008233156986534595,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978180170059204,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.52101135253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008321131579577923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978095531463623,
      "backward_entropy": 0.07701200246810913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.45635986328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008408880792558193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097799301147461,
      "backward_entropy": 0.07698004775577122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.59861755371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008495103567838669,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977877378463745,
      "backward_entropy": 0.07701188988155788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.92034912109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008582010865211487,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977745056152344,
      "backward_entropy": 0.07697828610738118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.6884002685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008670586161315441,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097758650779724,
      "backward_entropy": 0.07701173755857679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.5138397216797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008757296949625015,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977404117584229,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.14324951171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00884161051362753,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977208614349365,
      "backward_entropy": 0.07701150576273601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.83819580078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008928952738642693,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10976986885070801,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.89190673828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00901681836694479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976747274398804,
      "backward_entropy": 0.07701123423046535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.654296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00910400040447712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097649097442627,
      "backward_entropy": 0.07701105541653103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.76530456542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009192890487611294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10976212024688721,
      "backward_entropy": 0.0770108766025967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.21241760253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009275706484913826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097592830657959,
      "backward_entropy": 0.0769721335834927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.18186950683594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009357737377285957,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097562551498413,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.58444213867188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00944098737090826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10975300073623658,
      "backward_entropy": 0.07697046465343899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.21067810058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009523335844278336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974955558776855,
      "backward_entropy": 0.07701009511947632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.82489013671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009607076644897461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974588394165039,
      "backward_entropy": 0.07700991630554199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.94062042236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009694207459688187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974186658859253,
      "backward_entropy": 0.07696747779846191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.96641540527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00977658573538065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097378134727478,
      "backward_entropy": 0.0769663651784261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.638427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00986019428819418,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10973352193832397,
      "backward_entropy": 0.07700938648647732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.0043487548828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009941867552697659,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10972906351089477,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.03131103515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010023677721619606,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10972440242767334,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.10743713378906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010107726790010929,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10971945524215698,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.19337463378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010188490152359009,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10971440076828003,
      "backward_entropy": 0.07696077558729383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.81997680664062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010266490280628204,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970923900604249,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.87225341796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01034644152969122,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10970377922058105,
      "backward_entropy": 0.07700792286131117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.3971405029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010424776934087276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969815254211426,
      "backward_entropy": 0.07700767781999376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.21775817871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01050176378339529,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10969234704971313,
      "backward_entropy": 0.07695535156461927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.76756286621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010578788816928864,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10968632698059082,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.646240234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01065452303737402,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10968012809753418,
      "backward_entropy": 0.07695195409986708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.70753479003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010733276605606079,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967355966567993,
      "backward_entropy": 0.0769503911336263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.94715881347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010812774300575256,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966671705245971,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.33599853515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010894080623984337,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10965955257415771,
      "backward_entropy": 0.07694711287816365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.58514404296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010979048907756805,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10965194702148437,
      "backward_entropy": 0.07694549693001641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.4234619140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011061953380703926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10964419841766357,
      "backward_entropy": 0.07700623406304254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.06874084472656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011142817325890064,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10963628292083741,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.86181640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011225104331970215,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10962803363800049,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.36880493164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011306965723633766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10961954593658448,
      "backward_entropy": 0.07693874835968018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.561767578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01139222364872694,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1096105694770813,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.8649444580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011480881832540035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10960111618041993,
      "backward_entropy": 0.07700500223371717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.69635009765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011570172384381294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10959131717681884,
      "backward_entropy": 0.077004697587755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0116627998650074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958099365234375,
      "backward_entropy": 0.07700449890560573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.58477783203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01175376120954752,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095704436302185,
      "backward_entropy": 0.0769302248954773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.79156494140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011844182386994362,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10955961942672729,
      "backward_entropy": 0.07692827118767633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.0617218017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011934500187635422,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10954844951629639,
      "backward_entropy": 0.07700396908654107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.55918884277344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012027525343000889,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1095367431640625,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.82070922851562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012119992636144161,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10952472686767578,
      "backward_entropy": 0.07701634698443943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.64111328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012208512052893639,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10951263904571533,
      "backward_entropy": 0.07691943645477295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.8721160888672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012298834510147572,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10950003862380982,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.56712341308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012385619804263115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10948736667633056,
      "backward_entropy": 0.07700300216674805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.38319396972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012475980445742607,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10947405099868775,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.39256286621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012564645148813725,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10946048498153686,
      "backward_entropy": 0.07690948910183376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.98504638671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01265101507306099,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10944674015045167,
      "backward_entropy": 0.07700253857506646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.83622741699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012736395932734013,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10943270921707153,
      "backward_entropy": 0.07700239949756199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.91177368164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01281646452844143,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10941908359527588,
      "backward_entropy": 0.07700228028827244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.95367431640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012895520776510239,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10940519571304322,
      "backward_entropy": 0.07701633373896281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.13665771484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012976296246051788,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1093907356262207,
      "backward_entropy": 0.07700164450539483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.77574157714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013060715980827808,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10937553644180298,
      "backward_entropy": 0.07700122065014309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.48406982421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013148673810064793,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935966968536377,
      "backward_entropy": 0.07701634698443943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.89044189453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013237717561423779,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10934329032897949,
      "backward_entropy": 0.07700045241249932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.61907958984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013328593224287033,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1093263030052185,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.78245544433594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013419017195701599,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10930894613265991,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 256.44281005859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013512077741324902,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1092908263206482,
      "backward_entropy": 0.07688086562686497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.72679138183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01360869500786066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10927186012268067,
      "backward_entropy": 0.07687862714131673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.10557556152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01370398048311472,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1092525839805603,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.30673217773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013799592852592468,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10923280715942382,
      "backward_entropy": 0.07687412367926703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.96682739257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013892332091927528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10921297073364258,
      "backward_entropy": 0.07699739270740086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.90615844726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013981121592223644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10919314622879028,
      "backward_entropy": 0.07686805725097656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.84571838378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014071880839765072,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10917261838912964,
      "backward_entropy": 0.07686488495932685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.28559112548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014163454063236713,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10915160179138184,
      "backward_entropy": 0.07699628671010335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.80210876464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014249105006456375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10913095474243165,
      "backward_entropy": 0.0769960085550944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.40626525878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01433914341032505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10910923480987549,
      "backward_entropy": 0.07699567741817898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.09632873535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014427758753299713,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10908721685409546,
      "backward_entropy": 0.07699533965852526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.8300323486328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014519414864480495,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10906451940536499,
      "backward_entropy": 0.07701631387074788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.59384155273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014610552228987217,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10904157161712646,
      "backward_entropy": 0.07699455155266656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.34918212890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014697903767228127,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10901870727539062,
      "backward_entropy": 0.07699414094289143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.12310791015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014782647602260113,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10899628400802612,
      "backward_entropy": 0.07699357138739692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.18896484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014866524375975132,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1089738130569458,
      "backward_entropy": 0.07699301507737902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.95083618164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01494914386421442,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10895097255706787,
      "backward_entropy": 0.07699229982164171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.78846740722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01503211073577404,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10892758369445801,
      "backward_entropy": 0.0768231021033393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.8966827392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015112029388546944,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10890421867370606,
      "backward_entropy": 0.07699068387349446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.34561157226562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015189407393336296,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10888081789016724,
      "backward_entropy": 0.07681466473473443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.57965850830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015269947238266468,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10885642766952515,
      "backward_entropy": 0.07698914739820692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.93809509277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015346483327448368,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10883221626281739,
      "backward_entropy": 0.07698829306496514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.8245086669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015421540476381779,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10880770683288574,
      "backward_entropy": 0.0768010483847724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.51393127441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015494064427912235,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10878312587738037,
      "backward_entropy": 0.07698618041144477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.83815002441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015564613044261932,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10875841379165649,
      "backward_entropy": 0.07679218053817749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.92845153808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015635518357157707,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10873312950134277,
      "backward_entropy": 0.07698372999827068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.50741577148438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015710338950157166,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1087066650390625,
      "backward_entropy": 0.0770163271162245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.16383361816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01578659377992153,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10867946147918701,
      "backward_entropy": 0.07698121998045179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.28826904296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015864945948123932,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10865123271942138,
      "backward_entropy": 0.0770163271162245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.314208984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015946559607982635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10862174034118652,
      "backward_entropy": 0.07697857750786676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.68415069580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016028929501771927,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10859154462814331,
      "backward_entropy": 0.07676431867811415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.96214294433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016104785725474358,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10856208801269532,
      "backward_entropy": 0.07697605424457127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.86305236816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01617816835641861,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10853255987167358,
      "backward_entropy": 0.07697447141011556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.62579345703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01625346764922142,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10850245952606201,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.05512237548828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016329094767570496,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10847179889678955,
      "backward_entropy": 0.0770163271162245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 253.17005920410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016399508342146873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10844151973724366,
      "backward_entropy": 0.07697052425808376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.78575134277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016476351767778397,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10840935707092285,
      "backward_entropy": 0.07696893480088976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.2270050048828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016551364213228226,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10837682485580444,
      "backward_entropy": 0.07701633373896281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.5384292602539,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016625337302684784,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10834395885467529,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.82122802734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016696322709321976,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10831131935119628,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.80836486816406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01676942966878414,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10827775001525879,
      "backward_entropy": 0.0770163271162245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.3137664794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016842011362314224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10824359655380249,
      "backward_entropy": 0.07669803831312391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.48333740234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016917385160923004,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10820796489715576,
      "backward_entropy": 0.07669072681003147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.59197235107422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016993043944239616,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10817158222198486,
      "backward_entropy": 0.07701630062527126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.474853515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017064016312360764,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10813567638397217,
      "backward_entropy": 0.07701630062527126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.36264038085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017137156799435616,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10809866189956666,
      "backward_entropy": 0.0766666332880656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.6304931640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017214179039001465,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10805991888046265,
      "backward_entropy": 0.07665827539232042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.6001434326172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017291244119405746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1080204725265503,
      "backward_entropy": 0.07694896724489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.4388427734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01737169921398163,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10797934532165528,
      "backward_entropy": 0.07694672213660346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.34024047851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017445746809244156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10793924331665039,
      "backward_entropy": 0.07694423198699951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.10462951660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01752137765288353,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10789803266525269,
      "backward_entropy": 0.07694171534644233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.30982971191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0175948403775692,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1078567385673523,
      "backward_entropy": 0.07661867141723633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.2314910888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01766800507903099,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10781502723693848,
      "backward_entropy": 0.07693690061569214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.77513122558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017746413126587868,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10777111053466797,
      "backward_entropy": 0.07701628075705634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.94842529296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017824359238147736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10772640705108642,
      "backward_entropy": 0.07693208588494195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.62234497070312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01790352538228035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10768059492111207,
      "backward_entropy": 0.07658184236950344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.01307678222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01797998696565628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1076348066329956,
      "backward_entropy": 0.07692638370725843,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.29173278808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018057534471154213,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10758779048919678,
      "backward_entropy": 0.07656534512837727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.2911376953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018136465921998024,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10753967761993408,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.04161071777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018210317939519882,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10749237537384033,
      "backward_entropy": 0.0769158403078715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.71055603027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018285011872649193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10744425058364868,
      "backward_entropy": 0.07691272099812825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.70425415039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01836243085563183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10739446878433227,
      "backward_entropy": 0.07690957519743177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.87127685546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018440140411257744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10734388828277588,
      "backward_entropy": 0.07690657509697808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.1026153564453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01851925253868103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10729212760925293,
      "backward_entropy": 0.0769032637278239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 217.7531280517578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018595697358250618,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10724035501480103,
      "backward_entropy": 0.07701634698443943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.86705017089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018676040694117546,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10718648433685303,
      "backward_entropy": 0.07701634698443943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.70448303222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018759580329060555,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10713057518005371,
      "backward_entropy": 0.07646698421902126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 234.04676818847656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018842710182070732,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10707401037216187,
      "backward_entropy": 0.07645384470621745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.76811981201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018930062651634216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.107015061378479,
      "backward_entropy": 0.07688360744052464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.4435577392578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019010042771697044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10695781707763671,
      "backward_entropy": 0.07642817497253418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.8328857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01908903755247593,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10690032243728638,
      "backward_entropy": 0.07687384552425808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.31137084960938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01916317082941532,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10684458017349244,
      "backward_entropy": 0.07639787594477336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.7024154663086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019236408174037933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10678834915161133,
      "backward_entropy": 0.07686375247107612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.31658935546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01930631883442402,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10673249959945678,
      "backward_entropy": 0.07685799068874782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.7373809814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01937088742852211,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10667800903320312,
      "backward_entropy": 0.07685133483674791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 217.39988708496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019434167072176933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10662310123443604,
      "backward_entropy": 0.0768437385559082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.8821258544922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019502585753798485,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10656524896621704,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.7242431640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019571667537093163,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.106506085395813,
      "backward_entropy": 0.07682763205634223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.6292266845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019642813131213188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10644519329071045,
      "backward_entropy": 0.07681888341903687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.64907836914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019715821370482445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10638262033462524,
      "backward_entropy": 0.0762928459379408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 247.0654296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019791653379797935,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10631792545318604,
      "backward_entropy": 0.07701627413431804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.62818908691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01987411081790924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10624983310699462,
      "backward_entropy": 0.07679288917117649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.73388671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019953928887844086,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10618219375610352,
      "backward_entropy": 0.07701629400253296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.6389923095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020033687353134155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1061137318611145,
      "backward_entropy": 0.07677682903077868,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.82029724121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020113399252295494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10604443550109863,
      "backward_entropy": 0.07676828569836086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 246.78155517578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02019442617893219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10597378015518188,
      "backward_entropy": 0.07676050398084852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.51336669921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02028145268559456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10589964389801025,
      "backward_entropy": 0.07675336466895209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.49325561523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020371105521917343,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10582325458526612,
      "backward_entropy": 0.07674498028225368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.85719299316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020461050793528557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10574589967727661,
      "backward_entropy": 0.0767364501953125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.77210998535156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02054741233587265,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10566928386688232,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.12103271484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020630549639463425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10559327602386474,
      "backward_entropy": 0.07671586672465007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.3773193359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020713001489639282,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10551643371582031,
      "backward_entropy": 0.0760951042175293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.33465576171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02079007774591446,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10544133186340332,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.04888916015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020868651568889618,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10536459684371949,
      "backward_entropy": 0.07667721642388238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.90232849121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02094772644340992,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10528702735900879,
      "backward_entropy": 0.07666509019003974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.0831527709961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021027902141213417,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10520777702331544,
      "backward_entropy": 0.07665197054545085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.4647979736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021102413535118103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10513012409210205,
      "backward_entropy": 0.07663614220089382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.8714599609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021178295835852623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10505048036575318,
      "backward_entropy": 0.07661901579962836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.56268310546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021253501996397972,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10496999025344848,
      "backward_entropy": 0.07660226689444648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.57786560058594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021327901631593704,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1048888921737671,
      "backward_entropy": 0.07701626088884142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.75047302246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02140243910253048,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10480633974075318,
      "backward_entropy": 0.0759522517522176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.79299926757812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021479832008481026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10472111701965332,
      "backward_entropy": 0.07654653655158149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.75930786132812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021557480096817017,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10463480949401856,
      "backward_entropy": 0.07701614167955187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.6768035888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02163621596992016,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10454659461975098,
      "backward_entropy": 0.07650697231292725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.35467529296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02171391248703003,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1044581413269043,
      "backward_entropy": 0.07648656103346083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.60809326171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02179047465324402,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10436931848526002,
      "backward_entropy": 0.07701603571573894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.26278686523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021868456155061722,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10427855253219605,
      "backward_entropy": 0.07585565249125163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.44369506835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021950528025627136,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10418462753295898,
      "backward_entropy": 0.07583688365088569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.4466552734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022031789645552635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10408966541290283,
      "backward_entropy": 0.07639560434553358,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.30703735351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02211420238018036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10399311780929565,
      "backward_entropy": 0.07637072934044732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.3417510986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022198501974344254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10389418601989746,
      "backward_entropy": 0.07634400659137303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.850830078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022283343598246574,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10379375219345092,
      "backward_entropy": 0.07701575756072998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.63040161132812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0223744735121727,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10368810892105103,
      "backward_entropy": 0.07577004035313924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.19886779785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02245919220149517,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10358529090881348,
      "backward_entropy": 0.0757602055867513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.16600036621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022542385384440422,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10348215103149414,
      "backward_entropy": 0.0770154529147678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.7433624267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022627482190728188,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10337648391723633,
      "backward_entropy": 0.07573466830783421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.70774841308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02270943857729435,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10327175855636597,
      "backward_entropy": 0.07572301228841145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.46743774414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022793734446167946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1031645655632019,
      "backward_entropy": 0.0761210388607449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.79859924316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022879626601934433,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10305485725402833,
      "backward_entropy": 0.0760846667819553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.63485717773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02296219766139984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1029462456703186,
      "backward_entropy": 0.07604565885331896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.11746215820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023043163120746613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10283771753311158,
      "backward_entropy": 0.0760052998860677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.1549072265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02312033250927925,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10273101329803466,
      "backward_entropy": 0.07565852006276448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.980224609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02319623902440071,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10262376070022583,
      "backward_entropy": 0.07701477077272204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.54871368408203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023269938305020332,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1025170087814331,
      "backward_entropy": 0.07701455222235785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.98159790039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023339277133345604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10241262912750244,
      "backward_entropy": 0.0758255057864719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.30186462402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023406194522976875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10230947732925415,
      "backward_entropy": 0.07561067740122478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.05699157714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023473000153899193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10220515727996826,
      "backward_entropy": 0.07559759749306573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.4387969970703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02354617789387703,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10209518671035767,
      "backward_entropy": 0.07558082871966892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.91685485839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023618515580892563,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10198451280593872,
      "backward_entropy": 0.07556585470835368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.72422790527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023691285401582718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10187209844589233,
      "backward_entropy": 0.07557299402025011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.58224487304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023761769756674767,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1017599105834961,
      "backward_entropy": 0.07551330327987671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.33995056152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023828331381082535,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10165024995803833,
      "backward_entropy": 0.07701357205708821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.94444274902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02390257641673088,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10153390169143676,
      "backward_entropy": 0.07551942931281196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.22870635986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0239764004945755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1014173150062561,
      "backward_entropy": 0.0753394365310669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.70474243164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024047065526247025,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10130237340927124,
      "backward_entropy": 0.0754829380247328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.62185668945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02411845326423645,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10118557214736938,
      "backward_entropy": 0.07521696885426839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.94097900390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0241854190826416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10107123851776123,
      "backward_entropy": 0.07514918512768215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.30418395996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024253899231553078,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10095500946044922,
      "backward_entropy": 0.0754413472281562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.45050048828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02432096004486084,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10083895921707153,
      "backward_entropy": 0.07701370451185438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.27505493164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024386953562498093,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10072309970855713,
      "backward_entropy": 0.07494030396143596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.78804016113281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024458011612296104,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1006018877029419,
      "backward_entropy": 0.07701358530256483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.56452178955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02452479675412178,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1004833698272705,
      "backward_entropy": 0.07538518640730116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.34735107421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0245896615087986,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10036644935607911,
      "backward_entropy": 0.07536538441975911,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.50076293945312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024653587490320206,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10024945735931397,
      "backward_entropy": 0.07534526454077826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.50843811035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024716880172491074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10013244152069092,
      "backward_entropy": 0.07457056310441759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.35775756835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024780601263046265,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10001399517059326,
      "backward_entropy": 0.07530138227674696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.97531127929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024843409657478333,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09989536404609681,
      "backward_entropy": 0.07441256443659465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.11524963378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024907801300287247,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0997740387916565,
      "backward_entropy": 0.07432867421044244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.63121032714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02497122436761856,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0996525764465332,
      "backward_entropy": 0.07701348596149021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.93338012695312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02503388188779354,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09953104257583618,
      "backward_entropy": 0.07523130708270603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.56732177734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02509312517940998,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0994117259979248,
      "backward_entropy": 0.07405095630221897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.84556579589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025157134979963303,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09928727746009827,
      "backward_entropy": 0.07520087560017903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.42223358154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025221742689609528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09916176795959472,
      "backward_entropy": 0.07386014196607801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.90802764892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02528122253715992,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09903984069824219,
      "backward_entropy": 0.07375588681962755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.24595642089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02533649653196335,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0989213764667511,
      "backward_entropy": 0.07701196935441759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.14877319335938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02539174072444439,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09880202412605285,
      "backward_entropy": 0.07701154549916585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.933837890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02545079216361046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09867812395095825,
      "backward_entropy": 0.07512818442450629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.70915985107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025509754195809364,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09855402708053589,
      "backward_entropy": 0.07511057456334432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.53411865234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025566810742020607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.098430597782135,
      "backward_entropy": 0.07319381501939562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.31706237792969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025625303387641907,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09830556511878967,
      "backward_entropy": 0.07507777214050293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.310546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025677448138594627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0981871485710144,
      "backward_entropy": 0.07295848263634576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.47288513183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025731224566698074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09806621074676514,
      "backward_entropy": 0.07283722029791938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.77008056640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025786681100726128,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09794312715530396,
      "backward_entropy": 0.07271573278639051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.3071060180664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0258429404348135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09781755208969116,
      "backward_entropy": 0.07500396834479438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.50714874267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025893690064549446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09769662618637084,
      "backward_entropy": 0.07244686947928534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.77496337890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02594378963112831,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0975759744644165,
      "backward_entropy": 0.0749786032570733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.78057861328125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025998497381806374,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09745023250579835,
      "backward_entropy": 0.07700792286131117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.00169372558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026059763506054878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0973173201084137,
      "backward_entropy": 0.07203606764475505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.74191284179688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026123400777578354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09718204736709594,
      "backward_entropy": 0.07190477848052979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.22180938720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026186442002654076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09704713225364685,
      "backward_entropy": 0.071770621670617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.82675170898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02624599076807499,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09691495895385742,
      "backward_entropy": 0.0748627053366767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.31527709960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026306482031941414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09678124189376831,
      "backward_entropy": 0.0714825259314643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.18473815917969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02636394463479519,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09665025472640991,
      "backward_entropy": 0.0748208694987827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.73491668701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026420218870043755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09652036428451538,
      "backward_entropy": 0.07479976283179389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.017822265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026475224643945694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09639136791229248,
      "backward_entropy": 0.07477838463253444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.76648712158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026526520028710365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09626604318618774,
      "backward_entropy": 0.07085094186994764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.49610900878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02657848596572876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09613959789276123,
      "backward_entropy": 0.07473372088538276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.90493774414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02663351781666279,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09600934982299805,
      "backward_entropy": 0.0705201890733507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.43244171142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026692334562540054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0958740770816803,
      "backward_entropy": 0.07034928268856472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.97882843017578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02674955315887928,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09573999643325806,
      "backward_entropy": 0.07700931363635594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.5869903564453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026805737987160683,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09560735821723938,
      "backward_entropy": 0.07700955205493504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.65309143066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02686346136033535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0954729974269867,
      "backward_entropy": 0.06981925169626872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.40595245361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026917124167084694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09534264802932739,
      "backward_entropy": 0.07458513312869602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.22042846679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026971057057380676,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09521148204803467,
      "backward_entropy": 0.07456272178226048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.2576904296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02702760510146618,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09507646560668945,
      "backward_entropy": 0.07700975735982259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.44252014160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027085578069090843,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09493973851203918,
      "backward_entropy": 0.06904305352105035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.228515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027148516848683357,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09479724764823913,
      "backward_entropy": 0.06884166267183092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.94601440429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027212148532271385,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09465385675430298,
      "backward_entropy": 0.07448946767383152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.12858581542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02727934718132019,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09450747966766357,
      "backward_entropy": 0.06843990749782985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.90264129638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02734558656811714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09436171650886535,
      "backward_entropy": 0.07443706194559734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.42741394042969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02740868739783764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09422111511230469,
      "backward_entropy": 0.07440817356109619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.9925537109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02747105062007904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09408122301101685,
      "backward_entropy": 0.06782317161560059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.09205627441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027534283697605133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09394038915634155,
      "backward_entropy": 0.06760854853524102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.93887329101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027599338442087173,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09379709362983704,
      "backward_entropy": 0.07433174716101752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.12956237792969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027659451588988304,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09365867972373962,
      "backward_entropy": 0.07431325647566053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.59668731689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02771790139377117,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09352179765701293,
      "backward_entropy": 0.07429567972819011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.32212829589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027774665504693985,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09338605403900146,
      "backward_entropy": 0.07428275876575047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.40699768066406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02783563919365406,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0932462453842163,
      "backward_entropy": 0.07700890964931911,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.54408264160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027900302782654762,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09310280084609986,
      "backward_entropy": 0.06617154015435113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.97171020507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02796676941215992,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09295732975006103,
      "backward_entropy": 0.06592546568976508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.78079223632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028032490983605385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09281316995620728,
      "backward_entropy": 0.06567807992299397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.10858154296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028093328699469566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09267420768737793,
      "backward_entropy": 0.06541866726345485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.5960693359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02815522812306881,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09253437519073486,
      "backward_entropy": 0.06515555249320136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.25511932373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028219889849424362,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09239287972450257,
      "backward_entropy": 0.06490888860490587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.69493865966797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02827843464910984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09225786924362182,
      "backward_entropy": 0.06464509169260661,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.28494262695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028333012014627457,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09212738871574402,
      "backward_entropy": 0.06437376472685072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.47449493408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02838411182165146,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09200106859207154,
      "backward_entropy": 0.07404417461819118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.29960632324219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028436122462153435,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0918741226196289,
      "backward_entropy": 0.0638215806749132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.94294738769531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028485890477895737,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09174910783767701,
      "backward_entropy": 0.06353548500272962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.64923858642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028532670810818672,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09162746667861939,
      "backward_entropy": 0.07395764191945393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.74589538574219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028579557314515114,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09150622487068176,
      "backward_entropy": 0.07700887653562757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.06072235107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02862444892525673,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09138581156730652,
      "backward_entropy": 0.07389977905485365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.05554962158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028668388724327087,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09126731157302856,
      "backward_entropy": 0.062364565001593694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.10369110107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02870974875986576,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09115147590637207,
      "backward_entropy": 0.06205868721008301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.80270385742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028749069198966026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09103844165802003,
      "backward_entropy": 0.06175155109829373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.4798812866211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028787635266780853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09092628955841064,
      "backward_entropy": 0.07377635108100043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.15054321289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028822902590036392,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09081783294677734,
      "backward_entropy": 0.0737482574250963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.46746826171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02886957675218582,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09069601893424988,
      "backward_entropy": 0.06079941325717502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.53740692138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028918739408254623,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09057161808013917,
      "backward_entropy": 0.07368433475494385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.6462631225586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028966104611754417,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09044920206069947,
      "backward_entropy": 0.07365188995997111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.3913803100586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029014738276600838,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09032596349716186,
      "backward_entropy": 0.0736153523127238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.20974731445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029062902554869652,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09020304679870605,
      "backward_entropy": 0.07358196046617296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.68852996826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02911326102912426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09007750749588013,
      "backward_entropy": 0.059159530533684626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.3662338256836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029161818325519562,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08995420336723328,
      "backward_entropy": 0.058819724453820124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.07369995117188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02921011485159397,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08983159065246582,
      "backward_entropy": 0.05847528907987806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.35414123535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029260791838169098,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08970673084259033,
      "backward_entropy": 0.058131111992730036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.88418579101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029313793405890465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08958024978637695,
      "backward_entropy": 0.05778984228769938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.92880249023438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029367372393608093,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08945350646972657,
      "backward_entropy": 0.07338474194208781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.21107482910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029420476406812668,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08932849168777465,
      "backward_entropy": 0.0733418001068963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.5875244140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02947642281651497,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08920012712478638,
      "backward_entropy": 0.07330852084689671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.83036804199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02953166700899601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08907355666160584,
      "backward_entropy": 0.05638111299938626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.15924072265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0295870378613472,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08894611597061157,
      "backward_entropy": 0.05601511398951212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.71488952636719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029646722599864006,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08881509304046631,
      "backward_entropy": 0.07319788138071696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.19823455810547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029700184240937233,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08869264125823975,
      "backward_entropy": 0.07315203216340807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.5700912475586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029753969982266426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08856959342956543,
      "backward_entropy": 0.05491532882054647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.1453857421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029805775731801987,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08844929933547974,
      "backward_entropy": 0.07307603624131945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.57054901123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029859518632292747,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08832666873931885,
      "backward_entropy": 0.07304039266374376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.62627410888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029913585633039474,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08820359706878662,
      "backward_entropy": 0.07300982210371229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.3680877685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029965825378894806,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08808383345603943,
      "backward_entropy": 0.07297088040245904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.5149688720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030019661411643028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08796160221099854,
      "backward_entropy": 0.052983727720048696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.03434753417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030076686292886734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08783681392669677,
      "backward_entropy": 0.05258274409506056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.81718444824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030133983120322227,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08771253824234009,
      "backward_entropy": 0.05218210816383362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.3801498413086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030191533267498016,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08758881688117981,
      "backward_entropy": 0.0728522539138794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.162841796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030249396339058876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08746583461761474,
      "backward_entropy": 0.05137359102567037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.52871704101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030307546257972717,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08734359741210937,
      "backward_entropy": 0.07277561558617486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.69982147216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030364617705345154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08722330331802368,
      "backward_entropy": 0.05056609710057577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.80350494384766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030423564836382866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08710296154022217,
      "backward_entropy": 0.05017267333136664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.88201904296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030478516593575478,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08698651790618897,
      "backward_entropy": 0.04976404375500149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.20478439331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03054043836891651,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08686345219612121,
      "backward_entropy": 0.04936013619105021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.4036979675293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030595608055591583,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08674825429916382,
      "backward_entropy": 0.07256346278720432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.52306365966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030646054074168205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08663855791091919,
      "backward_entropy": 0.0725218719906277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.07636260986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030698688700795174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08652673959732056,
      "backward_entropy": 0.04810758431752523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.26366424560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030749419704079628,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08641716837882996,
      "backward_entropy": 0.07244541909959581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.60706329345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030802618712186813,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08630658388137817,
      "backward_entropy": 0.07239846388498943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.16405487060547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030856776982545853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0861964464187622,
      "backward_entropy": 0.07234455479515924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.74144744873047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03090905025601387,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08608906269073487,
      "backward_entropy": 0.04645357860459222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.83187866210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030963586643338203,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08598052263259888,
      "backward_entropy": 0.07223134570651585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.1166763305664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03101480007171631,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0858758270740509,
      "backward_entropy": 0.07217579417758518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.87896728515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03106706030666828,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08577072620391846,
      "backward_entropy": 0.0721178650856018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.92137145996094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.031116103753447533,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08566888570785522,
      "backward_entropy": 0.07701157199011908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.2179183959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031160172075033188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08557388186454773,
      "backward_entropy": 0.04441726870006985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.3472671508789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031204748898744583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08547909259796142,
      "backward_entropy": 0.04400867223739624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.40265655517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031250059604644775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0853851854801178,
      "backward_entropy": 0.04361643393834432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.963623046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03129692003130913,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08528990745544433,
      "backward_entropy": 0.04321661591529846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.97756958007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03134295716881752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08519705533981323,
      "backward_entropy": 0.042828920814726085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.9822006225586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03138953447341919,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0851051926612854,
      "backward_entropy": 0.042448209391699895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.4175033569336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03143385797739029,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08501616716384888,
      "backward_entropy": 0.07151138782501221,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.8040542602539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03147859126329422,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08492684364318848,
      "backward_entropy": 0.0714250538084242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.79476928710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03152255341410637,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08483900427818299,
      "backward_entropy": 0.04128604465060764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.84398651123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03156576678156853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0847521185874939,
      "backward_entropy": 0.07125136587354872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.240177154541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031609535217285156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0846646010875702,
      "backward_entropy": 0.0404954817559984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.22572326660156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031648363918066025,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08458113670349121,
      "backward_entropy": 0.07111650043063694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.53146362304688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03169216215610504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08449345827102661,
      "backward_entropy": 0.07105627324846056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.1798095703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03174061328172684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08440296649932862,
      "backward_entropy": 0.039262466960483126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.33563995361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03179076686501503,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08431270122528076,
      "backward_entropy": 0.0388755202293396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.07280731201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031843557953834534,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08422097563743591,
      "backward_entropy": 0.07081552346547444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.97089385986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03189333528280258,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08413220643997192,
      "backward_entropy": 0.03809007008870443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.59034729003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03194575011730194,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08404150605201721,
      "backward_entropy": 0.03769217928250631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.34394454956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03200181573629379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08394844532012939,
      "backward_entropy": 0.03729517923461066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.40180969238281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03205222263932228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08386182188987731,
      "backward_entropy": 0.07048504882388645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.94711303710938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032099898904561996,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0837776780128479,
      "backward_entropy": 0.07701483037736681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.88896179199219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03215045481920242,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0836922585964203,
      "backward_entropy": 0.07032667265997992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.32048034667969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03220116347074509,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08360779285430908,
      "backward_entropy": 0.070238815413581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.56645202636719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03225334361195564,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08352323174476624,
      "backward_entropy": 0.07013948758443196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.33268737792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03230665624141693,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08343802094459533,
      "backward_entropy": 0.0349515246020423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.94310760498047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032357107847929,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0833558201789856,
      "backward_entropy": 0.07701516813702053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.33171844482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03240509331226349,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08327649235725403,
      "backward_entropy": 0.06984457704755995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.219966888427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03245219588279724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08319891691207885,
      "backward_entropy": 0.06973368591732448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.83722686767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03249580040574074,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08312501907348632,
      "backward_entropy": 0.0696209536658393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.49712371826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03254126384854317,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08304872512817382,
      "backward_entropy": 0.033047394620047674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.37664794921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032587360590696335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08297286033630372,
      "backward_entropy": 0.032664358615875244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.20135498046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03263137862086296,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0828991949558258,
      "backward_entropy": 0.0693343546655443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.32759857177734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03266940265893936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08282999396324157,
      "backward_entropy": 0.06925143135918511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.00885772705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032709091901779175,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0827614426612854,
      "backward_entropy": 0.03150723377863566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.38509750366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032747332006692886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08269400596618652,
      "backward_entropy": 0.031126274002922907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.256168365478516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03278299421072006,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08262951374053955,
      "backward_entropy": 0.07701516151428223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.83562469482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03281634673476219,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08256723880767822,
      "backward_entropy": 0.06883203983306885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.48728942871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032854270190000534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08250164985656738,
      "backward_entropy": 0.02999872300359938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.00592803955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032890938222408295,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0824366569519043,
      "backward_entropy": 0.06861852275000678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.66380310058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03292664512991905,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08237338662147523,
      "backward_entropy": 0.06850096914503309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.71493530273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03296403959393501,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0823095440864563,
      "backward_entropy": 0.06838112407260472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.88694763183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03300042450428009,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08224735260009766,
      "backward_entropy": 0.028540819883346558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.9356689453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.033037204295396805,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08218576908111572,
      "backward_entropy": 0.07701549927393596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.16608428955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03307424858212471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08212404251098633,
      "backward_entropy": 0.02783726652463277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.05825805664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03311418741941452,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08206047415733338,
      "backward_entropy": 0.027482996384302776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.3534698486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03315809741616249,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08199460506439209,
      "backward_entropy": 0.02713706758287218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.846256256103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033209480345249176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08192434310913085,
      "backward_entropy": 0.026803645822736952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.1142578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033255741000175476,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.081857568025589,
      "backward_entropy": 0.06750117407904731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.72349548339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033302705734968185,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08179075717926025,
      "backward_entropy": 0.026110377576616075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.74823760986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03335040435194969,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08172453641891479,
      "backward_entropy": 0.025769811537530687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.1918716430664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03340000659227371,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08165768384933472,
      "backward_entropy": 0.02543666462103526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.16761016845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03345015272498131,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0815920352935791,
      "backward_entropy": 0.0670099589559767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.9205093383789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03349938616156578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08152725100517273,
      "backward_entropy": 0.06687082184685601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.70219612121582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033549144864082336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08146289587020875,
      "backward_entropy": 0.06672720776663886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.3490219116211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033592890948057175,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08140406608581544,
      "backward_entropy": 0.06657499074935913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.2792739868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03364164009690285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08134275078773498,
      "backward_entropy": 0.023869223064846463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.92276000976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033694785088300705,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08127861022949219,
      "backward_entropy": 0.06627404027514988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.83708953857422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033749427646398544,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08121439218521118,
      "backward_entropy": 0.06612143251630995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.325483322143555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03380410745739937,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08115077018737793,
      "backward_entropy": 0.0229728188779619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.88486099243164,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03385232016444206,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08109240531921387,
      "backward_entropy": 0.07701634698443943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.22447967529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033897340297698975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08103707432746887,
      "backward_entropy": 0.022382014327579074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.32685089111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03394336998462677,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08098106980323791,
      "backward_entropy": 0.022091218166881137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.77242279052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03398897498846054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08092472553253174,
      "backward_entropy": 0.021791120370229084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.88229370117188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034035574644804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08086845874786378,
      "backward_entropy": 0.02150286899672614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.41094207763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03408175706863403,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08081265091896057,
      "backward_entropy": 0.06501907772488064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.43803405761719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03412375599145889,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08076118230819702,
      "backward_entropy": 0.06485006544325086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.28164672851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03416586294770241,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08071045279502868,
      "backward_entropy": 0.020652305748727586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.796714782714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03420807048678398,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08066017627716064,
      "backward_entropy": 0.020378440618515015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.377197265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03424779698252678,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08061217069625855,
      "backward_entropy": 0.06429257657792833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.41576385498047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03428655117750168,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08056402206420898,
      "backward_entropy": 0.06411910719341701,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.0529670715332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03432704135775566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08051431179046631,
      "backward_entropy": 0.01955153875880771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.96536254882812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03436652570962906,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08046529293060303,
      "backward_entropy": 0.0638109180662367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.23845672607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03440772742033005,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08041538000106811,
      "backward_entropy": 0.06365566121207343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.98239517211914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03444919362664223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08036521673202515,
      "backward_entropy": 0.018717788987689547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.890865325927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034488365054130554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08031809329986572,
      "backward_entropy": 0.01845701535542806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.03912353515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03452546149492264,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08027271032333375,
      "backward_entropy": 0.01820174190733168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.85253143310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03456969931721687,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08022353649139405,
      "backward_entropy": 0.06294023990631104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.5803451538086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03461780771613121,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08017203211784363,
      "backward_entropy": 0.06276305516560872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.29875183105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034669410437345505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0801183819770813,
      "backward_entropy": 0.017458147472805448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.1766471862793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0347241647541523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08006376028060913,
      "backward_entropy": 0.017220934232076008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.383121490478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03477538749575615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08001118898391724,
      "backward_entropy": 0.01697527865568797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.5178451538086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03482472151517868,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07996083498001098,
      "backward_entropy": 0.016738472713364497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.89685821533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03487365320324898,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07991079688072204,
      "backward_entropy": 0.016504362225532532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.1160888671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034924786537885666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07985941171646119,
      "backward_entropy": 0.016272036565674677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.14525604248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03497537598013878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980878353118896,
      "backward_entropy": 0.0160389244556427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.32536697387695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03502676635980606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07975736856460572,
      "backward_entropy": 0.01580696139070723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.57343292236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035075053572654724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07970880270004273,
      "backward_entropy": 0.06120716200934516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.69306182861328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03512309864163399,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07966189980506896,
      "backward_entropy": 0.06100847986009386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.914066314697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035165876150131226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07961819171905518,
      "backward_entropy": 0.015138167474004958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.71601486206055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03520645573735237,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07957648634910583,
      "backward_entropy": 0.06061284409628974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.0549087524414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03524511680006981,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07953528165817261,
      "backward_entropy": 0.06042506297429403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.84919738769531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03528840094804764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07949095964431763,
      "backward_entropy": 0.06024404366811117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.45438003540039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03533712401986122,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07944363355636597,
      "backward_entropy": 0.01427712208694882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.28598022460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03538437932729721,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07939822673797607,
      "backward_entropy": 0.014076016015476651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.08830642700195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03543414920568466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07935196161270142,
      "backward_entropy": 0.01388071146276262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.09860610961914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03548114746809006,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07930790185928345,
      "backward_entropy": 0.059458381599850126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.46142578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035526808351278305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07926692962646484,
      "backward_entropy": 0.01350875778330697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.43467712402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0355764664709568,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07922381162643433,
      "backward_entropy": 0.013331468734476302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.19253540039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03562718257308006,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07918031215667724,
      "backward_entropy": 0.013160016801622178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.06973266601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035680048167705536,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07913655042648315,
      "backward_entropy": 0.012999618219004737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.96244812011719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.035732466727495193,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07909342050552368,
      "backward_entropy": 0.0770149098502265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.45713806152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03578443452715874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07905112504959107,
      "backward_entropy": 0.012674171891477373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.16871643066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03583734482526779,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07900832891464234,
      "backward_entropy": 0.012512062986691793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.45108032226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03589116036891937,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07896400690078735,
      "backward_entropy": 0.01234798464510176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.93643951416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03594828024506569,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07891720533370972,
      "backward_entropy": 0.01218430780702167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.07579040527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03600574657320976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07887156009674072,
      "backward_entropy": 0.012030018700493706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.44173431396484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036062248051166534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07882781624794007,
      "backward_entropy": 0.011884383029407926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.24661254882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03611920401453972,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07878469228744507,
      "backward_entropy": 0.011743463575839996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.26004028320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03617655113339424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07874212265014649,
      "backward_entropy": 0.01160996655623118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.82366180419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03623313456773758,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07870012521743774,
      "backward_entropy": 0.011473119258880615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.043521881103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036290138959884644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0786590814590454,
      "backward_entropy": 0.011344464288817512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.8464584350586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03634374961256981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07862161993980407,
      "backward_entropy": 0.01122497684425778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.49270629882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03639686480164528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0785851776599884,
      "backward_entropy": 0.011109169158670638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.47304916381836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036456990987062454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07854496240615845,
      "backward_entropy": 0.010995671153068542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.808204650878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036513566970825195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07850754261016846,
      "backward_entropy": 0.010881581240230136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.60292053222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036568157374858856,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07847163081169128,
      "backward_entropy": 0.010772479905022515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.70742416381836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03662100434303284,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07843716740608216,
      "backward_entropy": 0.05344290203518338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.843650817871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03667360916733742,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07840267419815064,
      "backward_entropy": 0.05308300919002957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.078041076660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03672352805733681,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07836965322494507,
      "backward_entropy": 0.010454278025362227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.508323669433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036772239953279495,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07833765745162964,
      "backward_entropy": 0.010349015394846598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.28712463378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0368172712624073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07830935716629028,
      "backward_entropy": 0.01024948474433687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.21396255493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03686407953500748,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07827947139739991,
      "backward_entropy": 0.010151810944080353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.39199447631836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036908939480781555,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07824995517730712,
      "backward_entropy": 0.051223821110195585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.53237915039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036953218281269073,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07822048068046569,
      "backward_entropy": 0.009943366878562503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.91584396362305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03699951991438866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07818883657455444,
      "backward_entropy": 0.009836129016346402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.32377624511719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03704391419887543,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07815817594528199,
      "backward_entropy": 0.0502451393339369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.754615783691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037090178579092026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07812641263008117,
      "backward_entropy": 0.049916340245140925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.937049865722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03713582083582878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07809531688690186,
      "backward_entropy": 0.009526671634780036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.531639099121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03718192130327225,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07806488275527954,
      "backward_entropy": 0.009432142807377709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.45433044433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03722737729549408,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0780348539352417,
      "backward_entropy": 0.048891948329077825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.31305694580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03727595880627632,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07800274491310119,
      "backward_entropy": 0.009246162242359586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.01300811767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03732605651021004,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07796997427940369,
      "backward_entropy": 0.048219508594936795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.988525390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03738119453191757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07793444395065308,
      "backward_entropy": 0.009067907101578183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.703269958496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03743593767285347,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07789983153343201,
      "backward_entropy": 0.04755428764555189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.784732818603516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037489280104637146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0778659462928772,
      "backward_entropy": 0.008899932106335958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.9276180267334,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03754002973437309,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0778343677520752,
      "backward_entropy": 0.00881922162241406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.49324035644531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03758601099252701,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07780655622482299,
      "backward_entropy": 0.008741538557741377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.75670623779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037630192935466766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07778040766716003,
      "backward_entropy": 0.04609559310807122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.982051849365234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03767647594213486,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07775219678878784,
      "backward_entropy": 0.008593767881393433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.98048400878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03772223740816116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0777246356010437,
      "backward_entropy": 0.04537043968836466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.86224365234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03776641935110092,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07769720554351807,
      "backward_entropy": 0.008438980413807763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.01483917236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03780919685959816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07767013311386109,
      "backward_entropy": 0.008360381755563948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.47994613647461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03785422816872597,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07764195203781128,
      "backward_entropy": 0.044369386302100286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.52648162841797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03789884224534035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07761424779891968,
      "backward_entropy": 0.044021023644341364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.38086700439453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03794926032423973,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0775823712348938,
      "backward_entropy": 0.04369820488823785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.8039608001709,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03800474479794502,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07754766941070557,
      "backward_entropy": 0.04337170388963488,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.37084197998047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0380563922226429,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07751618027687072,
      "backward_entropy": 0.008004675308863321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.3858642578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038105711340904236,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07748737335205078,
      "backward_entropy": 0.007944234543376498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.00978469848633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03815782815217972,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07745640277862549,
      "backward_entropy": 0.007886557115448846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.29682540893555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03820771351456642,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07742748260498047,
      "backward_entropy": 0.007829928563700782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.860652923583984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03825690224766731,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07739865779876709,
      "backward_entropy": 0.007770657539367676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.34616088867188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03830406069755554,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0773716390132904,
      "backward_entropy": 0.04115540782610575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.172513961791992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03835665434598923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07734172344207764,
      "backward_entropy": 0.007662987543476952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.134620666503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038405727595090866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07731488943099976,
      "backward_entropy": 0.040393021371629506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.33724975585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03845537081360817,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07728732824325561,
      "backward_entropy": 0.0400138431125217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.748430252075195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03850671276450157,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07725822925567627,
      "backward_entropy": 0.039645350641674466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.14105987548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03855485841631889,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07723122835159302,
      "backward_entropy": 0.007457297709253099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.504459381103516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038609642535448074,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07719951272010803,
      "backward_entropy": 0.03893673751089308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.926795959472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038664352148771286,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07716808319091797,
      "backward_entropy": 0.03858538799815708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.099143981933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03871799260377884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0771368384361267,
      "backward_entropy": 0.0073038554853863185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.49016189575195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03877173364162445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07710554599761962,
      "backward_entropy": 0.03789853718545702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.46257019042969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038823243230581284,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07707610130310058,
      "backward_entropy": 0.0072093937132093645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.1946792602539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03887403756380081,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07704682350158691,
      "backward_entropy": 0.007162539495362176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.17881774902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038931213319301605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0770124077796936,
      "backward_entropy": 0.007115351657072703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.825172424316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038987088948488235,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07697883248329163,
      "backward_entropy": 0.0070690736174583435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.975521087646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0390394851565361,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07694741487503051,
      "backward_entropy": 0.00702282041311264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.6336612701416,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03909093141555786,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07691725492477416,
      "backward_entropy": 0.006980233722262912,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.58399963378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039139363914728165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07688865661621094,
      "backward_entropy": 0.006938314686218898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.45325469970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03918622061610222,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07686140537261962,
      "backward_entropy": 0.006897076550457213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.26972198486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039230551570653915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07683585286140442,
      "backward_entropy": 0.006853582130538093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.084415435791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03927827626466751,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07680833339691162,
      "backward_entropy": 0.006814014166593552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.05879211425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0393279530107975,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07677918672561646,
      "backward_entropy": 0.0342691871854994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.88971710205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03937709331512451,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07675063610076904,
      "backward_entropy": 0.03393707672754923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.4881477355957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0394258014857769,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07672228813171386,
      "backward_entropy": 0.006705932319164276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.19517517089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039476390928030014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07669267654418946,
      "backward_entropy": 0.006671923316187329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.62076187133789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039528824388980865,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07666112184524536,
      "backward_entropy": 0.07700532012515598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.67611694335938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03957941755652428,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07663055658340454,
      "backward_entropy": 0.032663053936428495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.23308181762695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039632830768823624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07659831047058105,
      "backward_entropy": 0.006568936010201772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.57609748840332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03968530148267746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07656731009483338,
      "backward_entropy": 0.032023999426099986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.63062286376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039734747260808945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07653846740722656,
      "backward_entropy": 0.006511093427737554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.368885040283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03978835418820381,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07650643587112427,
      "backward_entropy": 0.006482713752322727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.25137710571289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03983888402581215,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07647638320922852,
      "backward_entropy": 0.03107069929440816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.631099700927734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039889972656965256,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07644631266593933,
      "backward_entropy": 0.0770021743244595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.67439651489258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03994280472397804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07641451358795166,
      "backward_entropy": 0.006399696071942647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.981687545776367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03999381139874458,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07638396024703979,
      "backward_entropy": 0.030146992868847318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.894350051879883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04004208371043205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07635513544082642,
      "backward_entropy": 0.029855416880713567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.80381774902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040087904781103134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07632755041122437,
      "backward_entropy": 0.0063110561006599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.9887466430664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04013383388519287,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07629855871200561,
      "backward_entropy": 0.02929795119497511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.5257682800293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040185656398534775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07626461982727051,
      "backward_entropy": 0.0062489426798290676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.23310470581055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040237002074718475,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0762302815914154,
      "backward_entropy": 0.028827369213104248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.28201675415039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04029018059372902,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07619431614875793,
      "backward_entropy": 0.0061870333221223615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.76588439941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04034266993403435,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07615897059440613,
      "backward_entropy": 0.028362423181533813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.6485595703125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04039342328906059,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07612500190734864,
      "backward_entropy": 0.07700909508599176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.46186828613281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040446050465106964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07608933448791504,
      "backward_entropy": 0.00610385172896915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.682342529296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04050033912062645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07605226039886474,
      "backward_entropy": 0.027641584475835163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.79302978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04055389016866684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0760156273841858,
      "backward_entropy": 0.006054355452458064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.26557159423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04060790687799454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0759784460067749,
      "backward_entropy": 0.0060306572251849705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.882604598999023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04066576808691025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07593767642974854,
      "backward_entropy": 0.0060061994526121355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.23924255371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04072020575404167,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07590012550354004,
      "backward_entropy": 0.005984369665384293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.29680633544922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04077506437897682,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0758620023727417,
      "backward_entropy": 0.07701108853022258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.75121307373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04083263501524925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07582048177719117,
      "backward_entropy": 0.005939627273215188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.89954376220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04088924080133438,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07577922344207763,
      "backward_entropy": 0.026048229800330266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.5020866394043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040948182344436646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.075736403465271,
      "backward_entropy": 0.005894603828589122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.329952239990234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0410083532333374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07569169998168945,
      "backward_entropy": 0.025640116797553167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.14067077636719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041066039353609085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07564937472343444,
      "backward_entropy": 0.00585115866528617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.131044387817383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041122738271951675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07560749650001526,
      "backward_entropy": 0.005831199801630444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.984861373901367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04117515683174133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07556936740875245,
      "backward_entropy": 0.005812709944115745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.610050201416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0412248931825161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07553333044052124,
      "backward_entropy": 0.005794785916805267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.56219482421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04127557948231697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07549591660499573,
      "backward_entropy": 0.005776381327046288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.69017028808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04132602736353874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07545816898345947,
      "backward_entropy": 0.005757681611511443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.32527160644531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04138178750872612,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07541518211364746,
      "backward_entropy": 0.005737960338592529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.54035186767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041436679661273956,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0753731369972229,
      "backward_entropy": 0.005720631943808662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.482038497924805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041488710790872574,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07533326745033264,
      "backward_entropy": 0.023824516269895766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.12761688232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041538093239068985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07529581785202026,
      "backward_entropy": 0.005686778161260817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.030031204223633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041586246341466904,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07525920867919922,
      "backward_entropy": 0.02343497673670451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.950761795043945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04163327440619469,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07522348165512086,
      "backward_entropy": 0.005655668675899506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.839109420776367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041679222136735916,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0751889705657959,
      "backward_entropy": 0.023041205273734197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.90597534179688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04172423109412193,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07515537738800049,
      "backward_entropy": 0.005630167822043101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.61278533935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04177405685186386,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07511624097824096,
      "backward_entropy": 0.022656323181258306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.66865158081055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04182261973619461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0750780999660492,
      "backward_entropy": 0.005600868413845698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.0931510925293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04187232255935669,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07503827214241028,
      "backward_entropy": 0.022304539879163105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.213959693908691,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04192408546805382,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07499587535858154,
      "backward_entropy": 0.00557112486826049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.15351676940918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04197219759225845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07495720386505127,
      "backward_entropy": 0.005558018055227067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.54379653930664,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04201934114098549,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07491888999938964,
      "backward_entropy": 0.07700604200363159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.33895492553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04206669703125954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07487930059432983,
      "backward_entropy": 0.005530763003561232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.7438850402832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042116448283195496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07483682036399841,
      "backward_entropy": 0.005515659848848979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.5579948425293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042167168110609055,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07479315996170044,
      "backward_entropy": 0.02136255469587114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.635648727416992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042218852788209915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07474780678749085,
      "backward_entropy": 0.005486373272207048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.516626358032227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04226918891072273,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07470356225967408,
      "backward_entropy": 0.00547223620944553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.42081069946289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04231836274266243,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07465969324111939,
      "backward_entropy": 0.005457965036233266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.927947998046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04236970841884613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07461326122283936,
      "backward_entropy": 0.005443724493185679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.643848419189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042421914637088776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.074565589427948,
      "backward_entropy": 0.0054303184151649475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.362491607666016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042470574378967285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07452134490013122,
      "backward_entropy": 0.005417923960420821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.46501922607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04251925274729729,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07447689771652222,
      "backward_entropy": 0.020474407407972548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.081085205078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04256901890039444,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07443108558654785,
      "backward_entropy": 0.02034843961397807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.765031814575195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04261878877878189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07438461780548096,
      "backward_entropy": 0.005384601652622223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.37216567993164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04266751930117607,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07433845996856689,
      "backward_entropy": 0.020115405321121216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.078758239746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04271309822797775,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07429544925689698,
      "backward_entropy": 0.02000040974881914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.69978332519531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04276234656572342,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07424783706665039,
      "backward_entropy": 0.019896460904015437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.308759689331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04281260818243027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07419899702072144,
      "backward_entropy": 0.005339647746748394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.272550582885742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04286058247089386,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0741527259349823,
      "backward_entropy": 0.019672923617892794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.19358825683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04290762543678284,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07410714626312256,
      "backward_entropy": 0.005322247743606567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.06322479248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042954932898283005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07406074404716492,
      "backward_entropy": 0.005314454436302185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.896236419677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043000344187021255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07401611208915711,
      "backward_entropy": 0.0053070709109306335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.763912200927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043049320578575134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07396711111068725,
      "backward_entropy": 0.005299628608756595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.529109954833984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043099481612443924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07391602993011474,
      "backward_entropy": 0.005291174683305953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.46397399902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04315179958939552,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07386164665222168,
      "backward_entropy": 0.01903712749481201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.687713623046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043204862624406815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07380616664886475,
      "backward_entropy": 0.018947059909502666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.137447357177734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04325537011027336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07375374436378479,
      "backward_entropy": 0.00526541057560179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.7925910949707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04330682009458542,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07369989156723022,
      "backward_entropy": 0.005258664902713563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.446542739868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04336020350456238,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07364321947097778,
      "backward_entropy": 0.00525159595741166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.583171844482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04341098666191101,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07358982563018798,
      "backward_entropy": 0.01855946249432034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.7369384765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04345846176147461,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07354011535644531,
      "backward_entropy": 0.005242215262518989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.78084945678711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04351029172539711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07348479032516479,
      "backward_entropy": 0.005237331406937705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.48624038696289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043564971536397934,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07342582941055298,
      "backward_entropy": 0.005232532819112142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.36188507080078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04361910745501518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07336708307266235,
      "backward_entropy": 0.005228085650338067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.572912216186523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043672651052474976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07330909967422486,
      "backward_entropy": 0.005225071890486611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.8937873840332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043724723160266876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07325247526168824,
      "backward_entropy": 0.005222326351536645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.81964111328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04377959668636322,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07319201231002807,
      "backward_entropy": 0.0052194322148958845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.87755584716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043839070945978165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07312535047531128,
      "backward_entropy": 0.017779385050137837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.64968490600586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0438995398581028,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07305710315704346,
      "backward_entropy": 0.017689544293615553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.95467758178711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043960973620414734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07298696041107178,
      "backward_entropy": 0.0052075522641340894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.31425857543945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04402216896414757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07291666865348816,
      "backward_entropy": 0.0052039408021503026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.88568115234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04408210515975952,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0728476107120514,
      "backward_entropy": 0.017432093620300293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.805084228515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04413783550262451,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07278360724449158,
      "backward_entropy": 0.0051997337076399065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.855419158935547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044194914400577545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07271736860275269,
      "backward_entropy": 0.005198073883851369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.39570236206055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04425124078989029,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07265119552612305,
      "backward_entropy": 0.017169737153583102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.583219528198242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0443088598549366,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0725827157497406,
      "backward_entropy": 0.017088770866394043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.5418586730957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04436551779508591,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07251527309417724,
      "backward_entropy": 0.005192811290423076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.029918670654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04442547261714935,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07244277000427246,
      "backward_entropy": 0.005191135323709912,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.56072235107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0444832481443882,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07237273454666138,
      "backward_entropy": 0.005190275195572112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.9832820892334,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04454217106103897,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07230038046836854,
      "backward_entropy": 0.016764786508348253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.669950485229492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044600002467632294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07222913503646851,
      "backward_entropy": 0.005188727130492528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.81310272216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044655878096818924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07216006517410278,
      "backward_entropy": 0.0051890090107917786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.332063674926758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044712044298648834,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07208983302116394,
      "backward_entropy": 0.005189166300826603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.329906463623047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044765446335077286,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07202283143997193,
      "backward_entropy": 0.01644491155942281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.123671531677246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04481732100248337,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07195749282836914,
      "backward_entropy": 0.005190731750594245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.15099334716797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044865816831588745,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07189653515815735,
      "backward_entropy": 0.005192489259772831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.01416778564453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044914234429597855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07183529734611512,
      "backward_entropy": 0.005194938017262353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.79378890991211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0449686236679554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07176485061645507,
      "backward_entropy": 0.005196521265639199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.906085014343262,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045025378465652466,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0716902494430542,
      "backward_entropy": 0.016054618689748976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.600622177124023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04507828503847122,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07162082791328431,
      "backward_entropy": 0.0051992009911272265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.799051284790039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045130688697099686,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07155173420906066,
      "backward_entropy": 0.015908413463168673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.9289436340332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04517965763807297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0714871883392334,
      "backward_entropy": 0.005203408499558766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.044864654541016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04523148015141487,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07141780853271484,
      "backward_entropy": 0.005205346064435111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.690101623535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045283861458301544,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07134699821472168,
      "backward_entropy": 0.005207604418198268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.92784881591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045337751507759094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07127339243888856,
      "backward_entropy": 0.005209525840149986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.531218528747559,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045391060411930084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07119982242584229,
      "backward_entropy": 0.00521117283238305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.2047061920166,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045440830290317535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07113128900527954,
      "backward_entropy": 0.005213644769456651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.5382137298584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04548843950033188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07106549739837646,
      "backward_entropy": 0.0052163004875183105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.10622024536133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045536089688539505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0709987998008728,
      "backward_entropy": 0.005218475229210324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.60403060913086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04558464512228966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07093034982681275,
      "backward_entropy": 0.005221077551444371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.16553497314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04563505947589874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0708582878112793,
      "backward_entropy": 0.005223174062040117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.827001571655273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04568516090512276,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07078620195388793,
      "backward_entropy": 0.015155740910106234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.90691375732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04573304206132889,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07071707248687745,
      "backward_entropy": 0.0052280595733059775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.78985595703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045780912041664124,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.070647132396698,
      "backward_entropy": 0.005230063365565406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.6674747467041,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045828692615032196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07057689428329468,
      "backward_entropy": 0.005232370148102443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.034833908081055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04587637633085251,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0705064058303833,
      "backward_entropy": 0.005235107408629524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.450645446777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04592303931713104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07043702602386474,
      "backward_entropy": 0.005237979607449638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.301380157470703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04596788436174393,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07036972045898438,
      "backward_entropy": 0.005240522738960054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.754802703857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046012964099645615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07030149698257446,
      "backward_entropy": 0.005242889953984154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.48321533203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04605723172426224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07023419141769409,
      "backward_entropy": 0.014672867125935025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.4053544998168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04610271751880646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07016434073448181,
      "backward_entropy": 0.005247626867559221,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.842247009277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04614444077014923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07010077238082886,
      "backward_entropy": 0.005251155545314153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.703886032104492,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.046186693012714386,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07003557682037354,
      "backward_entropy": 0.07701273759206136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.982098579406738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04622647911310196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06997438669204711,
      "backward_entropy": 0.005258650415473514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.530885696411133,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.046265047043561935,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06991481184959411,
      "backward_entropy": 0.07701215479109022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.704059600830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04630439355969429,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06985346078872681,
      "backward_entropy": 0.005267242590586345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.31395721435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04634546861052513,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06978817582130432,
      "backward_entropy": 0.00527076381776068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.68393325805664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046387139707803726,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06972123384475708,
      "backward_entropy": 0.014201752013630338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.092992782592773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046431250870227814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0696491003036499,
      "backward_entropy": 0.005275601314173805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.397233009338379,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0464756079018116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06957600116729737,
      "backward_entropy": 0.014103867941432528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.867467880249023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046517327427864075,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06950730085372925,
      "backward_entropy": 0.014054429199960496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.75726318359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04655953124165535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06943703889846801,
      "backward_entropy": 0.005282039443651835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.771516799926758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046602167189121246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06936540603637695,
      "backward_entropy": 0.013960303531752693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.431447982788086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04664615914225578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06929062604904175,
      "backward_entropy": 0.013916436168882582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.417627334594727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046689387410879135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06921699643135071,
      "backward_entropy": 0.013869718545013003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.59823989868164,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04673294350504875,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06914219856262208,
      "backward_entropy": 0.077013717757331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.144062042236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0467815101146698,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06905730962753295,
      "backward_entropy": 0.013784197469552359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.045677185058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046828918159008026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06897401809692383,
      "backward_entropy": 0.0052919193274445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.936601638793945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046875275671482086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06889216899871826,
      "backward_entropy": 0.005293437176280552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.814603805541992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04692159593105316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06880989670753479,
      "backward_entropy": 0.0052953023049566485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.884380340576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046967897564172745,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06872702836990356,
      "backward_entropy": 0.00529732389582528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.49712371826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047011349350214005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0686493158340454,
      "backward_entropy": 0.00530008143848843,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.031734466552734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04705597460269928,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0685687780380249,
      "backward_entropy": 0.01353872319062551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.94117736816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047104448080062866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0684801459312439,
      "backward_entropy": 0.013498612576060824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.208457946777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047155413776636124,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06838620901107788,
      "backward_entropy": 0.013458555771244897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.89306640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047205861657857895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06829268932342529,
      "backward_entropy": 0.013418769670857323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.372231483459473,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047256745398044586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0681977093219757,
      "backward_entropy": 0.005313405560122596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.297952651977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04730524122714996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06810701489448548,
      "backward_entropy": 0.005316882497734494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.444791793823242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047351568937301636,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06802018880844116,
      "backward_entropy": 0.013294506404134963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.437604904174805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04739866033196449,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0679314911365509,
      "backward_entropy": 0.005325614992115233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.155698776245117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047442805022001266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06784836053848267,
      "backward_entropy": 0.005330779900153478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.35163116455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04748798906803131,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06776230335235596,
      "backward_entropy": 0.013158780005243089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.586423873901367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04753312095999718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06767593026161194,
      "backward_entropy": 0.005341041419241164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.49899673461914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04757733643054962,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06759077906608582,
      "backward_entropy": 0.005346434811751048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.001991271972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04762065410614014,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06750730872154236,
      "backward_entropy": 0.013019654485914443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.318422317504883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047664158046245575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06742258071899414,
      "backward_entropy": 0.0053578027420573765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.889404296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04770682379603386,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06733930110931396,
      "backward_entropy": 0.005363728437158797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.133268356323242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04775150120258331,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06725101470947266,
      "backward_entropy": 0.005369194265868928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.54901885986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047795262187719345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.067164146900177,
      "backward_entropy": 0.0053745342625512015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.89710235595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047839123755693436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06707629561424255,
      "backward_entropy": 0.005379503385888206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.934693336486816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04788569360971451,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0669822096824646,
      "backward_entropy": 0.005384222914775212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.76360321044922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04792928323149681,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06689443588256835,
      "backward_entropy": 0.07701280381944445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.093584060668945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04797203838825226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06680785417556763,
      "backward_entropy": 0.005395116905371348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.37616729736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04801489785313606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06672054529190063,
      "backward_entropy": 0.005400631990697648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.381944179534912,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04805873706936836,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06663058400154113,
      "backward_entropy": 0.005405934320555793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.46100616455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04809897020459175,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06654843091964721,
      "backward_entropy": 0.005412004888057709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.65040397644043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04814135655760765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0664607048034668,
      "backward_entropy": 0.0054173461265034145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.37663269042969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048183877021074295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06637213230133057,
      "backward_entropy": 0.005422455983029472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.69805908203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048231832683086395,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06627079248428344,
      "backward_entropy": 0.0054262785447968375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.31144332885742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04828021302819252,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06616792678833008,
      "backward_entropy": 0.005429827504687839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.931299209594727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04833168163895607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06605736017227173,
      "backward_entropy": 0.005432287024127113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.825267791748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048381391912698746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0659504473209381,
      "backward_entropy": 0.012262680464320712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.080244064331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04842956364154816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06584659218788147,
      "backward_entropy": 0.005439440409342448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.779020309448242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0484781488776207,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06574099063873291,
      "backward_entropy": 0.005442904101477729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.779888153076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04852616414427757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06563620567321778,
      "backward_entropy": 0.005446694377395842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.422706604003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048574596643447876,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06552945971488952,
      "backward_entropy": 0.005449908061159981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.804931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048621632158756256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06542527675628662,
      "backward_entropy": 0.005453300972779592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.275270462036133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04867260530591011,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06531117558479309,
      "backward_entropy": 0.005456093284818862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.096172332763672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0487227700650692,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06519824266433716,
      "backward_entropy": 0.005459148436784744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.017504692077637,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04877038300037384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06509131193161011,
      "backward_entropy": 0.005463409754965041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.965818881988525,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04881576821208,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06498914957046509,
      "backward_entropy": 0.005468145426776674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.79926300048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04885830357670784,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06489322781562805,
      "backward_entropy": 0.005473204371001985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.751968383789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048900801688432693,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06479681134223939,
      "backward_entropy": 0.005478608939382765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.66699504852295,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04894242808222771,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06470191478729248,
      "backward_entropy": 0.005484152585268021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.802716255187988,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048983242362737656,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06460848450660706,
      "backward_entropy": 0.005489937547180388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.371065139770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04902150109410286,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06452149152755737,
      "backward_entropy": 0.005497116595506668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.277009963989258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04906022176146507,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06443222761154174,
      "backward_entropy": 0.005503416061401367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.176103591918945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04909925535321236,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06434153318405152,
      "backward_entropy": 0.005509663787153032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.447220802307129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04913856461644173,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06424960494041443,
      "backward_entropy": 0.005515820450252957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.388434410095215,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0491764210164547,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06416076421737671,
      "backward_entropy": 0.07701305548350017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.556921482086182,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04921296611428261,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06407473087310792,
      "backward_entropy": 0.011487755510542128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.552444458007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04924747720360756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06399345397949219,
      "backward_entropy": 0.005534950229856703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.698055267333984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049283478409051895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06390801072120667,
      "backward_entropy": 0.011402834620740678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.32567024230957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0493200309574604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06382038593292236,
      "backward_entropy": 0.005547621183925205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.203004837036133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04935786500573158,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06372926235198975,
      "backward_entropy": 0.005553507556517919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.082481384277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04939691722393036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0636340320110321,
      "backward_entropy": 0.005558520555496216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.983880043029785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04943704232573509,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06353521347045898,
      "backward_entropy": 0.005562888665331734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.75419998168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04947558417916298,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06344017982482911,
      "backward_entropy": 0.005567455457316505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.2395405769348145,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049517735838890076,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06333494186401367,
      "backward_entropy": 0.011170620719591776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.5871524810791,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049557290971279144,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06323601007461548,
      "backward_entropy": 0.011136309968100654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.887168884277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04959780350327492,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06313408613204956,
      "backward_entropy": 0.005577954567141003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.6734037399292,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04963831976056099,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06303173303604126,
      "backward_entropy": 0.005581604937712352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.680007934570312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04967717081308365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0629335880279541,
      "backward_entropy": 0.005585818654961056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.095592498779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04971621185541153,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06283424496650696,
      "backward_entropy": 0.00558982499771648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.481538772583008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049756214022636414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06273186206817627,
      "backward_entropy": 0.005593773391511705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.903484344482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049796197563409805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0626293420791626,
      "backward_entropy": 0.005598336872127321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.275718688964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04983535408973694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06252883672714234,
      "backward_entropy": 0.010880363484223684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.877782821655273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04987458139657974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06242775321006775,
      "backward_entropy": 0.005608323961496353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.07923126220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04991137981414795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06233338713645935,
      "backward_entropy": 0.0056144433716932935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.58693790435791,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04994846507906914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06223783493041992,
      "backward_entropy": 0.005620753599537743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.384675979614258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04998501017689705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06214327812194824,
      "backward_entropy": 0.00562709445754687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.781875610351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05002520978450775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.062037378549575806,
      "backward_entropy": 0.005631827645831638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.347733497619629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050065383315086365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06193089485168457,
      "backward_entropy": 0.005636252462863922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.953644752502441,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05010465905070305,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.061826658248901364,
      "backward_entropy": 0.010591427485148111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.187677383422852,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05014229938387871,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06172702312469482,
      "backward_entropy": 0.010549950102965036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.938570022583008,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05017930641770363,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06162868738174439,
      "backward_entropy": 0.07701296938790216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.783464431762695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05021815001964569,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061524462699890134,
      "backward_entropy": 0.00565769440597958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.721619606018066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05025530606508255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06142544746398926,
      "backward_entropy": 0.0056644512547387015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.528820037841797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05029100179672241,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06133042573928833,
      "backward_entropy": 0.005671842644611995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.408351421356201,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050328727811574936,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06122834086418152,
      "backward_entropy": 0.005677247212992774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.551502227783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050364140421152115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061132955551147464,
      "backward_entropy": 0.005683287150330014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.334840774536133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05039827525615692,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06104093194007874,
      "backward_entropy": 0.005689530736870236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.885906219482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05043046921491623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060954296588897706,
      "backward_entropy": 0.005696083108584086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.522246360778809,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05046410486102104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06086280941963196,
      "backward_entropy": 0.005701935125721825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.566776275634766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05049746483564377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06077144145965576,
      "backward_entropy": 0.005707246975766288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.384405136108398,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05053134262561798,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06067800521850586,
      "backward_entropy": 0.005712129589584138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.62432289123535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05056489631533623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060584962368011475,
      "backward_entropy": 0.005716666993167665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.238285064697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0506012924015522,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06048281192779541,
      "backward_entropy": 0.0057202523781193625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.130289077758789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050637137144804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06038168668746948,
      "backward_entropy": 0.005723139064179527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.187328338623047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05067161098122597,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060284531116485594,
      "backward_entropy": 0.005726692163281971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.03178596496582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05070877820253372,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06017879843711853,
      "backward_entropy": 0.005729615274402831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.962043762207031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050746750086545944,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06007052659988403,
      "backward_entropy": 0.005732712232404285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.76493263244629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05078312009572983,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05996708869934082,
      "backward_entropy": 0.005736548039648268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.738844871520996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05082118511199951,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05985793471336365,
      "backward_entropy": 0.005739823397662904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.637906074523926,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05085919052362442,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05974866151809692,
      "backward_entropy": 0.005743367804421319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.633262634277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05089719593524933,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05963870882987976,
      "backward_entropy": 0.005746413022279739,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.564096450805664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050934430211782455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059530383348464964,
      "backward_entropy": 0.0057490625315242344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.356311798095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05097086355090141,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05942450761795044,
      "backward_entropy": 0.009634876416789161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.112903594970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051007382571697235,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05931789875030517,
      "backward_entropy": 0.005756235784954495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.663330078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051044762134552,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059208035469055176,
      "backward_entropy": 0.005759465197722117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.691844940185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051084425300359726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05909044742584228,
      "backward_entropy": 0.005762082421117359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.381936073303223,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051125358790159225,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058968371152877806,
      "backward_entropy": 0.0057645117243131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.7259521484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05116438493132591,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05885196328163147,
      "backward_entropy": 0.005767477055390676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.762503623962402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051206957548856735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05872412919998169,
      "backward_entropy": 0.005770124081108306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.384986877441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05124891176819801,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058597993850708005,
      "backward_entropy": 0.005773615092039108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.844841957092285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051291074603796005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05847103595733642,
      "backward_entropy": 0.005777420269118415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.82374382019043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05133187025785446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058348429203033444,
      "backward_entropy": 0.005782057013776567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.012556076049805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05137370899319649,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05822209119796753,
      "backward_entropy": 0.005786826213200887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.297567367553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05141569301486015,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058095026016235354,
      "backward_entropy": 0.005792181524965499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.142375946044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05145490914583206,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05797637104988098,
      "backward_entropy": 0.005797588576873143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.439556121826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05149377882480621,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05785863995552063,
      "backward_entropy": 0.009161800146102905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.188909530639648,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05153157189488411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05774413347244263,
      "backward_entropy": 0.005810158948103587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.293556213378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051566898822784424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05763776302337646,
      "backward_entropy": 0.005817915416426129,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.22050666809082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051601484417915344,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0575337290763855,
      "backward_entropy": 0.005826081666681502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.620145797729492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05163544788956642,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05743107795715332,
      "backward_entropy": 0.0058338795271184705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.609167098999023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05166805535554886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057332676649093625,
      "backward_entropy": 0.005842139323552449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.023446083068848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05170092359185219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05723296403884888,
      "backward_entropy": 0.0058500754336516065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.420873641967773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051733292639255524,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057134485244750975,
      "backward_entropy": 0.005857795890834596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.362046241760254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05176737532019615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057029718160629274,
      "backward_entropy": 0.005864449259307649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.919445514678955,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05180152878165245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05692437291145325,
      "backward_entropy": 0.005870894011523988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.883832931518555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05183353275060654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056826448440551756,
      "backward_entropy": 0.005878220829698775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.38538932800293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0518636479973793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056734228134155275,
      "backward_entropy": 0.005885251694255405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.635785102844238,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05189640820026398,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05663255453109741,
      "backward_entropy": 0.005890483243597878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.960049629211426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05192859098315239,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05653262734413147,
      "backward_entropy": 0.005895993361870448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.128933906555176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05196098983287811,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05643165111541748,
      "backward_entropy": 0.005901186002625359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.440033912658691,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051992181688547134,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05633416771888733,
      "backward_entropy": 0.008524927000204722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.0364532470703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05202300846576691,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05623728036880493,
      "backward_entropy": 0.008488141828113131,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.663682460784912,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05205277353525162,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05614357590675354,
      "backward_entropy": 0.008452417949835459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.951024055480957,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052080873399972916,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05605514049530029,
      "backward_entropy": 0.07700839307573107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.810574531555176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05210816115140915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05596915483474731,
      "backward_entropy": 0.005919660131136577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.585376739501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0521368682384491,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05587754249572754,
      "backward_entropy": 0.005921528985102971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.632268905639648,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05216895043849945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055773568153381345,
      "backward_entropy": 0.005921690000428094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.025788307189941,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052201926708221436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05566617250442505,
      "backward_entropy": 0.005921379559569889,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.72355318069458,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05223429948091507,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05556056499481201,
      "backward_entropy": 0.005921115063958698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.6787428855896,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05226542428135872,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.055459094047546384,
      "backward_entropy": 0.005921139071385066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.052474975585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05229540541768074,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.055361390113830566,
      "backward_entropy": 0.008211095299985673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.981886863708496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052325740456581116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05526228547096253,
      "backward_entropy": 0.00592247520883878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.368113994598389,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052356332540512085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0551624596118927,
      "backward_entropy": 0.005924274937974082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.341708660125732,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052385102957487106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05506906509399414,
      "backward_entropy": 0.005927098294099172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.91617202758789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05241220071911812,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05498175621032715,
      "backward_entropy": 0.005931204391850365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1467185020446777,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052440617233514786,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05488930940628052,
      "backward_entropy": 0.00804847561650806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.260491847991943,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0524667426943779,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05480501055717468,
      "backward_entropy": 0.005938972863886092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.3461761474609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05249146372079849,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054725813865661624,
      "backward_entropy": 0.005944164676798714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.311241149902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05251564085483551,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05464826822280884,
      "backward_entropy": 0.005949385050270293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.186927318572998,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05253930762410164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05457230806350708,
      "backward_entropy": 0.005954808245102565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.242699146270752,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05256183445453644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054500234127044675,
      "backward_entropy": 0.005960546433925629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.277753829956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05258400738239288,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05442922115325928,
      "backward_entropy": 0.005966324359178543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.178623676300049,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052606526762247086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05435681343078613,
      "backward_entropy": 0.0059718771113289725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.190094947814941,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05262865126132965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05428585410118103,
      "backward_entropy": 0.00597792449924681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.141948699951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05265110731124878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05421354174613953,
      "backward_entropy": 0.005983944982290268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.074061393737793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05267391353845596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05413937568664551,
      "backward_entropy": 0.005988864849011104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.070374488830566,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052696358412504196,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05406607985496521,
      "backward_entropy": 0.07700686984592015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.00801420211792,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05271972343325615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05398968458175659,
      "backward_entropy": 0.005997365133629905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.942214012145996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05274195969104767,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053917044401168825,
      "backward_entropy": 0.00600164963139428,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.80756187438965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05276588723063469,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053837716579437256,
      "backward_entropy": 0.006004216770331065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.899232387542725,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052793294191360474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05374553203582764,
      "backward_entropy": 0.006005423764387767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.817654609680176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05281985178589821,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05365602970123291,
      "backward_entropy": 0.006006283064683278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.759882926940918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052846215665340424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05356771945953369,
      "backward_entropy": 0.006008193724685245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.636579513549805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05287250876426697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05347905158996582,
      "backward_entropy": 0.006009152779976527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.661256790161133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05289936810731888,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05338797569274902,
      "backward_entropy": 0.006009515788820054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8090453147888184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05292603000998497,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053297650814056394,
      "backward_entropy": 0.006010335766606861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.449634552001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05295119062066078,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05321294069290161,
      "backward_entropy": 0.007333649529351128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7592318058013916,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05297696217894554,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05312595963478088,
      "backward_entropy": 0.07700969113243951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.323554992675781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05300132930278778,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053044086694717406,
      "backward_entropy": 0.006016340106725693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.120650291442871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053026460111141205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05295865535736084,
      "backward_entropy": 0.006017538822359509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.3646931648254395,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053052835166454315,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05286876559257507,
      "backward_entropy": 0.006018870406680637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.96795654296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053079042583703995,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05277935266494751,
      "backward_entropy": 0.006020337343215942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.703234672546387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0531064011156559,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05268541574478149,
      "backward_entropy": 0.0060215116375022465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.406199932098389,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05313543975353241,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0525850772857666,
      "backward_entropy": 0.006022122999032338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.582329750061035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05316340550780296,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05248845815658569,
      "backward_entropy": 0.006022880474726359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.329282760620117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0531897097826004,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05239822864532471,
      "backward_entropy": 0.006024658679962158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.583428382873535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053215205669403076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.052310729026794435,
      "backward_entropy": 0.00602653788195716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.756112098693848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053241830319166183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.052219176292419435,
      "backward_entropy": 0.006028823554515839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.218097686767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05326886102557182,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05212591290473938,
      "backward_entropy": 0.00603111046883795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.183387756347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05329497903585434,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05203588008880615,
      "backward_entropy": 0.006033683402670754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.8587751388549805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05332023650407791,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05194917917251587,
      "backward_entropy": 0.006037045684125688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.916199684143066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05334537848830223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051862800121307374,
      "backward_entropy": 0.006040421624978383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6957827806472778,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05337228626012802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05176981091499329,
      "backward_entropy": 0.00604356038901541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.039242267608643,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053397003561258316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05168500542640686,
      "backward_entropy": 0.0060473283131917315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6722251176834106,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05342099070549011,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05160294771194458,
      "backward_entropy": 0.006051546169651879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3221113681793213,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05344308540225029,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05152782201766968,
      "backward_entropy": 0.006783623662259843,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.298222541809082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05346401780843735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05145748853683472,
      "backward_entropy": 0.006061876399649514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.550734996795654,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05348397046327591,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051390576362609866,
      "backward_entropy": 0.00606795565949546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.513075351715088,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053504303097724915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05132201313972473,
      "backward_entropy": 0.00607329151696629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.093218803405762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053524959832429886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05125182271003723,
      "backward_entropy": 0.006077871140506532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.042187690734863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053546492010354996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05117840766906738,
      "backward_entropy": 0.006082300924592548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.796122074127197,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05356881394982338,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05110182762145996,
      "backward_entropy": 0.006086333758301205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.003674314124509692,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053590621799230576,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.051027095317840575,
      "backward_entropy": 0.006563670519325469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.04544448852539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05361016094684601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0509604811668396,
      "backward_entropy": 0.006093593107329475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.005283243488520384,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053631942719221115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05088487267494202,
      "backward_entropy": 0.006095235546429952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.121163845062256,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05365142971277237,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05081787705421448,
      "backward_entropy": 0.006097227334976196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.301515579223633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05367007851600647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0507537841796875,
      "backward_entropy": 0.006098938071065479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.083831787109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05369041487574577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05068295001983643,
      "backward_entropy": 0.006099220779207017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.130972385406494,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05370984598994255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050615173578262326,
      "backward_entropy": 0.006099011335108016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.572914123535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053729623556137085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050546151399612424,
      "backward_entropy": 0.006099012990792592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.109668731689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05374908819794655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05047844648361206,
      "backward_entropy": 0.006099603242344326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.5115790367126465,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05377131700515747,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05040011405944824,
      "backward_entropy": 0.006342997981442345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.962347984313965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053793054074048996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05032327175140381,
      "backward_entropy": 0.006098153690497081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.419816493988037,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053816117346286774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050241279602050784,
      "backward_entropy": 0.006096741805473964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.478156328201294,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05383970960974693,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050157558917999265,
      "backward_entropy": 0.0060961854954560595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.775511741638184,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053861409425735474,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05008121132850647,
      "backward_entropy": 0.006257729397879707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.812442302703857,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053884416818618774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049999698996543884,
      "backward_entropy": 0.006096586998966005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.008281979709863663,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0539073646068573,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04991848468780517,
      "backward_entropy": 0.006097123026847839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.45998477935791,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05392785370349884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04984712302684784,
      "backward_entropy": 0.006099318050675922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.268543720245361,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053950902074575424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04976591467857361,
      "backward_entropy": 0.006100944760772917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.883305549621582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053973324596881866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04968696236610413,
      "backward_entropy": 0.006102476269006729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.410102844238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05399753153324127,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04960089325904846,
      "backward_entropy": 0.006103153857919905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.561859130859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05402269586920738,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04951151013374329,
      "backward_entropy": 0.0061041683786445195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.518374919891357,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054047588258981705,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04942285120487213,
      "backward_entropy": 0.006074688086907069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.842289924621582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054072219878435135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0493351936340332,
      "backward_entropy": 0.006106206940280067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.78668212890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05409718304872513,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.049246197938919066,
      "backward_entropy": 0.006031566196017795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6959104537963867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054122451692819595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049155968427658084,
      "backward_entropy": 0.006108463638358646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.015213966369629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05414624512195587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04907140731811523,
      "backward_entropy": 0.006110094487667084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.626286506652832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054170988500118256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048983269929885866,
      "backward_entropy": 0.0061119116014904445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2586669921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05419600009918213,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04889416098594666,
      "backward_entropy": 0.006113396750556098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.913372755050659,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05422067642211914,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04880646467208862,
      "backward_entropy": 0.006115398473209805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.172923564910889,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054244477301836014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04872217178344727,
      "backward_entropy": 0.006118021491501067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.133409023284912,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05426806956529617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04863859415054321,
      "backward_entropy": 0.00612069418032964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.363080978393555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05429144203662872,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04855585694313049,
      "backward_entropy": 0.006123780376381344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5277891159057617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054315198212862015,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04847158789634705,
      "backward_entropy": 0.00612674281001091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.014686107635498,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05433760583400726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04839240908622742,
      "backward_entropy": 0.006129951940642463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.700225830078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05435986444354057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04831409454345703,
      "backward_entropy": 0.006133950418896145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.930249214172363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05438368767499924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04822980165481568,
      "backward_entropy": 0.006137613621022966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6711440086364746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05440729111433029,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04814595878124237,
      "backward_entropy": 0.006140545010566711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2157803773880005,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05443008616566658,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04806526899337769,
      "backward_entropy": 0.006143761177857717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.412828207015991,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05445108190178871,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04799124002456665,
      "backward_entropy": 0.006146879659758674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.170703411102295,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05447095260024071,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04792160391807556,
      "backward_entropy": 0.006150530444251167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3771374225616455,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054492052644491196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047847014665603635,
      "backward_entropy": 0.0061531029641628265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.714107513427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05451202392578125,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04777677655220032,
      "backward_entropy": 0.006156096855799357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.183144569396973,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05453206226229668,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04770634770393371,
      "backward_entropy": 0.006159164425399568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1701502799987793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05455382540822029,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047628939151763916,
      "backward_entropy": 0.006160898754994075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.609440326690674,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05457375943660736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047559261322021484,
      "backward_entropy": 0.006164648052718904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.578625202178955,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05459374561905861,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047489380836486815,
      "backward_entropy": 0.006168382035361396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4123635292053223,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05461371690034866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04741989374160767,
      "backward_entropy": 0.006172991047302882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.506420612335205,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05463310703635216,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047353106737136844,
      "backward_entropy": 0.006178917156325446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.708876132965088,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0546526201069355,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04728548526763916,
      "backward_entropy": 0.0054713090260823565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.442996025085449,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054673295468091965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04721313714981079,
      "backward_entropy": 0.006187371495697234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.406888961791992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05469387397170067,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047141313552856445,
      "backward_entropy": 0.006191404329405891,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 6.195782941246871,
    "avg_log_Z": -0.05357469599694013,
    "success_rate": 1.0,
    "avg_reward": 81.0,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.02,
      "1": 0.09,
      "2": 0.89
    },
    "avg_forward_entropy": 0.051045850098133096,
    "avg_backward_entropy": 0.0075303308541576075,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}