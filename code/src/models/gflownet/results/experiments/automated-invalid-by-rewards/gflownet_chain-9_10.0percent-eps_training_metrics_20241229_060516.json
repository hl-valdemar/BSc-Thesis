{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07622671789593166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.07626383834415013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.01551818847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10984036922454835,
      "backward_entropy": 0.0759566757414076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.5559539794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -9.999999747378752e-05,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10983785390853881,
      "backward_entropy": 0.07596082157558864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.359130859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0001993190380744636,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10983521938323974,
      "backward_entropy": 0.07624261909061009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.1497039794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0002991747169289738,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10983245372772217,
      "backward_entropy": 0.07626952065361871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.2065887451172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00039940245915204287,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10982952117919922,
      "backward_entropy": 0.07627153396606445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.00120544433594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0004975406336598098,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10982654094696045,
      "backward_entropy": 0.07626557350158691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.37583923339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.000595406221691519,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10982350111007691,
      "backward_entropy": 0.07598048448562622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.57435607910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.000693904294166714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10982033014297485,
      "backward_entropy": 0.07598425282372369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.31588745117188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.000793348765000701,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981699228286743,
      "backward_entropy": 0.07598799467086792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.48316955566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.000892073439899832,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1098135232925415,
      "backward_entropy": 0.07627974616156684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.2490997314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0009878822602331638,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980989933013915,
      "backward_entropy": 0.0759947035047743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.41763305664062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0010837274603545666,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1098060965538025,
      "backward_entropy": 0.07628217008378771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.37710571289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0011811708100140095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1098021388053894,
      "backward_entropy": 0.07600106795628865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.95790100097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0012773406924679875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10979804992675782,
      "backward_entropy": 0.07628450128767225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.51705932617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013712260406464338,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10979382991790772,
      "backward_entropy": 0.07628532250722249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.90713500976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0014632876263931394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978950262069702,
      "backward_entropy": 0.07600945896572536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.66754150390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0015558418817818165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978498458862304,
      "backward_entropy": 0.07601194911532932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.62899780273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0016478726174682379,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978032350540161,
      "backward_entropy": 0.0760143068101671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.96640014648438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0017413945170119405,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977544784545898,
      "backward_entropy": 0.07634260919358996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.61289978027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018370825564488769,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977036952972412,
      "backward_entropy": 0.07628800471623738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.13059997558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0019304411252960563,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097651481628418,
      "backward_entropy": 0.0762884881761339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.62101745605469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00202323985286057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10975980758666992,
      "backward_entropy": 0.07602420780393812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.70529174804688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0021085876505821943,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10975451469421386,
      "backward_entropy": 0.07628919018639459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.40504455566406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002193042542785406,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10974911451339722,
      "backward_entropy": 0.0763668417930603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.47095489501953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002280103974044323,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10974347591400146,
      "backward_entropy": 0.07637129889594184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.92189025878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002364888321608305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10973776578903198,
      "backward_entropy": 0.07603032059139675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.23838806152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0024523839820176363,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10973181724548339,
      "backward_entropy": 0.07638043165206909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.5076141357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002543296664953232,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10972561836242675,
      "backward_entropy": 0.07629063394334581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.88575744628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002634095260873437,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10971930027008056,
      "backward_entropy": 0.07629115713967217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.3272705078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002724711550399661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971286296844482,
      "backward_entropy": 0.0760384135776096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.82298278808594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0028136291075497866,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970625877380372,
      "backward_entropy": 0.07640089591344197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.9296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0029026276897639036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1096995234489441,
      "backward_entropy": 0.07604263888465033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.32196044921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002988661639392376,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969266891479493,
      "backward_entropy": 0.07641065120697021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.0287628173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003076164983212948,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10968557596206666,
      "backward_entropy": 0.07604613569047716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.84344482421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00316612864844501,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967820882797241,
      "backward_entropy": 0.0762932300567627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.46401977539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00325720920227468,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967063903808594,
      "backward_entropy": 0.07629377312130398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.20256042480469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003351666731759906,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10966280698776246,
      "backward_entropy": 0.0760527319378323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.2263946533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0034424352925270796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965486764907836,
      "backward_entropy": 0.07605471875932482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.15968322753906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0035326811484992504,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1096467137336731,
      "backward_entropy": 0.07644140720367432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.90058135986328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003623012686148286,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10963844060897827,
      "backward_entropy": 0.07644640074835883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.75254821777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0037104312796145678,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10963011980056762,
      "backward_entropy": 0.07629602485232884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.52403259277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003800440113991499,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10962147712707519,
      "backward_entropy": 0.07629633612102932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.00839233398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003892533713951707,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10961246490478516,
      "backward_entropy": 0.07629681295818752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.6074676513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003981750924140215,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10960378646850585,
      "backward_entropy": 0.07629698514938354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.58523559570312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004073881544172764,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10959490537643432,
      "backward_entropy": 0.07647075917985705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.85374450683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004165484569966793,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958582162857056,
      "backward_entropy": 0.07606852054595947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.7021484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004260500427335501,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10957636833190917,
      "backward_entropy": 0.07629820373323229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.09874725341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004359755665063858,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10956655740737915,
      "backward_entropy": 0.07629928323957655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.15884399414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004451964050531387,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10955682992935181,
      "backward_entropy": 0.07607428232828777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.68479919433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004542101640254259,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1095469355583191,
      "backward_entropy": 0.07607548104392158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.26194763183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004633796866983175,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953692197799683,
      "backward_entropy": 0.07630038923687404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.52397155761719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00472645740956068,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10952692031860352,
      "backward_entropy": 0.07630077997843425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.4259033203125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004811374004930258,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10951722860336303,
      "backward_entropy": 0.07650869422488743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.44416046142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0048954370431602,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10950746536254882,
      "backward_entropy": 0.07630015081829494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.64224243164062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00497573334723711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10949773788452148,
      "backward_entropy": 0.07607973946465386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.20378112792969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005057206377387047,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948784351348877,
      "backward_entropy": 0.0762988461388482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.18539428710938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005134917795658112,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1094779133796692,
      "backward_entropy": 0.07652346293131511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.08522033691406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005218227859586477,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10946757793426513,
      "backward_entropy": 0.07652729087405735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.9179229736328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005304752849042416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945680141448974,
      "backward_entropy": 0.07629721694522434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.57679748535156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005391691345721483,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10944585800170899,
      "backward_entropy": 0.07653526465098064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.97195434570312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005485710222274065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10943440198898316,
      "backward_entropy": 0.07629737589094374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.0162811279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0055718994699418545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10942325592041016,
      "backward_entropy": 0.07629715071784125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.0807647705078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005658704321831465,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10941195487976074,
      "backward_entropy": 0.0765476557943556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.99639892578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005749050993472338,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10940039157867432,
      "backward_entropy": 0.07609009742736816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.67344665527344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005840798374265432,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10938856601715088,
      "backward_entropy": 0.0765561925040351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.56112670898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0059310742653906345,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10937665700912476,
      "backward_entropy": 0.07629720369974773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.11972045898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006024094298481941,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10936435461044311,
      "backward_entropy": 0.07629772027333577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.84471130371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006119650322943926,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935170650482177,
      "backward_entropy": 0.07656917307111952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.9470672607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00621604360640049,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10933878421783447,
      "backward_entropy": 0.07629926999409993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.14111328125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006310349330306053,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10932579040527343,
      "backward_entropy": 0.07657766342163086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.03795623779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006404295563697815,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10931261777877807,
      "backward_entropy": 0.07610279983944362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.48719787597656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006493386812508106,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1092995285987854,
      "backward_entropy": 0.07658548487557305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.0355682373047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006583835929632187,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10928604602813721,
      "backward_entropy": 0.07658925321367052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.40853881835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0066743572242558,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10927234888076783,
      "backward_entropy": 0.07610563437143962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.17367553710938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006766100879758596,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10925828218460083,
      "backward_entropy": 0.07659678988986546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.89900207519531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006859282031655312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10924391746520996,
      "backward_entropy": 0.07610772715674506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.08477783203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006945635657757521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10922967195510865,
      "backward_entropy": 0.07610827022128636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.45848083496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0070334444753825665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1092149257659912,
      "backward_entropy": 0.07610865434010823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.21746826171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007121747359633446,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10920000076293945,
      "backward_entropy": 0.07630191246668498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.34829711914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007210265379399061,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10918481349945068,
      "backward_entropy": 0.07630190584394667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.93499755859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007302984595298767,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10916897058486938,
      "backward_entropy": 0.07630221048990886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.8522491455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007397068664431572,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10915284156799317,
      "backward_entropy": 0.07611122396257189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.41233825683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007494818419218063,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10913609266281128,
      "backward_entropy": 0.07611247566011217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.5997314453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007594151422381401,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10911879539489747,
      "backward_entropy": 0.07662897639804417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.52700805664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007693878374993801,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10910117626190186,
      "backward_entropy": 0.07611507177352905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.358642578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007785068824887276,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10908410549163819,
      "backward_entropy": 0.07663634088304308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.30279541015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00787407997995615,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1090667724609375,
      "backward_entropy": 0.07663955953386095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.54603576660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007963075302541256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10904914140701294,
      "backward_entropy": 0.07611572742462158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.49403381347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008056331425905228,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1090308427810669,
      "backward_entropy": 0.07611619101630317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.3180923461914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008153349161148071,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10901187658309937,
      "backward_entropy": 0.07630506489011976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.44642639160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008244866505265236,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10899311304092407,
      "backward_entropy": 0.07611719767252605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.10000610351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008339104242622852,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1089738130569458,
      "backward_entropy": 0.07611760165956286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.66432189941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008432752452790737,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10895427465438842,
      "backward_entropy": 0.07630502515368992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.69309997558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008525996468961239,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10893452167510986,
      "backward_entropy": 0.07630486620797051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.5054168701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008621674962341785,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089141845703125,
      "backward_entropy": 0.07630489269892375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.11814880371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00871802493929863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10889346599578857,
      "backward_entropy": 0.07611917124854194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.365478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008817809633910656,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10887203216552735,
      "backward_entropy": 0.07612007194095188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.50503540039062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008914983831346035,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10885057449340821,
      "backward_entropy": 0.07667740186055501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.70689392089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00900945346802473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10882892608642578,
      "backward_entropy": 0.07612056202358669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.38255310058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009104604832828045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1088068127632141,
      "backward_entropy": 0.07630529668596056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.48613739013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009201985783874989,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1087841272354126,
      "backward_entropy": 0.07630534966786702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.2763671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009295287542045116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10876150131225586,
      "backward_entropy": 0.0763050185309516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.00604248046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009391025640070438,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1087382674217224,
      "backward_entropy": 0.0766945415072971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.20134735107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009488699957728386,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10871435403823852,
      "backward_entropy": 0.07612074746025933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.62646484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00958262663334608,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10869061946868896,
      "backward_entropy": 0.07670133643680149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.2765655517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009677528403699398,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10866636037826538,
      "backward_entropy": 0.07612025737762451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.7430877685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00977492518723011,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10864150524139404,
      "backward_entropy": 0.07630324363708496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.86329650878906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009873153641819954,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10861629247665405,
      "backward_entropy": 0.07671165466308594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.3570785522461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009971775114536285,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10859061479568481,
      "backward_entropy": 0.0763032767507765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.27212524414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01006579864770174,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10856491327285767,
      "backward_entropy": 0.07630291250016955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.92633056640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010162645019590855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10853866338729859,
      "backward_entropy": 0.07611995273166233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.13392639160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01026026252657175,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10851197242736817,
      "backward_entropy": 0.07630274030897352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.94076538085938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010355344042181969,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10848512649536132,
      "backward_entropy": 0.0767268803384569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.65757751464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010454387404024601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10845743417739868,
      "backward_entropy": 0.07611934343973796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.01534271240234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010551825165748596,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10842928886413575,
      "backward_entropy": 0.07673313220342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.21473693847656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010641475208103657,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1084014892578125,
      "backward_entropy": 0.07630301846398248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.77163696289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010732175782322884,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10837295055389404,
      "backward_entropy": 0.076738264825609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.05248260498047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010823940858244896,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10834310054779053,
      "backward_entropy": 0.07630335622363621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.68833923339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01091239508241415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10831327438354492,
      "backward_entropy": 0.07611238956451416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.4684600830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011001200415194035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10828307867050171,
      "backward_entropy": 0.07630322376887004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.93431091308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01109318993985653,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10825201272964477,
      "backward_entropy": 0.07610913780000475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.1013946533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011186006478965282,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10821998119354248,
      "backward_entropy": 0.07630368073781331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.0425567626953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011279805563390255,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10818729400634766,
      "backward_entropy": 0.07675385475158691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.60369873046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01137448474764824,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10815389156341552,
      "backward_entropy": 0.07675658331976996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.37489318847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011471603997051716,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10811961889266967,
      "backward_entropy": 0.07610350184970432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.32325744628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011568172834813595,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10808509588241577,
      "backward_entropy": 0.0761023031340705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.16909790039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011662673205137253,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.108050537109375,
      "backward_entropy": 0.07610089249081081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.37954711914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011760917492210865,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10801470279693604,
      "backward_entropy": 0.07609980636172825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.55899047851562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011856547556817532,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10797947645187378,
      "backward_entropy": 0.07677068975236681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.74038696289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011950078420341015,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10794475078582763,
      "backward_entropy": 0.07677325937483045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.78749084472656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012041972018778324,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10791000127792358,
      "backward_entropy": 0.07677572303348118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.6778335571289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012135196477174759,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10787453651428222,
      "backward_entropy": 0.07677826616499159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.66786193847656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01222520787268877,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10783929824829101,
      "backward_entropy": 0.07609337568283081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.4372100830078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012316759675741196,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10780327320098877,
      "backward_entropy": 0.07678303453657362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.21173095703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012412182055413723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10776565074920655,
      "backward_entropy": 0.0760912365383572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.3612518310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01250712014734745,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10772764682769775,
      "backward_entropy": 0.0760901239183214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.82933044433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012599666602909565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10768938064575195,
      "backward_entropy": 0.07608850797017415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.47472381591797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012696096673607826,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10764960050582886,
      "backward_entropy": 0.07630260785420735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.0731964111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01278735138475895,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10761047601699829,
      "backward_entropy": 0.07608607742521498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.6085205078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012884590774774551,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10756967067718506,
      "backward_entropy": 0.07679723368750678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.00865173339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012982459738850594,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10752809047698975,
      "backward_entropy": 0.07630210452609593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.91294860839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013079206459224224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10748599767684937,
      "backward_entropy": 0.07630205816692776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.02418518066406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01317770965397358,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10744189023971558,
      "backward_entropy": 0.0768047637409634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.20091247558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013276826590299606,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10739654302597046,
      "backward_entropy": 0.07630238268110487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.73574829101562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013374567963182926,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10735030174255371,
      "backward_entropy": 0.07680991623136732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.93301391601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01347834151238203,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10730156898498536,
      "backward_entropy": 0.07681285010443793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.4954376220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013583898544311523,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10725184679031372,
      "backward_entropy": 0.07630405161115858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.4141082763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013689050450921059,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10720131397247315,
      "backward_entropy": 0.07630499203999837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.449462890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01379822101444006,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10714883804321289,
      "backward_entropy": 0.0763064291742113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.16493225097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013909468427300453,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10709496736526489,
      "backward_entropy": 0.07630813784069485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.70848083496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014016694389283657,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1070412278175354,
      "backward_entropy": 0.07630929019716051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.31759643554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014121630229055882,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10698702335357665,
      "backward_entropy": 0.07631010479397243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 233.75074768066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014220968820154667,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10693438053131103,
      "backward_entropy": 0.07631026373969184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.77349853515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014326374046504498,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10687931776046752,
      "backward_entropy": 0.07631105846828884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.72886657714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014432822354137897,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10682306289672852,
      "backward_entropy": 0.07608462042278713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.77081298828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014539028517901897,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1067662000656128,
      "backward_entropy": 0.076312608189053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.75816345214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014641684480011463,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10670890808105468,
      "backward_entropy": 0.0760842031902737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.68370056152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014742950908839703,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1066508173942566,
      "backward_entropy": 0.07608330912060207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.65016174316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014842970296740532,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10659208297729492,
      "backward_entropy": 0.07608215014139812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.61849975585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014943649061024189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1065324068069458,
      "backward_entropy": 0.07608118322160509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.27468872070312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01504193339496851,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10647280216217041,
      "backward_entropy": 0.0763110982047187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.84147644042969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015135996975004673,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1064133882522583,
      "backward_entropy": 0.07607784536149767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.43341064453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015225403010845184,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10635361671447754,
      "backward_entropy": 0.07685942120022243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.08537292480469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015314427204430103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10629284381866455,
      "backward_entropy": 0.07607050736745198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.65695190429688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015398291870951653,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10623254776000976,
      "backward_entropy": 0.0768619113498264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.188232421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015482231974601746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10617105960845948,
      "backward_entropy": 0.07629840241538154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.00836181640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015569035895168781,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10610723495483398,
      "backward_entropy": 0.07605512936909993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.16908264160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015656104311347008,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10604264736175537,
      "backward_entropy": 0.07604999012417263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.10687255859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01574268937110901,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10597689151763916,
      "backward_entropy": 0.07686584525638157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.31517028808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015829812735319138,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1059105396270752,
      "backward_entropy": 0.07628503110673693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.84503173828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015919961035251617,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10584216117858887,
      "backward_entropy": 0.0768679247962104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.53421020507812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016012931242585182,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10577192306518554,
      "backward_entropy": 0.0768691963619656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.13287353515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016106395050883293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10570012331008911,
      "backward_entropy": 0.0760246647728814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.79574584960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01620502397418022,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10562516450881958,
      "backward_entropy": 0.0760204725795322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.6759796142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01630578748881817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10554872751235962,
      "backward_entropy": 0.07601679695977105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.22601318359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016403794288635254,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10547255277633667,
      "backward_entropy": 0.07687495814429389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.80067443847656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016499444842338562,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10539689064025878,
      "backward_entropy": 0.07626663313971625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.4794464111328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01659455895423889,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1053209662437439,
      "backward_entropy": 0.07626375887129042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.1083526611328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01669183000922203,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10524306297302247,
      "backward_entropy": 0.07626109653049046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.5419692993164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01679079234600067,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10516314506530762,
      "backward_entropy": 0.07599729961819118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.19264221191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016885945573449135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10508413314819336,
      "backward_entropy": 0.07599339220258924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.34058380126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016978906467556953,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10500473976135254,
      "backward_entropy": 0.0762520432472229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.13339233398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01706768572330475,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10492540597915649,
      "backward_entropy": 0.07624779144922893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.07000732421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017162354663014412,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1048424243927002,
      "backward_entropy": 0.0762443674935235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.22865295410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017256928607821465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10475913286209107,
      "backward_entropy": 0.07597473594877455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.08262634277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01734924130141735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10467569828033448,
      "backward_entropy": 0.07596998082266913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.0973663330078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01744098961353302,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10459131002426147,
      "backward_entropy": 0.07623316182030572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.72447204589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017541158944368362,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10450193881988526,
      "backward_entropy": 0.07623039351569282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.14759826660156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017642788589000702,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10441046953201294,
      "backward_entropy": 0.07622772455215454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 292.30413818359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017741654068231583,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10431952476501465,
      "backward_entropy": 0.0768926011191474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.19105529785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017852017655968666,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10422251224517823,
      "backward_entropy": 0.07622321446736653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.5595703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017961055040359497,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.104124915599823,
      "backward_entropy": 0.07595084773169623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.85643005371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018067361786961555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10402700901031495,
      "backward_entropy": 0.07594794034957886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.04083251953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018163638189435005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10393284559249878,
      "backward_entropy": 0.07594272163179186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.31187438964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0182575024664402,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10383903980255127,
      "backward_entropy": 0.0759369929631551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.66381072998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01834908127784729,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10374538898468018,
      "backward_entropy": 0.0762062735027737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.30811309814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018437383696436882,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.103652822971344,
      "backward_entropy": 0.07592380046844482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.11514282226562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01852250099182129,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10356100797653198,
      "backward_entropy": 0.07591615782843696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.27008056640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018610678613185883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10346633195877075,
      "backward_entropy": 0.07590885957082112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.43533325195312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0187018271535635,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10336843729019166,
      "backward_entropy": 0.07690147558848064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.81674194335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018793858587741852,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10326846837997436,
      "backward_entropy": 0.07589549488491482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.25188446044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018887873739004135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10316557884216308,
      "backward_entropy": 0.0758887595600552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.99903869628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01897800713777542,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10306372642517089,
      "backward_entropy": 0.07617407374911839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.18215942382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019069477915763855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10296025276184081,
      "backward_entropy": 0.07587355375289917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.9441375732422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01915866881608963,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10285661220550538,
      "backward_entropy": 0.0761641263961792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.91336059570312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019251907244324684,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1027490258216858,
      "backward_entropy": 0.07690355512830946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.51873779296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019344722852110863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10263922214508056,
      "backward_entropy": 0.07584985097249348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.13792419433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019435236230492592,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10252892971038818,
      "backward_entropy": 0.07614931133058336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.0625762939453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019529767334461212,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10241461992263794,
      "backward_entropy": 0.07614456282721625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.92009735107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019622916355729103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.102298903465271,
      "backward_entropy": 0.07582486338085598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.70814514160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019710423424839973,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10218653678894044,
      "backward_entropy": 0.07690491941240099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.69591522216797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01979929767549038,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10207325220108032,
      "backward_entropy": 0.07612454228931004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.1526336669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019884677603840828,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10196076631546021,
      "backward_entropy": 0.07579241196314494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.96521759033203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019972732290625572,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10184473991394043,
      "backward_entropy": 0.07690442932976617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.83696746826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020054157823324203,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10173162221908569,
      "backward_entropy": 0.07576725218031141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.82780456542969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020129971206188202,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10162149667739868,
      "backward_entropy": 0.07608746157752143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.01104736328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02020036056637764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10151360034942628,
      "backward_entropy": 0.07573552926381429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.60301208496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020272545516490936,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10140373706817626,
      "backward_entropy": 0.07606322235531277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.35464477539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02034611441195011,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10129172801971435,
      "backward_entropy": 0.07570228311750624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.32688903808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02042263001203537,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10117682218551635,
      "backward_entropy": 0.07603949970669216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.0954132080078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020501088351011276,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10105847120285034,
      "backward_entropy": 0.07689675357606676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.42037963867188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02058199793100357,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10093766450881958,
      "backward_entropy": 0.07689589924282497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.68067932128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020660344511270523,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10081768035888672,
      "backward_entropy": 0.0760041011704339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.72483825683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02074570022523403,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1006920576095581,
      "backward_entropy": 0.07561824056837294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.40536499023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020829521119594574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10056639909744262,
      "backward_entropy": 0.07560127973556519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.2294464111328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020913666114211082,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10043962001800537,
      "backward_entropy": 0.07597038480970594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.41583251953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021001003682613373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10030920505523681,
      "backward_entropy": 0.07556781503889295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.8394775390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021091081202030182,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10017526149749756,
      "backward_entropy": 0.07594854301876491,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.89739990234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021181825548410416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10003902912139892,
      "backward_entropy": 0.07553426424662273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.7634735107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021275920793414116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09989820718765259,
      "backward_entropy": 0.0759264628092448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.99322509765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021368931978940964,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09975696206092835,
      "backward_entropy": 0.07591490613089667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.9653091430664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021462760865688324,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09961437582969665,
      "backward_entropy": 0.07590333620707194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.34344482421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021551240235567093,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09947540163993836,
      "backward_entropy": 0.07546051343282063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.8631591796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021643009036779404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09933406710624695,
      "backward_entropy": 0.07544212208853827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.3865509033203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021735599264502525,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09919095039367676,
      "backward_entropy": 0.07688956790500218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.57081604003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02183024398982525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09904476404190063,
      "backward_entropy": 0.07540517383151585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.67567443847656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021923502907156944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09889771938323974,
      "backward_entropy": 0.07584031422932942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.895593643188477,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022020049393177032,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0987454116344452,
      "backward_entropy": 0.07582753896713257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.53759002685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022103071212768555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09860426783561707,
      "backward_entropy": 0.07534059551027086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.96527862548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02218327298760414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09846447706222534,
      "backward_entropy": 0.075313581360711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.22731018066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022256338968873024,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09833024740219116,
      "backward_entropy": 0.0757741928100586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.20275115966797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0223291777074337,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0981952428817749,
      "backward_entropy": 0.07525389062033759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.3131103515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02240021899342537,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09806081056594848,
      "backward_entropy": 0.07573326428731282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.30717468261719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022466769441962242,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09793009757995605,
      "backward_entropy": 0.07571068074968126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.0645294189453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022532746195793152,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09780022501945496,
      "backward_entropy": 0.07687499125798543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.80938720703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022599464282393456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09766916036605836,
      "backward_entropy": 0.07512338956197102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.979248046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022671498358249664,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09753283858299255,
      "backward_entropy": 0.07509177260928684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.38748168945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022744791582226753,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0973939061164856,
      "backward_entropy": 0.07505941390991211,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.4186782836914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022814761847257614,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09725719690322876,
      "backward_entropy": 0.07559451791975233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.96625518798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022882886230945587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09712032079696656,
      "backward_entropy": 0.07498859034644233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.62368774414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022949114441871643,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09698287248611451,
      "backward_entropy": 0.07554108566708034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.80504608154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02301560901105404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0968440055847168,
      "backward_entropy": 0.07490905125935872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.89620971679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023076074197888374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09670978784561157,
      "backward_entropy": 0.07548248105578953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.9282684326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023134857416152954,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09657703638076783,
      "backward_entropy": 0.07545126809014215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.7371597290039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02320409193634987,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09643356800079346,
      "backward_entropy": 0.07478396097819011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.27458953857422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02327226847410202,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.096291184425354,
      "backward_entropy": 0.07539448473188612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.83628845214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02333926036953926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09614934921264648,
      "backward_entropy": 0.07470443513658312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.0282211303711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023409422487020493,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09600279927253723,
      "backward_entropy": 0.07466375827789307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.58193969726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023478206247091293,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0958571970462799,
      "backward_entropy": 0.07683660586675008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.08574676513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023544011637568474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09571369886398315,
      "backward_entropy": 0.07457794745763142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.413330078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023608408868312836,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09556999206542968,
      "backward_entropy": 0.07682926124996609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.00222778320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023680130019783974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09541968107223511,
      "backward_entropy": 0.07448940806918675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.20701599121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0237527247518301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09526636600494384,
      "backward_entropy": 0.07444523440466987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.7945785522461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023826688528060913,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0951112151145935,
      "backward_entropy": 0.07440084881252712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.31559753417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023900164291262627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0949556589126587,
      "backward_entropy": 0.07435483402676052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.3071517944336,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02397332154214382,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0947998583316803,
      "backward_entropy": 0.0768181946542528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.69781494140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024044694378972054,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09464552998542786,
      "backward_entropy": 0.07504716846677992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.15113830566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02412210777401924,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0944846510887146,
      "backward_entropy": 0.07501458459430271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.58824157714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024201883003115654,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0943209707736969,
      "backward_entropy": 0.07498227225409614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.50647735595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024283548817038536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09415442943572998,
      "backward_entropy": 0.0749498340818617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.7846450805664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024364067241549492,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09398878812789917,
      "backward_entropy": 0.07406746016608344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.50259399414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024440206587314606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09382679462432861,
      "backward_entropy": 0.07401394844055176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.061588287353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02450924552977085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09367150068283081,
      "backward_entropy": 0.07395672798156738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.1252670288086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024570588022470474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09352445006370544,
      "backward_entropy": 0.07389536831114027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.886234283447266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024628521874547005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09338180422782898,
      "backward_entropy": 0.07383222712410821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.13607788085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0246780663728714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09324756860733033,
      "backward_entropy": 0.07376346323225233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.1650390625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02472594752907753,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09311273097991943,
      "backward_entropy": 0.07678218682607015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.61038208007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024771632626652718,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0929809033870697,
      "backward_entropy": 0.073617332511478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.99078369140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024823173880577087,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09284287691116333,
      "backward_entropy": 0.07354613145192464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.79175567626953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02488112635910511,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09269706010818482,
      "backward_entropy": 0.07676313983069526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.15184020996094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024937300011515617,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09255325794219971,
      "backward_entropy": 0.07675751050313313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.8456573486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025000901892781258,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09240068793296814,
      "backward_entropy": 0.07333600521087646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.94915771484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025072848424315453,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09223928451538085,
      "backward_entropy": 0.0743354426489936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.8688201904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02514784224331379,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09207509756088257,
      "backward_entropy": 0.0742896662818061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.24493408203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025226818397641182,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09190651178359985,
      "backward_entropy": 0.07424503564834595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.09556579589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025314370170235634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09173039197921753,
      "backward_entropy": 0.0742043587896559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.42652130126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025401759892702103,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0915549635887146,
      "backward_entropy": 0.0741629269387987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.2745361328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025487521663308144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0913818359375,
      "backward_entropy": 0.07296821806165907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.15451431274414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025573071092367172,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09120867252349854,
      "backward_entropy": 0.07290555371178521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.6283187866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025647535920143127,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09104747772216797,
      "backward_entropy": 0.07283333275053236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.20486450195312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025716807693243027,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0908918023109436,
      "backward_entropy": 0.07396182749006483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.92365264892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02578151412308216,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09074132442474366,
      "backward_entropy": 0.0738983154296875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.61860656738281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02584141120314598,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09059402942657471,
      "backward_entropy": 0.07382947868771023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.4618911743164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025900699198246002,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0904472827911377,
      "backward_entropy": 0.07249440087212457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.705078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02596106380224228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09029916524887086,
      "backward_entropy": 0.07368726200527614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.63911437988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02602100931107998,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09015233516693115,
      "backward_entropy": 0.0723103019926283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.8157501220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02607731521129608,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09000977873802185,
      "backward_entropy": 0.07353778680165608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.49400329589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02613823674619198,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08986263871192932,
      "backward_entropy": 0.07211867305967543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.20628356933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026193780824542046,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08972152471542358,
      "backward_entropy": 0.07668489880032009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.34463500976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02624177560210228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08958510160446168,
      "backward_entropy": 0.07329689131842719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.8765106201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026289671659469604,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08944653272628784,
      "backward_entropy": 0.07320753733317058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.8377685546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026342708617448807,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08930222988128662,
      "backward_entropy": 0.07311981254153782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.84761810302734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026395611464977264,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08915774822235108,
      "backward_entropy": 0.0766433940993415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.54827117919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026445578783750534,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08901740312576294,
      "backward_entropy": 0.07293664084540473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.99491119384766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026493363082408905,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08888185024261475,
      "backward_entropy": 0.07284080319934422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.74422836303711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026540135964751244,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08874741792678834,
      "backward_entropy": 0.07119684749179417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.57428741455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026582932099699974,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08861786127090454,
      "backward_entropy": 0.07263931963178846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.60418701171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026623336598277092,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08848957419395446,
      "backward_entropy": 0.07658218012915717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.61131286621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02666347287595272,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08836145401000976,
      "backward_entropy": 0.0724235905541314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.5521240234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026712847873568535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08822373151779175,
      "backward_entropy": 0.07066379653082953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.86500549316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02676704153418541,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08808004260063171,
      "backward_entropy": 0.07221782869762844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.620361328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026826132088899612,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08793247938156128,
      "backward_entropy": 0.07040519846810235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.50724792480469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026886265724897385,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08778445720672608,
      "backward_entropy": 0.07201700740390354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.57649230957031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026945821940898895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08763750195503235,
      "backward_entropy": 0.07191362645890978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.00130462646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02700367569923401,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08749455213546753,
      "backward_entropy": 0.07001103295220269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.34896850585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02705977112054825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08735456466674804,
      "backward_entropy": 0.071698440445794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.7989730834961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02712358348071575,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0872073769569397,
      "backward_entropy": 0.0764949984020657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.47734832763672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027186453342437744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08706194162368774,
      "backward_entropy": 0.06960818502638075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.74960327148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02724893018603325,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08691868185997009,
      "backward_entropy": 0.06947289572821723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.21186828613281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027315139770507812,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08677172660827637,
      "backward_entropy": 0.06933788458506267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.9223403930664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027378762140870094,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08662853240966797,
      "backward_entropy": 0.07116767432954577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.38453674316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027440505102276802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0864900529384613,
      "backward_entropy": 0.0690601799223158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.84530639648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02750767022371292,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08634612560272217,
      "backward_entropy": 0.06892342699898614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.8896484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027571702376008034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08620461225509643,
      "backward_entropy": 0.06877815061145359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.092529296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0276426263153553,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08605717420578003,
      "backward_entropy": 0.07071708308325873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.20037841796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02770663984119892,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08591545820236206,
      "backward_entropy": 0.07059605254067315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.8912124633789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027775824069976807,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08576915264129639,
      "backward_entropy": 0.07047871748606364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.69062805175781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027843523770570755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0856251835823059,
      "backward_entropy": 0.06819409132003784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.94200134277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027909889817237854,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08548340797424317,
      "backward_entropy": 0.06803904639350043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.05111312866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02798001654446125,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08533987998962403,
      "backward_entropy": 0.06788841883341472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.00796508789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028042567893862724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08520627021789551,
      "backward_entropy": 0.06772700945536296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.22509002685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02810603193938732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08507271409034729,
      "backward_entropy": 0.06756465964847141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.46409606933594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028171824291348457,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08493757247924805,
      "backward_entropy": 0.07638108730316162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.2974395751953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028235284611582756,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08480676412582397,
      "backward_entropy": 0.06956696510314941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.02812194824219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028302902355790138,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08467394709587098,
      "backward_entropy": 0.06943054993947347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.35456848144531,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.028366371989250183,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08454681634902954,
      "backward_entropy": 0.07635549704233806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.61719512939453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02842881716787815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08442035913467408,
      "backward_entropy": 0.06913616922166613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.64239501953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028490563854575157,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08429535627365112,
      "backward_entropy": 0.06898291243447198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.0018310546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028546657413244247,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08417531847953796,
      "backward_entropy": 0.06635961267683241,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.02432250976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028605610132217407,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08405205607414246,
      "backward_entropy": 0.06865718629625109,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.88740539550781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02866421639919281,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0839301586151123,
      "backward_entropy": 0.06597083806991577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.51331329345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02871968410909176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08381341695785523,
      "backward_entropy": 0.06576920880211724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.17007446289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028777973726391792,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08369361162185669,
      "backward_entropy": 0.06556563244925605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.74519348144531,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02883249707520008,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08357672691345215,
      "backward_entropy": 0.07624925507439508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.74774932861328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028890052810311317,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08345661163330079,
      "backward_entropy": 0.06513730684916179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.48188781738281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028950227424502373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08333398103713989,
      "backward_entropy": 0.06492346525192261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.38813018798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029012758284807205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08320936560630798,
      "backward_entropy": 0.06470819314320882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.34638214111328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0290762260556221,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08308612108230591,
      "backward_entropy": 0.0761931339899699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.93053436279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029135867953300476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08296754360198974,
      "backward_entropy": 0.06426793336868286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.46593475341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029196305200457573,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08284708857536316,
      "backward_entropy": 0.06685053639941746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.99445343017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029256053268909454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08272827863693237,
      "backward_entropy": 0.06380295753479004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.8356704711914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029315292835235596,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.082611083984375,
      "backward_entropy": 0.06644713878631592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.68745422363281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02937742881476879,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08249294757843018,
      "backward_entropy": 0.06332802110248142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.03108215332031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029440229758620262,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08237399458885193,
      "backward_entropy": 0.06308935085932414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.99524688720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02950567565858364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08225528001785279,
      "backward_entropy": 0.06285617748896281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.20372772216797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02957296371459961,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08213511109352112,
      "backward_entropy": 0.06565324465433757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.827014923095703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029642120003700256,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08201442956924439,
      "backward_entropy": 0.07606393761105007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.435302734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02970220521092415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08190370798110962,
      "backward_entropy": 0.062126086817847356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.54942321777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029758498072624207,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08179620504379273,
      "backward_entropy": 0.06185734272003174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.12210083007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029819058254361153,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08168517351150513,
      "backward_entropy": 0.06479170587327746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.28372192382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029876215383410454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08157938718795776,
      "backward_entropy": 0.06131638420952691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.86790466308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02994137443602085,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0814693808555603,
      "backward_entropy": 0.06434441275066799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.3621826171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03001631423830986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08135192394256592,
      "backward_entropy": 0.06083222230275472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.80518341064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03008653223514557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08124180436134339,
      "backward_entropy": 0.060588591628604464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.81187438964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030156591907143593,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.081131511926651,
      "backward_entropy": 0.060336536831325956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.76904296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030229385942220688,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08101847767829895,
      "backward_entropy": 0.06350625885857476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.1927719116211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03030196949839592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0809066653251648,
      "backward_entropy": 0.05982052617602878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.26362228393555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03037405200302601,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08079487085342407,
      "backward_entropy": 0.063059753841824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.25143432617188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030440082773566246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08069111108779907,
      "backward_entropy": 0.059268448087904185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.397193908691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030509764328598976,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08058576583862305,
      "backward_entropy": 0.0625824663374159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.79395294189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030570074915885925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08048808574676514,
      "backward_entropy": 0.05869230296876696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.66305541992188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03063332661986351,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0803900957107544,
      "backward_entropy": 0.058401525020599365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.71826934814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030692612752318382,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08029544353485107,
      "backward_entropy": 0.058097640673319496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.816650390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030751029029488564,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08019944429397582,
      "backward_entropy": 0.06152742438846164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.68234252929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03080294281244278,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08010889291763305,
      "backward_entropy": 0.05745104948679606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.92992401123047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030852237716317177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08002134561538696,
      "backward_entropy": 0.057112965318891734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.94603729248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030900586396455765,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07993491291999817,
      "backward_entropy": 0.056769867738087974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.59885025024414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030951611697673798,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0798490285873413,
      "backward_entropy": 0.056436512205335826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.00907897949219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030998826026916504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976869940757751,
      "backward_entropy": 0.06002612246407403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.78577423095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03105013072490692,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07968594431877137,
      "backward_entropy": 0.05576507250467936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.81322479248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031102027744054794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07960435152053832,
      "backward_entropy": 0.05543401506212023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.65138244628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031152350828051567,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07952179312705994,
      "backward_entropy": 0.0550882187154558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.29878234863281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03120923787355423,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07943438291549683,
      "backward_entropy": 0.05881547927856445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.55697631835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03126905485987663,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07934530377388001,
      "backward_entropy": 0.054422219594319664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.66844940185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031325336545705795,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07925920486450196,
      "backward_entropy": 0.058214386304219566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.164127349853516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031380243599414825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07917616367340088,
      "backward_entropy": 0.05790208445654975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.67816925048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03142920508980751,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07909890413284301,
      "backward_entropy": 0.05336658159891764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.83273315429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03147733211517334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07902127504348755,
      "backward_entropy": 0.05299800634384155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.1473846435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03152311593294144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07894535064697265,
      "backward_entropy": 0.05262020892567105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.47296905517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031576450914144516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07886579632759094,
      "backward_entropy": 0.052268233564164907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.64759826660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03162694349884987,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07878789901733399,
      "backward_entropy": 0.05190443330340915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.491294860839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03167658671736717,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07871136665344239,
      "backward_entropy": 0.055903494358062744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.64771270751953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03171946108341217,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07864184379577636,
      "backward_entropy": 0.05115494132041931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.93191528320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03176205977797508,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0785715937614441,
      "backward_entropy": 0.050767362117767334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.52542495727539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03180602192878723,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07850011587142944,
      "backward_entropy": 0.05482833915286594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.45209503173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03184632584452629,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0784295678138733,
      "backward_entropy": 0.04997689194149441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.04240798950195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03188695013523102,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0783603310585022,
      "backward_entropy": 0.04957582553227743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.60442352294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03192617744207382,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07829226851463318,
      "backward_entropy": 0.05370738771226671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.54126739501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03197220712900162,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07822160720825196,
      "backward_entropy": 0.04879234896765815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.55839538574219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03202070668339729,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07814798951148987,
      "backward_entropy": 0.0529961089293162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.3096923828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03206527233123779,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07807690501213074,
      "backward_entropy": 0.05262625217437744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.36082458496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03211405500769615,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07800122499465942,
      "backward_entropy": 0.052267230219311185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.82626342773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03216179460287094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07792451381683349,
      "backward_entropy": 0.04724007182651096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.54009246826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032211679965257645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07784473896026611,
      "backward_entropy": 0.05153888463973999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.248699188232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032263509929180145,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.077761709690094,
      "backward_entropy": 0.04644646247227987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.013118743896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032308537513017654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07768667936325073,
      "backward_entropy": 0.04603487915462918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.43742370605469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032352037727832794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07761510610580444,
      "backward_entropy": 0.045623458094067044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.3375244140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032401587814092636,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07753986120223999,
      "backward_entropy": 0.0452317131890191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.88933563232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03245373070240021,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0774646282196045,
      "backward_entropy": 0.04484971033202277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.08753967285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03250506520271301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07739081382751464,
      "backward_entropy": 0.04446517758899265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.94575500488281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03255223482847214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07731812000274658,
      "backward_entropy": 0.044061773353152804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.752220153808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03259725123643875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07724610567092896,
      "backward_entropy": 0.04854479100969103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.0179672241211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03264034166932106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07717424631118774,
      "backward_entropy": 0.043224957254197865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.25960540771484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03268358111381531,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07710352540016174,
      "backward_entropy": 0.047749055756462946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.34320831298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03272826224565506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07703195810317993,
      "backward_entropy": 0.04735437366697523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03277767077088356,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07695844173431396,
      "backward_entropy": 0.04198920064502292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.36111450195312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032834071666002274,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07687958478927612,
      "backward_entropy": 0.046622918711768255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.33134460449219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03289365395903587,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07679849863052368,
      "backward_entropy": 0.04627771178881327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.35577392578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032957371324300766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0767133355140686,
      "backward_entropy": 0.040890481736924916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.84799194335938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03302796185016632,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07662400603294373,
      "backward_entropy": 0.04562748803032769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.94857025146484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03309575840830803,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07653599381446838,
      "backward_entropy": 0.04019332594341702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.7940673828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03316141664981842,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07645139694213868,
      "backward_entropy": 0.0398325953218672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.75595092773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0332217663526535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07637050747871399,
      "backward_entropy": 0.03945167859395345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.182828903198242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03328382596373558,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07629104852676391,
      "backward_entropy": 0.039083255661858454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.90848541259766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03333784267306328,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07621583938598633,
      "backward_entropy": 0.043865733676486544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.7284164428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03339377045631409,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07613821029663086,
      "backward_entropy": 0.03829390141699049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.8837890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03344878926873207,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07606323957443237,
      "backward_entropy": 0.03790426254272461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.002769470214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03349963575601578,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.075989830493927,
      "backward_entropy": 0.042728417449527316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.389076232910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0335465632379055,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0759151816368103,
      "backward_entropy": 0.042327172226376004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.98278045654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033590249717235565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07584260702133179,
      "backward_entropy": 0.036658171150419444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.24380493164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03363540768623352,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07576721906661987,
      "backward_entropy": 0.041510581970214844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.31101608276367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03368812054395676,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07568625211715699,
      "backward_entropy": 0.035850356022516884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.405305862426758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0337374322116375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07561218738555908,
      "backward_entropy": 0.035458637608422175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.738344192504883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033779989928007126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07554075717926026,
      "backward_entropy": 0.03504401445388794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.93341064453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03381683677434921,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07547481656074524,
      "backward_entropy": 0.03461777501636081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.521577835083008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033850111067295074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07541184425354004,
      "backward_entropy": 0.034186489052242704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.590023040771484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03387864679098129,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0753525972366333,
      "backward_entropy": 0.03374607364336649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.60547637939453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03390730917453766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.075291109085083,
      "backward_entropy": 0.03860775629679362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.76009750366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033941056579351425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07522634863853454,
      "backward_entropy": 0.03289676705996195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.177589416503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033973101526498795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07516227960586548,
      "backward_entropy": 0.03248355786005656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.65851593017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03400491550564766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0750956654548645,
      "backward_entropy": 0.03207005394829644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.23234558105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034034162759780884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07503529787063598,
      "backward_entropy": 0.031659732262293495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.02633285522461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03406648710370064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07496936321258545,
      "backward_entropy": 0.03651180863380432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.55461120605469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03409744054079056,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07490532398223877,
      "backward_entropy": 0.03085651331477695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.03570556640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03412991017103195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07483811378479004,
      "backward_entropy": 0.03046062257554796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.104103088378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034162409603595734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07477033138275146,
      "backward_entropy": 0.03006791075070699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.62300109863281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03419165313243866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07470235824584961,
      "backward_entropy": 0.03487903541988797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.49540710449219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034224431961774826,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07463202476501465,
      "backward_entropy": 0.02927733129925198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.9866828918457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03425576910376549,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07456265687942505,
      "backward_entropy": 0.034089108308156334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.610107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03428760543465614,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07449482679367066,
      "backward_entropy": 0.028509596983591717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.0390510559082,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03431558981537819,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07443443536758423,
      "backward_entropy": 0.07272858752144708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.67826843261719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0343426913022995,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07437365651130676,
      "backward_entropy": 0.02774627341164483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.51288604736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03437644988298416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07430447936058045,
      "backward_entropy": 0.027390498254034255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.34734344482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03441327437758446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07423160076141358,
      "backward_entropy": 0.027049168944358826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.33502960205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03445422649383545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07415317893028259,
      "backward_entropy": 0.026720878150728013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.67362976074219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034497328102588654,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07407033443450928,
      "backward_entropy": 0.031512416071361966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.799800872802734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03454269841313362,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0739857316017151,
      "backward_entropy": 0.02608380549483829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.375038146972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03458371013402939,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07390043735504151,
      "backward_entropy": 0.03086451358265347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.01791381835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03462447226047516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07381770014762878,
      "backward_entropy": 0.025428536865446303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.70623779296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034667715430259705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07373244762420654,
      "backward_entropy": 0.025114513105816312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.854042053222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0347074456512928,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07365192174911499,
      "backward_entropy": 0.024794810348086886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.10281372070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03474830463528633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07356882095336914,
      "backward_entropy": 0.024480770031611126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.578880310058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03479740396142006,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07347655296325684,
      "backward_entropy": 0.024192785223325092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.82733917236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034846581518650055,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07338283061981202,
      "backward_entropy": 0.023906360069910686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.3792724609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03489740937948227,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07328681945800782,
      "backward_entropy": 0.023625946707195707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.149662017822266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03494684770703316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07319132089614869,
      "backward_entropy": 0.023343649175431993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.89084243774414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034994013607501984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07310103178024292,
      "backward_entropy": 0.02306325899230109,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.22908020019531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03504026681184769,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07301133871078491,
      "backward_entropy": 0.02278262045648363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.00530242919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03508712723851204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07292007207870484,
      "backward_entropy": 0.02250581979751587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.29413986206055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0351373553276062,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07282359004020691,
      "backward_entropy": 0.022240486409929063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.40342712402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03518642485141754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07272816896438598,
      "backward_entropy": 0.021976641482777066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.88758850097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03523866832256317,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07262808680534363,
      "backward_entropy": 0.021722924378183153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.16502380371094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03528958931565285,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07253018617630005,
      "backward_entropy": 0.026503557960192364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.70631408691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035338085144758224,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07243611216545105,
      "backward_entropy": 0.026237113608254328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.36863708496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03538469597697258,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07234770059585571,
      "backward_entropy": 0.020958075920740765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.835643768310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03543630614876747,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07225319147109985,
      "backward_entropy": 0.0207189056608412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.37810516357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03548366203904152,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0721613883972168,
      "backward_entropy": 0.02046809759404924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.3214111328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035533059388399124,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07206599116325378,
      "backward_entropy": 0.020223985115687054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.8757438659668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03558410331606865,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07196645736694336,
      "backward_entropy": 0.024948014153374568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.570125579833984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03563525900244713,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07186515927314759,
      "backward_entropy": 0.019742392831378512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.04856872558594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03568660467863083,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0717621088027954,
      "backward_entropy": 0.019501990742153592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.91443634033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03573991730809212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07165851593017578,
      "backward_entropy": 0.01927418178982205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.5926513671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03579631820321083,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07155200242996215,
      "backward_entropy": 0.01905885338783264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.47216033935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03585269674658775,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07144609093666077,
      "backward_entropy": 0.023759323689672682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.18561553955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035911675542593,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07133524417877198,
      "backward_entropy": 0.018640574481752183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.429737091064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03597163408994675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07122189998626709,
      "backward_entropy": 0.023328201638327703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.064476013183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03602674975991249,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07111302018165588,
      "backward_entropy": 0.018223086992899578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.814434051513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03607902675867081,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07100688219070435,
      "backward_entropy": 0.018001563019222684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.73336410522461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03612764552235603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0709070086479187,
      "backward_entropy": 0.017778648270501032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.16026306152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03617411479353905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07080895900726318,
      "backward_entropy": 0.017553615901205275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.70588493347168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03622140362858772,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07070703506469726,
      "backward_entropy": 0.02214450306362576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.61906433105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0362652949988842,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07060898542404175,
      "backward_entropy": 0.01710917552312215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.10400390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03631042689085007,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07050809860229493,
      "backward_entropy": 0.016891459623972576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.70376968383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036358051002025604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07040294408798217,
      "backward_entropy": 0.016682588391833834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.54292297363281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036408111453056335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07029638290405274,
      "backward_entropy": 0.016484571827782526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.43745422363281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03645631670951843,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07019411325454712,
      "backward_entropy": 0.016288277175691392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.47720718383789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036502812057733536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07009484171867371,
      "backward_entropy": 0.020793058805995517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.77508544921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03655043616890907,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06999362707138061,
      "backward_entropy": 0.01590326925118764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.830158233642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03659486398100853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06989668011665344,
      "backward_entropy": 0.020360281070073444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.85952758789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03663621097803116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0698005199432373,
      "backward_entropy": 0.015505058897866143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.23458480834961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0366818904876709,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06969716548919677,
      "backward_entropy": 0.019923751552899677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.29645347595215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036727454513311386,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06959434747695922,
      "backward_entropy": 0.015124196807543436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.1922492980957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036770131438970566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06949517726898194,
      "backward_entropy": 0.014932721853256226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.984935760498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036814481019973755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06939313411712647,
      "backward_entropy": 0.014751171072324118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.910539627075195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03685622662305832,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0692956030368805,
      "backward_entropy": 0.014568282498253716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.481143951416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036895595490932465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06920140981674194,
      "backward_entropy": 0.014383451806174384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.247227668762207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03693542629480362,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06910547018051147,
      "backward_entropy": 0.014202318257755704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.336795806884766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03697028011083603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06901639699935913,
      "backward_entropy": 0.01401449739933014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.68402862548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03700753673911095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06892269253730773,
      "backward_entropy": 0.018266548713048298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.336103439331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03705116733908653,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06881988048553467,
      "backward_entropy": 0.013671205275588565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.96772575378418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0370924174785614,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06872090697288513,
      "backward_entropy": 0.01350749284029007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.312477111816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03713036701083183,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06862895488739014,
      "backward_entropy": 0.01770659287770589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.228050231933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037166252732276917,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.068537837266922,
      "backward_entropy": 0.013175490829679701,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.15684127807617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037201788276433945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06844528913497924,
      "backward_entropy": 0.013010058965947893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.0357666015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037238407880067825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06834979057312011,
      "backward_entropy": 0.017138307293256123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.689558029174805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03727881982922554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06824797987937928,
      "backward_entropy": 0.01270009246137407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.64217758178711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03731599822640419,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06815292835235595,
      "backward_entropy": 0.012549474835395813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.597278594970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037356898188591,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06805015802383423,
      "backward_entropy": 0.012407670418421427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.190101623535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03739575296640396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06794974207878113,
      "backward_entropy": 0.012264125049114227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.10373306274414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037437014281749725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06784598827362061,
      "backward_entropy": 0.012130347390969595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.85026550292969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03748027980327606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06773895025253296,
      "backward_entropy": 0.012001199854744805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.70916748046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037526585161685944,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0676253855228424,
      "backward_entropy": 0.011876859598689608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.539180755615234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037574417889118195,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06750885248184205,
      "backward_entropy": 0.015895813703536987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.50260543823242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037622589617967606,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06739457845687866,
      "backward_entropy": 0.011641401383611891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.86296081542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03767082467675209,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06727903485298156,
      "backward_entropy": 0.01152710947725508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.29106521606445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03772173076868057,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06715810298919678,
      "backward_entropy": 0.0155069414112303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.32466125488281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03777646645903587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06703118085861207,
      "backward_entropy": 0.01131732596291436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.11432456970215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0378333143889904,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06690034866333008,
      "backward_entropy": 0.011222088502513038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.059328079223633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03788810595870018,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06677303314208985,
      "backward_entropy": 0.01112488243314955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.79985809326172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0379408560693264,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06664764881134033,
      "backward_entropy": 0.011022844248347811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.728757858276367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03799201548099518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06652657985687256,
      "backward_entropy": 0.010920157035191854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.16352844238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0380416065454483,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06640763282775879,
      "backward_entropy": 0.010816001229816012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.963597297668457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038093701004981995,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06628192663192749,
      "backward_entropy": 0.010717038479116228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.386297225952148,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038140248507261276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06616560220718384,
      "backward_entropy": 0.010611864427725473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.193187713623047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03818311542272568,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06605451703071594,
      "backward_entropy": 0.010502877334753672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.63652420043945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038225267082452774,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06594283580780029,
      "backward_entropy": 0.014329749676916335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.563356399536133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03827209398150444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06582236289978027,
      "backward_entropy": 0.010292644302050272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.461151123046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038316529244184494,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06570611000061036,
      "backward_entropy": 0.014100023441844516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.53671646118164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038362808525562286,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06558570861816407,
      "backward_entropy": 0.01009064581659105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.17744445800781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03841216489672661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06545960903167725,
      "backward_entropy": 0.009999250372250875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.607784271240234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03846808522939682,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06532096862792969,
      "backward_entropy": 0.009918546511067284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.996313095092773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03852342069149017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06518203020095825,
      "backward_entropy": 0.01371607267194324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.0645809173584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0385756716132164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06504866480827332,
      "backward_entropy": 0.009757576717270745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.636101722717285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03862645477056503,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06491763591766357,
      "backward_entropy": 0.009673357837729983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.00693130493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03867333009839058,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06479445695877076,
      "backward_entropy": 0.009583874709076352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.557645797729492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03872193768620491,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06466844081878662,
      "backward_entropy": 0.009499169058269925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.64846420288086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03876824304461479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06454744338989257,
      "backward_entropy": 0.009413386384646097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.45789337158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03881630301475525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06442352533340454,
      "backward_entropy": 0.009330739577611288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.25430679321289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03886594623327255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06429535150527954,
      "backward_entropy": 0.009251723686854044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.118879318237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03891704976558685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0641644537448883,
      "backward_entropy": 0.009176923169030083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.127691268920898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038966938853263855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06403658390045167,
      "backward_entropy": 0.009102738565868802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.862504959106445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03901316225528717,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06391593217849731,
      "backward_entropy": 0.009025593598683676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.754735946655273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03905867785215378,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06379711031913757,
      "backward_entropy": 0.008949077791637845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.6400089263916,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03910352289676666,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06367894411087036,
      "backward_entropy": 0.012589284943209754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.46417236328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0391477569937706,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06356106996536255,
      "backward_entropy": 0.008797155486212837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.9912567138671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039198968559503555,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06342902183532714,
      "backward_entropy": 0.07383318079842462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.01049041748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03924523666501045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06330782771110535,
      "backward_entropy": 0.0123369586136606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.85773468017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03929208964109421,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06318463087081909,
      "backward_entropy": 0.008591419292820824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.69833755493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03933947533369064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06305941343307495,
      "backward_entropy": 0.008525515596071878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.243473052978516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03938736766576767,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06293210983276368,
      "backward_entropy": 0.012096933192676969,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.65126037597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039436884224414825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06279942989349366,
      "backward_entropy": 0.008399054408073425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.59148406982422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039489272981882095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06266165971755981,
      "backward_entropy": 0.008343185815546248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.81428146362305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039540503174066544,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06252796649932861,
      "backward_entropy": 0.008288315600819059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.24136734008789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03959554806351662,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06238623857498169,
      "backward_entropy": 0.011824341283904182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.731815338134766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03964664787054062,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06225373148918152,
      "backward_entropy": 0.008183451990286509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.08040237426758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03969790041446686,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06212076544761658,
      "backward_entropy": 0.008130576875474717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.519569396972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03975044563412666,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06198333501815796,
      "backward_entropy": 0.008078633911079831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.406206130981445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03980046138167381,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06185048818588257,
      "backward_entropy": 0.008024576637479994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.518265247344971,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03984827548265457,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061722517013549805,
      "backward_entropy": 0.007969926628801558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.97895050048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039891671389341354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0616057276725769,
      "backward_entropy": 0.007912867599063449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.8264217376709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039935946464538574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06148543357849121,
      "backward_entropy": 0.007857490744855668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.427608966827393,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039981067180633545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06136308908462525,
      "backward_entropy": 0.0078045303622881574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.6874418258667,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04002205654978752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.061250698566436765,
      "backward_entropy": 0.007749574051962959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.419248580932617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040060557425022125,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06114397048950195,
      "backward_entropy": 0.011132145921389261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.3044490814209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04010052978992462,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06103387475013733,
      "backward_entropy": 0.007640866769684685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.732271194458008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04014177247881889,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06091886758804321,
      "backward_entropy": 0.007589902314874861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.21995162963867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04018178582191467,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06080787181854248,
      "backward_entropy": 0.00753931204477946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.39687728881836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0402243509888649,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06069148778915405,
      "backward_entropy": 0.007491915590233273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.894004821777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0402715727686882,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06056271195411682,
      "backward_entropy": 0.00744799276192983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.923622131347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040320586413145065,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06042912006378174,
      "backward_entropy": 0.007406156924035814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.447166442871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04037361219525337,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060285305976867674,
      "backward_entropy": 0.007368031475279067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.204347610473633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04042665287852287,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06014128923416138,
      "backward_entropy": 0.007330308357874553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.110410690307617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04047730192542076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.060002684593200684,
      "backward_entropy": 0.007290862500667572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.002819061279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040525805205106735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05986987352371216,
      "backward_entropy": 0.007249957157505883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.797080993652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040573589503765106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05973888039588928,
      "backward_entropy": 0.007209266225496928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.54400634765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040623102337121964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05960240364074707,
      "backward_entropy": 0.007170932988325755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.64190673828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04067535698413849,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05945796966552734,
      "backward_entropy": 0.007135656972726186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.519237518310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040726516395807266,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05931704044342041,
      "backward_entropy": 0.00710029982858234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.563751220703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04077669233083725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05917822122573853,
      "backward_entropy": 0.007064703438017104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.476259231567383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04082480072975159,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.059044837951660156,
      "backward_entropy": 0.007028595440917545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.72953414916992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04087105393409729,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05891637802124024,
      "backward_entropy": 0.006991969214545356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.055402755737305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040919169783592224,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05878314971923828,
      "backward_entropy": 0.006957140233781602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.21872901916504,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040966615080833435,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05865166187286377,
      "backward_entropy": 0.006922524008486006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.43993854522705,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04101228341460228,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05852499008178711,
      "backward_entropy": 0.010051675968699984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.7166666984558105,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04105518013238907,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05840590000152588,
      "backward_entropy": 0.010000625418292152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.27672576904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04109441488981247,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058297115564346316,
      "backward_entropy": 0.006815550641881095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.532564163208008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04113505035638809,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.058183479309082034,
      "backward_entropy": 0.006781088395251168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.421226501464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04117577150464058,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05806938409805298,
      "backward_entropy": 0.006747576925489638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.891115188598633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04122238606214523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05793968439102173,
      "backward_entropy": 0.006717575920952691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.37959289550781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04126966744661331,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057808172702789304,
      "backward_entropy": 0.006688563360108269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.602428436279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041321009397506714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.057665669918060304,
      "backward_entropy": 0.009720740218957266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.448633193969727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04137253016233444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.057522743940353394,
      "backward_entropy": 0.006636297537220849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.7540168762207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041424233466386795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05737882256507874,
      "backward_entropy": 0.0066111890806092154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.726213455200195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041477225720882416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05723119974136352,
      "backward_entropy": 0.006586945719189114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.79202651977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04152907058596611,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05708736181259155,
      "backward_entropy": 0.0065625910129812025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.207881927490234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04158324748277664,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05693732500076294,
      "backward_entropy": 0.009525789982742734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.0244197845459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04163840785622597,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05678560733795166,
      "backward_entropy": 0.006516941719584995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.22808837890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04169105365872383,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05664034485816956,
      "backward_entropy": 0.006493280745214886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.842291831970215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041742607951164246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05649765729904175,
      "backward_entropy": 0.006469302707248264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.236677169799805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041792016476392746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.056361055374145506,
      "backward_entropy": 0.006444737729099061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.458752632141113,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0418417826294899,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0562224805355072,
      "backward_entropy": 0.006421062681410048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.77143096923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041888438165187836,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05609308481216431,
      "backward_entropy": 0.006396500600708855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.824243545532227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04193459823727608,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05596482753753662,
      "backward_entropy": 0.006372280418872833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.03909028321504593,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04198143258690834,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05583367347717285,
      "backward_entropy": 0.006348821024099986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.360596656799316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04202314466238022,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05571877956390381,
      "backward_entropy": 0.0063235825962490505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.289325714111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042063646018505096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05560775399208069,
      "backward_entropy": 0.006298452201816771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.211008071899414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04210306331515312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0554997444152832,
      "backward_entropy": 0.0062734343939357335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.226682662963867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04214157909154892,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05539408326148987,
      "backward_entropy": 0.006248880591657426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.10260772705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04218147695064545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0552843451499939,
      "backward_entropy": 0.006225208441416423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.993358612060547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04222143441438675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05517508983612061,
      "backward_entropy": 0.006201581822501289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.932082176208496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042261578142642975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05506449341773987,
      "backward_entropy": 0.006179334388838874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.872498512268066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042300764471292496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05495646595954895,
      "backward_entropy": 0.006157545993725459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.6408748626709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042339012026786804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05485161542892456,
      "backward_entropy": 0.006135579198598862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.433929443359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0423787459731102,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054741543531417844,
      "backward_entropy": 0.006114965925614039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.66318416595459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04242085665464401,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054624295234680174,
      "backward_entropy": 0.006095452855030696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.736238479614258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04246174171566963,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054511308670043945,
      "backward_entropy": 0.006075638863775466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.17540168762207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04250044375658035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05440513491630554,
      "backward_entropy": 0.006055636952320735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.647330284118652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04254056513309479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.054293274879455566,
      "backward_entropy": 0.006037062240971459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.377285957336426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0425785630941391,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05418906211853027,
      "backward_entropy": 0.006018130315674676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.076866149902344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0426158644258976,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.05408656597137451,
      "backward_entropy": 0.07582700252532959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.514177322387695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04265362769365311,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05398220419883728,
      "backward_entropy": 0.005982882860634062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.901628494262695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04268953576683998,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.053883922100067136,
      "backward_entropy": 0.00849284397231208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.121517181396484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042726099491119385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05378291606903076,
      "backward_entropy": 0.005949776205751631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.77894592285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04276211932301521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05368332862854004,
      "backward_entropy": 0.005934733897447586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.9773006439209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04280197620391846,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053571450710296634,
      "backward_entropy": 0.005921112994352977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.180585861206055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04284404590725899,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.053453588485717775,
      "backward_entropy": 0.005907549212376277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.060636520385742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042887233197689056,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05333119034767151,
      "backward_entropy": 0.008338068922360739,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.195969581604004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042931392788887024,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05320572257041931,
      "backward_entropy": 0.005884520295593474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.706313133239746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04297309368848801,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05308849811553955,
      "backward_entropy": 0.005873252948125203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.642956733703613,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04301367700099945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05297476053237915,
      "backward_entropy": 0.005862271206246482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.559825897216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043053194880485535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05286496877670288,
      "backward_entropy": 0.0058508457409010995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.997314453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04309192672371864,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05275723934173584,
      "backward_entropy": 0.005840469979577594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.37666130065918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043130893260240555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05264893174171448,
      "backward_entropy": 0.005830211357937919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.374184608459473,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04317116364836693,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05253613591194153,
      "backward_entropy": 0.005820458134015401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.03293146565556526,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04321041703224182,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05242706537246704,
      "backward_entropy": 0.005810507883628209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.242712020874023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04324556514620781,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.052331918478012086,
      "backward_entropy": 0.005800120946433809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.572031021118164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043280262500047684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0522379994392395,
      "backward_entropy": 0.005790042794413037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.49216079711914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0433155819773674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05214196443557739,
      "backward_entropy": 0.005780312336153454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.063397407531738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04335145652294159,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05204427242279053,
      "backward_entropy": 0.005770715574423472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.319719314575195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043386805802583694,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05194825530052185,
      "backward_entropy": 0.0057615844739807975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.25035858154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04342283308506012,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.051849520206451415,
      "backward_entropy": 0.005753742323981391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.738874435424805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04345933347940445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05174942016601562,
      "backward_entropy": 0.005746018141508102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.077970504760742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043498314917087555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05164123773574829,
      "backward_entropy": 0.005738328728410933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.744495391845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043537501245737076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05153249502182007,
      "backward_entropy": 0.005730889323684905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.689425468444824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043575890362262726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05142623782157898,
      "backward_entropy": 0.005724276105562846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.020442962646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04361345246434212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05132277607917786,
      "backward_entropy": 0.005717669096257951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.558505058288574,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04365234822034836,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05121513605117798,
      "backward_entropy": 0.00571092673473888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.504469871520996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04369039461016655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05111035108566284,
      "backward_entropy": 0.005704425689246919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.445501327514648,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0437275655567646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05100909471511841,
      "backward_entropy": 0.005697191589408451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.81708335876465,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0437639057636261,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050911086797714236,
      "backward_entropy": 0.005689121782779694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.65083122253418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043803803622722626,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05080093145370483,
      "backward_entropy": 0.005682201435168584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.161152839660645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04384700581431389,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05067940354347229,
      "backward_entropy": 0.005676838258902232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.097079277038574,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04388795793056488,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05056523084640503,
      "backward_entropy": 0.005672290507290099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.136062622070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043925587087869644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0504632830619812,
      "backward_entropy": 0.005666101972262065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.028100967407227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043964628130197525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0503560721874237,
      "backward_entropy": 0.005661080694860882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.869047164916992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04400186613202095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.05025460720062256,
      "backward_entropy": 0.007573578920629289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.82762336730957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04404350742697716,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.05013871192932129,
      "backward_entropy": 0.005653696341647042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.048467133194208145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04408593475818634,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.050020134449005126,
      "backward_entropy": 0.005648975984917747,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.864465713500977,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04412397742271423,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04991734027862549,
      "backward_entropy": 0.005643739054600398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.734528541564941,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04416018724441528,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0498207837343216,
      "backward_entropy": 0.007476576500468784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.668417930603027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044195543974637985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04972793757915497,
      "backward_entropy": 0.005631932781802284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.444031715393066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04423021525144577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049637848138809205,
      "backward_entropy": 0.005624392380317052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.877771377563477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04426548257470131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04954530000686645,
      "backward_entropy": 0.005618058145046234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8591561317443848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044304125010967255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04944160878658295,
      "backward_entropy": 0.005611093093951543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.2236967086792,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04433982074260712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049348446726799014,
      "backward_entropy": 0.005604362736145656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.899425506591797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044375739991664886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04925482273101807,
      "backward_entropy": 0.005596625722116894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.049755096435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04441302642226219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.049156281352043155,
      "backward_entropy": 0.005589464058478673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.688030242919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04445049539208412,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04905737042427063,
      "backward_entropy": 0.005582299083471298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.888907432556152,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04448920488357544,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0489540696144104,
      "backward_entropy": 0.005575877924760182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.811173439025879,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04452785477042198,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0488514244556427,
      "backward_entropy": 0.005568608641624451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.714081764221191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044566355645656586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04875031113624573,
      "backward_entropy": 0.005559865385293961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.910446166992188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04460484907031059,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0486492395401001,
      "backward_entropy": 0.005551199118296306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.775596618652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04464534670114517,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04854114353656769,
      "backward_entropy": 0.005543033695883221,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.837189674377441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04468763247132301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04842707216739654,
      "backward_entropy": 0.00553453051381641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.343494415283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04472867399454117,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04831729531288147,
      "backward_entropy": 0.0055273158682717215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.710441589355469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044769566506147385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04820786118507385,
      "backward_entropy": 0.005521190249257618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.657435417175293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0448092482984066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048102831840515135,
      "backward_entropy": 0.0055153460966216195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.594212532043457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04484769329428673,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.048002755641937254,
      "backward_entropy": 0.005508396774530411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.033212661743164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04488501697778702,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04790705442428589,
      "backward_entropy": 0.005500463561879264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.917319297790527,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044920504093170166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04781782329082489,
      "backward_entropy": 0.005493234843015671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.402877807617188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04495641589164734,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04772670269012451,
      "backward_entropy": 0.005487661394808028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.19764518737793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044991593807935715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04763816297054291,
      "backward_entropy": 0.005482378933164809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4812698364257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04502807930111885,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04754498600959778,
      "backward_entropy": 0.0054775869680775535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.848644733428955,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04506169259548187,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04746250510215759,
      "backward_entropy": 0.0054720184869236415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.161128997802734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045093730092048645,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04738604128360748,
      "backward_entropy": 0.005466087410847346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.18809700012207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045125603675842285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.047309157252311704,
      "backward_entropy": 0.005462291340033214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.404278755187988,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04515999183058739,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04722371101379395,
      "backward_entropy": 0.005458176963859134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.94495964050293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045194726437330246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04713728129863739,
      "backward_entropy": 0.005454116811354955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.659923076629639,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04523187130689621,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04704163074493408,
      "backward_entropy": 0.005451896952258216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.439979553222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045267123728990555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04695314764976501,
      "backward_entropy": 0.005449406802654266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.349287033081055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04530366510152817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0468600332736969,
      "backward_entropy": 0.005447508560286628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.78744888305664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04534124955534935,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.046763491630554196,
      "backward_entropy": 0.005445232821835412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.915119171142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04537772759795189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04667184948921203,
      "backward_entropy": 0.005441511670748393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.04060173034668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045414503663778305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04657806754112244,
      "backward_entropy": 0.005439731809828017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.5867919921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04545237496495247,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04648052453994751,
      "backward_entropy": 0.005438821183310615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.839163780212402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0454893484711647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04638618230819702,
      "backward_entropy": 0.005438826150364346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.885923385620117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045527372509241104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.046288174390792844,
      "backward_entropy": 0.005439179639021556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.090349197387695,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.045567095279693604,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.046184754371643065,
      "backward_entropy": 0.07656452390882704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.521475791931152,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04561121016740799,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0460665225982666,
      "backward_entropy": 0.005435998241106669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.181347846984863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0456555113196373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04594816565513611,
      "backward_entropy": 0.005433368600077099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1418070793151855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04569747671484947,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04583777785301209,
      "backward_entropy": 0.005433525476190779,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.159293174743652,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04573727771639824,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04573466181755066,
      "backward_entropy": 0.006318704949484931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.072122573852539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045776788145303726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04563230872154236,
      "backward_entropy": 0.00543723710709148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.986368179321289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04581699147820473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04552799463272095,
      "backward_entropy": 0.005438557929462857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.910633087158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04585757851600647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04542321264743805,
      "backward_entropy": 0.00543761005004247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0211331844329834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045897699892520905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04532047510147095,
      "backward_entropy": 0.005436001552475823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.798860549926758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045934613794088364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.045229533314704896,
      "backward_entropy": 0.005433920356962416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.655435562133789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0459708496928215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.045140182971954344,
      "backward_entropy": 0.005434771378835042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.72793197631836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046007197350263596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04505024254322052,
      "backward_entropy": 0.005436369942294227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.3866548538208,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046042438596487045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044964995980262754,
      "backward_entropy": 0.005436054948303435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.291312217712402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046078529208898544,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04487721621990204,
      "backward_entropy": 0.005434172021018134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.193692207336426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04611537232995033,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044787359237670896,
      "backward_entropy": 0.005430923567877876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.855405807495117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04615287855267525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04469566345214844,
      "backward_entropy": 0.005426413069168727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.628149509429932,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04619227722287178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044596678018569945,
      "backward_entropy": 0.005424427489439647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.880996704101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04622964188456535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0445046603679657,
      "backward_entropy": 0.005424108770158555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.833315372467041,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04626769572496414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044410550594329835,
      "backward_entropy": 0.005423187381691403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.677458763122559,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046302713453769684,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.044327735900878906,
      "backward_entropy": 0.005973995559745365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.696043014526367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046338751912117004,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044241014122962954,
      "backward_entropy": 0.005420913298924764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.872819900512695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04637825861573219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04414258003234863,
      "backward_entropy": 0.005419288244512346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.03619384765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046419743448495865,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.044038301706314086,
      "backward_entropy": 0.005414631217718124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.978808403015137,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046462371945381165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04393041133880615,
      "backward_entropy": 0.00587467642294036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.31202507019043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04650354012846947,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.043827226758003233,
      "backward_entropy": 0.005407556891441345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.8608717918396,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046542391180992126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04373222589492798,
      "backward_entropy": 0.005406044009659026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.979146957397461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04658007621765137,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04364126920700073,
      "backward_entropy": 0.005405801037947337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6263999938964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04661816358566284,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0435496985912323,
      "backward_entropy": 0.005403512467940648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.70911979675293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046653494238853455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04346748888492584,
      "backward_entropy": 0.005403709494405323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.713726043701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046687789261341095,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.043389350175857544,
      "backward_entropy": 0.005737334489822388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.093306541442871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04672474041581154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04330146312713623,
      "backward_entropy": 0.005402410609854592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.000739097595215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04676138982176781,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0432153046131134,
      "backward_entropy": 0.005401284744342168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.76498031616211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046799249947071075,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04312597513198853,
      "backward_entropy": 0.0053971873389350045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.247586250305176,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046840254217386246,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04302583634853363,
      "backward_entropy": 0.005644860366980235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.778983116149902,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04688182473182678,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0429229199886322,
      "backward_entropy": 0.005394028292761909,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.886849880218506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04692251607775688,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.042823764681816104,
      "backward_entropy": 0.005393116424481074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.204667568206787,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046960871666669846,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04273291826248169,
      "backward_entropy": 0.00539272278547287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.170586109161377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046998150646686554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04264534711837768,
      "backward_entropy": 0.005394944300254186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.131505966186523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04703424498438835,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04256210923194885,
      "backward_entropy": 0.005397239079078038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.402209281921387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04706912487745285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04248368740081787,
      "backward_entropy": 0.005398182405365838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.614852905273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04710371792316437,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.042406785488128665,
      "backward_entropy": 0.005397712604867088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.958611965179443,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04713907465338707,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04232714772224426,
      "backward_entropy": 0.005397835953368081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.899193286895752,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0471733994781971,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.042251366376876834,
      "backward_entropy": 0.005398288783099916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.572249412536621,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047206856310367584,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04217836856842041,
      "backward_entropy": 0.005399384018447664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.062409400939941,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047238923609256744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0421094685792923,
      "backward_entropy": 0.005403625882334179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3440005779266357,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04727092385292053,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04204137623310089,
      "backward_entropy": 0.005405697557661269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.138002395629883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04730035737156868,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04198296070098877,
      "backward_entropy": 0.005406230688095093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.037090301513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04733084514737129,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04192115068435669,
      "backward_entropy": 0.0054056694110234576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.640833854675293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04736248403787613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041854843497276306,
      "backward_entropy": 0.005406355692280663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.435378074645996,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04739325866103172,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04179222881793976,
      "backward_entropy": 0.005328371706936095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.558990478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04742245003581047,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041735482215881345,
      "backward_entropy": 0.005404195023907555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.637757301330566,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04745093733072281,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.041681551933288576,
      "backward_entropy": 0.07676387495464748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.595701217651367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04747965186834335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041627296805381776,
      "backward_entropy": 0.005396112385723326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.544754028320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047508418560028076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04157358705997467,
      "backward_entropy": 0.005388968106773164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.335361003875732,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04753715917468071,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04152107238769531,
      "backward_entropy": 0.005378818346394433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.2520833015441895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04756566509604454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041468507051467894,
      "backward_entropy": 0.005372461345460679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.486679077148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0475928820669651,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041420483589172365,
      "backward_entropy": 0.005366959505610996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.374234199523926,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04762178286910057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04136691689491272,
      "backward_entropy": 0.005358013427919812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.183137893676758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047652408480644226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041307711601257326,
      "backward_entropy": 0.005348561124669181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.172122955322266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04768238961696625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.041251009702682494,
      "backward_entropy": 0.005340902341736687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.070860862731934,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04771234467625618,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04119479060173035,
      "backward_entropy": 0.005332230279843013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.968171119689941,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047744881361722946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04112963676452637,
      "backward_entropy": 0.005324523482057784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.014037609100342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04777903109788895,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04105877876281738,
      "backward_entropy": 0.005318618483013577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.842494010925293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04781195521354675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04099255204200745,
      "backward_entropy": 0.005311575200822618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9557244777679443,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04784544184803963,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04092467427253723,
      "backward_entropy": 0.005304449962245094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9244565963745117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047877293080091476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04086207151412964,
      "backward_entropy": 0.0053000715043809675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.60543441772461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047907669097185135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040804213285446166,
      "backward_entropy": 0.005298197269439697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.762423038482666,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04793883115053177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040743899345397946,
      "backward_entropy": 0.005296145876248677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.301005363464355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04796918109059334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04068648219108582,
      "backward_entropy": 0.005294559730423821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.649204730987549,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04800127074122429,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04062298536300659,
      "backward_entropy": 0.005294402440388997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.286380767822266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048032667487859726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04056148827075958,
      "backward_entropy": 0.005296462939845191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.21671199798584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04806484654545784,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04049746990203858,
      "backward_entropy": 0.005299160050021278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7500853538513184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0480976440012455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04043156504631042,
      "backward_entropy": 0.005301314095656077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.265256881713867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04812845587730408,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.04037342369556427,
      "backward_entropy": 0.00472692276040713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.2139387130737305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04815932735800743,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04031469225883484,
      "backward_entropy": 0.005302412228451835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.663289070129395,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0481901653110981,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.04025594592094421,
      "backward_entropy": 0.07676496108373006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.632472038269043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048222582787275314,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.040191429853439334,
      "backward_entropy": 0.005307321747144063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.296318531036377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04825302213430405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04013463854789734,
      "backward_entropy": 0.005308018376429875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.230474472045898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04828278347849846,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04008002877235413,
      "backward_entropy": 0.0053102196090751225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.61275863647461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04831215739250183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.04002598226070404,
      "backward_entropy": 0.005316026508808136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.178793907165527,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04834246635437012,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03996865749359131,
      "backward_entropy": 0.005322736998399098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.137789249420166,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04837200045585632,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03991410732269287,
      "backward_entropy": 0.07678381601969402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.756090521812439,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04840083047747612,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039861935377120974,
      "backward_entropy": 0.00533565878868103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.060825347900391,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04842773452401161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03981572985649109,
      "backward_entropy": 0.005344742288192113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.922317504882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0484541691839695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039771091938018796,
      "backward_entropy": 0.00535364697376887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.086464881896973,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048482462763786316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039720278978347776,
      "backward_entropy": 0.005362367050515281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.190433502197266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048513904213905334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03965899646282196,
      "backward_entropy": 0.005370936459965176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.908034324645996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04854556918144226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03959789276123047,
      "backward_entropy": 0.005375756157769097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7285300493240356,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04857616499066353,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03954060077667236,
      "backward_entropy": 0.005379607280095418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.378349781036377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048604145646095276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03949291110038757,
      "backward_entropy": 0.0053805700606769985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.926689147949219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04863226041197777,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03944439888000488,
      "backward_entropy": 0.0053823064598772265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.748178005218506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04866079241037369,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03939538598060608,
      "backward_entropy": 0.005379971530702379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.230179309844971,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048688530921936035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03934907913208008,
      "backward_entropy": 0.004419829282495711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.660085678100586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04871632531285286,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03930256962776184,
      "backward_entropy": 0.00537560342086686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.125422477722168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04874516278505325,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03925194442272186,
      "backward_entropy": 0.005376562476158142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.079550743103027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04877392575144768,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039201653003692626,
      "backward_entropy": 0.005377558370431264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.476368427276611,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04880374297499657,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039148730039596555,
      "backward_entropy": 0.005374999509917365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.4993672370910645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04883420839905739,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0390934944152832,
      "backward_entropy": 0.00537387447224723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0013463497161865,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0488637275993824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.039041554927825926,
      "backward_entropy": 0.005372925351063411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9935944080352783,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048891790211200714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03899402916431427,
      "backward_entropy": 0.005373639365037282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9517576694488525,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04891837760806084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03895139694213867,
      "backward_entropy": 0.005374059908919864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.358985424041748,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048943787813186646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038912230730056764,
      "backward_entropy": 0.005376087294684516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.717864990234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04896864667534828,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03887507021427154,
      "backward_entropy": 0.005377342303593953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.2911376953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04899374768137932,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038837045431137085,
      "backward_entropy": 0.0053785670962598585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.663078308105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049018315970897675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038800889253616334,
      "backward_entropy": 0.005379025306966569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.908276557922363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04904282093048096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03876589238643646,
      "backward_entropy": 0.005375971396764119,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.250226974487305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0490686409175396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03872559666633606,
      "backward_entropy": 0.005377086500326793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.178546905517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049095895141363144,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03868053555488586,
      "backward_entropy": 0.005377235925859875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.444504737854004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04912213981151581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03863990902900696,
      "backward_entropy": 0.005374680790636275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.024184226989746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04915228113532066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03858727812767029,
      "backward_entropy": 0.005369149148464203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.416416049003601,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04918338358402252,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03853184282779694,
      "backward_entropy": 0.005363430000013775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7040696144104004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049212101846933365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038483887910842896,
      "backward_entropy": 0.00535899152358373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9680063724517822,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04923923686146736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03844122290611267,
      "backward_entropy": 0.005354863074090745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.460454940795898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049265507608652115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038401633501052856,
      "backward_entropy": 0.0053498273094495135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6239213943481445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04929235205054283,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038360309600830075,
      "backward_entropy": 0.005344298978646596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3726699352264404,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04931787773966789,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03832299709320068,
      "backward_entropy": 0.005340750432676739,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.032968997955322,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04934142157435417,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03829227089881897,
      "backward_entropy": 0.00533761911922031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.590860605239868,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049365393817424774,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03825950622558594,
      "backward_entropy": 0.005337342205974791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.395506381988525,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049388010054826736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03823190629482269,
      "backward_entropy": 0.005334622744056914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.509187936782837,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.049412231892347336,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03819877505302429,
      "backward_entropy": 0.07680672407150269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.860566139221191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04943546652793884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03816854953765869,
      "backward_entropy": 0.0053319744765758514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6594481468200684,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049459218978881836,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03813613653182983,
      "backward_entropy": 0.005335129797458649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.478846788406372,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0494825653731823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038104969263076785,
      "backward_entropy": 0.005338368730412589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6083338260650635,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049504686146974564,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03807814717292786,
      "backward_entropy": 0.005339574482705858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.867455959320068,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04952647164463997,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03805241584777832,
      "backward_entropy": 0.00534061011340883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.66581392288208,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049549419432878494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.038022634387016294,
      "backward_entropy": 0.005343309707111782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3776328563690186,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049572866410017014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037990710139274596,
      "backward_entropy": 0.005348677022589577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5001718997955322,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04959529638290405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03796192705631256,
      "backward_entropy": 0.005354532765017616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3624160289764404,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049617256969213486,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037935110926628116,
      "backward_entropy": 0.0053587667644023895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3293142318725586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04963810369372368,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03791232705116272,
      "backward_entropy": 0.005361003180344899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.405100107192993,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04965810105204582,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03789215087890625,
      "backward_entropy": 0.005363399783770244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1731250286102295,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0496780090034008,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03787201046943665,
      "backward_entropy": 0.00536653565035926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.271331310272217,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04969686642289162,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037854325771331784,
      "backward_entropy": 0.005373811970154445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3333511352539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04971513897180557,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03783833980560303,
      "backward_entropy": 0.0053815655410289764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2059001922607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0497334748506546,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037821871042251584,
      "backward_entropy": 0.005389331115616692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.366828441619873,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04975028336048126,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03781107366085053,
      "backward_entropy": 0.005393128428194258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.337252616882324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049767736345529556,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03779834508895874,
      "backward_entropy": 0.005395367327663634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2152211666107178,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04978576675057411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03778391480445862,
      "backward_entropy": 0.005396235320303176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1374081373214722,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049803052097558975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037772062420845035,
      "backward_entropy": 0.005395616922113631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.242635726928711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049819301813840866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03776311278343201,
      "backward_entropy": 0.005396694772773319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.258079528808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04983631893992424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037751543521881106,
      "backward_entropy": 0.005397360771894455,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1548097133636475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049855343997478485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03773311376571655,
      "backward_entropy": 0.005398654689391454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.143528699874878,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04987433925271034,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037714651226997374,
      "backward_entropy": 0.005400270637538698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1424784660339355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049893155694007874,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.037696951627731325,
      "backward_entropy": 0.0034982172979248893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1026697158813477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04991348087787628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03767486810684204,
      "backward_entropy": 0.005398433241579268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1141877174377441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04993284121155739,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03765629529953003,
      "backward_entropy": 0.005394991073343489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0401766300201416,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049950532615184784,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0376437783241272,
      "backward_entropy": 0.005388656010230382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.978003978729248,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04996824637055397,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03763076961040497,
      "backward_entropy": 0.005383688128656811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.009491205215454,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04998699203133583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03761439621448517,
      "backward_entropy": 0.005377237995465596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0506919622421265,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05000544339418411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0375991553068161,
      "backward_entropy": 0.005369551893737581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0100884437561035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050022710114717484,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03758729100227356,
      "backward_entropy": 0.005364046742518743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.753010272979736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050039272755384445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037577849626541135,
      "backward_entropy": 0.005357742723491456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0311022996902466,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050057828426361084,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03756117820739746,
      "backward_entropy": 0.005354007913006676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.729321002960205,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050075139850378036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03754855394363403,
      "backward_entropy": 0.005351526455746757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9294967651367188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050093866884708405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03753014504909515,
      "backward_entropy": 0.005352196594079335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8311362266540527,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050112035125494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037513166666030884,
      "backward_entropy": 0.00535502615902159,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9988446831703186,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05013026297092438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03749595284461975,
      "backward_entropy": 0.005359349151452382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7801129817962646,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05014728009700775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03748257160186767,
      "backward_entropy": 0.005364087720712026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9838380813598633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050164595246315,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037467604875564574,
      "backward_entropy": 0.005371899240546756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.407867431640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050180789083242416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037456148862838747,
      "backward_entropy": 0.005379217780298657,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.594562292098999,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050199076533317566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03743751347064972,
      "backward_entropy": 0.005388481749428643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6975576877593994,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05021820217370987,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03741556406021118,
      "backward_entropy": 0.00540115518702401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8061445951461792,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05023737996816635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03739306628704071,
      "backward_entropy": 0.00541562835375468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.376285076141357,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05025603994727135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03737185001373291,
      "backward_entropy": 0.005432056056128608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6372742652893066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050275932997465134,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037346309423446654,
      "backward_entropy": 0.005449660536315706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.010619640350342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05029568821191788,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037321197986602786,
      "backward_entropy": 0.005467410716745589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4225313663482666,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05031748116016388,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03728938102722168,
      "backward_entropy": 0.005483311083581712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4000189304351807,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05033959448337555,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0372562825679779,
      "backward_entropy": 0.0031928070303466585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.202839374542236,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050361838191747665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03722274899482727,
      "backward_entropy": 0.005516943832238515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.9729228019714355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050384584814310074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037187892198562625,
      "backward_entropy": 0.005530719541841083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.9206953048706055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050408367067575455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03714997768402099,
      "backward_entropy": 0.0055423395501242746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6546016931533813,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05043306201696396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.037109392881393435,
      "backward_entropy": 0.005551864703496297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4219205379486084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05045663192868233,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03707183003425598,
      "backward_entropy": 0.005563476019435459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.660312533378601,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050479769706726074,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03703509569168091,
      "backward_entropy": 0.0031627414541112054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4073939323425293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05050143226981163,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03700376749038696,
      "backward_entropy": 0.005586778952015771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.349543809890747,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050522416830062866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03697497248649597,
      "backward_entropy": 0.005593615687555737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8742918968200684,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05054318904876709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03694608807563782,
      "backward_entropy": 0.005602864755524529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.321183204650879,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05056446045637131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036915987730026245,
      "backward_entropy": 0.005609278877576192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2925944328308105,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05058526247739792,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03688717782497406,
      "backward_entropy": 0.005615529086854722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.288343667984009,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05060569941997528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036859160661697386,
      "backward_entropy": 0.005622454815440708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9901740550994873,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050625596195459366,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03683304190635681,
      "backward_entropy": 0.005627551840411292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7944616079330444,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05064557492733002,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03680671453475952,
      "backward_entropy": 0.005631117771069209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2128820419311523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050664085894823074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0367847740650177,
      "backward_entropy": 0.005634038398663203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6232926845550537,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05068235844373703,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03676323592662811,
      "backward_entropy": 0.005637165572908189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.166546106338501,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050701286643743515,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03673990368843079,
      "backward_entropy": 0.005638056331210666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4506114721298218,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05071999132633209,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03671680092811584,
      "backward_entropy": 0.005640280743439992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8105452060699463,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05073798447847366,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036695423722267154,
      "backward_entropy": 0.0056439754035737776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7592113614082336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05075640231370926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03667220771312714,
      "backward_entropy": 0.005649140311612023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1127493381500244,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05077342316508293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03665342926979065,
      "backward_entropy": 0.0056531379620234174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0713658332824707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05079010874032974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03663603067398071,
      "backward_entropy": 0.005654461681842804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3997827768325806,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05080676078796387,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03661833107471466,
      "backward_entropy": 0.005656911267174615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0422613620758057,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05082274600863457,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03660261631011963,
      "backward_entropy": 0.005658924993541505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7022652626037598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050838690251111984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03658679127693176,
      "backward_entropy": 0.005661070346832275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.245449542999268,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05085481330752373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036570999026298526,
      "backward_entropy": 0.005659547530942493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2776453495025635,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05087338387966156,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03654763698577881,
      "backward_entropy": 0.07689687940809461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.862358808517456,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05089254677295685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03652245998382568,
      "backward_entropy": 0.005655312289794286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3233174085617065,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050912924110889435,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036493197083473206,
      "backward_entropy": 0.005654320120811462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.33699631690979,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050932250916957855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03646714687347412,
      "backward_entropy": 0.00565366612540351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.07736137509346008,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.050950296223163605,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03644582331180572,
      "backward_entropy": 0.0768928395377265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.091885566711426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05096643418073654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036430418491363525,
      "backward_entropy": 0.005643878959947162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2777804136276245,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05098363384604454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03641122579574585,
      "backward_entropy": 0.005641519609424803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.44107985496521,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05100003257393837,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03639463484287262,
      "backward_entropy": 0.002860407034556071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.417915105819702,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051016915589571,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036376142501831056,
      "backward_entropy": 0.005638393263022105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4045023918151855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05103423073887825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0363559901714325,
      "backward_entropy": 0.005639808045493232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7702831029891968,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0510517954826355,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.036334949731826785,
      "backward_entropy": 0.0028288871463802126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6437914371490479,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05106950178742409,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03631234169006348,
      "backward_entropy": 0.005648451132906808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7969608306884766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051085878163576126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.03629409372806549,
      "backward_entropy": 0.0028154003537363475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7405444383621216,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05110171064734459,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036278432607650755,
      "backward_entropy": 0.005655485308832592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.857482671737671,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05111757665872574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036262258887290955,
      "backward_entropy": 0.005658481683995988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2666101455688477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05113418400287628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036243975162506104,
      "backward_entropy": 0.005660080661376317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6250110864639282,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05115112289786339,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036224371194839476,
      "backward_entropy": 0.005662731826305389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6095972061157227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05116666853427887,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03620941638946533,
      "backward_entropy": 0.005663079933987724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.661637783050537,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05118108540773392,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036197888851165774,
      "backward_entropy": 0.005663168099191453,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.738299608230591,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051195647567510605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03618538975715637,
      "backward_entropy": 0.005665771663188934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.627549648284912,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0512109138071537,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03617111146450043,
      "backward_entropy": 0.005665663629770279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5649800896644592,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05122629553079605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03615584075450897,
      "backward_entropy": 0.0056689762406879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.651130199432373,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05124088376760483,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036142224073410036,
      "backward_entropy": 0.005675795177618663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6070500612258911,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051256339997053146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03612607717514038,
      "backward_entropy": 0.005681605802641975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0918967723846436,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051271550357341766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036110949516296384,
      "backward_entropy": 0.005685815380679237,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0834102630615234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05128597095608711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03609876036643982,
      "backward_entropy": 0.005687425120009316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0507283210754395,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05130068212747574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036085891723632815,
      "backward_entropy": 0.005687393662002351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5266541242599487,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05131583660840988,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03607134222984314,
      "backward_entropy": 0.005688135822614034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.05669347196817398,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05133107677102089,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.036055856943130495,
      "backward_entropy": 0.0026672730843226113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9689064025878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05134483426809311,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03604470491409302,
      "backward_entropy": 0.005696475919750001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.502672553062439,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05135997384786606,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03602924942970276,
      "backward_entropy": 0.07689777347776625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4754940271377563,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05137493088841438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.036014437675476074,
      "backward_entropy": 0.005704384297132492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4833909273147583,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0513899028301239,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03599920868873596,
      "backward_entropy": 0.005709668000539144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4609031677246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051404569298028946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035985279083251956,
      "backward_entropy": 0.00571243092417717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3674261569976807,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051419083029031754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035971814393997194,
      "backward_entropy": 0.0057147108018398285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9562317132949829,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05143442377448082,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03595558404922485,
      "backward_entropy": 0.00571768151389228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4236481189727783,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051449354737997055,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035939973592758176,
      "backward_entropy": 0.005723859700891707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9635009765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0514640249311924,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03592529296875,
      "backward_entropy": 0.005728334602382448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.278014898300171,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05147797614336014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03591312170028686,
      "backward_entropy": 0.005730923679139879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5030260682106018,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05149276927113533,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03589818775653839,
      "backward_entropy": 0.005734044230646557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.103200674057007,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051506407558918,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03588693141937256,
      "backward_entropy": 0.00573572392265002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3402115106582642,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05152185261249542,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03586980700492859,
      "backward_entropy": 0.0057388171553611755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.180645227432251,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051537200808525085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035852649807929994,
      "backward_entropy": 0.005743609948290719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.169281244277954,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051553331315517426,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03583273887634277,
      "backward_entropy": 0.0057500795357757145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2972935438156128,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05156997591257095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035811424255371094,
      "backward_entropy": 0.0057556894090440534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.4657154381275177,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05158637836575508,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03579053580760956,
      "backward_entropy": 0.07690870761871338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7010772228240967,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05160156264901161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03577334582805634,
      "backward_entropy": 0.00576938937107722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.45641443133354187,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05161673203110695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0357566773891449,
      "backward_entropy": 0.005772983862294091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8499425649642944,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05163079872727394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03574320673942566,
      "backward_entropy": 0.005775984790590074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4431655406951904,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051644325256347656,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035731160640716554,
      "backward_entropy": 0.005779566036330329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.03274301812052727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051658835262060165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03571643829345703,
      "backward_entropy": 0.005780666238731808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2075133323669434,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05167214199900627,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03570455312728882,
      "backward_entropy": 0.005785682549079259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8215494751930237,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05168548971414566,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03569212555885315,
      "backward_entropy": 0.005792328466971715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5842785835266113,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051698293536901474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03568134307861328,
      "backward_entropy": 0.005798165996869405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.4228159785270691,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05171132832765579,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035670092701911925,
      "backward_entropy": 0.005801810158623589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5473002195358276,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05172351002693176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03566116690635681,
      "backward_entropy": 0.005806044158008363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8033824563026428,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05173608660697937,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0356509655714035,
      "backward_entropy": 0.005809788074758317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8845419883728027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05174800753593445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035643094778060914,
      "backward_entropy": 0.005810507883628209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.134871006011963,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05176078528165817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035632464289665225,
      "backward_entropy": 0.005811697079075707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.40705251693725586,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0517735555768013,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03562155365943909,
      "backward_entropy": 0.0769148137834337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.38470518589019775,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051785439252853394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03561316430568695,
      "backward_entropy": 0.00581623117129008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4784775972366333,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051796805113554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03560553193092346,
      "backward_entropy": 0.0058224913146760725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.752419114112854,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05180838704109192,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.03559783101081848,
      "backward_entropy": 0.07691572772132026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.39242181181907654,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05181951820850372,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03559157550334931,
      "backward_entropy": 0.005825522045294444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.422743320465088,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05182991921901703,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03558735549449921,
      "backward_entropy": 0.0058265676101048785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0767945051193237,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05184090510010719,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03558114171028137,
      "backward_entropy": 0.005828505174981223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3880934715270996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051851872354745865,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.035575073957443235,
      "backward_entropy": 0.005829053206576241,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.392034649848938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051863521337509155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.03556633293628693,
      "backward_entropy": 0.005832774357663261,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 1.924061702080071,
    "avg_log_Z": -0.051133908331394196,
    "success_rate": 1.0,
    "avg_reward": 79.8,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.06,
      "1": 0.06,
      "2": 0.88
    },
    "avg_forward_entropy": 0.03629145464301109,
    "avg_backward_entropy": 0.00978800723122226,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}