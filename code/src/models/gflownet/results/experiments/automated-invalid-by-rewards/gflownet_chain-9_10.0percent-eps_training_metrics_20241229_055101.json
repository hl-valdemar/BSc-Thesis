{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07700569099850124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07698180940416124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.18026733398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958255529403686,
      "backward_entropy": 0.07701579729715984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.14891052246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00010000000474974513,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10957506895065308,
      "backward_entropy": 0.0770063665178087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.15821838378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0001999999221879989,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10956752300262451,
      "backward_entropy": 0.077015221118927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.0868682861328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00029725083732046187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10955991744995117,
      "backward_entropy": 0.07700764470630223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.37661743164062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0003956705331802368,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10955218076705933,
      "backward_entropy": 0.07699031300014919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.02488708496094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.000493970001116395,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954434871673584,
      "backward_entropy": 0.07699222697152032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.1748504638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005930294282734394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095363974571228,
      "backward_entropy": 0.07700940635469225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.2915802001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.000692721747327596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952832698822021,
      "backward_entropy": 0.07701290316051906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.8539581298828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0007918656920082867,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10952017307281495,
      "backward_entropy": 0.07699749204847547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.07850646972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0008906949078664184,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10951192378997802,
      "backward_entropy": 0.07699906826019287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.62957763671875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0009913236135616899,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10950348377227784,
      "backward_entropy": 0.07700060473548041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.74798583984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0010934927267953753,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10949485301971436,
      "backward_entropy": 0.07700206836064656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.54946899414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0011964782606810331,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948607921600342,
      "backward_entropy": 0.07701240645514594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.91807556152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001300628180615604,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10947713851928711,
      "backward_entropy": 0.07700476381513807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.05921936035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0014056825311854482,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10946801900863648,
      "backward_entropy": 0.07701325416564941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.55198669433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.001511543057858944,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945876836776733,
      "backward_entropy": 0.0770136449072096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.03208923339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001617603236809373,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10944941043853759,
      "backward_entropy": 0.07700530687967937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.70252990722656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001724720816127956,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10943994522094727,
      "backward_entropy": 0.07700914144515991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.25555419921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018315694760531187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10943037271499634,
      "backward_entropy": 0.07701463169521755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.85438537597656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0019382305908948183,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10942069292068482,
      "backward_entropy": 0.0770108699798584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.99742126464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0020453608594834805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10941107273101806,
      "backward_entropy": 0.07700056499905056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.25228881835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0021520815789699554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10940148830413818,
      "backward_entropy": 0.07699926694234212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.11302185058594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0022546714171767235,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10939199924468994,
      "backward_entropy": 0.07701291640599568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.6625518798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0023565255105495453,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938239097595215,
      "backward_entropy": 0.07701574431525336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.24974060058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0024589071981608868,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10937267541885376,
      "backward_entropy": 0.07699515422185262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.64825439453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002562472829595208,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10936280488967895,
      "backward_entropy": 0.07701436678568523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.37115478515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0026676016859710217,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935288667678833,
      "backward_entropy": 0.0770147442817688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.72511291503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0027725868858397007,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10934290885925294,
      "backward_entropy": 0.07701620790693495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.27882385253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0028783921152353287,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10933282375335693,
      "backward_entropy": 0.07698935932583278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.26905822753906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002981072524562478,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10932276248931885,
      "backward_entropy": 0.07701558536953396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 273.2478942871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0030818688683211803,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10931267738342285,
      "backward_entropy": 0.0770157774289449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.591552734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0031860468443483114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10930228233337402,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.77944946289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0032861509826034307,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10929198265075683,
      "backward_entropy": 0.0769831207063463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.12841796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0033855082001537085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10928159952163696,
      "backward_entropy": 0.07698146502176921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 256.0586853027344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003486118745058775,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10927102565765381,
      "backward_entropy": 0.0770162476433648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.08607482910156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003589560277760029,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10926017761230469,
      "backward_entropy": 0.07701630062527126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.1933898925781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003690839745104313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1092493176460266,
      "backward_entropy": 0.07697641849517822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.1570281982422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003794661955907941,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10923815965652466,
      "backward_entropy": 0.07701635360717773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.82208251953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003899222007021308,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10922684669494628,
      "backward_entropy": 0.07697294155756633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.36900329589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003999391105026007,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10921568870544433,
      "backward_entropy": 0.07701634036170112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.02694702148438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004097580444067717,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10920453071594238,
      "backward_entropy": 0.07701631387074788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.46078491210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004197991918772459,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10919311046600341,
      "backward_entropy": 0.0769673850801256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.94215393066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004298283718526363,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10918152332305908,
      "backward_entropy": 0.07701533370547825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.08680725097656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004394283052533865,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1091700553894043,
      "backward_entropy": 0.07701619466145833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.9918975830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004492939915508032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10915830135345458,
      "backward_entropy": 0.076961530579461,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.88002014160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004589970223605633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1091465711593628,
      "backward_entropy": 0.07695957024892171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.46298217773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004688195418566465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1091346025466919,
      "backward_entropy": 0.07695757018195258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.89834594726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0047867028042674065,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10912249088287354,
      "backward_entropy": 0.07701598273383246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.3889617919922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004883585497736931,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10911035537719727,
      "backward_entropy": 0.07701591650644939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.24256896972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004980916623026133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10909806489944458,
      "backward_entropy": 0.0769514110353258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.8667449951172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0050766607746481895,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10908564329147338,
      "backward_entropy": 0.0769492851363288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.72801208496094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005173954181373119,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10907291173934937,
      "backward_entropy": 0.07701574431525336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.42576599121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005270674824714661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10906002521514893,
      "backward_entropy": 0.07694498035642836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.7439422607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005368048790842295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10904693603515625,
      "backward_entropy": 0.07694282796647814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.30152893066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00546679925173521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10903396606445312,
      "backward_entropy": 0.07694064908557469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.38856506347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005563520826399326,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1090212345123291,
      "backward_entropy": 0.07701558536953396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.62347412109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005662806332111359,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10900816917419434,
      "backward_entropy": 0.0769360860188802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.36378479003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005763242486864328,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10899484157562256,
      "backward_entropy": 0.0769337879286872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.19984436035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005864935927093029,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10898126363754272,
      "backward_entropy": 0.0770155323876275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.37957763671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0059666926972568035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10896753072738648,
      "backward_entropy": 0.07692926459842259,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.98080444335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006068249698728323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10895364284515381,
      "backward_entropy": 0.07692692014906141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.47862243652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006171625107526779,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10893940925598145,
      "backward_entropy": 0.07700880368550618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.85585021972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006272311322391033,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.108925199508667,
      "backward_entropy": 0.07701550589667426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.84527587890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006372983567416668,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10891081094741821,
      "backward_entropy": 0.07691930400000678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.09317016601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006473212502896786,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10889623165130616,
      "backward_entropy": 0.0770154661602444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.44931030273438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006576589774340391,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10888124704360962,
      "backward_entropy": 0.07700648572709826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.7942352294922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006682632956653833,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10886585712432861,
      "backward_entropy": 0.07691102557712132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.17495727539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006789060775190592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10885025262832641,
      "backward_entropy": 0.07690829700893825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 272.2012939453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0068912506103515625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10883477926254273,
      "backward_entropy": 0.07690531677669948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.1555633544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006997046060860157,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10881878137588501,
      "backward_entropy": 0.07690239614910549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.19345092773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00710415281355381,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10880247354507447,
      "backward_entropy": 0.07700261804792616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.14024353027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007213400676846504,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10878577232360839,
      "backward_entropy": 0.07689660787582397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.7928924560547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00732450932264328,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10876871347427368,
      "backward_entropy": 0.07701540655559963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.01602172851562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007431065663695335,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10875186920166016,
      "backward_entropy": 0.07701540655559963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.08856201171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0075364182703197,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10873489379882813,
      "backward_entropy": 0.07699905501471625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.88031005859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007639772724360228,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10871789455413819,
      "backward_entropy": 0.07688425646887885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.8847198486328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007744224276393652,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10870051383972168,
      "backward_entropy": 0.07699685626559788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.02993774414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0078476807102561,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10868237018585206,
      "backward_entropy": 0.07687697807947795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.98333740234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007950418628752232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10866371393203736,
      "backward_entropy": 0.07687320974138048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.41661071777344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00805251207202673,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10864462852478027,
      "backward_entropy": 0.0770152144961887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.5680389404297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00815188605338335,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10862543582916259,
      "backward_entropy": 0.07701516151428223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.19525146484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008249002508819103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10860614776611328,
      "backward_entropy": 0.07686123583051893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.03675842285156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008344876579940319,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10858660936355591,
      "backward_entropy": 0.07701502905951606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 271.7303466796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008437789976596832,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1085671067237854,
      "backward_entropy": 0.07685242096583049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.04330444335938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008535290136933327,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10854676961898804,
      "backward_entropy": 0.07698520686891344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.93423461914062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008631943725049496,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10852625370025634,
      "backward_entropy": 0.07698350482516819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.792236328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008730542846024036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10850496292114258,
      "backward_entropy": 0.07683918211195204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.44313049316406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008829115889966488,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10848301649093628,
      "backward_entropy": 0.07701468467712402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.4031219482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008924516849219799,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10846065282821656,
      "backward_entropy": 0.07682998975118001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.57264709472656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009017069824039936,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10843808650970459,
      "backward_entropy": 0.07701453897688124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.58486938476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009106785990297794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10841532945632934,
      "backward_entropy": 0.07682005564371745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.67079162597656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009197455830872059,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10839192867279053,
      "backward_entropy": 0.07701432704925537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.926025390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009284473955631256,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1083686113357544,
      "backward_entropy": 0.07696947124269274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.99247741699219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009375585243105888,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1083442211151123,
      "backward_entropy": 0.07701408863067627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.76950073242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009461679495871067,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10832016468048096,
      "backward_entropy": 0.07696445782979329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.71697998046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00954669713973999,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10829583406448365,
      "backward_entropy": 0.07701377736197577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.08465576171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009634259156882763,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10827075242996216,
      "backward_entropy": 0.0769589212205675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.21640014648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009719309397041798,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10824562311172485,
      "backward_entropy": 0.07677947150336371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.16488647460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009804693050682545,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10822008848190308,
      "backward_entropy": 0.07695290777418348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.75827026367188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00989038497209549,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10819427967071533,
      "backward_entropy": 0.0767674446105957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.41868591308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009973577223718166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10816843509674072,
      "backward_entropy": 0.07676104704538982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.58050537109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010059582069516182,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10814181566238404,
      "backward_entropy": 0.07694313261244032,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 237.6659698486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010147126391530037,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10811464786529541,
      "backward_entropy": 0.07674866252475315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.86561584472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01023807842284441,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10808678865432739,
      "backward_entropy": 0.07693658272425334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.95814514160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010329954326152802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10805888175964355,
      "backward_entropy": 0.07673678133222792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.92486572265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010418365709483624,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10803130865097046,
      "backward_entropy": 0.07701230049133301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.836669921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010503172874450684,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10800395011901856,
      "backward_entropy": 0.07672406567467584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.58343505859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010585973970592022,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10797660350799561,
      "backward_entropy": 0.07671713829040527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.832275390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010671782307326794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1079483985900879,
      "backward_entropy": 0.07671042283376057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.16397094726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010758108459413052,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10791981220245361,
      "backward_entropy": 0.07701171769036187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.85960388183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010840950533747673,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10789145231246948,
      "backward_entropy": 0.07669656806521946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.13592529296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010921708308160305,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10786304473876954,
      "backward_entropy": 0.07690600554148357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.81944274902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011005556210875511,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10783374309539795,
      "backward_entropy": 0.07668121655782063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.91822814941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011089876294136047,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10780400037765503,
      "backward_entropy": 0.07689713107215033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.1563262939453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011172669939696789,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10777428150177001,
      "backward_entropy": 0.07689248190985785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.28915405273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011252552270889282,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10774476528167724,
      "backward_entropy": 0.07665737469991048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.69309997558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01132726576179266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10771589279174805,
      "backward_entropy": 0.07688181930118138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01140220370143652,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10768661499023438,
      "backward_entropy": 0.07687608400980632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.29339599609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01147982757538557,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10765647888183594,
      "backward_entropy": 0.07687034209569295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.84788513183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011556245386600494,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10762621164321899,
      "backward_entropy": 0.07686458693610297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.69573974609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01163169089704752,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10759581327438354,
      "backward_entropy": 0.07685865296257867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.32949829101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0117080332711339,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10756503343582154,
      "backward_entropy": 0.07685273223453098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.70928955078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01178574189543724,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10753358602523803,
      "backward_entropy": 0.07684675852457683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.26324462890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01186237670481205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.107502019405365,
      "backward_entropy": 0.07658240530225965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.35902404785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011938156560063362,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10747029781341552,
      "backward_entropy": 0.07683408922619289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 217.84799194335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012015104293823242,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10743787288665771,
      "backward_entropy": 0.07656208674112956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.51898193359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012096375226974487,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10740429162979126,
      "backward_entropy": 0.07700749238332112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 283.14617919921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012176239863038063,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10737053155899048,
      "backward_entropy": 0.07681490977605183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.38121032714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012264407239854336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10733468532562256,
      "backward_entropy": 0.07680917448467678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 298.81005859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012354079633951187,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10729808807373047,
      "backward_entropy": 0.07680343257056342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.71620178222656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01245202124118805,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10725948810577393,
      "backward_entropy": 0.07700773080190022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.18693542480469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012547223828732967,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10722098350524903,
      "backward_entropy": 0.0767931408352322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.09931945800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012637653388082981,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10718314647674561,
      "backward_entropy": 0.0767872863345676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.9786376953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012721776962280273,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10714621543884277,
      "backward_entropy": 0.07678058412339953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.0978546142578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012805424630641937,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10710910558700562,
      "backward_entropy": 0.07700788974761963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.67010498046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01288739312440157,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10707193613052368,
      "backward_entropy": 0.0770077175564236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.21499633789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012969426810741425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10703437328338623,
      "backward_entropy": 0.07645747396681044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.35328674316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01305162813514471,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10699625015258789,
      "backward_entropy": 0.07644602987501356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.48841857910156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013134639710187912,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10695695877075195,
      "backward_entropy": 0.07700721422831218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.04209899902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013216164894402027,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10691741704940796,
      "backward_entropy": 0.07673598660363092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.17164611816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01329939253628254,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10687708854675293,
      "backward_entropy": 0.07672817177242702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.11070251464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01338239386677742,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10683622360229492,
      "backward_entropy": 0.07639764414893256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.0880889892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01346520148217678,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10679488182067871,
      "backward_entropy": 0.07671189308166504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.14462280273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01354538556188345,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10675373077392578,
      "backward_entropy": 0.07700619432661268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 233.8041534423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013628301210701466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10671133995056152,
      "backward_entropy": 0.07635870907041761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 232.9225616455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013715831562876701,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10666697025299073,
      "backward_entropy": 0.07668662733501858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.10630798339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013807665556669235,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10662139654159546,
      "backward_entropy": 0.07667900456322564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.98716735839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013900699093937874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1065748691558838,
      "backward_entropy": 0.07632231050067478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 217.69200134277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013996227644383907,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10652710199356079,
      "backward_entropy": 0.07666365305582683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.91587829589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014093785546720028,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10647811889648437,
      "backward_entropy": 0.07629858122931586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.25796508789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014187437482178211,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10642976760864258,
      "backward_entropy": 0.076285719871521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.32518005371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014273788779973984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10638307332992554,
      "backward_entropy": 0.07627124256557888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.62110900878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014358434826135635,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10633631944656372,
      "backward_entropy": 0.07662851942910089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.54058837890625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014445040374994278,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10628831386566162,
      "backward_entropy": 0.07700591617160374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.74375915527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014533404260873795,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10623910427093505,
      "backward_entropy": 0.076608763800727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 232.39146423339844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014619971625506878,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10618994235992432,
      "backward_entropy": 0.07700549893909031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.10171508789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01471087709069252,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10613884925842285,
      "backward_entropy": 0.07658827304840088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.2439422607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014798136427998543,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10608824491500854,
      "backward_entropy": 0.07617941829893324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 232.49620056152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014884988777339458,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10603725910186768,
      "backward_entropy": 0.07700483004252116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 233.1850128173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014976061880588531,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1059842824935913,
      "backward_entropy": 0.07614660263061523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.41847229003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015070699155330658,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1059294581413269,
      "backward_entropy": 0.0765444901254442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.88719177246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015164065174758434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10587447881698608,
      "backward_entropy": 0.0765333440568712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.05374145507812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015258990228176117,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10581851005554199,
      "backward_entropy": 0.07700435320536296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.7491912841797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01535241212695837,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10576237440109253,
      "backward_entropy": 0.07700419425964355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.58560180664062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015443628653883934,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10570662021636963,
      "backward_entropy": 0.07649840248955621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.57745361328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015536138787865639,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10564936399459839,
      "backward_entropy": 0.07604420847362942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.32615661621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01562536135315895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10559235811233521,
      "backward_entropy": 0.07647275262408787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.64881896972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01571260578930378,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10553522109985351,
      "backward_entropy": 0.0764588647418552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.94847869873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015797970816493034,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10547802448272706,
      "backward_entropy": 0.07644428809483846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.94329833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015879159793257713,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10542149543762207,
      "backward_entropy": 0.07642910215589735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.4388885498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01596069149672985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10536426305770874,
      "backward_entropy": 0.07593957583109538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.56381225585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01604093611240387,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10530693531036377,
      "backward_entropy": 0.07639781634012859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.05908203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016117310151457787,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10525026321411132,
      "backward_entropy": 0.07589124308692084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.20448303222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01619795709848404,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10519125461578369,
      "backward_entropy": 0.07636408011118571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.15536499023438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01627861149609089,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10513148307800294,
      "backward_entropy": 0.07584183745914036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.58698272705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016358088701963425,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10507147312164307,
      "backward_entropy": 0.07581587632497151,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.57623291015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01643277518451214,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10501277446746826,
      "backward_entropy": 0.07699636618296306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.9440460205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016508202999830246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10495308637619019,
      "backward_entropy": 0.07575995392269558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.595703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016581259667873383,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10489344596862793,
      "backward_entropy": 0.0762712558110555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.8195343017578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016651609912514687,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10483443737030029,
      "backward_entropy": 0.07625008953942193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.8193817138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01672566682100296,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1047731876373291,
      "backward_entropy": 0.07622959878709581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.42787170410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01680014096200466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10471088886260986,
      "backward_entropy": 0.07563646634419759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.23162841796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01687389798462391,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10464813709259033,
      "backward_entropy": 0.0769894387986925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.3388214111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016949953511357307,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10458383560180665,
      "backward_entropy": 0.07557177543640137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.49732971191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017030343413352966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10451705455780029,
      "backward_entropy": 0.07554072803921169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.22573852539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017113124951720238,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10444800853729248,
      "backward_entropy": 0.07612639003329807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.62677001953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01719578169286251,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1043774127960205,
      "backward_entropy": 0.07547757360670301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.4249496459961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01727963052690029,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10430504083633423,
      "backward_entropy": 0.07698622014787462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.63348388671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017359623685479164,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10423358678817748,
      "backward_entropy": 0.07606440120273167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.30580139160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01743815839290619,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10416165590286255,
      "backward_entropy": 0.07604167196485731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.9892578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017514275386929512,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10408985614776611,
      "backward_entropy": 0.07698350482516819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.54546356201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01758618839085102,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10401954650878906,
      "backward_entropy": 0.07599194844563802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.86347961425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017654426395893097,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10395052433013915,
      "backward_entropy": 0.07596480846405029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.77687072753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017720161005854607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10388169288635254,
      "backward_entropy": 0.0752180814743042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.24853515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017789212986826897,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1038097620010376,
      "backward_entropy": 0.07590879334343804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.92767333984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01785992830991745,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10373579263687134,
      "backward_entropy": 0.07588145467970106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.140323638916016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01792958565056324,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10366144180297851,
      "backward_entropy": 0.0758533345328437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.5826187133789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017989862710237503,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10359123945236207,
      "backward_entropy": 0.07505178451538086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.43492126464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018047522753477097,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10352177619934082,
      "backward_entropy": 0.07500600814819336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.17787170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018109049648046494,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10344902276992798,
      "backward_entropy": 0.07575665579901801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.9966278076172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018172936514019966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.103374183177948,
      "backward_entropy": 0.07572507858276367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.36793518066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018241295590996742,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10329592227935791,
      "backward_entropy": 0.07487356000476414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.77334594726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01831122860312462,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10321590900421143,
      "backward_entropy": 0.07696488830778334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.40577697753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018380112946033478,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10313575267791748,
      "backward_entropy": 0.07478541798061794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.37477111816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01845276914536953,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10305240154266357,
      "backward_entropy": 0.07474088668823242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.1834716796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018529348075389862,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1029661774635315,
      "backward_entropy": 0.07469812366697523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.1358642578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018603459000587463,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1028786540031433,
      "backward_entropy": 0.07696187496185303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.41456604003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0186773594468832,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10278941392898559,
      "backward_entropy": 0.07460461060206096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.10472106933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01875097118318081,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10269895792007447,
      "backward_entropy": 0.07455635070800781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.04754638671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018823523074388504,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10260828733444213,
      "backward_entropy": 0.07544163862864177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.65901947021484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01889883540570736,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10251500606536865,
      "backward_entropy": 0.07540851169162327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.728759765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018966881558299065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10242520570755005,
      "backward_entropy": 0.07537127865685357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.89312744140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019038742408156395,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10233097076416016,
      "backward_entropy": 0.07435527112748888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.63121795654297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01911054365336895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1022347092628479,
      "backward_entropy": 0.07529759407043457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.1685791015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01917830854654312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10213963985443116,
      "backward_entropy": 0.0742443667517768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.67518615722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01924687810242176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10204298496246338,
      "backward_entropy": 0.07418827215830485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.1780242919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019317425787448883,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10194406509399415,
      "backward_entropy": 0.07517996099260119,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.09133911132812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019389865919947624,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10184305906295776,
      "backward_entropy": 0.07407951354980469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.18965148925781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01946200244128704,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10174084901809692,
      "backward_entropy": 0.07510267363654242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.58430480957031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019531244412064552,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1016394019126892,
      "backward_entropy": 0.07506093713972303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.30393981933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019595444202423096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10154068470001221,
      "backward_entropy": 0.07390012343724568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.04405975341797,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019661447033286095,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10143932104110717,
      "backward_entropy": 0.07694454325570001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.60154724121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0197257362306118,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10133872032165528,
      "backward_entropy": 0.07377062241236369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.46385192871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01979396492242813,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10123519897460938,
      "backward_entropy": 0.07370968659718831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.04393005371094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01986573450267315,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1011286735534668,
      "backward_entropy": 0.07483602894677056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.29591369628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019937973469495773,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10102123022079468,
      "backward_entropy": 0.07479261027442084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.6615447998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020010530948638916,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10091289281845092,
      "backward_entropy": 0.07474896642896864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.90545654296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02008463442325592,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10080264806747437,
      "backward_entropy": 0.07470536231994629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.47987365722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020158329978585243,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10069149732589722,
      "backward_entropy": 0.07341562377081977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.40553283691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02023482322692871,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10057761669158935,
      "backward_entropy": 0.07461553812026978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.36439514160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020315177738666534,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10046026706695557,
      "backward_entropy": 0.0745730267630683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.53972625732422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0203965175896883,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10034174919128418,
      "backward_entropy": 0.07324074374304877,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.85247039794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020471086725592613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10022718906402588,
      "backward_entropy": 0.07317454285091823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.3839874267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020543457940220833,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10011420249938965,
      "backward_entropy": 0.07310717635684544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.1374053955078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0206158347427845,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10000028610229492,
      "backward_entropy": 0.07693511247634888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.00872039794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0206917617470026,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09988218545913696,
      "backward_entropy": 0.07432760132683648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.87953186035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020760729908943176,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0997692584991455,
      "backward_entropy": 0.07427000999450684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.10269165039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02083176001906395,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09965417385101319,
      "backward_entropy": 0.07281353076299031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.34678649902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02090453915297985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09953705072402955,
      "backward_entropy": 0.07273797194163005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.54289245605469,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02098117396235466,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09941586256027221,
      "backward_entropy": 0.07692760891384548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.57510375976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021053925156593323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09929776787757874,
      "backward_entropy": 0.07258588737911648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.58775329589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02112390100955963,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09918115139007569,
      "backward_entropy": 0.07692368825276692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.62510681152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02120206691324711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09905698895454407,
      "backward_entropy": 0.07242737876044379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.13246154785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02127818949520588,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09893370866775512,
      "backward_entropy": 0.0723468926217821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.1421356201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02135258913040161,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0988111138343811,
      "backward_entropy": 0.07226257854037815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.96885681152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021432042121887207,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09868358969688415,
      "backward_entropy": 0.07375368807050917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.30486297607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021512074396014214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0985548734664917,
      "backward_entropy": 0.0720973014831543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.2856674194336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021587448194622993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09842990636825562,
      "backward_entropy": 0.07200802697075738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.19400024414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021658288314938545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09830782413482667,
      "backward_entropy": 0.0719118250740899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.8952865600586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021724320948123932,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0981898546218872,
      "backward_entropy": 0.07349991798400879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.70037841796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02178815007209778,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0980730652809143,
      "backward_entropy": 0.07170625527699788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.58029174804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02185724675655365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09795135259628296,
      "backward_entropy": 0.07160635789235432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.73304748535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02192613296210766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09783005714416504,
      "backward_entropy": 0.07150777180989583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.22935485839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021995438262820244,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09770766496658326,
      "backward_entropy": 0.07321308718787299,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.59583282470703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022065239027142525,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09758426547050476,
      "backward_entropy": 0.07314032978481716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.8505859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02213270775973797,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09746239185333253,
      "backward_entropy": 0.07120196686850654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.86093139648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022199591621756554,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0973406195640564,
      "backward_entropy": 0.07298554314507379,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.58405303955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02226850762963295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09721637964248657,
      "backward_entropy": 0.07098609871334499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.43144989013672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022336704656481743,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09709244966506958,
      "backward_entropy": 0.07688426971435547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.43125915527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022401340305805206,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09697132110595703,
      "backward_entropy": 0.07274214426676433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.18000030517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022467922419309616,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09684735536575317,
      "backward_entropy": 0.07063951757219103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.1728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02253403700888157,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0967236340045929,
      "backward_entropy": 0.0705183744430542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.35649108886719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02259516343474388,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0966038703918457,
      "backward_entropy": 0.07247567176818848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.54778289794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022653330117464066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09648618698120118,
      "backward_entropy": 0.07237792015075684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.74824523925781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022709064185619354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09637051820755005,
      "backward_entropy": 0.07011456622017755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.10303497314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022763801738619804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09625524282455444,
      "backward_entropy": 0.06997186607784694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.93878173828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022814916446805,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09614300727844238,
      "backward_entropy": 0.06982297367519802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.22415161132812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022869519889354706,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09602667093276977,
      "backward_entropy": 0.07195364104376899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.4228286743164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022920753806829453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09591355323791503,
      "backward_entropy": 0.06952234771516588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.4222183227539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022969810292124748,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09580168724060059,
      "backward_entropy": 0.06936325629552205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.6062774658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02301914058625698,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09568983912467957,
      "backward_entropy": 0.0692054099506802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.01032257080078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023072410374879837,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09557384848594666,
      "backward_entropy": 0.07149243354797363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.94902801513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023121140897274017,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09546238780021668,
      "backward_entropy": 0.07137195269266765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.98987579345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023171165958046913,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09534931182861328,
      "backward_entropy": 0.07125160429212782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.49229431152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023221055045723915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09523627758026124,
      "backward_entropy": 0.06856599118974474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.18196868896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02327345311641693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09512023329734802,
      "backward_entropy": 0.07100959618886311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.40744018554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02332250587642193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09500723481178283,
      "backward_entropy": 0.07088391648398505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.84359741210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02337285876274109,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09489274621009827,
      "backward_entropy": 0.0707581705517239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.79896545410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023422958329319954,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09477822780609131,
      "backward_entropy": 0.07063049740261501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.51167297363281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023479608818888664,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09465693235397339,
      "backward_entropy": 0.06773121489418878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.73635864257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02353418618440628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09453784823417663,
      "backward_entropy": 0.06756281852722168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.579833984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023586787283420563,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09442071914672852,
      "backward_entropy": 0.06738966041141087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.8476791381836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02363640069961548,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0943068265914917,
      "backward_entropy": 0.07011926174163818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.08496856689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0236844252794981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09419426918029786,
      "backward_entropy": 0.06702691316604614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.63194274902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023733679205179214,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09407999515533447,
      "backward_entropy": 0.06684072150124444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.7935791015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02377982996404171,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09396847486495971,
      "backward_entropy": 0.0696872075398763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.98419952392578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02382775954902172,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09385535717010499,
      "backward_entropy": 0.06953911648856269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.78390502929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023877186700701714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09374052286148071,
      "backward_entropy": 0.06626126501295301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.96036529541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023929178714752197,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09362283945083619,
      "backward_entropy": 0.06924666298760308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.79156494140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02397671341896057,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09350998997688294,
      "backward_entropy": 0.06909182336595324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.62586975097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024027250707149506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0933942198753357,
      "backward_entropy": 0.06893942091200086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.90603637695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02408049814403057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09327576756477356,
      "backward_entropy": 0.06548680199517144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.67058563232422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024134868755936623,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09315648674964905,
      "backward_entropy": 0.06863958305782741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.51492309570312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024187248200178146,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09303921461105347,
      "backward_entropy": 0.0684834983613756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.81170654296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024235157296061516,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0929267406463623,
      "backward_entropy": 0.07663006252712673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.5866928100586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024293212220072746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09280489683151245,
      "backward_entropy": 0.06469347741868761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.0300521850586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024351468309760094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09268237352371216,
      "backward_entropy": 0.0644973185327318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.3466339111328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024409623816609383,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09256136417388916,
      "backward_entropy": 0.07662182384067112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.293270111083984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024469083175063133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09243829250335693,
      "backward_entropy": 0.06410553720262316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.50623321533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024522166699171066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09232234954833984,
      "backward_entropy": 0.06754048003090753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.8969268798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0245746448636055,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09220674037933349,
      "backward_entropy": 0.06736785835689968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.6063003540039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024630054831504822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09208904504776001,
      "backward_entropy": 0.06346135007010566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.42153930664062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024685220792889595,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09197211265563965,
      "backward_entropy": 0.07659433947669135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.54627227783203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024738723412156105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09185730814933776,
      "backward_entropy": 0.0668591128455268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.12863159179688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024791376665234566,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09174280166625977,
      "backward_entropy": 0.06668002075619167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.94960021972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024845190346240997,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09162737131118774,
      "backward_entropy": 0.06257490979300605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.89920043945312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024901503697037697,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09150984287261962,
      "backward_entropy": 0.07656677563985188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.11743927001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02495606802403927,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09139463901519776,
      "backward_entropy": 0.06613344616360134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.69517517089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02500566840171814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09128425121307374,
      "backward_entropy": 0.06187156836191813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.13255310058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025054549798369408,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09117568135261536,
      "backward_entropy": 0.06162654028998481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.33473205566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025107912719249725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09106299877166749,
      "backward_entropy": 0.061387340227762856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.89691162109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02516375109553337,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09094803333282471,
      "backward_entropy": 0.07652568154864842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.71680450439453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025222284719347954,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09083133339881896,
      "backward_entropy": 0.06515731414159139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.63949203491211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025278661400079727,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09071708917617798,
      "backward_entropy": 0.06067568063735962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.38800048828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02533063478767872,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09060806035995483,
      "backward_entropy": 0.060425360997517906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.47897338867188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02538670413196087,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09049525856971741,
      "backward_entropy": 0.06017938587400648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.62129974365234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025446347892284393,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09037927389144898,
      "backward_entropy": 0.05993576182259454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.25469970703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025504332035779953,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09026607871055603,
      "backward_entropy": 0.06415190961625841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.43833923339844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025564268231391907,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09015097618103027,
      "backward_entropy": 0.07648571332295735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.2769775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025626197457313538,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09003465175628662,
      "backward_entropy": 0.059199902746412486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.76351928710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025688938796520233,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08991866707801818,
      "backward_entropy": 0.0589568747414483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.9034652709961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02574869617819786,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08980609774589539,
      "backward_entropy": 0.06333784924613105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.81526184082031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025809500366449356,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08969292640686036,
      "backward_entropy": 0.07647010352876452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.76103210449219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025870444253087044,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08957900404930115,
      "backward_entropy": 0.06292161676618788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.0530014038086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025930531322956085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08946661949157715,
      "backward_entropy": 0.05793036354912652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.71337127685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025988740846514702,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08935742378234864,
      "backward_entropy": 0.05766611629062229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.56163024902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0260462649166584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08924925327301025,
      "backward_entropy": 0.06226564115948147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.32505798339844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026100948452949524,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08914493322372437,
      "backward_entropy": 0.07644208272298177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.89366912841797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0261510219424963,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08904510736465454,
      "backward_entropy": 0.06178771787219577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.6513671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026203058660030365,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08894418478012085,
      "backward_entropy": 0.05653809838824802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.23181915283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02625223994255066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08884624242782593,
      "backward_entropy": 0.05623897578981188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.71179962158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026298973709344864,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08875088691711426,
      "backward_entropy": 0.05593052175309923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.17699432373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026346225291490555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08865525126457215,
      "backward_entropy": 0.05561827288733588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.70972442626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0263957679271698,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08855836391448975,
      "backward_entropy": 0.055311222871144615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.89309692382812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026442507281899452,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08846392631530761,
      "backward_entropy": 0.060221579339769155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.56499099731445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02648867666721344,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0883712887763977,
      "backward_entropy": 0.05467197630140516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.31381225585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026530256494879723,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08828372359275818,
      "backward_entropy": 0.059655573632982045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.74806213378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02657439559698105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08819348216056824,
      "backward_entropy": 0.059374610582987465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.39960479736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026618139818310738,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08810378313064575,
      "backward_entropy": 0.05370120538605584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.30024719238281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0266624316573143,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08801325559616088,
      "backward_entropy": 0.05880868434906006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.38910675048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026710866019129753,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08792060613632202,
      "backward_entropy": 0.05305780304802789,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.70540618896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026757270097732544,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08783060312271118,
      "backward_entropy": 0.05824994378619724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.23406982421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02680209092795849,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0877431571483612,
      "backward_entropy": 0.05241316888067457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.8859634399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026844793930649757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08765757083892822,
      "backward_entropy": 0.05207731988694933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.02875518798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026888910681009293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0875714898109436,
      "backward_entropy": 0.051745490895377264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.6474609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026928618550300598,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08749005794525147,
      "backward_entropy": 0.05140348937776354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.50782775878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02696959860622883,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08740745782852173,
      "backward_entropy": 0.05675298637813992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.22545623779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027016038075089455,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08732040524482727,
      "backward_entropy": 0.05645936727523804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.15524291992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02706356905400753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08723337650299072,
      "backward_entropy": 0.056167794598473444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.2105941772461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02710890583693981,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08714893460273743,
      "backward_entropy": 0.05005894104639689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.52638244628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027154888957738876,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08706371784210205,
      "backward_entropy": 0.05556446976131863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.16034698486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027200572192668915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08697939515113831,
      "backward_entropy": 0.04937544133928087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.52336883544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027245650067925453,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08689582347869873,
      "backward_entropy": 0.04903124438391791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.08468627929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02728996053338051,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08681323528289794,
      "backward_entropy": 0.05464263094796075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.60684204101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027336785569787025,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08672910928726196,
      "backward_entropy": 0.05433405107922024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.508487701416016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027388591319322586,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0866416871547699,
      "backward_entropy": 0.05403683914078607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.27050018310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027434663847088814,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.086558997631073,
      "backward_entropy": 0.05371948083241781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.08953857421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027478672564029694,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08647838830947877,
      "backward_entropy": 0.053391857279671565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.5021743774414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0275217704474926,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08640052080154419,
      "backward_entropy": 0.053064849641588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.64393615722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027564845979213715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08632329106330872,
      "backward_entropy": 0.05273993147744073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.41276550292969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02761469967663288,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08624184131622314,
      "backward_entropy": 0.04619270231988695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.4544677734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02766524814069271,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08615927696228028,
      "backward_entropy": 0.045857634809282094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.32462310791016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027715392410755157,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08607708215713501,
      "backward_entropy": 0.05183669924736023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.41415405273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027764635160565376,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08599643707275391,
      "backward_entropy": 0.04515875048107571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.21900939941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02781342715024948,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08591731786727905,
      "backward_entropy": 0.04480817251735263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.29879760742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02786136418581009,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08583929538726806,
      "backward_entropy": 0.050899638070000544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.03646850585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027911270037293434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08576021194458008,
      "backward_entropy": 0.050582541359795466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.280338287353516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027959221974015236,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08568385243415833,
      "backward_entropy": 0.075901640786065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.45065307617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02800382673740387,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08561074733734131,
      "backward_entropy": 0.049921241071489125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.14718627929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028046686202287674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08553916215896606,
      "backward_entropy": 0.042990138133366905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.2721939086914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028090838342905045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08546707630157471,
      "backward_entropy": 0.049244291252560086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.05626678466797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028137873858213425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08539389967918395,
      "backward_entropy": 0.04892160495122274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.5519790649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028187518939375877,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0853201687335968,
      "backward_entropy": 0.04191509882609049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.57662963867188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028237825259566307,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0852470874786377,
      "backward_entropy": 0.04829393492804633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.23202514648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028288627043366432,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08517407178878784,
      "backward_entropy": 0.041211320294274226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.03103637695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02834046259522438,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08510171175003052,
      "backward_entropy": 0.040862964259253606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.9495620727539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02839324250817299,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08503003120422363,
      "backward_entropy": 0.040519015656577215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.7991714477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028446314856410027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08495861887931824,
      "backward_entropy": 0.0401698284678989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.25162506103516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028497057035565376,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0848894715309143,
      "backward_entropy": 0.04669674899843004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.44698715209961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028547195717692375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08482148647308349,
      "backward_entropy": 0.03945397006140815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.03451538085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02859407663345337,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08475674986839295,
      "backward_entropy": 0.0460204283396403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.981136322021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028643716126680374,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08469104766845703,
      "backward_entropy": 0.038733979066212974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.9345703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02868903987109661,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0846292495727539,
      "backward_entropy": 0.045345564683278404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.9184455871582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02873345836997032,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08456945419311523,
      "backward_entropy": 0.04500502016809252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.68450164794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028775254264473915,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.084511798620224,
      "backward_entropy": 0.04465953177875943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.91046905517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02881891280412674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08445298671722412,
      "backward_entropy": 0.03731332884894477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.080997467041016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02886403724551201,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08439407348632813,
      "backward_entropy": 0.04399131072892083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.5193862915039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02890503965318203,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08433864116668702,
      "backward_entropy": 0.0436444448100196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.477787017822266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02894780971109867,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08428242206573486,
      "backward_entropy": 0.04330260223812527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.92853546142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028988096863031387,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08422809839248657,
      "backward_entropy": 0.042953305774264865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.74005889892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02902924083173275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0841740608215332,
      "backward_entropy": 0.03553774290614658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.58926391601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029071740806102753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08411895036697388,
      "backward_entropy": 0.04226362705230713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.08518981933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029114915058016777,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08406433463096619,
      "backward_entropy": 0.03482550713751051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.093563079833984,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029157239943742752,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08401083350181579,
      "backward_entropy": 0.07559142510096233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.4676742553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029195401817560196,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08395950198173523,
      "backward_entropy": 0.034107466538747154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.37822723388672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029234502464532852,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08390822410583496,
      "backward_entropy": 0.04086836179097494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.43921661376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029276054352521896,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08385663032531739,
      "backward_entropy": 0.033399601777394615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.75384521484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029321303591132164,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08380385041236878,
      "backward_entropy": 0.03306449121899075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.34733581542969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029366957023739815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0837520718574524,
      "backward_entropy": 0.03987794783380297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.39340209960938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02941412664949894,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08369898796081543,
      "backward_entropy": 0.03955882787704468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.27909851074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02946154773235321,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08364715576171874,
      "backward_entropy": 0.03208169009950426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.20624542236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029511697590351105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0835942268371582,
      "backward_entropy": 0.03892656498485141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.67914581298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02956172451376915,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08354203104972839,
      "backward_entropy": 0.03861457440588209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.02123260498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029612790793180466,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08348970413208008,
      "backward_entropy": 0.03112131357192993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.88201904296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029666604474186897,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08343716859817504,
      "backward_entropy": 0.030811852878994413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.9429817199707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02971590869128704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08338764309883118,
      "backward_entropy": 0.030492368671629164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.4652328491211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02976105734705925,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08334074020385743,
      "backward_entropy": 0.03736244307623969,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.2208480834961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029808025807142258,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08329368829727173,
      "backward_entropy": 0.03704226679272122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.75614929199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02985585480928421,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08324708938598632,
      "backward_entropy": 0.02953313456641303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.68736267089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02990109845995903,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08320224881172181,
      "backward_entropy": 0.03641382522053189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.423484802246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02994765155017376,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08315672278404236,
      "backward_entropy": 0.036096036434173584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.68065643310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02999325655400753,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08311234712600708,
      "backward_entropy": 0.028581132491429646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.7984504699707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03004196658730507,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08306692838668824,
      "backward_entropy": 0.02827339039908515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.22666931152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03008628636598587,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08302351236343383,
      "backward_entropy": 0.027953442600038316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.720455169677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030127672478556633,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08298261165618896,
      "backward_entropy": 0.02763791216744317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.94098663330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030165325850248337,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08294321894645691,
      "backward_entropy": 0.027309464083777532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.584129333496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030205504968762398,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08290241956710816,
      "backward_entropy": 0.02698964046107398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.28760528564453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03024480678141117,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08286383748054504,
      "backward_entropy": 0.03385525941848755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.18476867675781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030285386368632317,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0828251600265503,
      "backward_entropy": 0.03354411323865255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.05725860595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03032318688929081,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08278832435607911,
      "backward_entropy": 0.026076907912890118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.23770141601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030364934355020523,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08274988532066345,
      "backward_entropy": 0.03292882442474365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.30082702636719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030410688370466232,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08271055221557617,
      "backward_entropy": 0.032642364501953125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.40530395507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030458614230155945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08267101049423217,
      "backward_entropy": 0.025228886140717402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.97999572753906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.030507061630487442,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08263192772865295,
      "backward_entropy": 0.07533996635013157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.32440185546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030554981902241707,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08259396553039551,
      "backward_entropy": 0.03180423378944397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.31430053710938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030604008585214615,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0825568437576294,
      "backward_entropy": 0.03153443998760647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.65943908691406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.030663449317216873,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08251701593399048,
      "backward_entropy": 0.07536028491126166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.92068862915039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030723445117473602,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08247715830802918,
      "backward_entropy": 0.023966289228863187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.703606605529785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030782151967287064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08243896961212158,
      "backward_entropy": 0.02373723520172967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.90242004394531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030833035707473755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08240320682525634,
      "backward_entropy": 0.030574907859166462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.98708724975586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03088623471558094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08236806988716125,
      "backward_entropy": 0.023245144221517775,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.90235900878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030938437208533287,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08233383893966675,
      "backward_entropy": 0.03007907999886407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.21141815185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03098686784505844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08230130076408386,
      "backward_entropy": 0.02982134289211697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.7737808227539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031037157401442528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08226805925369263,
      "backward_entropy": 0.022526317172580294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.108436584472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031091105192899704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08223423957824708,
      "backward_entropy": 0.02230121029747857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.9511489868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031144270673394203,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08220161199569702,
      "backward_entropy": 0.0220810870329539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.9898681640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031197648495435715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08216912746429443,
      "backward_entropy": 0.02186042732662625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.88951110839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031247563660144806,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08213858604431153,
      "backward_entropy": 0.028633395830790203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.95059204101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03129437565803528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08210963010787964,
      "backward_entropy": 0.021412026551034715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.99225616455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031343307346105576,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08207992315292359,
      "backward_entropy": 0.02815104855431451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.94850158691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03139316290616989,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0820503830909729,
      "backward_entropy": 0.027914497587415908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.99652099609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03143966197967529,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08202201128005981,
      "backward_entropy": 0.020737909608417086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.79228973388672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03148704394698143,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08199331760406495,
      "backward_entropy": 0.02743048800362481,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.70398712158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03153688460588455,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08196434974670411,
      "backward_entropy": 0.020297593540615506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.646331787109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0315888449549675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08193497657775879,
      "backward_entropy": 0.020087190800242953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.896663665771484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031637221574783325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08190681934356689,
      "backward_entropy": 0.02674962083498637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.42626190185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03168361261487007,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08187920451164246,
      "backward_entropy": 0.02651286290751563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.69947052001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03173220902681351,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0818511962890625,
      "backward_entropy": 0.026279643177986145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.90221405029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03178180381655693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08182310461997985,
      "backward_entropy": 0.02605445186297099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.51527404785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03183342516422272,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0817947506904602,
      "backward_entropy": 0.019005911217795476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.330265045166016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03188851475715637,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08176612854003906,
      "backward_entropy": 0.025630335013071697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.987300872802734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03194095194339752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0817381501197815,
      "backward_entropy": 0.018602041734589472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.652976989746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03199123218655586,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08171087503433228,
      "backward_entropy": 0.02519995801978641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.346595764160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03203980252146721,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08168441653251649,
      "backward_entropy": 0.018188456694285076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.52944946289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032083261758089066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08165848851203919,
      "backward_entropy": 0.017967396312289767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.2283706665039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03212825953960419,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08163251876831054,
      "backward_entropy": 0.02451545496781667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.69916534423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03217869624495506,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08160550594329834,
      "backward_entropy": 0.017550688650872972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.77537727355957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03223053365945816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08157932758331299,
      "backward_entropy": 0.017361644241544936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.36597442626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0322783961892128,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08155499696731568,
      "backward_entropy": 0.017177070180575054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.042213439941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032331548631191254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08152970075607299,
      "backward_entropy": 0.017004703481992085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.502567291259766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0323825404047966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08150510787963867,
      "backward_entropy": 0.023538149065441556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.26304626464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032433222979307175,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08148089647293091,
      "backward_entropy": 0.023351498776011996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.81356811523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032483749091625214,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08145706057548523,
      "backward_entropy": 0.023167893290519714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.8773250579834,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03253665566444397,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08143304586410523,
      "backward_entropy": 0.02299106948905521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.9955062866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032584983855485916,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08141002655029297,
      "backward_entropy": 0.01614361670282152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.621768951416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03263744339346886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08138673305511475,
      "backward_entropy": 0.01598178015814887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.24913787841797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032686516642570496,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08136398196220399,
      "backward_entropy": 0.01581465204556783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.39607238769531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03273787349462509,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08134056329727173,
      "backward_entropy": 0.022283941507339478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.78018188476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03279299661517143,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08131680488586426,
      "backward_entropy": 0.015494116478496127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.5182876586914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032849036157131195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08129345178604126,
      "backward_entropy": 0.01534383495648702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.334228515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03290601447224617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08127040863037109,
      "backward_entropy": 0.015200775530603197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.30033874511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03296384960412979,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08124788403511048,
      "backward_entropy": 0.01506402095158895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.04405212402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03302226588129997,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08122553825378417,
      "backward_entropy": 0.01493036581410302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.41108703613281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0330786369740963,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08120391964912414,
      "backward_entropy": 0.021414862738715276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.00957489013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033131882548332214,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08118318319320679,
      "backward_entropy": 0.021275185876422457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.53573608398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03319280594587326,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08116165399551392,
      "backward_entropy": 0.014539966980616251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.017906188964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03325650468468666,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08113964796066284,
      "backward_entropy": 0.02102969917986128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.9617919921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03331886976957321,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08111821413040161,
      "backward_entropy": 0.014298212197091844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.383445739746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033375781029462814,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08109735250473023,
      "backward_entropy": 0.020774039957258437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.878698348999023,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.033428892493247986,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08107664585113525,
      "backward_entropy": 0.07570587264166938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.46976089477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03347918763756752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08105654716491699,
      "backward_entropy": 0.013896468612882826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.216386795043945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03352944180369377,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.081036376953125,
      "backward_entropy": 0.02034209006362491,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.542604446411133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03357620909810066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08101720809936523,
      "backward_entropy": 0.01363291260268953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.35356521606445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03362094238400459,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08099844455718994,
      "backward_entropy": 0.020061343908309937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.741790771484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0336674228310585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0809792160987854,
      "backward_entropy": 0.013376285632451376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.96734619140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033711351454257965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08095998167991639,
      "backward_entropy": 0.013243343267175887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.441556930541992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03375566750764847,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08094079494476318,
      "backward_entropy": 0.013108728660477532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.60696792602539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03379787877202034,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0809216856956482,
      "backward_entropy": 0.01949104004436069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.03809356689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03384235501289368,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08090224862098694,
      "backward_entropy": 0.019353947705692716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.482425689697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03388513997197151,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08088372945785523,
      "backward_entropy": 0.012716321481598748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.31133270263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03392840921878815,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08086485862731933,
      "backward_entropy": 0.012587804761197832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.049415588378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03396937623620033,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08084570169448853,
      "backward_entropy": 0.012453392975860171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.81550598144531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03401270508766174,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08082659244537353,
      "backward_entropy": 0.01881116959783766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.777143478393555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034059640020132065,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08080716133117676,
      "backward_entropy": 0.012203808459970687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.62319564819336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03410445153713226,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.080787992477417,
      "backward_entropy": 0.018569471107588872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.36147689819336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03415119647979736,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0807686686515808,
      "backward_entropy": 0.018453389406204224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.775794982910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03419988229870796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08074933290481567,
      "backward_entropy": 0.018345067898432415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.98298645019531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03424694389104843,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08072959780693054,
      "backward_entropy": 0.018227077192730375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.12594223022461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034297119826078415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08070969581604004,
      "backward_entropy": 0.011617714332209693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.97916030883789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03434739261865616,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08068966865539551,
      "backward_entropy": 0.01800808310508728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.47930908203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03439776971936226,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08066949844360352,
      "backward_entropy": 0.011392181118329367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.41030216217041,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03445078432559967,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08064918518066407,
      "backward_entropy": 0.017798486683103774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.45555114746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03449876233935356,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08062952756881714,
      "backward_entropy": 0.01769342687394884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.743114471435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03454729914665222,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806100308895111,
      "backward_entropy": 0.011067708333333334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.70048522949219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03459400683641434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08059091567993164,
      "backward_entropy": 0.017496347427368164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.23005294799805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03464314714074135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08057186603546143,
      "backward_entropy": 0.010873725016911825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.238285064697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03469178080558777,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08055319786071777,
      "backward_entropy": 0.010785784986284044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.484052658081055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03473970293998718,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08053466081619262,
      "backward_entropy": 0.01726453834109836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.719364166259766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03478572890162468,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08051644563674927,
      "backward_entropy": 0.010611430638366275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.58648681640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034832410514354706,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08049784898757935,
      "backward_entropy": 0.017116477092107136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.31151580810547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034879691898822784,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08047899603843689,
      "backward_entropy": 0.017043312390645344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.718135833740234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034930747002363205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08046038150787353,
      "backward_entropy": 0.010361459520128038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.124834060668945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034983597695827484,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08044167757034301,
      "backward_entropy": 0.016933469308747187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.483552932739258,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035031337291002274,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08042310476303101,
      "backward_entropy": 0.01687390274471707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.305728912353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03507600724697113,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08040488958358764,
      "backward_entropy": 0.010133244925075106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.886486053466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03512062504887581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08038697838783264,
      "backward_entropy": 0.010057643055915833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.897132873535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03516359627246857,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08036922216415406,
      "backward_entropy": 0.016690611839294434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.6979923248291,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03520938754081726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08035123348236084,
      "backward_entropy": 0.009909861617618136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.62067985534668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035253528505563736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0803333044052124,
      "backward_entropy": 0.009838421311643388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.22929763793945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03529619425535202,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08031535744667054,
      "backward_entropy": 0.01652780506345961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.74575805664062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035340141505002975,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08029705882072449,
      "backward_entropy": 0.016476111279593572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.667579650878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03538783639669418,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08027865886688232,
      "backward_entropy": 0.009630862209531996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.41846466064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03543507680296898,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08026043176651002,
      "backward_entropy": 0.016385702623261347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.39824676513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03548566251993179,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08024177551269532,
      "backward_entropy": 0.009502195649676852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.26376724243164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03553568944334984,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0802232027053833,
      "backward_entropy": 0.009440965122646756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.36842346191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035585299134254456,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08020474910736083,
      "backward_entropy": 0.009382756219969856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.998197555541992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035633884370326996,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08018622398376465,
      "backward_entropy": 0.016234527031580608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.917821884155273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03568054735660553,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08016758561134338,
      "backward_entropy": 0.01619171102841695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.98845291137695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03572549298405647,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08014882802963257,
      "backward_entropy": 0.016146439645025466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.86370086669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03577180579304695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0801303207874298,
      "backward_entropy": 0.009131550788879395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.79111099243164,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03581935539841652,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08011188507080078,
      "backward_entropy": 0.07614470852745904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.72673797607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03586629778146744,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08009322881698608,
      "backward_entropy": 0.009016176064809164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.4503173828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03591402620077133,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08007439374923705,
      "backward_entropy": 0.008957424097590976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.328657150268555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03596021980047226,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08005546927452087,
      "backward_entropy": 0.015968912177615695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.276111602783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03600519523024559,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08003686666488648,
      "backward_entropy": 0.008844881421989866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.193634033203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036049000918865204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08001846075057983,
      "backward_entropy": 0.008791680137316385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.1182975769043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03609292954206467,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08000031709671021,
      "backward_entropy": 0.008739698264333937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.159198760986328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.036136895418167114,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07998213768005372,
      "backward_entropy": 0.07623547977871364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.59291076660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036179427057504654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07996364831924438,
      "backward_entropy": 0.008633680641651154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.10854148864746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03622688353061676,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07994468212127685,
      "backward_entropy": 0.00857942303021749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.229393005371094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03627140820026398,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07992584705352783,
      "backward_entropy": 0.01575057042969598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.37102127075195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03631988540291786,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0799066185951233,
      "backward_entropy": 0.008473677767647637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.49307632446289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036370083689689636,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07988694906234742,
      "backward_entropy": 0.00842107915216022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.626232147216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036419812589883804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0798675537109375,
      "backward_entropy": 0.008370320002237955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.29347229003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03646780923008919,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0798482596874237,
      "backward_entropy": 0.008320163521501753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.93027877807617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03652049973607063,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982863783836365,
      "backward_entropy": 0.015614451633559333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.1687240600586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03657357767224312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07980897426605224,
      "backward_entropy": 0.00822636236747106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.057198524475098,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03663363680243492,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07978904247283936,
      "backward_entropy": 0.015581683980094062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.600034713745117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036687977612018585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976927757263183,
      "backward_entropy": 0.008140177362494998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.358158111572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03673870116472244,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0797492265701294,
      "backward_entropy": 0.015533342957496643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.23707962036133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03679001331329346,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07972946166992187,
      "backward_entropy": 0.015507651699913872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.550880432128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03684179112315178,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07970978617668152,
      "backward_entropy": 0.007997172574202219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.96303939819336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03689269348978996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07968984842300415,
      "backward_entropy": 0.007948479718632169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.80449676513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03694411367177963,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07966973781585693,
      "backward_entropy": 0.007900183399518331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.752330780029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03699355199933052,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07964961528778076,
      "backward_entropy": 0.015390094783571031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.198060989379883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03703875094652176,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07962986230850219,
      "backward_entropy": 0.015357534090677897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.625276565551758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03708130493760109,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07961007356643676,
      "backward_entropy": 0.007752044333351983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.489770889282227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03712251037359238,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07959001064300537,
      "backward_entropy": 0.015285144249598185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.24514250457286835,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03716285154223442,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07957015037536622,
      "backward_entropy": 0.007649472190274132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.93335723876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03719867020845413,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07955048084259034,
      "backward_entropy": 0.007598137689961327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.65031814575195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0372430644929409,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07953059673309326,
      "backward_entropy": 0.015186965465545654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.543643951416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037291452288627625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07951063513755799,
      "backward_entropy": 0.007515220178498162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.40780258178711,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03733944892883301,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0794906735420227,
      "backward_entropy": 0.07648942205641004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.769014358520508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03738729655742645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07947035431861878,
      "backward_entropy": 0.015135059754053751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.697805404663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037432439625263214,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0794502854347229,
      "backward_entropy": 0.015117660164833069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.35079574584961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037475284188985825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0794304609298706,
      "backward_entropy": 0.007360360688633389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.620344161987305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03751972317695618,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0794105887413025,
      "backward_entropy": 0.015089046623971727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.76185417175293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0375618152320385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0793907642364502,
      "backward_entropy": 0.007288002305560642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.00123596191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037603020668029785,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07937079668045044,
      "backward_entropy": 0.015059944656160142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.660003662109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.037646058946847916,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07935068607330323,
      "backward_entropy": 0.0765419536166721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.92580795288086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03768787533044815,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07933052778244018,
      "backward_entropy": 0.007183219823572371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.516435623168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03773249685764313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07931003570556641,
      "backward_entropy": 0.007148448791768815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.65338134765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037775758653879166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07928951978683471,
      "backward_entropy": 0.007112787001662784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.49079895019531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03782162442803383,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07926874160766602,
      "backward_entropy": 0.007077168259355757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.295143127441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03786996006965637,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07924778461456299,
      "backward_entropy": 0.0070426008767551845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.186378479003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037917979061603546,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07922701835632324,
      "backward_entropy": 0.007007372048166063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.12810516357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03796825557947159,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07920578718185425,
      "backward_entropy": 0.006973764962620205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.998689651489258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03801560774445534,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07918480634689332,
      "backward_entropy": 0.006940580904483795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.82040023803711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03806263580918312,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07916391491889954,
      "backward_entropy": 0.014906361699104309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.8756046295166,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0381106436252594,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07914267182350158,
      "backward_entropy": 0.01488562093840705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.81268882751465,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038157179951667786,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07912165522575379,
      "backward_entropy": 0.014864199691348605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.558185577392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03820228576660156,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07910066246986389,
      "backward_entropy": 0.006801372600926293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.307594299316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03824765607714653,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0790796160697937,
      "backward_entropy": 0.014821045928531222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.397674560546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03829436004161835,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07905821800231934,
      "backward_entropy": 0.006737280637025833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.275104522705078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038340840488672256,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0790366291999817,
      "backward_entropy": 0.014788167344199287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.430994033813477,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038387347012758255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07901507019996643,
      "backward_entropy": 0.006675354308552212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.079608917236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03843251243233681,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07899349927902222,
      "backward_entropy": 0.014760361777411567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.851375579833984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03847783803939819,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07897192239761353,
      "backward_entropy": 0.0066162289844618905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.55119323730469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03852799907326698,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07894983291625976,
      "backward_entropy": 0.01473975678284963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.7651424407959,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03857918828725815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07892717123031616,
      "backward_entropy": 0.014737288157145182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.64964485168457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03862999752163887,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07890474200248718,
      "backward_entropy": 0.014738099442587959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.169193267822266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03868056833744049,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07888227105140685,
      "backward_entropy": 0.006519188483556111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.7857551574707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03873195871710777,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07885981798171997,
      "backward_entropy": 0.014750093221664429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.77155113220215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038787707686424255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07883641719818116,
      "backward_entropy": 0.006480886290470759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.221818923950195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03884147107601166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0788132905960083,
      "backward_entropy": 0.006463088095188141,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.627811431884766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03889458253979683,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07879027128219604,
      "backward_entropy": 0.014778320988019308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.96968460083008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038948144763708115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07876715660095215,
      "backward_entropy": 0.0064264121982786394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.89667320251465,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039003245532512665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07874383926391601,
      "backward_entropy": 0.006405856046411727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.952378273010254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03905745595693588,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07872063517570496,
      "backward_entropy": 0.006384896735350291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.690074920654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03910838067531586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07869765758514405,
      "backward_entropy": 0.006362887720266978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.565889358520508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03915867954492569,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0786746621131897,
      "backward_entropy": 0.014787251750628153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.1545524597168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039208658039569855,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0786516547203064,
      "backward_entropy": 0.006317781077490913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.99332046508789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0392606221139431,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07862815260887146,
      "backward_entropy": 0.014786243438720703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.52767562866211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03931436687707901,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0786042869091034,
      "backward_entropy": 0.014787500103314718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.13336944580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03936867043375969,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07858028411865234,
      "backward_entropy": 0.006258792761299346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.798355102539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03942219913005829,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07855643033981323,
      "backward_entropy": 0.006240062001678679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.90571403503418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03947370871901512,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0785327434539795,
      "backward_entropy": 0.014802032046847872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.99391174316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039524853229522705,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07850919961929322,
      "backward_entropy": 0.01481020450592041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.233102798461914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03957666456699371,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07848522663116456,
      "backward_entropy": 0.006184622645378113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.86958694458008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03962444141507149,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07846195697784424,
      "backward_entropy": 0.014827259712749057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.60060119628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03967434540390968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07843823432922363,
      "backward_entropy": 0.006148880554570092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.39287757873535,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03972511738538742,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0784143090248108,
      "backward_entropy": 0.014842960569593642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.34188461303711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03977544605731964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07839035987854004,
      "backward_entropy": 0.006115386055575477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.2232666015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03982654958963394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07836615443229675,
      "backward_entropy": 0.006098589135540856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.074634552001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039878133684396744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07834177017211914,
      "backward_entropy": 0.014864464600880941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.968189239501953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039929185062646866,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07831724882125854,
      "backward_entropy": 0.07684775193532307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.82655334472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039979755878448486,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07829279303550721,
      "backward_entropy": 0.006043244981103473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.765453338623047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.040030818432569504,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07826834917068481,
      "backward_entropy": 0.07685471243328518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.54862976074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04008123278617859,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.078243887424469,
      "backward_entropy": 0.006001637213759952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.41026306152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04013235121965408,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07821895480155945,
      "backward_entropy": 0.005981982168224122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.131309509277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040184181183576584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07819416522979736,
      "backward_entropy": 0.014863461256027222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.324058532714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04023757576942444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07816860675811768,
      "backward_entropy": 0.005943908045689265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.852323532104492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04029020667076111,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07814321517944336,
      "backward_entropy": 0.005924807654486762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.156837463378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040338579565286636,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07811866402626037,
      "backward_entropy": 0.005905241188075807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.94290542602539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0403914675116539,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07809302806854249,
      "backward_entropy": 0.005887693829006619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.18292999267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04044828936457634,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07806650400161744,
      "backward_entropy": 0.0058702147669262355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.480098724365234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04050269350409508,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0780405044555664,
      "backward_entropy": 0.005851703385512034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.294795989990234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04056089371442795,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07801340222358703,
      "backward_entropy": 0.005834283100234138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.38222122192383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04061901941895485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07798632383346557,
      "backward_entropy": 0.0058176881737179225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.64399528503418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040679365396499634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07795870304107666,
      "backward_entropy": 0.014860494269265069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.85396957397461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04073463752865791,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07793211340904235,
      "backward_entropy": 0.014859512448310852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.63588523864746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040789902210235596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07790538072586059,
      "backward_entropy": 0.005766901705000136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.06901550292969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040843140333890915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07787930369377136,
      "backward_entropy": 0.005749954945511288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.991241455078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04089784622192383,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07785226106643676,
      "backward_entropy": 0.014851591653294034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.4920196533203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04094943031668663,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07782606482505798,
      "backward_entropy": 0.005718369450834062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.314863204956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04099695384502411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07780081629753113,
      "backward_entropy": 0.0057037220233016545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.04166793823242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04104311019182205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07777585983276367,
      "backward_entropy": 0.014858889910909865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.415725231170654,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04109035059809685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07775065302848816,
      "backward_entropy": 0.005674900280104743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.101009368896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04113396629691124,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07772629261016846,
      "backward_entropy": 0.014865510993533664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.035602569580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041176535189151764,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07770196795463562,
      "backward_entropy": 0.005645996166600121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.03651030361652374,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0412181057035923,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07767785787582397,
      "backward_entropy": 0.005630292826228672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.484458923339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04125547409057617,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07765482664108277,
      "backward_entropy": 0.005615361862712436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.38361358642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04129478707909584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0776311993598938,
      "backward_entropy": 0.014865019255214267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.77143096923828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04133576154708862,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07760699987411498,
      "backward_entropy": 0.014864352014329698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.25979471206665,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041380494832992554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07758160829544067,
      "backward_entropy": 0.00557228136393759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.45186233520508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04142177477478981,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07755718231201172,
      "backward_entropy": 0.014863236082924737,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.568267822265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041466858237981796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07753154635429382,
      "backward_entropy": 0.005544190605481465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.5,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041510842740535736,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0775061309337616,
      "backward_entropy": 0.01486509210533566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.567968368530273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04155377671122551,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.077480947971344,
      "backward_entropy": 0.014866011010275947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.70037078857422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04159681126475334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07745573520660401,
      "backward_entropy": 0.005504071298572753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.290199279785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041642166674137115,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07742968797683716,
      "backward_entropy": 0.014858232604132758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.281848907470703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04168636351823807,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07740389108657837,
      "backward_entropy": 0.005474393566449483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.110342979431152,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04173074662685394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0773779571056366,
      "backward_entropy": 0.005461583534876506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.14144515991211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041772834956645966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07735266685485839,
      "backward_entropy": 0.005448358754316966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.007123947143555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04181738197803497,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07732654809951782,
      "backward_entropy": 0.00543500648604499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.83009719848633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041861969977617264,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0773003339767456,
      "backward_entropy": 0.014846396115091112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.810758590698242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041909947991371155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07727289199829102,
      "backward_entropy": 0.005409913344515694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.55741882324219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04195766896009445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07724540233612061,
      "backward_entropy": 0.005398661726050907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.606945037841797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042007263749837875,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0772172451019287,
      "backward_entropy": 0.0053872403999169665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.890697002410889,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042056381702423096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07718918323516846,
      "backward_entropy": 0.005376026034355164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.40715789794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04210174083709717,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07716233134269715,
      "backward_entropy": 0.014850839972496033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.135835647583008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04214710742235184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07713536024093628,
      "backward_entropy": 0.005355449186431037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.815391540527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042193565517663956,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07710791826248169,
      "backward_entropy": 0.005346217917071449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.566859245300293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04224199429154396,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07707964181900025,
      "backward_entropy": 0.0148634049627516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.768191814422607,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04228775575757027,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07705227732658386,
      "backward_entropy": 0.005326614197757509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.200210571289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042330000549554825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07702614068984985,
      "backward_entropy": 0.014867272641923692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.842632293701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04237140715122223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07700022459030151,
      "backward_entropy": 0.005307291944821675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.754764556884766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04241305962204933,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0769741415977478,
      "backward_entropy": 0.014874012933837043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.329029083251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042454931885004044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07694786787033081,
      "backward_entropy": 0.0052878935303952955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.85422897338867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04249810054898262,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07692101001739501,
      "backward_entropy": 0.005277806686030494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.71040344238281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04254361614584923,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07689316272735595,
      "backward_entropy": 0.07693866888682048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.56057357788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04259125515818596,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07686430215835571,
      "backward_entropy": 0.005259345389074749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.27464485168457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042640797793865204,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0768346905708313,
      "backward_entropy": 0.005251099665959676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.0920991897583,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042689792811870575,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07680516242980957,
      "backward_entropy": 0.01489096548822191,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.589601516723633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04273609444499016,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07677662372589111,
      "backward_entropy": 0.0052351173427369856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.97565460205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04278332740068436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0767475962638855,
      "backward_entropy": 0.005227789282798767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.877843856811523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04283022880554199,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0767186164855957,
      "backward_entropy": 0.005220375127262539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.664268493652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042876772582530975,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0766896665096283,
      "backward_entropy": 0.005212422460317612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.677522659301758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04292520508170128,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07665982842445374,
      "backward_entropy": 0.014915789167086283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.18522834777832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04297320544719696,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07663006782531738,
      "backward_entropy": 0.005196867717636956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.21488952636719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04301968589425087,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07660114765167236,
      "backward_entropy": 0.005189355876710679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.377758026123047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0430680550634861,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07657067775726319,
      "backward_entropy": 0.005182296865516239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.325488567352295,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04311597719788551,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07654063701629639,
      "backward_entropy": 0.07695274882846409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.184720993041992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04316021874547005,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0765121340751648,
      "backward_entropy": 0.005169130033916897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.550585746765137,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04320438206195831,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07648360729217529,
      "backward_entropy": 0.014945661028226217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.75127029418945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04324621707201004,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07645606994628906,
      "backward_entropy": 0.014948566754659018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.908798217773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04329145327210426,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07642683982849122,
      "backward_entropy": 0.014951215849982368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.015886306762695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043336495757102966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07639766335487366,
      "backward_entropy": 0.014953593413035074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.539456367492676,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04338245093822479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07636778354644776,
      "backward_entropy": 0.0051349492536650766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.623811721801758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043427079916000366,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07633852958679199,
      "backward_entropy": 0.014960320459471809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.923431396484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043471548706293106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07630921602249145,
      "backward_entropy": 0.005122172335783641,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.217656135559082,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043519116938114166,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07627830505371094,
      "backward_entropy": 0.014966168337398104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.501432418823242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04356411471962929,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0762485146522522,
      "backward_entropy": 0.005110005537668864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.17911434173584,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043611034750938416,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07621769905090332,
      "backward_entropy": 0.014971839057074653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.073526382446289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04365650936961174,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07618749737739564,
      "backward_entropy": 0.005097781618436177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.03966999053955,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04369955509901047,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07615846395492554,
      "backward_entropy": 0.00509155210521486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.973316192626953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04374152049422264,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07612990140914917,
      "backward_entropy": 0.014977526333596971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.90878677368164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04378253221511841,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0761017382144928,
      "backward_entropy": 0.01498002310593923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.740154266357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04382265731692314,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07607393264770508,
      "backward_entropy": 0.014982496698697409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.631019592285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04386409372091293,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07604531645774841,
      "backward_entropy": 0.005068965670135286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.616239547729492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043906744569540024,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07601591348648071,
      "backward_entropy": 0.005063871542612712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.287900924682617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04394936561584473,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07598642110824586,
      "backward_entropy": 0.0050587960415416295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.860171794891357,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04399413987994194,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07595563530921937,
      "backward_entropy": 0.014992781811290316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.838442802429199,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044035520404577255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07592665553092956,
      "backward_entropy": 0.005049731582403183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.896577835083008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04407382383942604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07589945197105408,
      "backward_entropy": 0.005045110152827369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.796534538269043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04411463066935539,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0758705973625183,
      "backward_entropy": 0.005040388968255784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.325554847717285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044152431190013885,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07584348917007447,
      "backward_entropy": 0.005035781611998876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.7767333984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04418961703777313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07581664323806762,
      "backward_entropy": 0.005031159354580773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.409151077270508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04422835633158684,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0757887601852417,
      "backward_entropy": 0.015004765656259324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.85387420654297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04426959156990051,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0757592797279358,
      "backward_entropy": 0.07697182231479222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.766939163208008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04431089386343956,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0757296621799469,
      "backward_entropy": 0.015008451210127937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.01015567779541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044352270662784576,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0756999135017395,
      "backward_entropy": 0.005014090902275509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.946701049804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044392697513103485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07567063570022584,
      "backward_entropy": 0.005010175208250682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.256674766540527,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044432226568460464,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07564184069633484,
      "backward_entropy": 0.015012100338935852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.040863037109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04446990042924881,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07561416625976562,
      "backward_entropy": 0.005002199775642819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.76308536529541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04450901970267296,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07558536529541016,
      "backward_entropy": 0.004998396254248089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.404571533203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04454737901687622,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07555704712867736,
      "backward_entropy": 0.015014217959509956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.73014259338379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0445881262421608,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07552708387374878,
      "backward_entropy": 0.004990882757637236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.572371482849121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044629987329244614,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0754962682723999,
      "backward_entropy": 0.01501307057009803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.010387420654297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04467078298330307,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07546597719192505,
      "backward_entropy": 0.015012196368641324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.4435396194458,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044711653143167496,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07543565034866333,
      "backward_entropy": 0.015011004275745816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.300580978393555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04475153982639313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07540582418441773,
      "backward_entropy": 0.004975205494297875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.069297790527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04479258880019188,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07537509202957153,
      "backward_entropy": 0.00497109732694096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.83089542388916,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04483674466609955,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07534215450286866,
      "backward_entropy": 0.004967029723856185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.966703414916992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044878553599119186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0753106951713562,
      "backward_entropy": 0.004963022139337327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.37115478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04492131620645523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07527847290039062,
      "backward_entropy": 0.004958902796109517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.750446319580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04496084153652191,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07524844408035278,
      "backward_entropy": 0.0049548351930247415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.31494140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045001547783613205,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0752174973487854,
      "backward_entropy": 0.014990086356798807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.614779472351074,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045042287558317184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07518637180328369,
      "backward_entropy": 0.004947384612427818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0002161891316063702,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045081011950969696,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07515665888786316,
      "backward_entropy": 0.004943828201956219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.343563079833984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045115869492292404,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07512974739074707,
      "backward_entropy": 0.004940304905176163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.24781608581543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045152340084314346,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07510139942169189,
      "backward_entropy": 0.004937186423275206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.689037322998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04519025608897209,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07507190704345704,
      "backward_entropy": 0.014973767929606967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.84063148498535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04522743821144104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07504287362098694,
      "backward_entropy": 0.004931577377849155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.761795043945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04526495933532715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07501347064971924,
      "backward_entropy": 0.004928716354899936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.36466979980469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04530278965830803,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07498372197151185,
      "backward_entropy": 0.004925905416409175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.445566177368164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045344918966293335,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07495058178901673,
      "backward_entropy": 0.014960789018207125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.381182670593262,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045385874807834625,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07491824626922608,
      "backward_entropy": 0.014956952797042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.636938095092773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04542577266693115,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0748866081237793,
      "backward_entropy": 0.004917531377739376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.252636909484863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045467715710401535,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07485324144363403,
      "backward_entropy": 0.004914820608165529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.188626289367676,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0455084927380085,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07482067346572877,
      "backward_entropy": 0.004912148333258099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.126273155212402,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0455482117831707,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07478882670402527,
      "backward_entropy": 0.014939662483003404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.065461158752441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045586977154016495,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07475762367248535,
      "backward_entropy": 0.004906996256775326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.007966995239258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04562487453222275,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07472697496414185,
      "backward_entropy": 0.004904533839888043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.892375946044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04566298425197601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07469598054885865,
      "backward_entropy": 0.004902159174283345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.883088111877441,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04570326209068298,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07466318607330322,
      "backward_entropy": 0.07698100805282593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.881107807159424,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04574250429868698,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0746311068534851,
      "backward_entropy": 0.004896949148840374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.60507583618164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045779820531606674,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07460052967071533,
      "backward_entropy": 0.014907469352086386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.801867961883545,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04581836611032486,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07456876039505005,
      "backward_entropy": 0.014901293648613824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.527008056640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045855049043893814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07453858852386475,
      "backward_entropy": 0.004890315234661102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.449782371520996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04589201882481575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07450787425041198,
      "backward_entropy": 0.004888431065612369,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.057897567749023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04592923820018768,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07447692155838012,
      "backward_entropy": 0.014883132444487678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8226616382598877,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04596864804625511,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07444391250610352,
      "backward_entropy": 0.004884689632389281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.607041835784912,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046005114912986755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07441340684890747,
      "backward_entropy": 0.004882937918106715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.355945587158203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04603990539908409,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07438430786132813,
      "backward_entropy": 0.014864279164208306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.535041809082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046074166893959045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07435556650161743,
      "backward_entropy": 0.004879524310429891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.2510404586792,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046106964349746704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07432805299758911,
      "backward_entropy": 0.004877910017967224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.933526992797852,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0461394228041172,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07430073022842407,
      "backward_entropy": 0.004876379751496845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.2963924407959,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046172548085451126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0742726445198059,
      "backward_entropy": 0.01483732627497779,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.395257472991943,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046208206564188004,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07424211502075195,
      "backward_entropy": 0.004873508380519019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.719149589538574,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04624226316809654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07421300411224366,
      "backward_entropy": 0.004872273239824507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.30866813659668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046276792883872986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07418326139450074,
      "backward_entropy": 0.004870932549238205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.929268836975098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046312738209962845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07415204048156739,
      "backward_entropy": 0.004869821170965831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.249693393707275,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04634802043437958,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07412142753601074,
      "backward_entropy": 0.014804489082760282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6076948642730713,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04638173431158066,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07409205436706542,
      "backward_entropy": 0.014799152811368307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.363663673400879,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046413056552410126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0740649402141571,
      "backward_entropy": 0.014793586399820115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.722888946533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0464450903236866,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07403706312179566,
      "backward_entropy": 0.014787228571044074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.78858184814453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0464768186211586,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.074009370803833,
      "backward_entropy": 0.014780874053637186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.621369361877441,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046510159969329834,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07398000955581666,
      "backward_entropy": 0.004866216331720352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.616209030151367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04654305800795555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07395098805427551,
      "backward_entropy": 0.0048655424680974745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.031980514526367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04657745733857155,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07392032146453857,
      "backward_entropy": 0.01475954552491506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.973158359527588,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0466141439974308,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07388724088668823,
      "backward_entropy": 0.014752353231112162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.404821395874023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04664909467101097,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07385576367378235,
      "backward_entropy": 0.014745298359129164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.251733779907227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04668343439698219,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07382479310035706,
      "backward_entropy": 0.0048637427389621735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.159343719482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04671910032629967,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07379230260848998,
      "backward_entropy": 0.004863560199737549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.238923072814941,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046755947172641754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07375850677490234,
      "backward_entropy": 0.0048633408215310835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.576754570007324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04679194092750549,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.073725426197052,
      "backward_entropy": 0.004862921105490791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3764564990997314,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04682813212275505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07369205951690674,
      "backward_entropy": 0.004862623082266914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.14814567565918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046861663460731506,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07366142272949219,
      "backward_entropy": 0.004862376385264927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.358426094055176,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04689748212695122,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07362810373306275,
      "backward_entropy": 0.014693876107533773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.284069061279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04693349450826645,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07359452843666077,
      "backward_entropy": 0.0048619041012393106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.907525062561035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046969667077064514,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07356060743331909,
      "backward_entropy": 0.004861801034874386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.420276641845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047005027532577515,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07352743148803711,
      "backward_entropy": 0.004861548956897523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.796314239501953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047041501849889755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07349326014518738,
      "backward_entropy": 0.014660166369544135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.234119415283203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04707713425159454,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07345930337905884,
      "backward_entropy": 0.014651061760054694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.911941528320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04711385443806648,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07342423796653748,
      "backward_entropy": 0.004861486040883594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.836156845092773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0471506267786026,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07338893413543701,
      "backward_entropy": 0.0048620204130808515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.950220108032227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0471874438226223,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07335343956947327,
      "backward_entropy": 0.014623413483301798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.85323429107666,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04722518473863602,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0733167588710785,
      "backward_entropy": 0.014613585339652168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.603466033935547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0472637414932251,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07327899932861329,
      "backward_entropy": 0.07698551151487562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.394021034240723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04730214551091194,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07324117422103882,
      "backward_entropy": 0.004864970015154945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.336791038513184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04733948037028313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.073204505443573,
      "backward_entropy": 0.004866043312682046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.187774181365967,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04737582057714462,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07316871881484985,
      "backward_entropy": 0.004867130683528053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.152513027191162,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04741038382053375,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07313482761383057,
      "backward_entropy": 0.014562298854192099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.352214813232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04744332656264305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07310273647308349,
      "backward_entropy": 0.004870300905572044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.082808017730713,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04747840389609337,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07306797504425049,
      "backward_entropy": 0.014541647500462003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.072283744812012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04751181975007057,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07303500175476074,
      "backward_entropy": 0.004873121364249123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.021615982055664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04754458740353584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07300271987915039,
      "backward_entropy": 0.014519765973091125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.97249984741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047576822340488434,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07297080755233765,
      "backward_entropy": 0.004875862350066503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.818790435791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04760853201150894,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07293941974639892,
      "backward_entropy": 0.004877224978473451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.914556503295898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047643400728702545,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0729039192199707,
      "backward_entropy": 0.0048789141906632316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.880462646484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04767657816410065,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07287036180496216,
      "backward_entropy": 0.00488048502140575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.6944580078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047708265483379364,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07283839583396912,
      "backward_entropy": 0.004882391956117418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.628739356994629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04774035885930061,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07280582189559937,
      "backward_entropy": 0.004884120490815904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.783055782318115,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0477728545665741,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07277237176895142,
      "backward_entropy": 0.004886214517884784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.500218391418457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047803860157728195,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07274086475372314,
      "backward_entropy": 0.004887976994117101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.577795028686523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04783529415726662,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0727087140083313,
      "backward_entropy": 0.014422333902782865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.530590057373047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04786623641848564,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07267698049545288,
      "backward_entropy": 0.004890681554873784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8297510147094727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04789673164486885,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07264578342437744,
      "backward_entropy": 0.014396496944957309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.065954208374023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04792506992816925,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0726171851158142,
      "backward_entropy": 0.0048929668135113185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.585121154785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047954998910427094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07258617877960205,
      "backward_entropy": 0.0048945508897304535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.564053058624268,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04798807576298714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07255100011825562,
      "backward_entropy": 0.004895840254094865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.82639217376709,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048019617795944214,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07251777052879334,
      "backward_entropy": 0.014346450567245483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7510993480682373,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04805237427353859,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07248252630233765,
      "backward_entropy": 0.004898710797230403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.003001964185386896,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04808272048830986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07245051860809326,
      "backward_entropy": 0.004900032861365212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.597021102905273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04811002314090729,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0724225401878357,
      "backward_entropy": 0.014307722449302673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.637325286865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04813898727297783,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07239203453063965,
      "backward_entropy": 0.004902799510293537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.06561279296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048171963542699814,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07235604524612427,
      "backward_entropy": 0.00490376353263855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.345621585845947,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04820426180958748,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07232069969177246,
      "backward_entropy": 0.004904950658480327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.314804553985596,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04823506623506546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07228728532791137,
      "backward_entropy": 0.01425485478507148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.644169807434082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04826449975371361,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07225567102432251,
      "backward_entropy": 0.0049070628980795545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.139281272888184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04829183965921402,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07222687005996704,
      "backward_entropy": 0.004907845622963375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.67852783203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0483207069337368,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07219581604003907,
      "backward_entropy": 0.004908211529254913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.599750280380249,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048351868987083435,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07216122150421142,
      "backward_entropy": 0.01419494715001848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.1678338050842285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048380814492702484,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07212958335876465,
      "backward_entropy": 0.014181516236729093,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.140113353729248,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04840859770774841,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07209937572479248,
      "backward_entropy": 0.004911738551325268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.113780975341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04843532294034958,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07207051515579224,
      "backward_entropy": 0.004913356569078233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.629925727844238,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04846104979515076,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07204307317733764,
      "backward_entropy": 0.014142021536827087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0033182220067828894,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04848675802350044,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07201546430587769,
      "backward_entropy": 0.00491580863793691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.62828826904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048509884625673294,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07199165225028992,
      "backward_entropy": 0.014113840129640367,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.021013259887695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04853659123182297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07196234464645386,
      "backward_entropy": 0.0049178339540958405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.947845458984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048564013093709946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07193185687065125,
      "backward_entropy": 0.004918733404742347,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.955630302429199,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04859373718500137,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07189791202545166,
      "backward_entropy": 0.004919383260938857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.312090873718262,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04862211272120476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0718659520149231,
      "backward_entropy": 0.004919445349110497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.450395345687866,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04865181818604469,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07183189392089843,
      "backward_entropy": 0.004919425066974428,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.436171531677246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04867937043309212,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07180090546607971,
      "backward_entropy": 0.004919252047936122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.104504585266113,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04870503768324852,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07177259922027587,
      "backward_entropy": 0.0140000581741333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.443721771240234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048732347786426544,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07174150943756104,
      "backward_entropy": 0.004920449107885361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3952622413635254,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04876191169023514,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07170683145523071,
      "backward_entropy": 0.01396968960762024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.273998260498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0487893745303154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0716751754283905,
      "backward_entropy": 0.004922245525651508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.821533203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048819053918123245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07164017558097839,
      "backward_entropy": 0.004923306405544281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.700368404388428,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04884996637701988,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07160285711288453,
      "backward_entropy": 0.004925046530034807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.671152114868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04887944832444191,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07156752347946167,
      "backward_entropy": 0.00492682225174374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.644105911254883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04890765994787216,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07153397798538208,
      "backward_entropy": 0.013904415898852877,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.537381172180176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04893464595079422,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07150238752365112,
      "backward_entropy": 0.00493024993273947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.296034097671509,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0489630363881588,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07146837711334228,
      "backward_entropy": 0.004931579861376021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2838943004608154,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04898945242166519,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07143726348876953,
      "backward_entropy": 0.004933448301421272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.539289474487305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04901399463415146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07140912413597107,
      "backward_entropy": 0.00493470248248842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.796441078186035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04903770610690117,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07138221263885498,
      "backward_entropy": 0.0049358755350112915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.732095241546631,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04906477779150009,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0713496446609497,
      "backward_entropy": 0.004937441398700078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.463352203369141,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04909161105751991,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07131720781326294,
      "backward_entropy": 0.004939367373784383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.872360229492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049117375165224075,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07128638029098511,
      "backward_entropy": 0.004941148062547048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.616641044616699,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04914382845163345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07125424146652222,
      "backward_entropy": 0.00494319780005349,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.387053966522217,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04917006939649582,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07122226953506469,
      "backward_entropy": 0.0049452752702765996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.362105369567871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04919527471065521,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07119191884994507,
      "backward_entropy": 0.004947040643956926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.673885345458984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04921959340572357,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0711628556251526,
      "backward_entropy": 0.004949050645033519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.625052452087402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04924466833472252,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07113239765167237,
      "backward_entropy": 0.004950735304090712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.432074069976807,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04927042871713638,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07110069990158081,
      "backward_entropy": 0.013720981776714325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.394964694976807,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04929601773619652,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07106910943984986,
      "backward_entropy": 0.004953807840744655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.3579020500183105,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04932143911719322,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07103761434555053,
      "backward_entropy": 0.013694286346435547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.321220874786377,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04934671148657799,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07100626230239868,
      "backward_entropy": 0.013681118686993917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.19105863571167,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04937184602022171,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07097496390342713,
      "backward_entropy": 0.004958844433228175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.00361562124453485,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04939606413245201,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07094503045082093,
      "backward_entropy": 0.013655213018258413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.146656036376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0494178831577301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07091913223266602,
      "backward_entropy": 0.00496276468038559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.30992317199707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04943910986185074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07089409828186036,
      "backward_entropy": 0.004964918726020389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.103708267211914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04946216940879822,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07086567282676696,
      "backward_entropy": 0.0049671923948658835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.121994495391846,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04948454350233078,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07083823680877685,
      "backward_entropy": 0.004969936692052417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.147106170654297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04950705170631409,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07081034183502197,
      "backward_entropy": 0.01359857784377204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.0384297370910645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0495312325656414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07077956199645996,
      "backward_entropy": 0.004975654184818268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.0153656005859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04955451935529709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07074998021125793,
      "backward_entropy": 0.004977936959928936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9943184852600098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04957706853747368,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07072176933288574,
      "backward_entropy": 0.0049805281062920885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.927392959594727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049598902463912964,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0706946849822998,
      "backward_entropy": 0.004982757899496291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.897377014160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049622442573308945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07066437005996704,
      "backward_entropy": 0.004984899527496762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.003254450624808669,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0496467649936676,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07063241004943847,
      "backward_entropy": 0.013531227906545004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.906424045562744,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04966870695352554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0706045687198639,
      "backward_entropy": 0.004990253183576796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8868982791900635,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04969002678990364,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07057765722274781,
      "backward_entropy": 0.013512223958969116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.592633247375488,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0497107207775116,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07055190801620484,
      "backward_entropy": 0.013501265810595619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.765489101409912,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04973399639129639,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07052110433578491,
      "backward_entropy": 0.013491284516122606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.55098819732666,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04975723475217819,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07049043774604798,
      "backward_entropy": 0.00500113061732716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.696452617645264,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049782004207372665,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0704566478729248,
      "backward_entropy": 0.005003708104292552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8897881507873535,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04980665072798729,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07042280435562134,
      "backward_entropy": 0.07698151800367567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.755028486251831,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04982960969209671,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0703919529914856,
      "backward_entropy": 0.005009921060668098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.193413734436035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049851756542921066,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07036258578300476,
      "backward_entropy": 0.005012440184752147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.563776016235352,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04987623915076256,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07032882571220397,
      "backward_entropy": 0.013430490261978574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.213334083557129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049900565296411514,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07029513120651246,
      "backward_entropy": 0.005017005321052339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.153404235839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049926258623600006,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07025864124298095,
      "backward_entropy": 0.005019426345825195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.456078052520752,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0499531514942646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07021968364715576,
      "backward_entropy": 0.005021573768721687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.419323444366455,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04997958987951279,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07018172740936279,
      "backward_entropy": 0.005023290299706989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.590034246444702,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05000558868050575,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07014424800872802,
      "backward_entropy": 0.013372007343504164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.471307754516602,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05003044381737709,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07010923624038697,
      "backward_entropy": 0.00502518109149403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5408308506011963,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05005804821848869,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07006864547729492,
      "backward_entropy": 0.005026084681351979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.515946865081787,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05008435249328613,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07003048658370972,
      "backward_entropy": 0.005026518470711178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.237260341644287,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05010950565338135,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06999447345733642,
      "backward_entropy": 0.013310447335243225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.469925880432129,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050134316086769104,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06995911598205566,
      "backward_entropy": 0.013292953372001648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.447251081466675,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05015807971358299,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06992576122283936,
      "backward_entropy": 0.005026100410355462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.847616672515869,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050180938094854355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06989400386810303,
      "backward_entropy": 0.005025618192222383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.10394811630249,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050204452127218246,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06986081600189209,
      "backward_entropy": 0.005025263461801741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.44975471496582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050227824598550797,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.069827800989151,
      "backward_entropy": 0.005025132248799006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3596527576446533,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050252515822649,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06979172825813293,
      "backward_entropy": 0.00502530112862587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.671199321746826,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05027620121836662,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06975759267807007,
      "backward_entropy": 0.013186891873677572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3159730434417725,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05030043423175812,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06972225308418274,
      "backward_entropy": 0.07697825961642796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.294260263442993,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05032365769147873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06968887448310852,
      "backward_entropy": 0.00502612441778183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6387649774551392,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050345998257398605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06965717673301697,
      "backward_entropy": 0.005026157945394516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.129149436950684,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0503668487071991,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06962845325469971,
      "backward_entropy": 0.005026604152388043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.463862895965576,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05038924887776375,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06959624290466308,
      "backward_entropy": 0.00502747462855445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.817190647125244,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05041225254535675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06956269741058349,
      "backward_entropy": 0.005027963883346981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.567309379577637,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05043511092662811,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06952932476997375,
      "backward_entropy": 0.005028500739071105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.752386093139648,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050459977239370346,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06949154138565064,
      "backward_entropy": 0.0130567095345921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.290031909942627,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05048445984721184,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06945455074310303,
      "backward_entropy": 0.005029225928915871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.364391326904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05050932615995407,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06941664218902588,
      "backward_entropy": 0.013022343317667643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.649173736572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05053599178791046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06937466263771057,
      "backward_entropy": 0.005029741674661636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.612764835357666,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0505620501935482,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06933394670486451,
      "backward_entropy": 0.005029456896914376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5283068418502808,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05058765411376953,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06929402351379395,
      "backward_entropy": 0.005029795898331536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.060310363769531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050611499696969986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06925762295722962,
      "backward_entropy": 0.0050315095318688285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0104329586029053,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05063576623797417,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06921981573104859,
      "backward_entropy": 0.012945964104599424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.960404396057129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050659045577049255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06918438673019409,
      "backward_entropy": 0.005035813070005841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.485993504524231,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05068419501185417,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06914416551589966,
      "backward_entropy": 0.005038592136568493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.889103412628174,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05070755258202553,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06910789608955384,
      "backward_entropy": 0.005041578163703282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.385717868804932,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050731346011161804,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06907060146331787,
      "backward_entropy": 0.005044327841864692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.355381965637207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050754889845848083,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06903353929519654,
      "backward_entropy": 0.012893701593081156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4450334310531616,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05077813193202019,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.068997061252594,
      "backward_entropy": 0.005050588399171829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.434992790222168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050799716264009476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06896429061889649,
      "backward_entropy": 0.005053102142281002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.690301895141602,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050819847732782364,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06893459558486939,
      "backward_entropy": 0.01286048690478007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.241466999053955,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05084073543548584,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06890297532081605,
      "backward_entropy": 0.012850004765722487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.617056369781494,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050861604511737823,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06887117624282837,
      "backward_entropy": 0.005061813112762239,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.792599678039551,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05088314041495323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06883760690689086,
      "backward_entropy": 0.0050651004744900596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.158656120300293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050903864204883575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06880586743354797,
      "backward_entropy": 0.005067954460779826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7558743953704834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05092456564307213,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06877398490905762,
      "backward_entropy": 0.005070909857749939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.738471269607544,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05094457417726517,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06874358654022217,
      "backward_entropy": 0.005073999365170796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.156855583190918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05096394941210747,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0687143325805664,
      "backward_entropy": 0.005077285485135185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.40242338180542,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050985366106033325,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06868036389350891,
      "backward_entropy": 0.012778050369686551,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.684178352355957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05100727453827858,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06864549517631531,
      "backward_entropy": 0.012764993641111586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.665679693222046,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05102833732962608,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06861226558685303,
      "backward_entropy": 0.005083765006727642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.292129039764404,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0510486476123333,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06858068704605103,
      "backward_entropy": 0.012739318940374587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3176671266555786,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05106956884264946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06854748129844665,
      "backward_entropy": 0.005087660418616401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9178991317749023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051089052110910416,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06851764917373657,
      "backward_entropy": 0.005089319414562649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.596932888031006,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05110855773091316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06848762035369874,
      "backward_entropy": 0.005090734197033776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.579594373703003,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05112740769982338,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06845913529396057,
      "backward_entropy": 0.005091801285743713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.564457893371582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05114573612809181,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06843149662017822,
      "backward_entropy": 0.0050935298204422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5499956607818604,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05116355046629906,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06840503215789795,
      "backward_entropy": 0.005095278223355611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.063701152801514,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05118083953857422,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06837985515594483,
      "backward_entropy": 0.005096262941757838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2638992071151733,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05119902640581131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06835236549377441,
      "backward_entropy": 0.005097396671772003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.003537178039551,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05121589079499245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06832847595214844,
      "backward_entropy": 0.005096604426701863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7310028076171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051233671605587006,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06830217838287353,
      "backward_entropy": 0.00509604521923595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.47440767288208,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0512516126036644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06827532052993775,
      "backward_entropy": 0.005095566312472026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2315996885299683,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05126897618174553,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06825007200241089,
      "backward_entropy": 0.005094293918874528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6631510257720947,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05128525197505951,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06822730302810669,
      "backward_entropy": 0.012523318330446878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6429991722106934,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05130193382501602,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06820300817489625,
      "backward_entropy": 0.012506764796045091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.827254772186279,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05131887272000313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06817790269851684,
      "backward_entropy": 0.005094530681769053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.202622890472412,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05133665353059769,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06815075278282165,
      "backward_entropy": 0.012473515338367887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3863234519958496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05135330185294151,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06812623739242554,
      "backward_entropy": 0.005095156944460339,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3725271224975586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05136958137154579,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06810250282287597,
      "backward_entropy": 0.0050959355301327175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5365347862243652,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05138552561402321,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0680793821811676,
      "backward_entropy": 0.005097060567802853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.345331907272339,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.051401786506175995,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.06805540919303894,
      "backward_entropy": 0.07697101434071858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1675374507904053,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05141771212220192,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06803202629089355,
      "backward_entropy": 0.005099719597233666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.94961404800415,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05143275856971741,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06801050305366516,
      "backward_entropy": 0.005102339718076918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.605483531951904,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051450084894895554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06798317432403564,
      "backward_entropy": 0.005104624148872163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2902297973632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05146821588277817,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06795367002487182,
      "backward_entropy": 0.005106947074333827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1403186321258545,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05148576945066452,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06792563796043397,
      "backward_entropy": 0.005108863115310669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.775826930999756,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05150216072797775,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0679006278514862,
      "backward_entropy": 0.005110306044419606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.245969295501709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0515206903219223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06786994338035583,
      "backward_entropy": 0.005111899640825059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.457824230194092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05153864994645119,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0678405523300171,
      "backward_entropy": 0.00511408183309767,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2158448696136475,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05155736207962036,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06780880093574523,
      "backward_entropy": 0.012303175197707282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.594589710235596,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05157546326518059,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06777855157852172,
      "backward_entropy": 0.005120272851652569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.365056991577148,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05159546062350273,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06774337291717529,
      "backward_entropy": 0.005122974514961243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2492668628692627,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05161592736840248,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06770684719085693,
      "backward_entropy": 0.005125419961081611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2250890731811523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051636237651109695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06767052412033081,
      "backward_entropy": 0.005128251181708442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2008750438690186,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051656343042850494,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.06763464212417603,
      "backward_entropy": 0.005130656477477815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.233396053314209,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051676224917173386,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.06759944558143616,
      "backward_entropy": 0.012239597737789154,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 4.562204600942787,
    "avg_log_Z": -0.05068411394953728,
    "success_rate": 1.0,
    "avg_reward": 68.9,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.03,
      "1": 0.23,
      "2": 0.74
    },
    "avg_forward_entropy": 0.06912641006708146,
    "avg_backward_entropy": 0.00903818517923355,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}