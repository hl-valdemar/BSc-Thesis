{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 0.45454545454545453
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13827670812606813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13845601081848144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.99180603027344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18287205696105957,
      "backward_entropy": 0.13835041522979735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.28713989257812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -9.99999901978299e-05,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18286160628000894,
      "backward_entropy": 0.138358473777771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.5110321044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00020010577281937003,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1828508973121643,
      "backward_entropy": 0.13828299045562745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.8013916015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00029987734160386026,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1828399896621704,
      "backward_entropy": 0.1382864832878113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.5373077392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00040004431502893567,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18282880385716757,
      "backward_entropy": 0.13846416473388673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.09353637695312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0004985986743122339,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18281739950180054,
      "backward_entropy": 0.13829327821731568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.92234802246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005962496506981552,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18280581633249918,
      "backward_entropy": 0.13829641342163085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.03323364257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0006921335007064044,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18279409408569336,
      "backward_entropy": 0.1384692907333374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.2808074951172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0007879042532294989,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18278213342030844,
      "backward_entropy": 0.13830265998840333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.1768341064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0008823777316138148,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18276999394098917,
      "backward_entropy": 0.138305401802063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.5314178466797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0009789298055693507,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18275749683380127,
      "backward_entropy": 0.13830819129943847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.0902862548828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0010753097012639046,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18274476130803427,
      "backward_entropy": 0.13842711448669434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.9390869140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0011716585140675306,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18273178736368814,
      "backward_entropy": 0.13831400871276855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.2098846435547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0012669431744143367,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18271871407826742,
      "backward_entropy": 0.13831689357757568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.29684448242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013626053696498275,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1827055017153422,
      "backward_entropy": 0.13831965923309325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.7430877685547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0014572400832548738,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269258737564087,
      "backward_entropy": 0.13832223415374756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.12486267089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0015524846967309713,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18267929553985596,
      "backward_entropy": 0.13845475912094116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.8097381591797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0016481629572808743,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18266580502192178,
      "backward_entropy": 0.1383275032043457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.8865203857422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0017429485451430082,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1826521356900533,
      "backward_entropy": 0.13832991123199462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.47649383544922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018380805850028992,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18263818820317587,
      "backward_entropy": 0.13833229541778563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.14108276367188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001930941711179912,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18262412150700888,
      "backward_entropy": 0.13847498893737792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.7998809814453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002023212844505906,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18260985612869263,
      "backward_entropy": 0.13833636045455933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.68081665039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0021162484772503376,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18259523312250772,
      "backward_entropy": 0.13848562240600587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.51832580566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002208808669820428,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18258041143417358,
      "backward_entropy": 0.13834002017974853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.32258605957031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0023024387191981077,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18256531159083048,
      "backward_entropy": 0.13849296569824218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.4012451171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002394627081230283,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1825502117474874,
      "backward_entropy": 0.138487708568573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.5769500732422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0024902259465306997,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18253459533055624,
      "backward_entropy": 0.13834453821182252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.9574432373047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002585007343441248,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18251878023147583,
      "backward_entropy": 0.1383459210395813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.750244140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002678966149687767,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18250274658203125,
      "backward_entropy": 0.13848990201950073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.16067504882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0027673267759382725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.182486891746521,
      "backward_entropy": 0.1384902000427246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.22020721435547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0028570981230586767,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18247063954671225,
      "backward_entropy": 0.13851622343063355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.66429138183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002945187035948038,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18245426813761392,
      "backward_entropy": 0.1383499026298523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.11528015136719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003033032640814781,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18243749936421713,
      "backward_entropy": 0.1384909987449646,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.10147094726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003117977175861597,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18242077032725015,
      "backward_entropy": 0.13849103450775146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.8846435546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003206087276339531,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18240342537562051,
      "backward_entropy": 0.13835192918777467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.1038818359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0032909815199673176,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18238606055577597,
      "backward_entropy": 0.13835244178771972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.034912109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003374760737642646,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18236859639485678,
      "backward_entropy": 0.1385350465774536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.97474670410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0034560617059469223,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18235158920288086,
      "backward_entropy": 0.1384907364845276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.07540893554688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0035411478020250797,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1823342243830363,
      "backward_entropy": 0.13849048614501952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.20591735839844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0036265600938349962,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18231654167175293,
      "backward_entropy": 0.13835315704345702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.678218841552734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0037111127749085426,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18229868014653525,
      "backward_entropy": 0.13854531049728394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.74353790283203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0037870181258767843,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18228129545847574,
      "backward_entropy": 0.13835238218307494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.37681579589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0038609348703175783,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18226377169291177,
      "backward_entropy": 0.1383514881134033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.9862518310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003936342895030975,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1822457512219747,
      "backward_entropy": 0.1383506178855896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.14059448242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004016717430204153,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18222715457280478,
      "backward_entropy": 0.13834993839263915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.64579010009766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004099726676940918,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18220814069112143,
      "backward_entropy": 0.13834939002990723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.0380630493164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004178416915237904,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1821893254915873,
      "backward_entropy": 0.13834848403930664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.24180603027344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004255449865013361,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18217060963312784,
      "backward_entropy": 0.13855915069580077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.40184783935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004334405064582825,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1821515162785848,
      "backward_entropy": 0.13834617137908936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.3982696533203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004411553032696247,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18213244279225668,
      "backward_entropy": 0.1384809970855713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.76763153076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004492413718253374,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18211283286412558,
      "backward_entropy": 0.1384800672531128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.14541625976562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004572743084281683,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18209290504455566,
      "backward_entropy": 0.13856592178344726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.11045837402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004654034972190857,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18207242091496786,
      "backward_entropy": 0.13847782611846923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.29188537597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004741163458675146,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1820507844289144,
      "backward_entropy": 0.13834016323089598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.28333282470703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004828780423849821,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18202857176462808,
      "backward_entropy": 0.1385709047317505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.25823211669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004913719836622477,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1820063591003418,
      "backward_entropy": 0.13847455978393555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.58241271972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004995505791157484,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18198394775390625,
      "backward_entropy": 0.1384732484817505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.7834014892578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005081916693598032,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18196062246958414,
      "backward_entropy": 0.13857529163360596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.9729995727539,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005169079639017582,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1819368600845337,
      "backward_entropy": 0.1385766863822937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.0989532470703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005254937335848808,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18191279967625937,
      "backward_entropy": 0.13857803344726563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.28916931152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005343018099665642,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1818880240122477,
      "backward_entropy": 0.138331139087677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.89509582519531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005431146360933781,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18186251322428384,
      "backward_entropy": 0.138466739654541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.86878967285156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005517890211194754,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18183682362238565,
      "backward_entropy": 0.13858182430267335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.32029724121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005603404249995947,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18181087573369345,
      "backward_entropy": 0.1384632706642151,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.50858306884766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00568431057035923,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1817852258682251,
      "backward_entropy": 0.13832377195358275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.51782989501953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00576137425377965,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18175995349884033,
      "backward_entropy": 0.13832107782363892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.37109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005832374561578035,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18173507849375406,
      "backward_entropy": 0.13858535289764404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.9436492919922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005903972778469324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18170974651972452,
      "backward_entropy": 0.13831474781036376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.62132263183594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00598295871168375,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1816828449567159,
      "backward_entropy": 0.13858680725097655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.2340850830078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0060578021220862865,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18165618181228638,
      "backward_entropy": 0.13858747482299805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.5791778564453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006134528201073408,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18162874380747476,
      "backward_entropy": 0.138305926322937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.20326232910156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00621993001550436,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18159963687260947,
      "backward_entropy": 0.1385889768600464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.34657287597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006306772120296955,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18157023191452026,
      "backward_entropy": 0.13830043077468873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.5202178955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006403620354831219,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18153862158457437,
      "backward_entropy": 0.13843915462493897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.15558624267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006505063734948635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18150589863459268,
      "backward_entropy": 0.13843724727630616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.7311553955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0066038090735673904,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.181473175684611,
      "backward_entropy": 0.13843512535095215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.95693969726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006705754436552525,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18143979708353677,
      "backward_entropy": 0.13829171657562256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.4353485107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00680867675691843,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18140592177708945,
      "backward_entropy": 0.13828940391540528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.9376983642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006914030760526657,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18137133121490479,
      "backward_entropy": 0.13842947483062745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.90284729003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007017781957983971,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18133652210235596,
      "backward_entropy": 0.1384272813796997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.07964324951172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007120097521692514,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1813024878501892,
      "backward_entropy": 0.13828177452087403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.3363800048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007217953912913799,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18126885096232095,
      "backward_entropy": 0.13827853202819823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.74515533447266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007316675037145615,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18123437960942587,
      "backward_entropy": 0.138419246673584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.39391326904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007413342595100403,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18120004733403525,
      "backward_entropy": 0.1382718563079834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.02667236328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007505960762500763,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18116597334543863,
      "backward_entropy": 0.13826779127120972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.28616333007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007596288342028856,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18113150199254355,
      "backward_entropy": 0.1382635712623596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.28164672851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007690132595598698,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.181095818678538,
      "backward_entropy": 0.1384058713912964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.680419921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007783742155879736,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18105963865915933,
      "backward_entropy": 0.13840224742889404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.12896728515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00787345226854086,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18102367719014487,
      "backward_entropy": 0.1383982539176941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.7115478515625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007961466908454895,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18098745743433634,
      "backward_entropy": 0.1386012315750122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.90739440917969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008048116229474545,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18095103899637857,
      "backward_entropy": 0.13824064731597902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.88425064086914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008133845403790474,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1809145212173462,
      "backward_entropy": 0.13823517560958862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.31944274902344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008208272978663445,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1808795134226481,
      "backward_entropy": 0.13860132694244384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.4401092529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00828602910041809,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18084311485290527,
      "backward_entropy": 0.13837445974349977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.86270141601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008365138433873653,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18080592155456543,
      "backward_entropy": 0.13836902379989624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.72921752929688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008446061052381992,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.180768092473348,
      "backward_entropy": 0.13820966482162475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.29049682617188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008532094769179821,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18072899182637533,
      "backward_entropy": 0.13860119581222535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.6327896118164,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008623907342553139,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18068798383076987,
      "backward_entropy": 0.13860135078430175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.95587921142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008713515475392342,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18064661820729574,
      "backward_entropy": 0.13834831714630128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.59758758544922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008797748014330864,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18060592810312906,
      "backward_entropy": 0.1381845712661743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.92138671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008879316970705986,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18056549628575644,
      "backward_entropy": 0.13817720413208007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.6331329345703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008959594182670116,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1805244286855062,
      "backward_entropy": 0.13832985162734984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.70680236816406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009044678881764412,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1804816722869873,
      "backward_entropy": 0.13860102891921997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.74546813964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009128570556640625,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18043863773345947,
      "backward_entropy": 0.13831675052642822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.5272674560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00921529158949852,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18039540449778238,
      "backward_entropy": 0.1381467580795288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.0497283935547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009303063154220581,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18035205205281576,
      "backward_entropy": 0.13830358982086183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.72450256347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009393117390573025,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18030746777852377,
      "backward_entropy": 0.13860070705413818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.62966918945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009481036104261875,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18026246627171835,
      "backward_entropy": 0.13829015493392943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.78494262695312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009575080126523972,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1802157163619995,
      "backward_entropy": 0.13860058784484863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.47933197021484,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009671143256127834,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18016823132832846,
      "backward_entropy": 0.1386006474494934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.43636322021484,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009764887392520905,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18012064695358276,
      "backward_entropy": 0.13860063552856444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.953857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009854559786617756,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18007330099741617,
      "backward_entropy": 0.13826298713684082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.2755126953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009949609637260437,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1800238291422526,
      "backward_entropy": 0.13825681209564208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.4783935546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010041692294180393,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17997382084528604,
      "backward_entropy": 0.13824999332427979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.55121612548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010133377276360989,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17992305755615234,
      "backward_entropy": 0.13824291229248048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.274169921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010223486460745335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17987237373987833,
      "backward_entropy": 0.13823559284210205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.63185119628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010311889462172985,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1798215707143148,
      "backward_entropy": 0.13822816610336303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.52896118164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010396599769592285,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1797709067662557,
      "backward_entropy": 0.13804035186767577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.72727966308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010485860519111156,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17971861362457275,
      "backward_entropy": 0.13803085088729858,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.4074935913086,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010579441674053669,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17966487010320029,
      "backward_entropy": 0.1386002779006958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.75340270996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010669258423149586,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1796116828918457,
      "backward_entropy": 0.13801193237304688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.26974487304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010755538009107113,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17955891291300455,
      "backward_entropy": 0.13818881511688233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.2502899169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01084988098591566,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1795034408569336,
      "backward_entropy": 0.13818106651306153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.57015991210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010949504561722279,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1794458826382955,
      "backward_entropy": 0.13798224925994873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.9818572998047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01105364691466093,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17938631772994995,
      "backward_entropy": 0.13860089778900148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.41019439697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011157982982695103,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1793256402015686,
      "backward_entropy": 0.1379639983177185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.8174591064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011258510872721672,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17926470438639322,
      "backward_entropy": 0.13815062046051024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.8952865600586,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011358139105141163,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17920331160227457,
      "backward_entropy": 0.1386016607284546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.96266174316406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01145304273813963,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17914257446924844,
      "backward_entropy": 0.1386016607284546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.03724670410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011554504744708538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17907893657684326,
      "backward_entropy": 0.13812341690063476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.83180236816406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011658132076263428,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17901355028152466,
      "backward_entropy": 0.13860204219818115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.2595672607422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011758137494325638,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17894816398620605,
      "backward_entropy": 0.1386021375656128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.26187896728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011858873069286346,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1788817048072815,
      "backward_entropy": 0.13809409141540527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.16509246826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011956857517361641,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17881562312444052,
      "backward_entropy": 0.13808345794677734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.33004760742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012049656361341476,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17874979972839355,
      "backward_entropy": 0.13807175159454346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.65599060058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012143104337155819,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17868212858835855,
      "backward_entropy": 0.137849760055542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.67698669433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012227572500705719,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1786158283551534,
      "backward_entropy": 0.13804563283920288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.37416076660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012306759133934975,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1785506804784139,
      "backward_entropy": 0.13803093433380126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.27572631835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012392835691571236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17848289012908936,
      "backward_entropy": 0.138016939163208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.21250915527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012480969540774822,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17841356992721558,
      "backward_entropy": 0.13800277709960937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.45262145996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01257242914289236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17834186553955078,
      "backward_entropy": 0.13798847198486328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.4957504272461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012664166279137135,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17826996246973673,
      "backward_entropy": 0.13797417879104615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.96400451660156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012751477770507336,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17819847663243613,
      "backward_entropy": 0.13860046863555908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.37806701660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01284115482121706,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1781258980433146,
      "backward_entropy": 0.13794364929199218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.81642150878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01293238252401352,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17805143197377524,
      "backward_entropy": 0.137928307056427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.19342041015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013027041219174862,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1779749592145284,
      "backward_entropy": 0.13791294097900392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.23603057861328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013118975795805454,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1778984268506368,
      "backward_entropy": 0.13859951496124268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.9488067626953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013202527537941933,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17782390117645264,
      "backward_entropy": 0.13859891891479492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.0532989501953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013286462984979153,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17774832248687744,
      "backward_entropy": 0.13764584064483643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.18052673339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013376649469137192,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17766918738683066,
      "backward_entropy": 0.13762799501419068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.7284393310547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013462557457387447,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17758967479070029,
      "backward_entropy": 0.1385974407196045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.205078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013552749529480934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1775075594584147,
      "backward_entropy": 0.13780567646026612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.32979583740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013644714839756489,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17742363611857095,
      "backward_entropy": 0.1375708818435669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.44998168945312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013732519000768661,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17734030882517496,
      "backward_entropy": 0.13755046129226683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.69158172607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013822214677929878,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17725497484207153,
      "backward_entropy": 0.13774757385253905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.88999938964844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013910124078392982,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1771696408589681,
      "backward_entropy": 0.1385951519012451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.14793395996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013997833244502544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17708295583724976,
      "backward_entropy": 0.13770573139190673,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.96697998046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014088965952396393,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17699321111043295,
      "backward_entropy": 0.13768417835235597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.86146545410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014174144715070724,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17690529425938925,
      "backward_entropy": 0.13744181394577026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.02976989746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014267058111727238,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.176813006401062,
      "backward_entropy": 0.13859283924102783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.38084411621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014355707913637161,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1767216920852661,
      "backward_entropy": 0.13859219551086427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.6005096435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014446019195020199,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17662837107976279,
      "backward_entropy": 0.1375917911529541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.77552795410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01453810092061758,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1765333612759908,
      "backward_entropy": 0.13756775856018066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.60492706298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014629827812314034,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17643757661183676,
      "backward_entropy": 0.13754308223724365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.5504150390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014719413593411446,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17634193102518717,
      "backward_entropy": 0.13751758337020875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.24354553222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014814795926213264,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17624302705128989,
      "backward_entropy": 0.13727158308029175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.25157165527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014909619465470314,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17614352703094482,
      "backward_entropy": 0.1372455358505249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.92115783691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015003519132733345,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1760431925455729,
      "backward_entropy": 0.13744120597839354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.81221008300781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015098536387085915,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17594053347905478,
      "backward_entropy": 0.1374144434928894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.1508026123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01518516056239605,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17584123214085898,
      "backward_entropy": 0.13716259002685546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.3611297607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015275724232196808,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17573891083399454,
      "backward_entropy": 0.13735625743865967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.47457885742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015365654602646828,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17563519875208536,
      "backward_entropy": 0.13732597827911378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.22694396972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01546251866966486,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17552649974822998,
      "backward_entropy": 0.13707523345947265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.81546020507812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015560301952064037,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17541621128718057,
      "backward_entropy": 0.13858406543731688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.13931274414062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015656836330890656,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17530514796574911,
      "backward_entropy": 0.1370161771774292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.80435180664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015753909945487976,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1751920779546102,
      "backward_entropy": 0.13720299005508424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.28675842285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015856832265853882,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17507404088974,
      "backward_entropy": 0.13717050552368165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.5973663330078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015959879383444786,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1749568978945414,
      "backward_entropy": 0.13692543506622315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.46385955810547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01606069691479206,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17483965555826822,
      "backward_entropy": 0.1385815382003784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.71389770507812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016156286001205444,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17472426096598306,
      "backward_entropy": 0.13858067989349365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.3598861694336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016251109540462494,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17460848887761435,
      "backward_entropy": 0.13682444095611573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.8297119140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016341304406523705,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17449422677357992,
      "backward_entropy": 0.13699212074279785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.97264099121094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016438618302345276,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17437507708867392,
      "backward_entropy": 0.13857791423797608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.79811096191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016528895124793053,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1742589275042216,
      "backward_entropy": 0.13671457767486572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.80809783935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016618795692920685,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17414116859436035,
      "backward_entropy": 0.13667807579040528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.36373138427734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016706112772226334,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1740235686302185,
      "backward_entropy": 0.13857439756393433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.63668823242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016791677102446556,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17390634616216025,
      "backward_entropy": 0.13660061359405518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.39960479736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016876941546797752,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17378747463226318,
      "backward_entropy": 0.13674676418304443,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.94500732421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01695650815963745,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.173671027024587,
      "backward_entropy": 0.13670084476470948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.748779296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017032967880368233,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1735555330912272,
      "backward_entropy": 0.13856809139251708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.75621032714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01711035892367363,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17343813180923462,
      "backward_entropy": 0.13642725944519044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.84153747558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01719430275261402,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17331546545028687,
      "backward_entropy": 0.13856502771377563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.98544311523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017280511558055878,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17319043477376303,
      "backward_entropy": 0.13634082078933715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.9263153076172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01737082004547119,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17306220531463623,
      "backward_entropy": 0.13629800081253052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.04403686523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017464378848671913,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17293047904968262,
      "backward_entropy": 0.13625518083572388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.22935485839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0175628624856472,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17279454072316489,
      "backward_entropy": 0.13637065887451172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.1020050048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01766139641404152,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17265627781550089,
      "backward_entropy": 0.13617020845413208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.9491729736328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01776079088449478,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17251678307851157,
      "backward_entropy": 0.13612630367279052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.5792007446289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017866045236587524,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1723719835281372,
      "backward_entropy": 0.1385621428489685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.98597717285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017967302352190018,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17222837607065836,
      "backward_entropy": 0.13617550134658812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.92736053466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018063411116600037,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1720893383026123,
      "backward_entropy": 0.1361226797103882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.04551696777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01815488561987877,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17195355892181396,
      "backward_entropy": 0.13606734275817872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.2114715576172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018246976658701897,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1718143622080485,
      "backward_entropy": 0.13601021766662597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.81185913085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01834779791533947,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1716686487197876,
      "backward_entropy": 0.13595571517944335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.02388763427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018452409654855728,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17151878277460733,
      "backward_entropy": 0.1359017848968506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.37895965576172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018551377579569817,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.171371857325236,
      "backward_entropy": 0.1385587215423584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.56778717041016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018641125410795212,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17123005787531534,
      "backward_entropy": 0.13568732738494874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.61767578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01872684620320797,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1710904836654663,
      "backward_entropy": 0.13571540117263795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.68024444580078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018816594034433365,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17094739278157553,
      "backward_entropy": 0.13855390548706054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.5807342529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018900884315371513,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17080875237782797,
      "backward_entropy": 0.1355847954750061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.8386993408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018985498696565628,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1706688404083252,
      "backward_entropy": 0.13543987274169922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.01884460449219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019070271402597427,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1705274979273478,
      "backward_entropy": 0.138547945022583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.6146469116211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01915336586534977,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17038625478744507,
      "backward_entropy": 0.1353759288787842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.21533966064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019234677776694298,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17024465401967367,
      "backward_entropy": 0.1353020191192627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.64358520507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019314533099532127,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1701027750968933,
      "backward_entropy": 0.13522593975067138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.41778564453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019398808479309082,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16995614767074585,
      "backward_entropy": 0.1350976347923279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.33734130859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019481610506772995,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1698099970817566,
      "backward_entropy": 0.1385371208190918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.6313018798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019562674686312675,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16966350873311362,
      "backward_entropy": 0.13499375581741332,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.22134399414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019644159823656082,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16951513290405273,
      "backward_entropy": 0.1349113702774048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.51766967773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019727442413568497,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16936266422271729,
      "backward_entropy": 0.13482677936553955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.6895751953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019818352535367012,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1692027449607849,
      "backward_entropy": 0.1347223162651062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.256591796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01990431174635887,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16904425621032715,
      "backward_entropy": 0.13465685844421388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.18795776367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019997194409370422,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16887791951497397,
      "backward_entropy": 0.1345701813697815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.43832397460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020090945065021515,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1687092979749044,
      "backward_entropy": 0.1344813346862793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.6204833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020185740664601326,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16853884855906168,
      "backward_entropy": 0.13441643714904786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.15462875366211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02028539404273033,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16836398839950562,
      "backward_entropy": 0.13430387973785402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.70033264160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0203737560659647,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16819735368092856,
      "backward_entropy": 0.13852145671844482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.53980255126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020469876006245613,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16802473862965903,
      "backward_entropy": 0.13411942720413209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.44499206542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02056078426539898,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16785464684168497,
      "backward_entropy": 0.13402353525161742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.90989685058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020649509504437447,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1676861047744751,
      "backward_entropy": 0.13392481803894044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.572998046875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02073768898844719,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16751627127329508,
      "backward_entropy": 0.13851619958877565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.4871368408203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020826319232583046,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16734683513641357,
      "backward_entropy": 0.13372404575347902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.0354995727539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020915355533361435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16717753807703653,
      "backward_entropy": 0.13362476825714112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.59420776367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020998729392886162,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16701388359069824,
      "backward_entropy": 0.13362115621566772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.565185546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021082516759634018,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16684724887212118,
      "backward_entropy": 0.1335200071334839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.972900390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021170275285840034,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1666752298672994,
      "backward_entropy": 0.1334218144416809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.14768981933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021263347938656807,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1664963165918986,
      "backward_entropy": 0.13332550525665282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.77243041992188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021357499063014984,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16631470123926798,
      "backward_entropy": 0.13309850692749023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.49600219726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021453140303492546,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16613147656122842,
      "backward_entropy": 0.13312747478485107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.55196380615234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021548200398683548,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16594839096069336,
      "backward_entropy": 0.13850607872009277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.10203552246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021636594086885452,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16577027241388956,
      "backward_entropy": 0.13291623592376708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.27570343017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02172280102968216,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16559259096781412,
      "backward_entropy": 0.13264454603195192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.17311096191406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0218065045773983,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1654142439365387,
      "backward_entropy": 0.13251702785491942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.08718872070312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0218867976218462,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16523849964141846,
      "backward_entropy": 0.13256983757019042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.2011260986328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021969636902213097,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16505897045135498,
      "backward_entropy": 0.13244922161102296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.65757751464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022058581933379173,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16487248738606772,
      "backward_entropy": 0.1384915828704834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.490478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022151213139295578,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16468157370885214,
      "backward_entropy": 0.1319953203201294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.56710815429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02223927341401577,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1644936998685201,
      "backward_entropy": 0.1318570375442505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.88377380371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022327138110995293,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1643043359120687,
      "backward_entropy": 0.13848717212677003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.250732421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022407062351703644,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16412177681922913,
      "backward_entropy": 0.13848364353179932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.55333709716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022483687847852707,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16393977403640747,
      "backward_entropy": 0.13167012929916383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.09395599365234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022557618096470833,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16376022497812906,
      "backward_entropy": 0.1312434196472168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.0486602783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02262885682284832,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16358184814453125,
      "backward_entropy": 0.13107538223266602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.69345092773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022705139592289925,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16339580217997232,
      "backward_entropy": 0.13121261596679687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.55563354492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022786464542150497,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16320407390594482,
      "backward_entropy": 0.1310619592666626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.36903381347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022868437692523003,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1630107561747233,
      "backward_entropy": 0.1305701732635498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.1861801147461,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02295060269534588,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1628152330716451,
      "backward_entropy": 0.13845680952072142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.5446014404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02302595227956772,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1626277764638265,
      "backward_entropy": 0.13058722019195557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.21063995361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02310243993997574,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16243781646092734,
      "backward_entropy": 0.13003115653991698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.33421325683594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02317614108324051,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1622498631477356,
      "backward_entropy": 0.13844211101531984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.34716033935547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023251160979270935,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16205926736195883,
      "backward_entropy": 0.1296462297439575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.91099548339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023323416709899902,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1618702014287313,
      "backward_entropy": 0.12944812774658204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.30155944824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023402979597449303,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.161672314008077,
      "backward_entropy": 0.12925843000411988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.99407196044922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023480776697397232,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16147351264953613,
      "backward_entropy": 0.13842406272888183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.60035705566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023557526990771294,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16127481063206991,
      "backward_entropy": 0.12933958768844606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.51670837402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023631086573004723,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16107775767644247,
      "backward_entropy": 0.12864866256713867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.67701721191406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02371361292898655,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16086995601654053,
      "backward_entropy": 0.13841263055801392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.7777862548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023801717907190323,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1606534719467163,
      "backward_entropy": 0.12878201007843018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.12574005126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02389693818986416,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16042778889338175,
      "backward_entropy": 0.12860682010650634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.2139434814453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02399180643260479,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16020373503367105,
      "backward_entropy": 0.12842386960983276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.54689025878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024087250232696533,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.159976859887441,
      "backward_entropy": 0.12762727737426757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.4381103515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024181153625249863,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15974914034207663,
      "backward_entropy": 0.12740931510925294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.68861389160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024277769029140472,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15951714913050333,
      "backward_entropy": 0.1271873116493225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.6751708984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024377062916755676,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15928192933400473,
      "backward_entropy": 0.12696406841278077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.42010498046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024482229724526405,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1590389609336853,
      "backward_entropy": 0.12746695280075074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.6178436279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024581890553236008,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1588028073310852,
      "backward_entropy": 0.1272639274597168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.77162170410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02468148246407509,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15856442848841348,
      "backward_entropy": 0.12705857753753663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.40711212158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024785276502370834,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15832090377807617,
      "backward_entropy": 0.1260463237762451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.41375732421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024885376915335655,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15808152159055075,
      "backward_entropy": 0.12580504417419433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.21156311035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024982541799545288,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1578466296195984,
      "backward_entropy": 0.1384150505065918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.21438598632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02507120370864868,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1576226751009623,
      "backward_entropy": 0.12529902458190917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.72730255126953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02516469918191433,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1573904554049174,
      "backward_entropy": 0.12594420909881593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.1155242919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025252027437090874,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1571665108203888,
      "backward_entropy": 0.1247679591178894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.47504425048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02534116618335247,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15694010257720947,
      "backward_entropy": 0.12448923587799073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.085205078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025426404550671577,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1567184031009674,
      "backward_entropy": 0.12420063018798828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.92085266113281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025507356971502304,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1564993659655253,
      "backward_entropy": 0.12389640808105469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.33417510986328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02558848261833191,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15627825260162354,
      "backward_entropy": 0.12463116645812988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.015785217285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02566821500658989,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15605845053990683,
      "backward_entropy": 0.12326509952545166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.38160705566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02573920600116253,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15585043032964072,
      "backward_entropy": 0.12293161153793335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.39195251464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025813201442360878,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15563627084096274,
      "backward_entropy": 0.12259376049041748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.42818450927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025888096541166306,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15542320410410562,
      "backward_entropy": 0.12224981784820557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.16716766357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025961991399526596,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15520886580149332,
      "backward_entropy": 0.12189936637878418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.5944366455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026037707924842834,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15499496459960938,
      "backward_entropy": 0.12155253887176513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.5257110595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026117771863937378,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15477575858434042,
      "backward_entropy": 0.1225170612335205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.33621978759766,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02620045468211174,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15455440680185953,
      "backward_entropy": 0.13832805156707764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.77397155761719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026276005432009697,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1543429990609487,
      "backward_entropy": 0.1383198618888855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.58479309082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026348819956183434,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15413508812586466,
      "backward_entropy": 0.12011513710021973,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.01385498046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02642139419913292,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1539286176363627,
      "backward_entropy": 0.12120723724365234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.8913345336914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026493504643440247,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15372259418169656,
      "backward_entropy": 0.11935266256332397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.63435363769531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02656520903110504,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15351690848668417,
      "backward_entropy": 0.12051162719726563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.02500915527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026632752269506454,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15331677595774332,
      "backward_entropy": 0.1185341238975525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.87124633789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026704005897045135,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1531104544798533,
      "backward_entropy": 0.11979169845581054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.93254852294922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02677859738469124,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1528992454210917,
      "backward_entropy": 0.1382603645324707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.0786361694336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026852311566472054,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15268844366073608,
      "backward_entropy": 0.11725794076919556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.57080078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026923926547169685,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15248157580693564,
      "backward_entropy": 0.11869982481002808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.20210266113281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026996681466698647,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15227077404658,
      "backward_entropy": 0.11833033561706544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.34303283691406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027069412171840668,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15206193923950195,
      "backward_entropy": 0.13823107481002808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.11549377441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027145255357027054,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15184777975082397,
      "backward_entropy": 0.11546049118041993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.84263610839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027224401012063026,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15163023273150125,
      "backward_entropy": 0.11500668525695801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.72284698486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02730768360197544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15140525499979654,
      "backward_entropy": 0.11454931497573853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.21373748779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02738940343260765,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15118354558944702,
      "backward_entropy": 0.11635024547576904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.18111419677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02746197208762169,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15097267429033914,
      "backward_entropy": 0.11358736753463745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.0599365234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027530010789632797,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15076486269632974,
      "backward_entropy": 0.11308246850967407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.17253112792969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027596082538366318,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15055840214093527,
      "backward_entropy": 0.11257002353668213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.54878997802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027662357315421104,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15035327275594076,
      "backward_entropy": 0.11456881761550904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.3833999633789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027730152010917664,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15014304717381796,
      "backward_entropy": 0.13817369937896729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.65524291992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027798255905508995,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14993407328923544,
      "backward_entropy": 0.1136583924293518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.4172134399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027870040386915207,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14972052971522012,
      "backward_entropy": 0.11047230958938599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.05585479736328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02794005163013935,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1495127479235331,
      "backward_entropy": 0.10994008779525757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.22716522216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028008095920085907,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1493045687675476,
      "backward_entropy": 0.10940325260162354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.02888488769531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028078299015760422,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1490951975186666,
      "backward_entropy": 0.11177350282669067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.93633270263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028142841532826424,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14889533321062723,
      "backward_entropy": 0.11127305030822754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.30265045166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028201783075928688,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.148701141277949,
      "backward_entropy": 0.10772944688796997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.20582580566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028260041028261185,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1485085884730021,
      "backward_entropy": 0.11022721529006958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.14238739013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028323203325271606,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14830917119979858,
      "backward_entropy": 0.10970637798309327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.38909912109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028381196781992912,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14811686674753824,
      "backward_entropy": 0.10596961975097656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.46701049804688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028444895520806313,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14791934688886008,
      "backward_entropy": 0.1086275577545166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.28030776977539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028513599187135696,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14771753549575806,
      "backward_entropy": 0.10808265209197998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.3436279296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028572894632816315,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14752906560897827,
      "backward_entropy": 0.10423496961593628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.40746307373047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02863703854382038,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14733277757962546,
      "backward_entropy": 0.10696259737014771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.52876281738281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028698096051812172,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.147142231464386,
      "backward_entropy": 0.10638859272003173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.36040496826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02875823713839054,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1469514568646749,
      "backward_entropy": 0.10243308544158936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.9599380493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028821492567658424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1467560132344564,
      "backward_entropy": 0.10182631015777588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.8445816040039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028887685388326645,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14655748009681702,
      "backward_entropy": 0.10122507810592651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.2191619873047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028954582288861275,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1463594933350881,
      "backward_entropy": 0.10061755180358886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.31997680664062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02902515046298504,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14615406592686972,
      "backward_entropy": 0.1035348653793335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.03978729248047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029099399223923683,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14594324429829916,
      "backward_entropy": 0.1380324363708496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.7387924194336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029169412329792976,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14573760827382407,
      "backward_entropy": 0.09873569607734681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.78119659423828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02924088016152382,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14552825689315796,
      "backward_entropy": 0.10181407928466797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.99356842041016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029308421537280083,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14532401164372763,
      "backward_entropy": 0.09740843772888183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.72974395751953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029378442093729973,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14511730273564658,
      "backward_entropy": 0.09674419164657592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.68800354003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02945021167397499,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14490769306818643,
      "backward_entropy": 0.09607623815536499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.29336547851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029525866732001305,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14469399054845175,
      "backward_entropy": 0.09541547894477845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.12886810302734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029606932774186134,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14447468519210815,
      "backward_entropy": 0.09883893728256225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.64061737060547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02968503162264824,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14425931374231973,
      "backward_entropy": 0.09409188628196716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.57206726074219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029760105535387993,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14404616753260294,
      "backward_entropy": 0.09763182401657104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.51824951171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029834790155291557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14383333921432495,
      "backward_entropy": 0.09271991848945618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.27427673339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029909631237387657,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14362390836079916,
      "backward_entropy": 0.09203206300735474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.45106506347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0299934484064579,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14340313275655112,
      "backward_entropy": 0.09580162763595582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.05570220947266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030079863965511322,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14318042000134787,
      "backward_entropy": 0.0907022476196289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.8913345336914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030161166563630104,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14296579360961914,
      "backward_entropy": 0.09459130764007569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.06983947753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03024139441549778,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14275388916333517,
      "backward_entropy": 0.08930686712265015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.73486328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0303210336714983,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14254305760065714,
      "backward_entropy": 0.08860301375389099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.1490936279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030401961877942085,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1423303484916687,
      "backward_entropy": 0.08790310621261596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.7439422607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0304859671741724,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1421168843905131,
      "backward_entropy": 0.09206303358078002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.93509674072266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030576152727007866,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14189581076304117,
      "backward_entropy": 0.09146537780761718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.7415771484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03066335618495941,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14168178041776022,
      "backward_entropy": 0.08586645722389222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.78469848632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030749570578336716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14147247870763144,
      "backward_entropy": 0.08519024848937988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.65076446533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030840231105685234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14125844836235046,
      "backward_entropy": 0.08959029912948609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.9962158203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030927782878279686,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1410519282023112,
      "backward_entropy": 0.08384958505630494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.71723937988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03102082386612892,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1408345103263855,
      "backward_entropy": 0.08317258358001708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.20479583740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031110305339097977,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14062469204266867,
      "backward_entropy": 0.08768702149391175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.24237060546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03119838982820511,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14041786392529806,
      "backward_entropy": 0.0817983865737915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.79309844970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031286656856536865,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14021068811416626,
      "backward_entropy": 0.08109142780303955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.01125717163086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03137154132127762,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14000744620958963,
      "backward_entropy": 0.08038053512573243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.78848266601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03144943341612816,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13981181383132935,
      "backward_entropy": 0.13799999952316283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.78328323364258,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03153269737958908,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1396107574303945,
      "backward_entropy": 0.08434627056121827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.65621566772461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03160976618528366,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13942060867945352,
      "backward_entropy": 0.07819857001304627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.04838562011719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03168127313256264,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13923986752827963,
      "backward_entropy": 0.08294763565063476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.15792465209961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031749602407217026,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13906343777974448,
      "backward_entropy": 0.07672442197799682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.21488952636719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03180916979908943,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13889841238657633,
      "backward_entropy": 0.07596527934074401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.05709838867188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03187217935919762,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1387267311414083,
      "backward_entropy": 0.07521381378173828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.64501190185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03194786235690117,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13854252298672995,
      "backward_entropy": 0.07450941205024719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.88873291015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03202405944466591,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13836127519607544,
      "backward_entropy": 0.07381384372711182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.1902084350586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03210250288248062,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13817950089772543,
      "backward_entropy": 0.07312849164009094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.93633270263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03217720240354538,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13800260424613953,
      "backward_entropy": 0.07806274890899659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.343299865722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03225540742278099,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1378216048081716,
      "backward_entropy": 0.07173283696174622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.40985107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03232623264193535,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13765194018681845,
      "backward_entropy": 0.0710192084312439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.89995574951172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03239031508564949,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13749136527379355,
      "backward_entropy": 0.075993812084198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.4305648803711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03245464712381363,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13733255863189697,
      "backward_entropy": 0.06957404017448425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.087791442871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03252008929848671,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13716912269592285,
      "backward_entropy": 0.06886271238327027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.82330322265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03257940709590912,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13701486587524414,
      "backward_entropy": 0.06814050674438477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.3130874633789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032632648944854736,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1368667483329773,
      "backward_entropy": 0.06739416718482971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.73104095458984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032686639577150345,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1367182731628418,
      "backward_entropy": 0.06665796041488647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.96086883544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03274288401007652,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1365667184193929,
      "backward_entropy": 0.06592385768890381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.1292953491211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032803524285554886,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1364108920097351,
      "backward_entropy": 0.06520954370498658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.01074981689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03286213427782059,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.136256605386734,
      "backward_entropy": 0.06448827981948853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.66103744506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03292084112763405,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13610311349232992,
      "backward_entropy": 0.06376761794090272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.36128234863281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032974064350128174,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13595600922902426,
      "backward_entropy": 0.1376589298248291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.0632095336914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03303385525941849,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13580161333084106,
      "backward_entropy": 0.06231887936592102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.09750366210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033094048500061035,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13564841945966086,
      "backward_entropy": 0.06161479353904724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.21828079223633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03315262496471405,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13549773891766867,
      "backward_entropy": 0.06091088652610779,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.888031005859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03320585936307907,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13535414139429727,
      "backward_entropy": 0.0660666584968567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.5950698852539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03325632959604263,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13521400094032288,
      "backward_entropy": 0.059476053714752196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.2398681640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03330814093351364,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1350736916065216,
      "backward_entropy": 0.06464212536811828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.02188873291016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033364955335855484,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13492772976557413,
      "backward_entropy": 0.058087438344955444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.529541015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03342452645301819,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13478065530459085,
      "backward_entropy": 0.0633003294467926,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.54844665527344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033478934317827225,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13463996847470602,
      "backward_entropy": 0.0626191794872284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.04663848876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03353218361735344,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1344998081525167,
      "backward_entropy": 0.056044363975524904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.02377700805664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03358995541930199,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13435373703638712,
      "backward_entropy": 0.05537159442901611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.18804931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033644743263721466,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1342137853304545,
      "backward_entropy": 0.05469542145729065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.8557243347168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03370009362697601,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13407081365585327,
      "backward_entropy": 0.054017829895019534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.3245849609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033750977367162704,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13393397132555643,
      "backward_entropy": 0.053338027000427245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.56547546386719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03380332514643669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1337946355342865,
      "backward_entropy": 0.05267267227172852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.74434661865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03385669365525246,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13365411758422852,
      "backward_entropy": 0.0520072877407074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.69632720947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03391486778855324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13350893060366312,
      "backward_entropy": 0.05739443898200989,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.9827880859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03397563472390175,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13336183627446493,
      "backward_entropy": 0.05071873664855957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.77140045166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03403512388467789,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13321712613105774,
      "backward_entropy": 0.05008089542388916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.39048767089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0340951569378376,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1330713431040446,
      "backward_entropy": 0.05557400584220886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.83023834228516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03416302055120468,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13291864593823752,
      "backward_entropy": 0.05501492023468017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.035030364990234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03423271328210831,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13276442885398865,
      "backward_entropy": 0.04824978709220886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.05712890625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03429913520812988,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13261747360229492,
      "backward_entropy": 0.1374330997467041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.37528991699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03436943516135216,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13246816396713257,
      "backward_entropy": 0.04709306955337524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.49687957763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03444128856062889,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13231754302978516,
      "backward_entropy": 0.052804839611053464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.91054153442383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034512825310230255,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13216779629389444,
      "backward_entropy": 0.04595229923725128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.76231002807617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03457829728722572,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13202435771624246,
      "backward_entropy": 0.05169190764427185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.32624816894531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03464074432849884,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13188536961873373,
      "backward_entropy": 0.044798025488853456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.26666259765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034703586250543594,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13174510995546976,
      "backward_entropy": 0.044225302338600156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.50259780883789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03476671129465103,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13160314162572226,
      "backward_entropy": 0.04365584850311279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.486366271972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03482704237103462,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1314647396405538,
      "backward_entropy": 0.043087619543075564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.34404754638672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03488478064537048,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1313304305076599,
      "backward_entropy": 0.13746564388275145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.28749084472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0349438339471817,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13119617104530334,
      "backward_entropy": 0.04196962118148804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.31853485107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03500399738550186,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1310606598854065,
      "backward_entropy": 0.04142304062843323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.43164825439453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03506375849246979,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13092756271362305,
      "backward_entropy": 0.04088737964630127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.89156341552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03512861207127571,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13079110781351724,
      "backward_entropy": 0.04037400484085083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.1955795288086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03519957512617111,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13064882159233093,
      "backward_entropy": 0.04619225859642029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.23354721069336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03527763858437538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13049903512001038,
      "backward_entropy": 0.039405667781829835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.39269256591797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035351984202861786,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1303563416004181,
      "backward_entropy": 0.04526869654655456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.08139419555664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03542270511388779,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13021816809972128,
      "backward_entropy": 0.044787296652793886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.05730438232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03548824042081833,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1300861438115438,
      "backward_entropy": 0.03797354996204376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.4514045715332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03555119037628174,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12995857993761697,
      "backward_entropy": 0.037492218613624576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.819263458251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03561338037252426,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12983036041259766,
      "backward_entropy": 0.037014696002006534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.3858642578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03566918149590492,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12970836957295737,
      "backward_entropy": 0.036525437235832216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.78995132446289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03572683408856392,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1295835276444753,
      "backward_entropy": 0.13755043745040893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.8456039428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035784658044576645,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12946081161499023,
      "backward_entropy": 0.03558454215526581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.78560256958008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035849347710609436,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1293299992879232,
      "backward_entropy": 0.041347140073776247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.3697509765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035911235958337784,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12920077641805014,
      "backward_entropy": 0.03467876315116882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.14838409423828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035969000309705734,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12907691796620688,
      "backward_entropy": 0.040416696667671205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.31151580810547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03602521866559982,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12895546356836954,
      "backward_entropy": 0.13754792213439943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.32288360595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036083634942770004,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12883160511652628,
      "backward_entropy": 0.03951841592788696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.00798034667969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03614957258105278,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1287031372388204,
      "backward_entropy": 0.03911876082420349,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.183197021484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03621678426861763,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1285735766092936,
      "backward_entropy": 0.03255037665367126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.532230377197266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036282867193222046,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12844401597976685,
      "backward_entropy": 0.03215247690677643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.68180847167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03634864464402199,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12831604480743408,
      "backward_entropy": 0.031765294075012204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.38500213623047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03641919419169426,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1281831463177999,
      "backward_entropy": 0.03759092390537262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.42272186279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03649592027068138,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12804476420084634,
      "backward_entropy": 0.03724917471408844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.65774154663086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036572664976119995,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1279061237970988,
      "backward_entropy": 0.030686894059181215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.86666107177734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036643754690885544,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1277718941370646,
      "backward_entropy": 0.036539942026138306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.167911529541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036717455834150314,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1276355187098185,
      "backward_entropy": 0.036186623573303225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.01835632324219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03678462281823158,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.127506822347641,
      "backward_entropy": 0.029581180214881896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.44204330444336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03685249760746956,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12737546364466348,
      "backward_entropy": 0.03545193076133728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.57438659667969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03692008554935455,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12724757194519043,
      "backward_entropy": 0.028854000568389892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.591800689697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03699612244963646,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12710913022359213,
      "backward_entropy": 0.03477984964847565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.422119140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03707045316696167,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12697288393974304,
      "backward_entropy": 0.028188225626945496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.46495819091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0371520109474659,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1268282930056254,
      "backward_entropy": 0.02786787450313568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.53346252441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037235431373119354,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12668337424596152,
      "backward_entropy": 0.033889162540435794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.35041809082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03731878474354744,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12653919061024985,
      "backward_entropy": 0.027262744307518006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1806391477584839,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037403404712677,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.126393993695577,
      "backward_entropy": 0.02696478068828583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.13687133789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037477001547813416,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1262610654036204,
      "backward_entropy": 0.02664456069469452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.56534576416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0375513881444931,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1261274218559265,
      "backward_entropy": 0.02632952332496643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.90254592895508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037628356367349625,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12599160273869833,
      "backward_entropy": 0.03242846727371216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.55817413330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03770060837268829,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12586204210917154,
      "backward_entropy": 0.025724145770072936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.5971450805664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03777015581727028,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12573484579722086,
      "backward_entropy": 0.031826889514923094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.20125579833984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0378408208489418,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12560415267944336,
      "backward_entropy": 0.03153408467769623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.32710266113281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037912946194410324,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12547442317008972,
      "backward_entropy": 0.02483718693256378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.654788970947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037985894829034805,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12534291545550028,
      "backward_entropy": 0.030981776118278504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.91682434082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038057852536439896,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1252111295859019,
      "backward_entropy": 0.02426854372024536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.73739242553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03813086822628975,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12507855892181396,
      "backward_entropy": 0.023992058634757996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.00438690185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03820152208209038,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1249495546023051,
      "backward_entropy": 0.023719009757041932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.94761657714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038275230675935745,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12481826543807983,
      "backward_entropy": 0.029951542615890503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.32029724121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038351453840732574,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12468322118123372,
      "backward_entropy": 0.02320568561553955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.07682037353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038428302854299545,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12454726298650105,
      "backward_entropy": 0.02295130491256714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.65618133544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03850590065121651,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12441210945447286,
      "backward_entropy": 0.022700950503349304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.76478576660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038587551563978195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1242711345354716,
      "backward_entropy": 0.022461754083633424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.267974853515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03867455571889877,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12412597735722859,
      "backward_entropy": 0.02223391681909561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.2998046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03875959664583206,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12398378054300944,
      "backward_entropy": 0.022003822028636932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.36617279052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03884249925613403,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12384251753489177,
      "backward_entropy": 0.02176559716463089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.93217086791992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03892529755830765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12370101610819499,
      "backward_entropy": 0.021526074409484862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.02375793457031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03900637850165367,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12356126308441162,
      "backward_entropy": 0.021284161508083342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.78213500976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03908974677324295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12342023849487305,
      "backward_entropy": 0.021053314208984375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.458892822265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039173174649477005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12327919403711955,
      "backward_entropy": 0.020826196670532225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.97043991088867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039254963397979736,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12313982844352722,
      "backward_entropy": 0.02737198770046234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.27780151367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039332084357738495,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12300610542297363,
      "backward_entropy": 0.027162685990333557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.27116394042969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039409950375556946,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12287282943725586,
      "backward_entropy": 0.02696084976196289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.77947235107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03949189558625221,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12273283799489339,
      "backward_entropy": 0.01994604468345642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.72013473510742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03957271948456764,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12259584665298462,
      "backward_entropy": 0.02659604549407959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.6755599975586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039650555700063705,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12246203422546387,
      "backward_entropy": 0.026414138078689576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.47557067871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03973236307501793,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12232226133346558,
      "backward_entropy": 0.02624128758907318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.44095993041992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03981606662273407,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1221822698911031,
      "backward_entropy": 0.026075345277786256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.05757141113281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039896342903375626,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12204439441363017,
      "backward_entropy": 0.01894400417804718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.71239471435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039978764951229095,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.121904323498408,
      "backward_entropy": 0.01874886155128479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.5523910522461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040066324174404144,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12175788482030232,
      "backward_entropy": 0.018558159470558167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.02989959716797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040156904608011246,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12160905202229817,
      "backward_entropy": 0.01837077587842941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.11370849609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040241967886686325,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12146697441736858,
      "backward_entropy": 0.018180131912231445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.57122039794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04032912477850914,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1213209331035614,
      "backward_entropy": 0.018000966310501097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.33562469482422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0404130257666111,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12118101119995117,
      "backward_entropy": 0.02496595084667206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.15031814575195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040502190589904785,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12103561560312907,
      "backward_entropy": 0.02483084946870804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.28702163696289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040589340031147,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1208919088045756,
      "backward_entropy": 0.017486029863357545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.17705535888672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04067302495241165,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12075281143188477,
      "backward_entropy": 0.024550677835941316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.35199737548828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040758777409791946,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12061129013697307,
      "backward_entropy": 0.0171499639749527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.24602508544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0408446229994297,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12046962976455688,
      "backward_entropy": 0.016987374424934386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.59588623046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04093031585216522,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12032739321390788,
      "backward_entropy": 0.016827100515365602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.667964935302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04101783409714699,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12018322944641113,
      "backward_entropy": 0.0166721910238266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.950620651245117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04110199585556984,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12004215518633525,
      "backward_entropy": 0.016515983641147612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.46462631225586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041181668639183044,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11990829308827718,
      "backward_entropy": 0.016360099613666534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.38591384887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04126221686601639,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11977307001749675,
      "backward_entropy": 0.023636794090270995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.06327819824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04134322330355644,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11963621775309245,
      "backward_entropy": 0.016059011220932007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.23033905029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04142836481332779,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11949547131856282,
      "backward_entropy": 0.015917712450027467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.530467987060547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04152186959981918,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11934443314870198,
      "backward_entropy": 0.023303575813770294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.84394073486328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04160991683602333,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11920031905174255,
      "backward_entropy": 0.023202672600746155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.969778060913086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04169464856386185,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11906028787295024,
      "backward_entropy": 0.02309873104095459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.60789489746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0417732335627079,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11892780661582947,
      "backward_entropy": 0.022988510131835938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.4322509765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04185451567173004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11879224578539531,
      "backward_entropy": 0.015257841348648072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.949562072753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04194272309541702,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11864771445592244,
      "backward_entropy": 0.022796431183815004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.27412033081055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0420309342443943,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11850321292877197,
      "backward_entropy": 0.015013214945793153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.08003234863281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042115822434425354,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11836252609888713,
      "backward_entropy": 0.014890563488006592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.64261627197266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04220421612262726,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11821785569190979,
      "backward_entropy": 0.014772942662239075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.339411735534668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04229416325688362,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11807092030843098,
      "backward_entropy": 0.022451901435852052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.39485931396484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042376019060611725,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1179349422454834,
      "backward_entropy": 0.022367089986801147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.939056396484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04246168211102486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1177922785282135,
      "backward_entropy": 0.014433303475379944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.622798919677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04254749044775963,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1176498035589854,
      "backward_entropy": 0.014322729408740997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.420719146728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04263211041688919,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11750932534535725,
      "backward_entropy": 0.014216642081737518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.44561767578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04271402209997177,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11737253268559773,
      "backward_entropy": 0.02205789089202881,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.16628646850586,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.042796388268470764,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1172341803709666,
      "backward_entropy": 0.13856258392333984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.105712890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0428730733692646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11710336804389954,
      "backward_entropy": 0.013905055820941925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.93584442138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04294778034090996,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11697489023208618,
      "backward_entropy": 0.01380181461572647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.944438934326172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043022528290748596,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11684664090474446,
      "backward_entropy": 0.013703924417495728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.58415222167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04309399425983429,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11672269304593404,
      "backward_entropy": 0.013606888055801392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.59541702270508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043168604373931885,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11659397681554158,
      "backward_entropy": 0.013510040938854218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.83754539489746,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04324303939938545,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11646453539530437,
      "backward_entropy": 0.021536776423454286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.6332950592041,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04331269860267639,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11634164055188496,
      "backward_entropy": 0.021466684341430665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.75813865661621,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04337966442108154,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11622289816538493,
      "backward_entropy": 0.013231195509433746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.7298355102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04344246909022331,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11610963940620422,
      "backward_entropy": 0.013139481842517852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.208621978759766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04350944980978966,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11598936716715495,
      "backward_entropy": 0.013051815330982208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.854249000549316,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04357559233903885,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1158708930015564,
      "backward_entropy": 0.021205796301364897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.579242706298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04363609477877617,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11575975020726521,
      "backward_entropy": 0.01288236528635025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.6779899597168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04369302839040756,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11565321683883667,
      "backward_entropy": 0.02107180207967758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.87908935546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043751638382673264,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11554406086603801,
      "backward_entropy": 0.012711615860462188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.15049362182617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043810151517391205,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11543444792429607,
      "backward_entropy": 0.020942717790603638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.395648956298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04387174919247627,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11531993746757507,
      "backward_entropy": 0.012549839913845062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.60933303833008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04392976686358452,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1152103344599406,
      "backward_entropy": 0.012468662112951279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.76554489135742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04398791119456291,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11510069171587627,
      "backward_entropy": 0.01239066869020462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.44147872924805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04404927417635918,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11498544613520305,
      "backward_entropy": 0.01231706067919731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.2330093383789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044110413640737534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11487104495366414,
      "backward_entropy": 0.012246190011501313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.816864013671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04417913407087326,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11474498112996419,
      "backward_entropy": 0.012180865556001664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.633180618286133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04424838721752167,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11461792389551799,
      "backward_entropy": 0.012116201221942902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.55754280090332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044315073639154434,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11449467142422994,
      "backward_entropy": 0.012052443623542786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.45435333251953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04437950998544693,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11437489589055379,
      "backward_entropy": 0.020527127385139465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.88814163208008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044445063918828964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11425336201985677,
      "backward_entropy": 0.011932913959026337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.23640823364258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04450995475053787,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11413262287775676,
      "backward_entropy": 0.011874919384717941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.61224365234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04457579180598259,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11401005585988362,
      "backward_entropy": 0.011817948520183563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.37522506713867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04465175420045853,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1138721505800883,
      "backward_entropy": 0.011766289174556733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.57865905761719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044729143381118774,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11373194058736165,
      "backward_entropy": 0.020410527288913728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.05374526977539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044809356331825256,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11358730991681416,
      "backward_entropy": 0.011670706421136856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.88972854614258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04489051178097725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11344097057978313,
      "backward_entropy": 0.011624065786600113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.585159301757812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04497246816754341,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1132929523785909,
      "backward_entropy": 0.011577098071575165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.79443359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04504915326833725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11315351724624634,
      "backward_entropy": 0.011531149595975876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.8626937866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04512251541018486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11301889022191365,
      "backward_entropy": 0.011484421789646149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.03749465942383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04520060122013092,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11287691195805867,
      "backward_entropy": 0.011440827697515487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.903358459472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04527837038040161,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11273513237635295,
      "backward_entropy": 0.01139841452240944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.622337341308594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04535584896802902,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11259389917055766,
      "backward_entropy": 0.011357131600379943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.640869140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04543149471282959,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11245479186375935,
      "backward_entropy": 0.02029622197151184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.41212844848633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04550702124834061,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11231579383214314,
      "backward_entropy": 0.01127510666847229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.59807586669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0455809086561203,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11217919985453288,
      "backward_entropy": 0.011234329640865326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.247093200683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04565941169857979,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11203527450561523,
      "backward_entropy": 0.011195141077041625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.1087532043457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04573747143149376,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11189061403274536,
      "backward_entropy": 0.01115567609667778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.97836685180664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04581516981124878,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11174680789311726,
      "backward_entropy": 0.02022039592266083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.84033966064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04589246213436127,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1116031010945638,
      "backward_entropy": 0.01107739433646202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.705814361572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045969437807798386,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11145917574564616,
      "backward_entropy": 0.011038172245025634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.74871826171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04604613035917282,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1113162636756897,
      "backward_entropy": 0.011002971231937409,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.214881896972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04611952230334282,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11117784182230632,
      "backward_entropy": 0.02013673633337021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.16697311401367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04619599133729935,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11103389660517375,
      "backward_entropy": 0.010932699590921403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.83594512939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04627373814582825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1108876367410024,
      "backward_entropy": 0.01089881658554077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.0299186706543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04635411500930786,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11073596278826396,
      "backward_entropy": 0.010866156965494155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.441585540771484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046433866024017334,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1105852723121643,
      "backward_entropy": 0.010833879560232162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.99710464477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0465160496532917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11043016115824382,
      "backward_entropy": 0.01080285683274269,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.165679931640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04659593105316162,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11027891437212627,
      "backward_entropy": 0.010772204399108887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.24267578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04667222499847412,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11013384660085042,
      "backward_entropy": 0.019982334971427918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.32683563232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04675419256091118,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10997834801673889,
      "backward_entropy": 0.010711698979139327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.81914138793945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04683532565832138,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10982346534729004,
      "backward_entropy": 0.01068216860294342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.64543533325195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04691718891263008,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10966694355010986,
      "backward_entropy": 0.010652891546487808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.58332347869873,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04699970409274101,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10950876275698344,
      "backward_entropy": 0.010624051094055176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.10689353942871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04707534238696098,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10936304926872253,
      "backward_entropy": 0.01059454381465912,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.10689926147461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04714628681540489,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10922574003537495,
      "backward_entropy": 0.01056467741727829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.5094223022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04721599072217941,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10909020900726318,
      "backward_entropy": 0.01979455053806305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.95170783996582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04729197919368744,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10894238948822021,
      "backward_entropy": 0.010507827997207642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.348541259765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04736324027180672,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1088032325108846,
      "backward_entropy": 0.010480352491140366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.121578216552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047431737184524536,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10866949955622356,
      "backward_entropy": 0.010453221201896668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.59894943237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047500718384981155,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1085339883963267,
      "backward_entropy": 0.010426878929138184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.12663459777832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04756864905357361,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10840026537577312,
      "backward_entropy": 0.010401219129562378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.354019165039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047634150832891464,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10827080408732097,
      "backward_entropy": 0.010375762730836869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.989660263061523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04769449681043625,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10815096894900005,
      "backward_entropy": 0.010350243747234344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.46379852294922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04775317385792732,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10803399483362834,
      "backward_entropy": 0.010325288772583008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.423675537109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047817718237638474,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10790503025054932,
      "backward_entropy": 0.010301759093999862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.56502151489258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04788312688469887,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10777388016382854,
      "backward_entropy": 0.010278989374637604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.18156051635742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047950781881809235,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10763772328694661,
      "backward_entropy": 0.010257232934236526,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.84640121459961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048018984496593475,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10749998688697815,
      "backward_entropy": 0.010236237943172456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.05316925048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04808620363473892,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10736383994420369,
      "backward_entropy": 0.01021585613489151,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.795623779296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04816272109746933,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10720907648404439,
      "backward_entropy": 0.01019730493426323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.788272857666016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04823887348175049,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10705457131067912,
      "backward_entropy": 0.010179590433835983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.311002731323242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048316143453121185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10689749320348103,
      "backward_entropy": 0.010162565857172012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.30630111694336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048390038311481476,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10674635569254558,
      "backward_entropy": 0.019283932447433472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.25020980834961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04846234619617462,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10659923156102498,
      "backward_entropy": 0.010130442678928375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.116737365722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04853466898202896,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10645117362340291,
      "backward_entropy": 0.010115724802017213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.99290657043457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048607029020786285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10630242029825847,
      "backward_entropy": 0.01010167896747589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.797847747802734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04867646470665932,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10615963737169902,
      "backward_entropy": 0.010087307542562485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.722843170166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048749107867479324,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.106009840965271,
      "backward_entropy": 0.010074103623628617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.42409896850586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048821739852428436,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10585865378379822,
      "backward_entropy": 0.010061931610107423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.44980239868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04889721795916557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10570140679677327,
      "backward_entropy": 0.010050521790981292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.44972801208496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048972390592098236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10554396112759908,
      "backward_entropy": 0.010039815306663513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.34901428222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04904583841562271,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10539001226425171,
      "backward_entropy": 0.019067052006721496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.028324127197266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0491262823343277,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10522078474362691,
      "backward_entropy": 0.019048702716827393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.330795288085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04920586571097374,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10505292812983195,
      "backward_entropy": 0.019029243290424346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.234703063964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04928179085254669,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10489269097646077,
      "backward_entropy": 0.019007351994514466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.878156661987305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049360163509845734,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10472653309504192,
      "backward_entropy": 0.009994075447320939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.21363067626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04943641647696495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10456463694572449,
      "backward_entropy": 0.00998537540435791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.665119171142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04951787367463112,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1043902337551117,
      "backward_entropy": 0.018945316970348357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.1638298034668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049592602998018265,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10423099001248677,
      "backward_entropy": 0.009970326721668244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.212980270385742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04966701567173004,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10407269994417827,
      "backward_entropy": 0.009963507950305938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.741979598999023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049736857414245605,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10392320156097412,
      "backward_entropy": 0.009956905245780944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.560667991638184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04980393871665001,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10377977291742961,
      "backward_entropy": 0.009949496388435364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.65818405151367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04986568167805672,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10364842414855957,
      "backward_entropy": 0.009941132366657257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.551734924316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04992828145623207,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10351482033729553,
      "backward_entropy": 0.009931896626949311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.93167495727539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04999450966715813,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10337234536806743,
      "backward_entropy": 0.009922674298286438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.29123306274414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05005979165434837,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10323150952657063,
      "backward_entropy": 0.009913839399814606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.301332473754883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050125602632761,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10308895508448283,
      "backward_entropy": 0.01870550811290741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.63958168029785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05018910393118858,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10295106967290242,
      "backward_entropy": 0.009896789491176606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.77650260925293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05025187507271767,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10281457503636678,
      "backward_entropy": 0.009888253360986709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.0909481048584,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05031115934252739,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10268598794937134,
      "backward_entropy": 0.018603959679603578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.04129409790039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050368763506412506,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10256091753641765,
      "backward_entropy": 0.00987139791250229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.275785446166992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050429120659828186,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1024283766746521,
      "backward_entropy": 0.018537048995494843,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.94231414794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05048900842666626,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10229678948720296,
      "backward_entropy": 0.009857145696878433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.540459632873535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05055687949061394,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10214575131734212,
      "backward_entropy": 0.00984974130988121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.465240478515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050620853900909424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10200383265813191,
      "backward_entropy": 0.009843923151493073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.093265533447266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050686851143836975,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10185656944910686,
      "backward_entropy": 0.00983901247382164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.586259841918945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05075334385037422,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10170716047286987,
      "backward_entropy": 0.009835779666900635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.68097496032715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050817374140024185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10156338413556416,
      "backward_entropy": 0.009832143783569336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.442352294921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0508805587887764,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1014215350151062,
      "backward_entropy": 0.009827898442745208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.133595943450928,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05094154551625252,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1012848714987437,
      "backward_entropy": 0.009822620451450348,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.40387535095215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05099773406982422,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10116034746170044,
      "backward_entropy": 0.018250983953475953,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.46501922607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05105382204055786,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10103578368822734,
      "backward_entropy": 0.01820935308933258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.327110290527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051112692803144455,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10090357065200806,
      "backward_entropy": 0.009802260249853135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.102035522460938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05117407441139221,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10076448321342468,
      "backward_entropy": 0.009797576814889908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.034114837646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05123341828584671,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10063006480534871,
      "backward_entropy": 0.00979195088148117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.9304084777832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051292430609464645,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10049558679262798,
      "backward_entropy": 0.018061578273773193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.852825164794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05135239660739899,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10035943984985352,
      "backward_entropy": 0.018023501336574554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.628395080566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05141185596585274,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1002240777015686,
      "backward_entropy": 0.009778327494859695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.74673080444336,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0514737106859684,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.10008163253466289,
      "backward_entropy": 0.13862897157669068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.454612731933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051533591002225876,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09994377692540486,
      "backward_entropy": 0.009772679209709168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.3381233215332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051594413816928864,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09980294108390808,
      "backward_entropy": 0.009771224111318588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.3740234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05165601149201393,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0996598203976949,
      "backward_entropy": 0.009769221395254135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.09286117553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05171699821949005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09951778252919515,
      "backward_entropy": 0.009767953306436539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.553680419921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05177884176373482,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09937272469202678,
      "backward_entropy": 0.017781463265419007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.787020683288574,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051844216883182526,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09921769301096599,
      "backward_entropy": 0.009770550578832627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.239152908325195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05190427973866463,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09907718499501546,
      "backward_entropy": 0.017722709476947783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.172353744506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051962535828351974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09894088904062907,
      "backward_entropy": 0.009770562499761581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.19838333129883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052019111812114716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09880814949671428,
      "backward_entropy": 0.009771915525197983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.365763664245605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05207822844386101,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09866846601168315,
      "backward_entropy": 0.009773201495409011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.92313003540039,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.052134107798337936,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0985374649365743,
      "backward_entropy": 0.13862913846969604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.414451599121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052192628383636475,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09839874505996704,
      "backward_entropy": 0.009773746877908707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.825679779052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052254818379879,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09824974338213603,
      "backward_entropy": 0.009774360805749893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.597977161407471,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05231491103768349,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09810600678126018,
      "backward_entropy": 0.009775343537330627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.24354362487793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052370309829711914,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09797507524490356,
      "backward_entropy": 0.009775267541408538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.62553596496582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05242568254470825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09784344832102458,
      "backward_entropy": 0.009776561707258224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.12679672241211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052479542791843414,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09771578510602315,
      "backward_entropy": 0.009776721894741058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.960113525390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052538879215717316,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09757265448570251,
      "backward_entropy": 0.00977770984172821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.88009262084961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05260036513209343,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0974228282769521,
      "backward_entropy": 0.009778033196926116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.66315841674805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05266110971570015,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09727571407953899,
      "backward_entropy": 0.009778580069541931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.10124206542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05272391811013222,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0971220334370931,
      "backward_entropy": 0.009779921174049378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.96817398071289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05278712511062622,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09696680307388306,
      "backward_entropy": 0.009780289232730865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.843093872070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052850816398859024,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0968095858891805,
      "backward_entropy": 0.009782414138317107,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.3765811920166,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052914880216121674,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09665095806121826,
      "backward_entropy": 0.009784676879644395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.21757125854492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05297790467739105,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09649493296941121,
      "backward_entropy": 0.017126455903053284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.45965003967285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053043972700834274,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09633022546768188,
      "backward_entropy": 0.009787806868553161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.80232048034668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05311005562543869,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09616512060165405,
      "backward_entropy": 0.00978793129324913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.722793579101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05317353457212448,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09600724776585896,
      "backward_entropy": 0.009787806868553161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.862178802490234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05323472619056702,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09585522611935933,
      "backward_entropy": 0.016985374689102172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.767715454101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05329511687159538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09570515155792236,
      "backward_entropy": 0.009788819402456284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.66517448425293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05335472151637077,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09555777907371521,
      "backward_entropy": 0.009787952899932862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.71192741394043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05341373011469841,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09541038672129314,
      "backward_entropy": 0.009788262844085693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.365520477294922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05347342789173126,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09526127576828003,
      "backward_entropy": 0.016830992698669434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.467195510864258,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.053531065583229065,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09511808554331462,
      "backward_entropy": 0.13862905502319336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.357730865478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053589630872011185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09497090180714925,
      "backward_entropy": 0.00978531539440155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.15004539489746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05364883691072464,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09482274452845256,
      "backward_entropy": 0.009783416241407394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.096799850463867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05370604619383812,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0946799914042155,
      "backward_entropy": 0.009781384468078613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.005502700805664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05376279726624489,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09453789393107097,
      "backward_entropy": 0.009779712557792664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.915456771850586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053817879408597946,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09440018733342488,
      "backward_entropy": 0.009779362380504609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.922233581542969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053872670978307724,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09426313638687134,
      "backward_entropy": 0.009778427332639695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.663938522338867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05392463505268097,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09413403272628784,
      "backward_entropy": 0.016485552489757537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.932652473449707,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05397801101207733,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09399998188018799,
      "backward_entropy": 0.009779225289821624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.56820297241211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05402731895446777,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09387818972269694,
      "backward_entropy": 0.00977955460548401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.084197998046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05407695472240448,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09375484784444173,
      "backward_entropy": 0.009780506789684295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.236324310302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054130781441926956,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09361839294433594,
      "backward_entropy": 0.009782028943300247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.310705184936523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05418586730957031,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09347718954086304,
      "backward_entropy": 0.009785957634449005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.01885986328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05424061417579651,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09333680073420207,
      "backward_entropy": 0.0097888745367527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.581551551818848,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0542963482439518,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09319354097048442,
      "backward_entropy": 0.138627827167511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.284250259399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05434900149703026,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09305989742279053,
      "backward_entropy": 0.009791216999292373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.41682815551758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054400309920310974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09292988975842793,
      "backward_entropy": 0.009791938215494156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.570980072021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05445427820086479,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09279121955235799,
      "backward_entropy": 0.00979321300983429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.44926643371582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054509375244379044,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.092648446559906,
      "backward_entropy": 0.009795962274074555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.673093795776367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054565612226724625,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09250094493230183,
      "backward_entropy": 0.009802352637052536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.940296173095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054621513932943344,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09235372145970662,
      "backward_entropy": 0.009810690581798554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.89089012145996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054675813764333725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09221076965332031,
      "backward_entropy": 0.009820681065320969,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.99945640563965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05472835898399353,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09207421541213989,
      "backward_entropy": 0.00982605516910553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.326465606689453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054782189428806305,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09193247556686401,
      "backward_entropy": 0.009833531081676483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.686344146728516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05483568087220192,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09179218610127766,
      "backward_entropy": 0.01586160957813263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.610986709594727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054887574166059494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0916571319103241,
      "backward_entropy": 0.009841111302375794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.060302734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05493820086121559,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09152526656786601,
      "backward_entropy": 0.01578621715307236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.945674896240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054988887161016464,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09139323234558105,
      "backward_entropy": 0.015750178694725038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.961357116699219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05504223704338074,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0912515918413798,
      "backward_entropy": 0.1386282205581665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.808794021606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05509273707866669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09111937880516052,
      "backward_entropy": 0.009858830273151398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.142169952392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05514317750930786,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09098753333091736,
      "backward_entropy": 0.00986030176281929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.035741806030273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05519489943981171,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09085140625635783,
      "backward_entropy": 0.015596367418766022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.16778564453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05524773150682449,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09071167310078938,
      "backward_entropy": 0.009860316663980484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.805025100708008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05529901757836342,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09057692686716716,
      "backward_entropy": 0.00985841006040573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.695284843444824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055351622402668,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09043691555658977,
      "backward_entropy": 0.009859035164117813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.282472610473633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05540153756737709,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09030520915985107,
      "backward_entropy": 0.009860506653785706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.778223037719727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055451519787311554,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0901729663213094,
      "backward_entropy": 0.009862013906240464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.107284545898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055504146963357925,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09003146489461263,
      "backward_entropy": 0.015339583158493042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.023696899414062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05555659160017967,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08989010254542033,
      "backward_entropy": 0.009869106113910675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.935710906982422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05560878664255142,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08974963426589966,
      "backward_entropy": 0.009871897846460342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.850370407104492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055660754442214966,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08960947394371033,
      "backward_entropy": 0.00987398698925972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.757978439331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055712468922138214,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08947062492370605,
      "backward_entropy": 0.0098746657371521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.665756225585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05576402321457863,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0893319050470988,
      "backward_entropy": 0.009875352680683135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.729158401489258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055815499275922775,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08919264872868855,
      "backward_entropy": 0.009877631068229675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.383660316467285,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055867958813905716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08905065059661865,
      "backward_entropy": 0.009877864271402359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.413021087646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05591883510351181,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08891393740971883,
      "backward_entropy": 0.009876631200313568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.319128036499023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05596952885389328,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08877789974212646,
      "backward_entropy": 0.009874090552330017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.189505577087402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05602017045021057,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08864148457845052,
      "backward_entropy": 0.009872763603925704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.099176406860352,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05606939643621445,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08850983778635661,
      "backward_entropy": 0.009870177507400513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.093801498413086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05611606314778328,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08838719129562378,
      "backward_entropy": 0.009865720570087434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.976943969726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056165456771850586,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08825500806172688,
      "backward_entropy": 0.009861542284488678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.90489387512207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05621501803398132,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08812110622723897,
      "backward_entropy": 0.009861380606889725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.702917098999023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05626453086733818,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08798730373382568,
      "backward_entropy": 0.009860484302043915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.72658348083496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05631661042571068,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08784409364064534,
      "backward_entropy": 0.009862537682056426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.940166473388672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056368451565504074,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08770094315210979,
      "backward_entropy": 0.009866124391555786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.806639671325684,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056416261941194534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08757187922795613,
      "backward_entropy": 0.009868209809064865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.208343505859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05646158754825592,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0874521533648173,
      "backward_entropy": 0.00986710786819458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.399185180664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05650969594717026,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.087322731812795,
      "backward_entropy": 0.009865941107273101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.859597206115723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056557897478342056,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08719252546628316,
      "backward_entropy": 0.009865648299455642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.63170337677002,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05660242214798927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08707527319590251,
      "backward_entropy": 0.009864123165607452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.605599403381348,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056645020842552185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08696389198303223,
      "backward_entropy": 0.009864873439073562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.567995071411133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05668571963906288,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08685922622680664,
      "backward_entropy": 0.009864265471696854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.28079891204834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05672474950551987,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08676019310951233,
      "backward_entropy": 0.009863343834877015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.427719116210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056763529777526855,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08666165669759114,
      "backward_entropy": 0.009863287955522538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.892948150634766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05680573359131813,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08655083179473877,
      "backward_entropy": 0.009863775968551636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.7295308113098145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056848540902137756,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08643769224484761,
      "backward_entropy": 0.009863968938589096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.388436317443848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056888286024332047,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08633516232172649,
      "backward_entropy": 0.009864770621061326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.017619132995605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056926529854536057,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08623736103375752,
      "backward_entropy": 0.009867161512374878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.965883255004883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056964628398418427,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0861394206682841,
      "backward_entropy": 0.009870964288711547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.548816680908203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057002633810043335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08604137102762859,
      "backward_entropy": 0.009876997768878936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.089109420776367,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0570417195558548,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.08593897024790446,
      "backward_entropy": 0.13861987590789795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.81328010559082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05708305910229683,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0858277678489685,
      "backward_entropy": 0.009894266724586487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.188672065734863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05712396651506424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08571761846542358,
      "backward_entropy": 0.009905575215816498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.163084983825684,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05716318637132645,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08561338980992635,
      "backward_entropy": 0.013804009556770325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.5837860107421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05720075964927673,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08551568786303203,
      "backward_entropy": 0.009923719614744187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.21219825744629,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057235684245824814,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08542793989181519,
      "backward_entropy": 0.00992894172668457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.579765319824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05727427080273628,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0853264331817627,
      "backward_entropy": 0.00993393436074257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.51126480102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057312462478876114,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0852271020412445,
      "backward_entropy": 0.009936820715665817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.94791030883789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057352859526872635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08511946598688762,
      "backward_entropy": 0.009941154718399048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.955307006835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05739399418234825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08500887950261433,
      "backward_entropy": 0.009946049749851226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.931922912597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05743339657783508,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08490439256032307,
      "backward_entropy": 0.009950968623161315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.326139450073242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05747107043862343,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0848068396250407,
      "backward_entropy": 0.01348666250705719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.27167797088623,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057508379220962524,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08471087614695232,
      "backward_entropy": 0.009950416535139084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.622737884521484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057545438408851624,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08461584647496541,
      "backward_entropy": 0.009947405010461808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.789510726928711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05758338049054146,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08451836307843526,
      "backward_entropy": 0.009942124038934708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.486379623413086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05761990323662758,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08442542950312297,
      "backward_entropy": 0.009938065707683564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.46681785583496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05765744298696518,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08432873090108235,
      "backward_entropy": 0.009933394193649293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.029447555541992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057699281722307205,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08421742916107178,
      "backward_entropy": 0.009924621134996415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.582883834838867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05774025246500969,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0841100811958313,
      "backward_entropy": 0.009912127256393432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.19118881225586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05778297036886215,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0839959979057312,
      "backward_entropy": 0.013036677241325378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.108020782470703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0578262023627758,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08387937148412068,
      "backward_entropy": 0.009892474114894866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.0432071685791,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05787001922726631,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0837593674659729,
      "backward_entropy": 0.009890136122703553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.198606491088867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05791414529085159,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0836380124092102,
      "backward_entropy": 0.009888307005167008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.891220092773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057959720492362976,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0835111935933431,
      "backward_entropy": 0.00988713875412941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.1897029876709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05800533667206764,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08338465293248494,
      "backward_entropy": 0.00988556295633316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.72052574157715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05805344507098198,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08324869970480601,
      "backward_entropy": 0.009885657578706741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.486281394958496,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05810144916176796,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08311278621355693,
      "backward_entropy": 0.01270523965358734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.8164005279541,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05814818665385246,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08298120896021526,
      "backward_entropy": 0.009889934211969376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.148510932922363,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0581972636282444,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0828409343957901,
      "backward_entropy": 0.009893590956926346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.307584762573242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058242540806531906,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08271468182404836,
      "backward_entropy": 0.012587006390094756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.175054550170898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05828671529889107,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0825926810503006,
      "backward_entropy": 0.00989832803606987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.244548797607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058328814804553986,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0824777881304423,
      "backward_entropy": 0.009901661425828934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.144904136657715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05837131664156914,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08236156900723775,
      "backward_entropy": 0.009905493259429932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.11818504333496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058412887156009674,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0822492241859436,
      "backward_entropy": 0.009906485676765442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.031357765197754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05845455452799797,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08213795224825542,
      "backward_entropy": 0.009900027513504028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.883865356445312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058494340628385544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08203325668970744,
      "backward_entropy": 0.009894657135009765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.906173706054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058538347482681274,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08191181222597758,
      "backward_entropy": 0.009894153475761414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.854765892028809,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058581601828336716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08179207146167755,
      "backward_entropy": 0.009899057447910309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.650667190551758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058624088764190674,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08167451620101929,
      "backward_entropy": 0.009906770288944244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.555593490600586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058668073266744614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08155104517936707,
      "backward_entropy": 0.009915261715650558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.809715270996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0587133914232254,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08142252266407013,
      "backward_entropy": 0.009924118965864181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.504144668579102,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05875639617443085,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08130282163619995,
      "backward_entropy": 0.009931136667728425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.582402229309082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058799587190151215,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08118267854054768,
      "backward_entropy": 0.009936152398586274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.364108085632324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05884179845452309,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08106632033983867,
      "backward_entropy": 0.009939526021480561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.274714469909668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05888412520289421,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08095037937164307,
      "backward_entropy": 0.009938444942235947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.218935012817383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05892680957913399,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08083248138427734,
      "backward_entropy": 0.009939677268266677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.139128684997559,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05896950140595436,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08071552713712056,
      "backward_entropy": 0.009935851395130157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.537463665008545,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0590122826397419,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08059867719809215,
      "backward_entropy": 0.00992957353591919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.517359733581543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059053197503089905,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0804877628882726,
      "backward_entropy": 0.009928467869758605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.922285079956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05909216031432152,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08038429419199626,
      "backward_entropy": 0.009925799071788787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.545351028442383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05913161113858223,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08027944962183635,
      "backward_entropy": 0.009922011196613312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.77644157409668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05917279049754143,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08016701539357503,
      "backward_entropy": 0.009921005368232727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.025296211242676,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059214312583208084,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08005436261494954,
      "backward_entropy": 0.009920742362737656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.3363261222839355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059255242347717285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07994108398755391,
      "backward_entropy": 0.009925898164510727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.815998077392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05929429456591606,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07983554899692535,
      "backward_entropy": 0.009930641949176788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.493346214294434,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05933632329106331,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07971779505411784,
      "backward_entropy": 0.009940192848443986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.820108413696289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059378575533628464,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07959901293118794,
      "backward_entropy": 0.011490240693092346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.910640716552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05941999703645706,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07948316633701324,
      "backward_entropy": 0.009959053248167038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.719639778137207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059463053941726685,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07935993870099385,
      "backward_entropy": 0.009974399209022522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.662997245788574,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05950504541397095,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07924119631449382,
      "backward_entropy": 0.011415614932775497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5554444789886475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05954611301422119,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07912619908650716,
      "backward_entropy": 0.009997473657131195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.0487589836120605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05958426743745804,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07902169227600098,
      "backward_entropy": 0.01000976637005806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.97322654724121,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059620849788188934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07892287770907085,
      "backward_entropy": 0.010021904110908508,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.406143188476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05966053530573845,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07881136735280354,
      "backward_entropy": 0.010037226974964142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.864664077758789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059701770544052124,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07869382699330647,
      "backward_entropy": 0.01005219593644142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.232662200927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0597432442009449,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07857560118039449,
      "backward_entropy": 0.010065601766109466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.150724411010742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05978601053357124,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07845254739125569,
      "backward_entropy": 0.011248464882373809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.237847328186035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05982978641986847,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07832662264506023,
      "backward_entropy": 0.010084561258554458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4326648712158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05987251549959183,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0782041847705841,
      "backward_entropy": 0.010092900693416595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.88433265686035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05991188436746597,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0780954658985138,
      "backward_entropy": 0.01009683832526207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.03670820966362953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0599525161087513,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07798283298810323,
      "backward_entropy": 0.010095607489347458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.702611446380615,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059989094734191895,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0778854489326477,
      "backward_entropy": 0.01009538397192955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.6755781173706055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060024186968803406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07779327034950256,
      "backward_entropy": 0.010095669329166413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.262764930725098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06005788967013359,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0777060588200887,
      "backward_entropy": 0.010095585137605667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.47861671447754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06009235978126526,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07761677106221516,
      "backward_entropy": 0.010091845691204072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.13358211517334,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060128774493932724,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07751982907454173,
      "backward_entropy": 0.010089574754238129,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.567127227783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06016576662659645,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07742064197858174,
      "backward_entropy": 0.01008603498339653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.001014709472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060205649584531784,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07730934023857117,
      "backward_entropy": 0.010087086260318756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.50030517578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06024575233459473,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07719786961873372,
      "backward_entropy": 0.010086599737405777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.0815372467041,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06028371676802635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07709535459677379,
      "backward_entropy": 0.010080341994762421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.552946090698242,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06032300367951393,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07698862254619598,
      "backward_entropy": 0.13862053155899048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2037487030029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06036679074168205,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07686494787534077,
      "backward_entropy": 0.010639317333698273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.63688850402832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06040734425187111,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07675303022066753,
      "backward_entropy": 0.010054738819599151,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.572912216186523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06044825538992882,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07663884262243907,
      "backward_entropy": 0.010054262727499009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.277897357940674,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06048938259482384,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07652338842550914,
      "backward_entropy": 0.010056214779615403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1442673206329346,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06052844598889351,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07641612986723582,
      "backward_entropy": 0.010057243704795837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.292222023010254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060564592480659485,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07632029056549072,
      "backward_entropy": 0.010057196021080017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.512218475341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06060031056404114,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0762254148721695,
      "backward_entropy": 0.010058379173278809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.205414772033691,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060640014708042145,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07611437638600667,
      "backward_entropy": 0.010065342485904693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.1497163772583,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06067877635359764,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07600764433542888,
      "backward_entropy": 0.01006939560174942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.098099708557129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06071677803993225,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07590387761592865,
      "backward_entropy": 0.010073219239711762,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.04731273651123,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06075413525104523,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07580231130123138,
      "backward_entropy": 0.010077953338623047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.003764152526855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060790956020355225,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07570241888364156,
      "backward_entropy": 0.010084418207406997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.018012762069702,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060827236622571945,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07560442884763081,
      "backward_entropy": 0.010091161727905274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.79488182067871,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06086084991693497,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07551713784535725,
      "backward_entropy": 0.01009594276547432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.765365600585938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060897279530763626,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0754190484682719,
      "backward_entropy": 0.010099875926971435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.526552200317383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06093510240316391,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07531628012657166,
      "backward_entropy": 0.010100528597831726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9584639072418213,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06097642332315445,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07519970337549846,
      "backward_entropy": 0.01010325700044632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.826074123382568,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06101451441645622,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0750962495803833,
      "backward_entropy": 0.010103566944599152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.670184135437012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06105086952447891,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07499954601128896,
      "backward_entropy": 0.010105299949645995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.207242965698242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061086732894182205,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07490399479866028,
      "backward_entropy": 0.010109291225671769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.28662109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06112530454993248,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0747972031434377,
      "backward_entropy": 0.010117442905902862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8727099895477295,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061164937913417816,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07468742628892262,
      "backward_entropy": 0.01012050062417984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.671909809112549,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06120159104466438,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07458896438280742,
      "backward_entropy": 0.010123015940189361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.43884563446045,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06123657524585724,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0744972825050354,
      "backward_entropy": 0.13861889839172364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.952126502990723,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06127111613750458,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07440717021624248,
      "backward_entropy": 0.010127240419387817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.879351615905762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06130747124552727,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07430878281593323,
      "backward_entropy": 0.010134939104318619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.052948951721191,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06134539842605591,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07420354584852855,
      "backward_entropy": 0.010146091878414153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7958736419677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06138361990451813,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07409701744715373,
      "backward_entropy": 0.010157816857099534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.363235473632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06141877919435501,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07400368650754292,
      "backward_entropy": 0.01016242876648903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.462843894958496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06145669147372246,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07389854888121287,
      "backward_entropy": 0.010173040628433227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.432991027832031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061492711305618286,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0738012542327245,
      "backward_entropy": 0.010180676728487015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.075447082519531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06152702122926712,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07371107240517934,
      "backward_entropy": 0.010185518860816955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.372525215148926,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06156088411808014,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07362261414527893,
      "backward_entropy": 0.010190579295158386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.643190383911133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06159329041838646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07353986303011577,
      "backward_entropy": 0.010194190591573716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.87637996673584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06162641942501068,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07345418632030487,
      "backward_entropy": 0.010197684168815613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.280616283416748,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0616619698703289,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07336039344469707,
      "backward_entropy": 0.010196404159069061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.097402572631836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06169598549604416,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07327209909756978,
      "backward_entropy": 0.010196766257286072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.008451461791992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06173127517104149,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07318017880121867,
      "backward_entropy": 0.01019083634018898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.20629358291626,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061767905950546265,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07308329145113628,
      "backward_entropy": 0.010184422880411149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6176085472106934,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061802685260772705,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07299406329790752,
      "backward_entropy": 0.010175284743309022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.05082035809755325,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06183476373553276,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07291598121325175,
      "backward_entropy": 0.010162441432476044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.124629020690918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0618634857237339,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07285116116205852,
      "backward_entropy": 0.01014786958694458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.612769603729248,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06189114972949028,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07279046376546223,
      "backward_entropy": 0.010132130235433578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.611387252807617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06191904470324516,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07272835075855255,
      "backward_entropy": 0.010120826959609985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.041491985321045,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0619489960372448,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07265846927960713,
      "backward_entropy": 0.010111027210950852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.026484489440918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06197793036699295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07259233295917511,
      "backward_entropy": 0.010103961080312729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.4739251136779785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062005795538425446,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07253018021583557,
      "backward_entropy": 0.010095569491386413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.826485633850098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062033772468566895,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07246653735637665,
      "backward_entropy": 0.010089175403118133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4850728511810303,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06206490099430084,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0723906656106313,
      "backward_entropy": 0.010087747126817703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.360257625579834,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062094029039144516,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07232166330019633,
      "backward_entropy": 0.01009092703461647,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.8953657150268555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06212318688631058,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07225227852662404,
      "backward_entropy": 0.010095805674791337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.706178665161133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06215139850974083,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07218630115191142,
      "backward_entropy": 0.010102147608995438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.657960891723633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06218067929148674,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07211602727572124,
      "backward_entropy": 0.010109853744506837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.203693866729736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06221091374754906,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0720419039328893,
      "backward_entropy": 0.010118478536605835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.795159816741943,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06224123761057854,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07196620603402455,
      "backward_entropy": 0.010133087635040283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.517335891723633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06227052956819534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07189419865608215,
      "backward_entropy": 0.010149110108613968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.099795341491699,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06230064108967781,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07181950906912486,
      "backward_entropy": 0.010162454098463058,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.075566291809082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06233072653412819,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07174429297447205,
      "backward_entropy": 0.010178083926439286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.028539657592773,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06236058101058006,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07167046268781026,
      "backward_entropy": 0.010190687328577041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.321365356445312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06239315867424011,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07158620158831279,
      "backward_entropy": 0.010203015059232712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3456954956054688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0624261349439621,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0715011755625407,
      "backward_entropy": 0.010211315751075745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.195205688476562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06245671957731247,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07142540315786998,
      "backward_entropy": 0.010218676924705506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.584120750427246,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06248822808265686,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07134497165679932,
      "backward_entropy": 0.1386163353919983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2983596324920654,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06251867115497589,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07126749555269878,
      "backward_entropy": 0.010249333083629608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.549899578094482,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06254711002111435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07119721174240112,
      "backward_entropy": 0.010268492996692658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.747721672058105,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06257457286119461,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07113075256347656,
      "backward_entropy": 0.008556100726127624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.492411136627197,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06260592490434647,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07104914387067159,
      "backward_entropy": 0.010305118560791016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.930730819702148,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06263613700866699,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07097109158833821,
      "backward_entropy": 0.010326194763183593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2314722537994385,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0626668855547905,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07089211046695709,
      "backward_entropy": 0.010340982675552368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.623228073120117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06269565224647522,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07081965605417888,
      "backward_entropy": 0.010359425842761994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.400956630706787,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06272878497838974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0707318385442098,
      "backward_entropy": 0.01037096232175827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.884318351745605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06276046484708786,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0706491470336914,
      "backward_entropy": 0.010381630063056946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.986465454101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06279365718364716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07056086262067159,
      "backward_entropy": 0.010393130034208298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.920371055603027,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06282896548509598,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07046489417552948,
      "backward_entropy": 0.010400737822055816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1589112281799316,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06286591291427612,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0703648030757904,
      "backward_entropy": 0.008371435105800629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.263493061065674,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0629001334309578,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07027452190717061,
      "backward_entropy": 0.010398948937654496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.541357040405273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06293275952339172,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07018989324569702,
      "backward_entropy": 0.010400050133466721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.480841636657715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06296668201684952,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07010023792584737,
      "backward_entropy": 0.010401822626590729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.263279438018799,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06300166249275208,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07000708083311717,
      "backward_entropy": 0.00824732482433319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.237391948699951,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06303577125072479,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06991772850354512,
      "backward_entropy": 0.010398057103157044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.228355407714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06306888163089752,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06983351707458496,
      "backward_entropy": 0.01038784086704254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.232938766479492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06310219317674637,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06974875926971436,
      "backward_entropy": 0.010376548767089844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.117292404174805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06313759088516235,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06965598464012146,
      "backward_entropy": 0.01036681979894638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.041922569274902,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06317295879125595,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0695637712876002,
      "backward_entropy": 0.010356360673904419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.007189750671387,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06320854276418686,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0694693773984909,
      "backward_entropy": 0.010352535545825959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.957789421081543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06324400752782822,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06937592228253682,
      "backward_entropy": 0.010346156358718873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.955432891845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06327925622463226,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06928473214308421,
      "backward_entropy": 0.010334514081478119,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.836547374725342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0633128359913826,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06919917464256287,
      "backward_entropy": 0.01032702550292015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.57367992401123,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06334652751684189,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06911325951417287,
      "backward_entropy": 0.010319143533706665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.05701448395848274,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06338316202163696,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06901599963506062,
      "backward_entropy": 0.010316903889179229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.765201568603516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06341585516929626,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06893471876780193,
      "backward_entropy": 0.01030862182378769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.726963520050049,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06344784796237946,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06885657707850139,
      "backward_entropy": 0.010300557315349578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.326323509216309,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06347920745611191,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06878074010213216,
      "backward_entropy": 0.007712408900260925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.391587257385254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06351276487112045,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06869613130887349,
      "backward_entropy": 0.010289955884218216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.605590343475342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06354720890522003,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06860883037249248,
      "backward_entropy": 0.010285118967294693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.565945148468018,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06358081847429276,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06852457920710246,
      "backward_entropy": 0.01028164178133011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.055919237434864044,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.063613660633564,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06844309469064076,
      "backward_entropy": 0.010279491543769836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.767723083496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0636429637670517,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.068376158674558,
      "backward_entropy": 0.010271476954221726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.243002414703369,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06367530673742294,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06829796731472015,
      "backward_entropy": 0.010263761878013611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6334903240203857,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0637081190943718,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0682166616121928,
      "backward_entropy": 0.010265375673770904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.3862385749816895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06373921781778336,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06814221541086833,
      "backward_entropy": 0.01026386022567749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.340707302093506,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06376967579126358,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0680708388487498,
      "backward_entropy": 0.010261435806751252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.573237419128418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06379970908164978,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0680006096760432,
      "backward_entropy": 0.010262517631053925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.754136085510254,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06383194774389267,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.06792166829109192,
      "backward_entropy": 0.007340686768293381,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.494999408721924,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0638653114438057,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06783795356750488,
      "backward_entropy": 0.010277339071035386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4673397541046143,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06389714032411575,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06775941451390584,
      "backward_entropy": 0.01028946042060852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4508302211761475,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06392763555049896,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06768515209356944,
      "backward_entropy": 0.010305607318878173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.115488529205322,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06395681202411652,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0676155686378479,
      "backward_entropy": 0.01032249629497528,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.7791829109191895,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06398578733205795,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0675458014011383,
      "backward_entropy": 0.010344034433364869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.435364723205566,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06401523947715759,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06747429569562276,
      "backward_entropy": 0.010365131497383117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.0189900398254395,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06404844671487808,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06738905608654022,
      "backward_entropy": 0.0103858582675457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.0009989738464355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06408090144395828,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06730639934539795,
      "backward_entropy": 0.010407189279794693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.593235492706299,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06411240249872208,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06722840170065562,
      "backward_entropy": 0.010420915484428406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.680667757987976,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06414401531219482,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06715014080206554,
      "backward_entropy": 0.010431866347789764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.502028465270996,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06417302787303925,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06708269317944844,
      "backward_entropy": 0.010433848202228545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6434499025344849,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06420237571001053,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06701411803563435,
      "backward_entropy": 0.010434237122535706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.829610347747803,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0642295554280281,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06695335110028584,
      "backward_entropy": 0.010433722287416458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2146036624908447,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06425631791353226,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06689475973447163,
      "backward_entropy": 0.010429177433252335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.3371381759643555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0642818808555603,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06684093674023946,
      "backward_entropy": 0.010421191155910493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.735996246337891,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06430809944868088,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06678465008735657,
      "backward_entropy": 0.010413137078285218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.702780246734619,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06433404237031937,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06672959526379903,
      "backward_entropy": 0.010403773933649062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.666506767272949,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06435979157686234,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06667502721150716,
      "backward_entropy": 0.010395418107509612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.7261128425598145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06438546627759933,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06661981344223022,
      "backward_entropy": 0.010391119122505187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.125924110412598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.064412422478199,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06656123201052348,
      "backward_entropy": 0.010382913053035736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5520565509796143,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06444008648395538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06649867693583171,
      "backward_entropy": 0.010381681472063064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.046408653259277,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06446576863527298,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06644327441851298,
      "backward_entropy": 0.01038111224770546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.53772497177124,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06449227035045624,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06638383865356445,
      "backward_entropy": 0.010386881232261658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.474668979644775,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0645182877779007,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06632739802201588,
      "backward_entropy": 0.010386748611927033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.4648237228393555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06454556435346603,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0662669042746226,
      "backward_entropy": 0.010384311527013778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.904257297515869,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06457247585058212,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06620759765307109,
      "backward_entropy": 0.010382606089115143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.3946757316589355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06459979712963104,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06614707410335541,
      "backward_entropy": 0.01037999764084816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.696917533874512,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06462690234184265,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06608650088310242,
      "backward_entropy": 0.010382741689682007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9099807739257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06465626507997513,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06601652503013611,
      "backward_entropy": 0.010392958670854569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.305236339569092,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.064684197306633,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06595222651958466,
      "backward_entropy": 0.0104009248316288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.298055171966553,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06471176445484161,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06588880221048991,
      "backward_entropy": 0.010411358624696731,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.654477119445801,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06473863869905472,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.065829336643219,
      "backward_entropy": 0.010413611680269242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.016725063323975,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06476597487926483,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.06576795876026154,
      "backward_entropy": 0.010417089611291886,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 6.71804355237633,
    "avg_log_Z": -0.063277445435524,
    "success_rate": 1.0,
    "avg_reward": 85.1,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.01,
      "1": 0.05,
      "2": 0.94
    },
    "avg_forward_entropy": 0.06934736505150796,
    "avg_backward_entropy": 0.011470122389495374,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}