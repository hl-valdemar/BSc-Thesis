{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 0.47368421052631576
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 0.7368421052631579
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07666370603773329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07666370603773329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07666370603773329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07666370603773329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07666370603773329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07620025343365139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.02638244628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10945868492126465,
      "backward_entropy": 0.07634397347768147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.0337371826172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -9.99999901978299e-05,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10946829319000244,
      "backward_entropy": 0.07621839311387804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.52816772460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00020013470202684402,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10947777032852173,
      "backward_entropy": 0.07635745737287733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.39511108398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00030037632677704096,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10948708057403564,
      "backward_entropy": 0.07636417282952203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.43833923339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0003996920131612569,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1094961404800415,
      "backward_entropy": 0.07637067635854085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.81605529785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0004994581104256213,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10950499773025513,
      "backward_entropy": 0.07628829611672296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.81040954589844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0005990412319079041,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10951372385025024,
      "backward_entropy": 0.07669499185350206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.70220947265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0006989601533859968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10952234268188477,
      "backward_entropy": 0.07669991917080349,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.20599365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0007980524096637964,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10953080654144287,
      "backward_entropy": 0.07639655801984999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.6540985107422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0008965894812718034,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953915119171143,
      "backward_entropy": 0.07635362943013509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.1083526611328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.000995247857645154,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10954734086990356,
      "backward_entropy": 0.07671467463175456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.47007751464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0010927740950137377,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10955536365509033,
      "backward_entropy": 0.07638474967744616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.51675415039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001189908478409052,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10956332683563233,
      "backward_entropy": 0.07672468158933851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.937744140625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0012868582271039486,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10957115888595581,
      "backward_entropy": 0.07641471094555324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.37086486816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013836066937074065,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10957884788513184,
      "backward_entropy": 0.07643197642432319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.76898193359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0014807895058766007,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10958645343780518,
      "backward_entropy": 0.07643791039784749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.8271026611328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.001576888607814908,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10959395170211791,
      "backward_entropy": 0.07645779185824925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.3293914794922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0016729477792978287,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10960133075714111,
      "backward_entropy": 0.07647161351309882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.75257873535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001768319052644074,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10960853099822998,
      "backward_entropy": 0.07675254344940186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.71597290039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018630428239703178,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10961557626724243,
      "backward_entropy": 0.07646067937215169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.68954467773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001958672422915697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10962257385253907,
      "backward_entropy": 0.07676117950015598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.6448211669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002052819123491645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1096293568611145,
      "backward_entropy": 0.07647158039940728,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.5202178955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002147274324670434,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10963605642318726,
      "backward_entropy": 0.07647734218173557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.04017639160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0022418724838644266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10964272022247315,
      "backward_entropy": 0.07648324966430664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.095458984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0023358925245702267,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10964927673339844,
      "backward_entropy": 0.07656068272060818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.51699829101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0024295684415847063,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10965566635131836,
      "backward_entropy": 0.07678132587009007,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.4413604736328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002522882539778948,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10966194868087768,
      "backward_entropy": 0.07650001843770345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.9514617919922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0026174583472311497,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966817140579224,
      "backward_entropy": 0.07659474346372816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.43070983886719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002709742169827223,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967419147491456,
      "backward_entropy": 0.07651054196887547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.20425415039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0027979486621916294,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1096799612045288,
      "backward_entropy": 0.07651518450842963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.31805419921875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0028887679800391197,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10968575477600098,
      "backward_entropy": 0.0766246583726671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.73553466796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0029804371297359467,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969146490097045,
      "backward_entropy": 0.07663439379798041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.66127014160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0030727919656783342,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10969712734222412,
      "backward_entropy": 0.07680606842041016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.155029296875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00316657405346632,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970275402069092,
      "backward_entropy": 0.07665359973907471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.12022399902344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003259873017668724,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10970828533172608,
      "backward_entropy": 0.07666299078199598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.630126953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0033527561463415623,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971369743347167,
      "backward_entropy": 0.07681600915061103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 244.45269775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0034433482214808464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971893072128296,
      "backward_entropy": 0.07681905561023289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.51246643066406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003536480013281107,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10972414016723633,
      "backward_entropy": 0.07668992545869616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.0897979736328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003630187129601836,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10972923040390015,
      "backward_entropy": 0.07655834489398533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.45558166503906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0037215871270745993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10973411798477173,
      "backward_entropy": 0.07682821485731336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.87522888183594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003811805509030819,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10973886251449586,
      "backward_entropy": 0.0767153369055854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.27313232421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0039008953608572483,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974348783493042,
      "backward_entropy": 0.0768337713347541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 228.61383056640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003993803169578314,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10974807739257812,
      "backward_entropy": 0.0767315493689643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.87550354003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0040880064480006695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097525954246521,
      "backward_entropy": 0.07683956623077393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.6291046142578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0041808015666902065,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975693464279175,
      "backward_entropy": 0.07674754328197902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.186279296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004275091923773289,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976119041442871,
      "backward_entropy": 0.07658935917748345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 259.1416931152344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004368825815618038,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10976531505584716,
      "backward_entropy": 0.0767628616756863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.97396850585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004466025624424219,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976932048797608,
      "backward_entropy": 0.07659819391038683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.1775360107422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004565127193927765,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977323055267334,
      "backward_entropy": 0.0767777231004503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 211.85235595703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004663269035518169,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10977694988250733,
      "backward_entropy": 0.07660704188876682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.92718505859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004761143587529659,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10978059768676758,
      "backward_entropy": 0.07685895760854085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.41465759277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004860000219196081,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978409051895141,
      "backward_entropy": 0.07661560508939955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.29933166503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0049576666206121445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10978745222091675,
      "backward_entropy": 0.07661975092358059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.43801879882812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005057295318692923,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10979068279266357,
      "backward_entropy": 0.07681253221299914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.40731811523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005154683254659176,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10979374647140502,
      "backward_entropy": 0.07686907715267605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.92327880859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005251201335340738,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10979663133621216,
      "backward_entropy": 0.07682532734341091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.67372131347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005347033962607384,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10979936122894288,
      "backward_entropy": 0.07663576470481025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.3016815185547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005444117356091738,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980194807052612,
      "backward_entropy": 0.07683765225940281,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.25535583496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005539224948734045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10980439186096191,
      "backward_entropy": 0.07687809069951375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 293.1923828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005634841043502092,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980664491653443,
      "backward_entropy": 0.07664667235480414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.7146301269531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005735364742577076,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980876684188842,
      "backward_entropy": 0.07665044731563991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.40850830078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005838253069669008,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10981078147888183,
      "backward_entropy": 0.07686123583051893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.1715393066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005936502479016781,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10981261730194092,
      "backward_entropy": 0.07665798399183485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.2504425048828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0060374541208148,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10981433391571045,
      "backward_entropy": 0.07687234878540039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.5961456298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0061399019323289394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10981589555740356,
      "backward_entropy": 0.07666574584113227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.24813842773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006240923888981342,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981727838516235,
      "backward_entropy": 0.07689330312940809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.0783233642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006339797284454107,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981848239898681,
      "backward_entropy": 0.07689528995090061,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.87742614746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006437716074287891,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10981950759887696,
      "backward_entropy": 0.0766762031449212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 227.0542449951172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006532910745590925,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10982034206390381,
      "backward_entropy": 0.07689770062764485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.82615661621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006629203446209431,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10982105731964112,
      "backward_entropy": 0.07690029011832343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.00637817382812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006725786253809929,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109821617603302,
      "backward_entropy": 0.07690681351555718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.78651428222656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.006819320842623711,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10982203483581543,
      "backward_entropy": 0.0769110123316447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.30738830566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00691231107339263,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10982226133346558,
      "backward_entropy": 0.07690433661142985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.91429138183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007004911545664072,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10982229709625244,
      "backward_entropy": 0.07690558168623182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.79898071289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007100222632288933,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10982213020324708,
      "backward_entropy": 0.07692297299702962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.87083435058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00719665689393878,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1098217248916626,
      "backward_entropy": 0.07669837607277764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.76824951171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007294262293726206,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10982108116149902,
      "backward_entropy": 0.07670138941870795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.1730499267578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007389897480607033,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10982025861740112,
      "backward_entropy": 0.0769343376159668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.42990112304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007487828843295574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981919765472412,
      "backward_entropy": 0.07691223091549343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.4052734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0075828274711966515,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10981788635253906,
      "backward_entropy": 0.0769412914911906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.1800537109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007679946720600128,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981640815734864,
      "backward_entropy": 0.07691464159223768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.28671264648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0077798753045499325,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981473922729493,
      "backward_entropy": 0.07691589991251628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.5185546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007880838587880135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981286764144897,
      "backward_entropy": 0.07691715823279487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.07859802246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007984073832631111,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10981080532073975,
      "backward_entropy": 0.07691842317581177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.6176300048828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008087788708508015,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10980852842330932,
      "backward_entropy": 0.07695653703477648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.06202697753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008189564570784569,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980613231658935,
      "backward_entropy": 0.07672562864091662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 243.50453186035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008290846832096577,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1098035454750061,
      "backward_entropy": 0.07696198092566596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 260.69329833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008393424563109875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10980076789855957,
      "backward_entropy": 0.07673056920369466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.74252319335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008497991599142551,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097977876663208,
      "backward_entropy": 0.07692405250337389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 291.9716491699219,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00859831552952528,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097947120666504,
      "backward_entropy": 0.07696959045198229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.66357421875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008703094907104969,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10979135036468506,
      "backward_entropy": 0.07697200112872654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.26553344726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008809227496385574,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1097877025604248,
      "backward_entropy": 0.076974352200826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.4084014892578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008915534242987633,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10978385210037231,
      "backward_entropy": 0.07697660393185085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 193.2441864013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009019725024700165,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1097798466682434,
      "backward_entropy": 0.07692884074317084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.21319580078125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009121840819716454,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10977569818496705,
      "backward_entropy": 0.07698073652055529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.7378387451172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009225253015756607,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10977131128311157,
      "backward_entropy": 0.07693034410476685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.77894592285156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00932483933866024,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10976681709289551,
      "backward_entropy": 0.07698407438066271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.05796813964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00942495558410883,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10976209640502929,
      "backward_entropy": 0.07675235138999091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 226.11903381347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009526582434773445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10975714921951293,
      "backward_entropy": 0.0769322779443529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.31044006347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009628423489630222,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10975196361541747,
      "backward_entropy": 0.0769886506928338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 274.9725036621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009729483164846897,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10974658727645874,
      "backward_entropy": 0.07675800058576795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.53321838378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00983389001339674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10974087715148925,
      "backward_entropy": 0.07693437072965834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.47901916503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009939560666680336,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10973488092422486,
      "backward_entropy": 0.07676194773779975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.6056213378906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010045060887932777,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1097286581993103,
      "backward_entropy": 0.0767638882001241,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.9746551513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010152534581720829,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10972211360931397,
      "backward_entropy": 0.07693669531080458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 256.6153564453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010255794040858746,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10971550941467285,
      "backward_entropy": 0.07693737083011204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.20884704589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010361425578594208,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10970853567123413,
      "backward_entropy": 0.07676928573184544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.46527099609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010462741367518902,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10970152616500854,
      "backward_entropy": 0.07693866888682048,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.40777587890625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010561276227235794,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10969438552856445,
      "backward_entropy": 0.07700047228071424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 242.27613830566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010658674873411655,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10968708992004395,
      "backward_entropy": 0.076773292488522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.4755859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010757636278867722,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1096794605255127,
      "backward_entropy": 0.07700239949756199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.28434753417969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01085631176829338,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10967159271240234,
      "backward_entropy": 0.07677563031514485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 241.67538452148438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010949580930173397,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10966376066207886,
      "backward_entropy": 0.07700410154130724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 273.2978515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011044949293136597,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10965557098388672,
      "backward_entropy": 0.07677739196353489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.29295349121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011144429445266724,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10964691638946533,
      "backward_entropy": 0.07694125175476074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.40673828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011244265362620354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10963796377182007,
      "backward_entropy": 0.07694154315524632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.4871063232422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01134326308965683,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10962879657745361,
      "backward_entropy": 0.07678035232755873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.99838256835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01144096627831459,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10961943864822388,
      "backward_entropy": 0.0767812000380622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.43997192382812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011539042927324772,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10960978269577026,
      "backward_entropy": 0.07700825399822658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.4062042236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011634555645287037,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10960001945495605,
      "backward_entropy": 0.07678280936347114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.09878540039062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01173209398984909,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10958982706069946,
      "backward_entropy": 0.07694217231538561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.79811096191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011830377392470837,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10957931280136109,
      "backward_entropy": 0.07678430610232884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.31307983398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01192825473845005,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1095685362815857,
      "backward_entropy": 0.07678498162163629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 273.07696533203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012025859206914902,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10955747365951538,
      "backward_entropy": 0.07694220542907715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.2060852050781,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012127259746193886,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10954586267471314,
      "backward_entropy": 0.07701116138034397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.490234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012230903841555119,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10953376293182374,
      "backward_entropy": 0.07701157199011908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.82574462890625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012332657352089882,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.109521484375,
      "backward_entropy": 0.07701194948620266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.0432586669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012433351948857307,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10950896739959717,
      "backward_entropy": 0.07678826649983723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.63858032226562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012533452361822128,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10949616432189942,
      "backward_entropy": 0.0770126183827718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.3662109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012634002603590488,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10948320627212524,
      "backward_entropy": 0.07701291640599568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.94351196289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0127338832244277,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10946989059448242,
      "backward_entropy": 0.07694164911905925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.8950653076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012836340814828873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10945596694946289,
      "backward_entropy": 0.07694146368238661,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.07269287109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012937285006046295,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10944182872772217,
      "backward_entropy": 0.07679020033942328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 274.523681640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013035692274570465,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10942753553390502,
      "backward_entropy": 0.07679026656680638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.08917236328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01313735730946064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10941258668899537,
      "backward_entropy": 0.07694069544474284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.5499725341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013238215819001198,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10939738750457764,
      "backward_entropy": 0.07679059770372179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.64088439941406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013334316201508045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10938255786895752,
      "backward_entropy": 0.07679049173990886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.24456787109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013432074338197708,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10936720371246338,
      "backward_entropy": 0.07701459195878771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.90074157714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013530685566365719,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10935142040252685,
      "backward_entropy": 0.07701471779081556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.28016662597656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013625599443912506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10933563709259034,
      "backward_entropy": 0.07679000828001234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.6151123046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013717744499444962,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10931982994079589,
      "backward_entropy": 0.07678955131106907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.10581970214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013804769143462181,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10930420160293579,
      "backward_entropy": 0.07693698671129015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.56451416015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013891509734094143,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10928826332092285,
      "backward_entropy": 0.07693618535995483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.68736267089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01398218423128128,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10927157402038574,
      "backward_entropy": 0.07693541712231106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.6562957763672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014069967903196812,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10925483703613281,
      "backward_entropy": 0.07693454954359266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.05320739746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014156097546219826,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1092379331588745,
      "backward_entropy": 0.07701526085535686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.42259216308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01424316130578518,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10922057628631592,
      "backward_entropy": 0.07678532600402832,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.93896484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01433005090802908,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10920287370681762,
      "backward_entropy": 0.07678446504804823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.75128173828125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01441964041441679,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10918447971343995,
      "backward_entropy": 0.07701539331012303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.82647705078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014507777988910675,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10916591882705688,
      "backward_entropy": 0.0767827828725179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.44097900390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014596663415431976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10914688110351563,
      "backward_entropy": 0.07692853609720866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.6221160888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014689224772155285,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1091269850730896,
      "backward_entropy": 0.07692750957277086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.72178649902344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014779097400605679,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10910707712173462,
      "backward_entropy": 0.07692641682094997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 254.79261779785156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014872455038130283,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10908631086349488,
      "backward_entropy": 0.07692535056008233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.95201110839844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014969094656407833,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10906468629837036,
      "backward_entropy": 0.07701559861501057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.75148010253906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015065501444041729,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10904266834259033,
      "backward_entropy": 0.07701563172870213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.0905303955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015157601796090603,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10902087688446045,
      "backward_entropy": 0.07692237695058186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.7160186767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015246682800352573,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1089990496635437,
      "backward_entropy": 0.07677545150121053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.1819305419922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015337249264121056,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10897653102874756,
      "backward_entropy": 0.07677434550391303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.09283447265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015427207574248314,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10895365476608276,
      "backward_entropy": 0.07701573769251506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.72520446777344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015514557249844074,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10893075466156006,
      "backward_entropy": 0.07701576418346828,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.65061950683594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01560284849256277,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10890727043151856,
      "backward_entropy": 0.07677124606238471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.59716796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01568993367254734,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10888347625732422,
      "backward_entropy": 0.07677022616068523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 237.95323181152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01577594131231308,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10885941982269287,
      "backward_entropy": 0.07676910691791111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 237.0285186767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015864992514252663,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1088343620300293,
      "backward_entropy": 0.07676809363894993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.28106689453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015956943854689598,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10880835056304931,
      "backward_entropy": 0.07691127724117702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 270.6850280761719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01604713872075081,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10878190994262696,
      "backward_entropy": 0.07701590326097277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.90882873535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01614193059504032,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10875368118286133,
      "backward_entropy": 0.07690869437323676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.72621154785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016232213005423546,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1087255597114563,
      "backward_entropy": 0.07676413986417982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.0390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016322126612067223,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10869684219360351,
      "backward_entropy": 0.07690579361385769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.36248779296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016406575217843056,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10866822004318237,
      "backward_entropy": 0.07676179541481866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.98257446289062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016493134200572968,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10863765478134155,
      "backward_entropy": 0.07701598935657078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.45924377441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016580747440457344,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10860600471496581,
      "backward_entropy": 0.07690071397357517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.080078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016668280586600304,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.108573579788208,
      "backward_entropy": 0.07689903842078315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.29010009765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016755374148488045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10854038000106811,
      "backward_entropy": 0.07675691445668538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.6865692138672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016840176656842232,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10850781202316284,
      "backward_entropy": 0.07701606220669216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 236.0283203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016927359625697136,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10847405195236207,
      "backward_entropy": 0.07689363426632351,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.1172866821289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01701769419014454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10843892097473144,
      "backward_entropy": 0.0768918858634101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.56195068359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017103107646107674,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10840425491333008,
      "backward_entropy": 0.07701610194312201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.80215454101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01718812622129917,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10836888551712036,
      "backward_entropy": 0.0767495764626397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.63543701171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01727317087352276,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10833282470703125,
      "backward_entropy": 0.07688577307595147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.25869750976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01735985092818737,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10829555988311768,
      "backward_entropy": 0.07674638430277507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.61012268066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01744535192847252,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10825791358947753,
      "backward_entropy": 0.07674471537272136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 334.0946960449219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017529703676700592,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10821987390518188,
      "backward_entropy": 0.07687886555989583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.43101501464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017623448744416237,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10817885398864746,
      "backward_entropy": 0.07687666681077746,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.75437927246094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01771809533238411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10813682079315186,
      "backward_entropy": 0.07687437534332275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.95257568359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017813459038734436,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10809392929077148,
      "backward_entropy": 0.07687213023503621,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 237.07493591308594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01790560595691204,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1080512523651123,
      "backward_entropy": 0.0770161681705051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.16110229492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017999937757849693,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10800721645355224,
      "backward_entropy": 0.07673570844862196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.92771911621094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01809220388531685,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10796318054199219,
      "backward_entropy": 0.07701617479324341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.75962829589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018184876069426537,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10791839361190796,
      "backward_entropy": 0.07701618141598171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.2568359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018276602029800415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10787312984466553,
      "backward_entropy": 0.07686024241977268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 202.65567016601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018367575481534004,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10782732963562011,
      "backward_entropy": 0.07701617479324341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 234.77069091796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018459100276231766,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10778064727783203,
      "backward_entropy": 0.07672658893797132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 266.4376525878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01855326071381569,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10773247480392456,
      "backward_entropy": 0.07672464847564697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.26849365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018651964142918587,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10768224000930786,
      "backward_entropy": 0.07672281397713555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.5333251953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018745778128504753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10763269662857056,
      "backward_entropy": 0.07672075430552165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 266.8996276855469,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01883557252585888,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10758373737335206,
      "backward_entropy": 0.07701615492502849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 217.94618225097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018930191174149513,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10753247737884522,
      "backward_entropy": 0.07684160603417291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.2041015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019026082009077072,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10748002529144288,
      "backward_entropy": 0.07671408520804511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.03433227539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019120212644338608,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10742751359939576,
      "backward_entropy": 0.07683581776089138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.54103088378906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01921210251748562,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10737488269805909,
      "backward_entropy": 0.07701612843407525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.86639404296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019303401932120323,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1073216438293457,
      "backward_entropy": 0.07682949966854519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 234.07412719726562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019394079223275185,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10726807117462159,
      "backward_entropy": 0.07701610194312201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.52890014648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019487380981445312,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10721311569213868,
      "backward_entropy": 0.0768229431576199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.15574645996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019578341394662857,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1071580171585083,
      "backward_entropy": 0.07681939336988661,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 249.52891540527344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01967012882232666,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10710196495056153,
      "backward_entropy": 0.07701604896121556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.57666015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01976553350687027,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10704374313354492,
      "backward_entropy": 0.07701602909300062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.05125427246094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01985790580511093,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10698590278625489,
      "backward_entropy": 0.0770160092247857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.00845336914062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01995198242366314,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10692667961120605,
      "backward_entropy": 0.07680541939205593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.05897521972656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02004712074995041,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10686640739440918,
      "backward_entropy": 0.07701596948835585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 233.54217529296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020142557099461555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10680532455444336,
      "backward_entropy": 0.07679825358920628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.83880615234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020240025594830513,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10674266815185547,
      "backward_entropy": 0.07679463095135158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.47052001953125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020338386297225952,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10667891502380371,
      "backward_entropy": 0.07701594299740261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.0890884399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020435020327568054,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10661518573760986,
      "backward_entropy": 0.07678714063432482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.7098159790039,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020522983744740486,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1065546989440918,
      "backward_entropy": 0.07701590326097277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 230.87271118164062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02060605399310589,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10649538040161133,
      "backward_entropy": 0.07667062017652723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.73934936523438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02069290727376938,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10643376111984253,
      "backward_entropy": 0.07666722933451335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.04678344726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020780811086297035,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10637091398239136,
      "backward_entropy": 0.07666383849249946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.2445068359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02086719684302807,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1063078761100769,
      "backward_entropy": 0.07666032181845771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.6669921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020953459665179253,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1062440276145935,
      "backward_entropy": 0.07676148414611816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.8262176513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021036261692643166,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10618088245391846,
      "backward_entropy": 0.07665291097429064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.77427673339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021121490746736526,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10611578226089477,
      "backward_entropy": 0.07675222555796306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.85690307617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021206535398960114,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10604979991912841,
      "backward_entropy": 0.0766453742980957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.64422607421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021288150921463966,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10598452091217041,
      "backward_entropy": 0.07674243052800496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.54689025878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021371005102992058,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10591769218444824,
      "backward_entropy": 0.07673725816938612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.7832794189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021456489339470863,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10584886074066162,
      "backward_entropy": 0.07673223813374837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 231.57821655273438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02154167927801609,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10577917098999023,
      "backward_entropy": 0.07701537344190809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.8209991455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02163001336157322,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10570702552795411,
      "backward_entropy": 0.07662559880150689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.67054748535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02172003872692585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1056330919265747,
      "backward_entropy": 0.07671669456693861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.9306640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021810542792081833,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10555796623229981,
      "backward_entropy": 0.07671141624450684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.8297576904297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02189929224550724,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10548248291015624,
      "backward_entropy": 0.0770152144961887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.90968322753906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0219864659011364,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.1054067850112915,
      "backward_entropy": 0.07701518800523546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.22007751464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022073470056056976,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10533022880554199,
      "backward_entropy": 0.07701516813702053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.1189422607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02215917780995369,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10525343418121338,
      "backward_entropy": 0.07668810420566136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.01773071289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022243723273277283,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10517629384994506,
      "backward_entropy": 0.07659669717152913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.36427307128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022327225655317307,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10509878396987915,
      "backward_entropy": 0.07659195529090033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.6434326171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022408638149499893,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10502146482467652,
      "backward_entropy": 0.07658697499169244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.1920928955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022484654560685158,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10494619607925415,
      "backward_entropy": 0.07666081852383083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.59078979492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022562718018889427,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10486884117126465,
      "backward_entropy": 0.07657600111431545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.86419677734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02264089323580265,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10479081869125366,
      "backward_entropy": 0.07701485686832005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 276.9927673339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022716745734214783,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10471341609954835,
      "backward_entropy": 0.07663846015930176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 245.01373291015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022799985483288765,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10463051795959473,
      "backward_entropy": 0.0770147509045071,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.01272583007812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022887755185365677,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10454373359680176,
      "backward_entropy": 0.0770147376590305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.4517059326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022972140461206436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1044580578804016,
      "backward_entropy": 0.07654847039116754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.72425842285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023057719692587852,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10437055826187133,
      "backward_entropy": 0.0765426688724094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 214.66171264648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023144278675317764,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10428131818771362,
      "backward_entropy": 0.07653677463531494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.15635681152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02323247864842415,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10418974161148072,
      "backward_entropy": 0.07659251160091823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 290.34063720703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023319188505411148,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10409802198410034,
      "backward_entropy": 0.07652466826968723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.49319458007812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023413414135575294,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10400075912475586,
      "backward_entropy": 0.0765756434864468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.39634704589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02350437641143799,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10390450954437255,
      "backward_entropy": 0.07651287979549831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 212.69386291503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023593654856085777,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10380836725234985,
      "backward_entropy": 0.07650641600290935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 182.55526733398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023684566840529442,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10371003150939942,
      "backward_entropy": 0.07649994558758205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.0645751953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023774471133947372,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10361106395721435,
      "backward_entropy": 0.0765388740433587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.9974365234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02386077307164669,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10351374149322509,
      "backward_entropy": 0.07701408863067627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.59140014648438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02394256927073002,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10341863632202149,
      "backward_entropy": 0.07701393630769518,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.67587280273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024023903533816338,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10332307815551758,
      "backward_entropy": 0.07650726371341282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.21932983398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024104537442326546,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10322699546813965,
      "backward_entropy": 0.07649623685412937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.98487854003906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024183381348848343,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10313116312026978,
      "backward_entropy": 0.07701342635684544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.72320556640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024259641766548157,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10303633213043213,
      "backward_entropy": 0.07647281222873265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.80679321289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024335628375411034,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1029406189918518,
      "backward_entropy": 0.07643551296657985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.38348388671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02441156841814518,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10284404754638672,
      "backward_entropy": 0.07644809616936578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.6511993408203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02448853850364685,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10274577140808105,
      "backward_entropy": 0.07643557257122463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.79110717773438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02456633932888508,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10264586210250855,
      "backward_entropy": 0.07640759150187175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.83828735351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024646049365401268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10254356861114503,
      "backward_entropy": 0.07641017436981201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.7418975830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02472718618810177,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10243902206420899,
      "backward_entropy": 0.07639718055725098,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.62652587890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02480640821158886,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10233492851257324,
      "backward_entropy": 0.07637955082787408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.27052307128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02488390915095806,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10223116874694824,
      "backward_entropy": 0.07636961672041151,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.38214111328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024966716766357422,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10212249755859375,
      "backward_entropy": 0.07636029190487331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.28500366210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025046467781066895,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10201539993286132,
      "backward_entropy": 0.07635041740205553,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.3457489013672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025125911459326744,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10190782546997071,
      "backward_entropy": 0.07701071103413899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.40167236328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025206249207258224,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.1017988681793213,
      "backward_entropy": 0.07631403870052761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.32867431640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025285637006163597,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10168944597244263,
      "backward_entropy": 0.07631985346476237,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.34291076660156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02536369115114212,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1015804648399353,
      "backward_entropy": 0.07630911138322619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.41020965576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025441231206059456,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10147086381912232,
      "backward_entropy": 0.07629811763763428,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.0732421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025515366345643997,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1013635277748108,
      "backward_entropy": 0.07628647486368816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.29591369628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0255929883569479,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10125244855880737,
      "backward_entropy": 0.07623885737525092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.01704406738281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0256736408919096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10113792419433594,
      "backward_entropy": 0.07622395621405707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.27455139160156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025749394670128822,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.1010270357131958,
      "backward_entropy": 0.0762522750430637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.95445251464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025824973359704018,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10091534852981568,
      "backward_entropy": 0.07619184917873806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.9497528076172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025903508067131042,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10079996585845948,
      "backward_entropy": 0.07622826761669582,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.01513671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02598422020673752,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10068217515945435,
      "backward_entropy": 0.07615953021579319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.8413543701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02606775239109993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10056120157241821,
      "backward_entropy": 0.0761439667807685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.35693359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026148175820708275,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.10044221878051758,
      "backward_entropy": 0.07619323333104451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.81422424316406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026227079331874847,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10032397508621216,
      "backward_entropy": 0.07611046897040473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.09536743164062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.026308409869670868,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.10020227432250976,
      "backward_entropy": 0.07700727383295695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.28826904296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02638457901775837,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.10008465051651001,
      "backward_entropy": 0.07607407040066189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.83099365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026461418718099594,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09996534585952759,
      "backward_entropy": 0.07614102628495958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.7408447265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026537105441093445,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09984656572341918,
      "backward_entropy": 0.07603474458058675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.74234008789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02661057375371456,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09972925186157226,
      "backward_entropy": 0.0761121114095052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.06808471679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026687826961278915,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09960798621177673,
      "backward_entropy": 0.07609789239035712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.6928253173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026762306690216064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09948835372924805,
      "backward_entropy": 0.07597365644243029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.80715942382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02683577872812748,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09936904907226562,
      "backward_entropy": 0.07595210605197483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.92506408691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026908932253718376,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09924888610839844,
      "backward_entropy": 0.07592942979600695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.58514404296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026979928836226463,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09913010597229004,
      "backward_entropy": 0.07603483067618476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.61328125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027048785239458084,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09901250600814819,
      "backward_entropy": 0.07700296243031819,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.28016662597656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027118882164359093,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0988925814628601,
      "backward_entropy": 0.07585540082719591,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.44041442871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027190398424863815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09877052903175354,
      "backward_entropy": 0.07598274283938938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.21356201171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027259930968284607,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09864987134933471,
      "backward_entropy": 0.07596467600928412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.2212371826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027332523837685585,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0985259771347046,
      "backward_entropy": 0.07577735847896999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.2724609375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02740442380309105,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0984024703502655,
      "backward_entropy": 0.07699988947974311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.71600341796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02747310884296894,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09828157424926758,
      "backward_entropy": 0.07591021060943604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.66928100585938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027539897710084915,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09816181063652038,
      "backward_entropy": 0.0769983794954088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.94242858886719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027606971561908722,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09804067611694336,
      "backward_entropy": 0.07566544744703504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.85118103027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027671316638588905,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09792183637619019,
      "backward_entropy": 0.07563414176305135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.22913360595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027735205367207527,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09780258536338807,
      "backward_entropy": 0.07560165723164876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.48130798339844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.027795683592557907,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.09768660068511963,
      "backward_entropy": 0.07699424028396606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.2724380493164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02785499580204487,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09757102727890014,
      "backward_entropy": 0.07553287347157796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.30531311035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027911558747291565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09745829105377198,
      "backward_entropy": 0.07549668020672268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.40228271484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027968458831310272,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09734432697296143,
      "backward_entropy": 0.07545935445361668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 231.9467315673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02802947349846363,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09722567796707153,
      "backward_entropy": 0.07570780648125543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.00851440429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028097540140151978,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0970994472503662,
      "backward_entropy": 0.07538872957229614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.81121826171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02816339023411274,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09697459936141968,
      "backward_entropy": 0.0756626394059923,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.03684997558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028229163959622383,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09684977531433106,
      "backward_entropy": 0.07563910219404432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.27870178222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02829587087035179,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09672373533248901,
      "backward_entropy": 0.07561548550923665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.85687255859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028362218290567398,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09659780263900757,
      "backward_entropy": 0.0755913257598877,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.2959442138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028431836515665054,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09646836519241334,
      "backward_entropy": 0.07556796073913574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.37269592285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028501806780695915,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09633814096450806,
      "backward_entropy": 0.07516548368665907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.85169982910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028575770556926727,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0962037205696106,
      "backward_entropy": 0.07552159494823879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.75564575195312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028652332723140717,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09606674909591675,
      "backward_entropy": 0.07509346140755548,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.41372680664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02872854843735695,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09592982530593872,
      "backward_entropy": 0.07505660586886936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.66925811767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028807135298848152,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09579062461853027,
      "backward_entropy": 0.07502075698640612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.1607666015625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02888297475874424,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0956541657447815,
      "backward_entropy": 0.07698318693372938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.25067138671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028955647721886635,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09552142024040222,
      "backward_entropy": 0.07494313187069362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.08255004882812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029028570279479027,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.095388263463974,
      "backward_entropy": 0.07698260413275824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.31449890136719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02909962087869644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0952572226524353,
      "backward_entropy": 0.07486134767532349,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.46131896972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029168885201215744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09512803554534913,
      "backward_entropy": 0.0753241777420044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.54508972167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029238546267151833,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09499802589416503,
      "backward_entropy": 0.07477358976999919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.72146606445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029303517192602158,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09487228393554688,
      "backward_entropy": 0.0752633743815952,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.50481414794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029369421303272247,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0947453260421753,
      "backward_entropy": 0.07523145940568712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 169.43905639648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029433362185955048,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09462112784385682,
      "backward_entropy": 0.0746296379301283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.7191925048828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029499346390366554,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09449434280395508,
      "backward_entropy": 0.0745810071627299,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.16551208496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02956986613571644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09436283111572266,
      "backward_entropy": 0.0751345952351888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.1824493408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029635636135935783,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09423725605010987,
      "backward_entropy": 0.07510058085123698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.975830078125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029702164232730865,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09411051273345947,
      "backward_entropy": 0.07443661159939235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.60795593261719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02976248785853386,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0939904808998108,
      "backward_entropy": 0.07502799563937718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.20209503173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029820891097187996,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09387249946594238,
      "backward_entropy": 0.07432748211754693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.6838607788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029878372326493263,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09375506043434143,
      "backward_entropy": 0.07426932122972277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.28826141357422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02993512898683548,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09363809823989869,
      "backward_entropy": 0.07490605115890503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.02228546142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0299910269677639,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09352155923843383,
      "backward_entropy": 0.07486324177847968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 209.55389404296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03004646860063076,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09340541958808898,
      "backward_entropy": 0.07408270570966932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.88217163085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030108561739325523,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0932822585105896,
      "backward_entropy": 0.07402295536465114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.94158935546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030173014849424362,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09315657615661621,
      "backward_entropy": 0.07396379444334242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.19189453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030237583443522453,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09303119778633118,
      "backward_entropy": 0.07469990518358019,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.2215118408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030299926176667213,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09290866255760193,
      "backward_entropy": 0.07465834087795681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.29148864746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030362702906131744,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09278615713119506,
      "backward_entropy": 0.07461649841732448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.76966857910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03042798861861229,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09266123175621033,
      "backward_entropy": 0.07372419039408366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.59552001953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030494114384055138,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09253541231155396,
      "backward_entropy": 0.07366292344199286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.1265106201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030562208965420723,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09240765571594238,
      "backward_entropy": 0.07360106706619263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.70331573486328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03063427284359932,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09227589368820191,
      "backward_entropy": 0.07445633411407471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.9362030029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030703265219926834,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.092147958278656,
      "backward_entropy": 0.07441479629940456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.20126342773438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030772725120186806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09201972484588623,
      "backward_entropy": 0.07340933216942681,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.83824157714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03084632381796837,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09188780784606934,
      "backward_entropy": 0.07334587309095594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.20193481445312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030918920412659645,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0917574405670166,
      "backward_entropy": 0.07428952058156331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.18222045898438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03099372237920761,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09162492752075195,
      "backward_entropy": 0.0732132461335924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.16735076904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03106629103422165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09149539470672607,
      "backward_entropy": 0.07420266999138726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.30462646484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031137367710471153,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09136860370635987,
      "backward_entropy": 0.07307395007875231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.11387634277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031211111694574356,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09123953580856323,
      "backward_entropy": 0.07411128944820827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.86402893066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03128635510802269,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0911097526550293,
      "backward_entropy": 0.07293589909871419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.75878143310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03136294707655907,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09097942113876342,
      "backward_entropy": 0.07286752594841851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.33438110351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03143611550331116,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09085339307785034,
      "backward_entropy": 0.0727952520052592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.83502197265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03150942549109459,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09072767496109009,
      "backward_entropy": 0.07272071308559841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.15933227539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031582657247781754,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09060229063034057,
      "backward_entropy": 0.07264227999581231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.47093963623047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03165481239557266,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.09047845602035523,
      "backward_entropy": 0.07381737894482082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.69276428222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031723879277706146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09035842418670655,
      "backward_entropy": 0.0724745856391059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.49537658691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031793706119060516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09023835659027099,
      "backward_entropy": 0.07238832447263929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.33238220214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03185945376753807,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0901229739189148,
      "backward_entropy": 0.07229736116197374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.41094970703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0319259837269783,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.09000712037086486,
      "backward_entropy": 0.07220476203494602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.70250701904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03199231997132301,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08989209532737732,
      "backward_entropy": 0.07211141453848945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.27063751220703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03205695003271103,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0897789478302002,
      "backward_entropy": 0.07693325810962254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.70426940917969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03212098404765129,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08966755867004395,
      "backward_entropy": 0.07191738155153063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.4433135986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03217905014753342,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08956246376037598,
      "backward_entropy": 0.07181406021118164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.9157257080078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.032242417335510254,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08945277929306031,
      "backward_entropy": 0.07692439688576592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.17462158203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03230712190270424,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08934247493743896,
      "backward_entropy": 0.07161586152182685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.31620025634766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032374244183301926,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08923065662384033,
      "backward_entropy": 0.07310397095150417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.82575225830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03243723884224892,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08912307024002075,
      "backward_entropy": 0.0730312532848782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.03166198730469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03249826282262802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08901824951171874,
      "backward_entropy": 0.07130608293745253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.78750610351562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03255576267838478,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08891695737838745,
      "backward_entropy": 0.0728751089837816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.80136108398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03261519595980644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08881450891494751,
      "backward_entropy": 0.07279513279596965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.6194839477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03267392888665199,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08871326446533204,
      "backward_entropy": 0.0709604024887085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.7572250366211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0327320322394371,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08861316442489624,
      "backward_entropy": 0.07262924644682142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.67845916748047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03278711810708046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08851631879806518,
      "backward_entropy": 0.07071669234169854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.0341796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0328415185213089,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08842015266418457,
      "backward_entropy": 0.07058745622634888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.63106536865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03289807215332985,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08832261562347413,
      "backward_entropy": 0.07045949167675442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.06974792480469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032951973378658295,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08822824954986572,
      "backward_entropy": 0.07032788462109035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.8997802734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03300563246011734,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0881345510482788,
      "backward_entropy": 0.07217404577467176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.10794067382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03306295722723007,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08803849220275879,
      "backward_entropy": 0.07006651825375027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.6735954284668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03311927616596222,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08794339895248413,
      "backward_entropy": 0.06993235482109918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.80326843261719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03317050263285637,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08785358667373658,
      "backward_entropy": 0.07188942697313097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.22833251953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033218055963516235,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08776735067367554,
      "backward_entropy": 0.0717827214135064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.0343780517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033270109444856644,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08767828941345215,
      "backward_entropy": 0.07168164518144396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.48120880126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03332589194178581,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08758679032325745,
      "backward_entropy": 0.06936105092366536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.13548278808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03338104486465454,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08749617338180542,
      "backward_entropy": 0.06921973493364122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.94863891601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033440038561820984,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08740366697311401,
      "backward_entropy": 0.07138966851764256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.76750183105469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033495619893074036,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08731434345245362,
      "backward_entropy": 0.0689437985420227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.30237579345703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.033549997955560684,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08722702264785767,
      "backward_entropy": 0.07682693666881985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.1429214477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03360152617096901,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08714240789413452,
      "backward_entropy": 0.07107202212015788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.30365753173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033654507249593735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08705739974975586,
      "backward_entropy": 0.06849008136325413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.720458984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03370622918009758,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08697403073310853,
      "backward_entropy": 0.07084848483403523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.8504867553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033760301768779755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08688930273056031,
      "backward_entropy": 0.06817643509970771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.97393035888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03381546959280968,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08680448532104493,
      "backward_entropy": 0.06802108552720812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.72467803955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033871524035930634,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08671965599060058,
      "backward_entropy": 0.07051429483625624,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.86675262451172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03392869234085083,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08663500547409057,
      "backward_entropy": 0.06770902872085571,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.78437042236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033986445516347885,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08655054569244384,
      "backward_entropy": 0.07029049926333958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.74755096435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03404349461197853,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08646721839904785,
      "backward_entropy": 0.06738595167795818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.39140319824219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03409867733716965,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08638585805892944,
      "backward_entropy": 0.06721240282058716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.47351837158203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03415310010313988,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0863053560256958,
      "backward_entropy": 0.06703200605180529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.2049331665039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034203555434942245,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08622841835021973,
      "backward_entropy": 0.06684201955795288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.53284454345703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03425280749797821,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08615288138389587,
      "backward_entropy": 0.06965290175543891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.78964233398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034298818558454514,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08608049154281616,
      "backward_entropy": 0.06950871149698894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.8951873779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034343261271715164,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08601007461547852,
      "backward_entropy": 0.06936098469628228,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.15478515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03439417481422424,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08593547940254212,
      "backward_entropy": 0.0692239867316352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.656005859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03444282338023186,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08586323261260986,
      "backward_entropy": 0.06584178076850043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.11149597167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03449035435914993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08579218983650208,
      "backward_entropy": 0.06563139624065822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.41246032714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03453811630606651,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0857214629650116,
      "backward_entropy": 0.06541652811898126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.71271514892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03458957374095917,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08564869165420533,
      "backward_entropy": 0.06863835122850206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.92648315429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03464151546359062,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08557686805725098,
      "backward_entropy": 0.06849147876103719,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.58001708984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03469233959913254,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08550663590431214,
      "backward_entropy": 0.06478177176581489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.45779418945312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03474687412381172,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0854347825050354,
      "backward_entropy": 0.06457538074917263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.34674835205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03480394184589386,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08536254167556763,
      "backward_entropy": 0.06437695026397705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.20915222167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03486185148358345,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08529077768325806,
      "backward_entropy": 0.06417770518196954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.85940551757812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034915585070848465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08522247076034546,
      "backward_entropy": 0.06396316819720799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.0412139892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034970495849847794,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08515434265136719,
      "backward_entropy": 0.06374911467234294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.38555908203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035027697682380676,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08508578538894654,
      "backward_entropy": 0.06353606780370076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.12532043457031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035086434334516525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08501677513122559,
      "backward_entropy": 0.06331492132610744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.18151092529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035145148634910583,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08494930267333985,
      "backward_entropy": 0.06309723191791111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.9549789428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0352044440805912,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08488233089447021,
      "backward_entropy": 0.06287630399068196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.05107116699219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03526190668344498,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08481727838516236,
      "backward_entropy": 0.0668301052517361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.9118423461914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03531895577907562,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08475337028503419,
      "backward_entropy": 0.06666332483291626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.73392486572266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03537295386195183,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08469176292419434,
      "backward_entropy": 0.0621624920103285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.29093933105469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03542420640587807,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08463225364685059,
      "backward_entropy": 0.06629829274283515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.5363998413086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03547341376543045,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0845747709274292,
      "backward_entropy": 0.0661047167248196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.62078857421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03552128002047539,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0845181941986084,
      "backward_entropy": 0.06590357091691759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.87726593017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03557205572724342,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08446083068847657,
      "backward_entropy": 0.06107825703091092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.23102569580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0356217622756958,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08440470099449157,
      "backward_entropy": 0.06550490193896824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.94625854492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03566938266158104,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08435040712356567,
      "backward_entropy": 0.06529451078838772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.52391815185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03571829944849014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08429572582244874,
      "backward_entropy": 0.06020812855826484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.5746955871582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03576912358403206,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08424113988876343,
      "backward_entropy": 0.06487814585367839,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.64685821533203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0358152873814106,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0841895580291748,
      "backward_entropy": 0.06465996636284722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.29277801513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03586128354072571,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08413903713226319,
      "backward_entropy": 0.05932305918799506,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.41553497314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03590650111436844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08408921360969543,
      "backward_entropy": 0.0642175210846795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.0542221069336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035945963114500046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08404223918914795,
      "backward_entropy": 0.06397729449801975,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.19601440429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03598717227578163,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08399540185928345,
      "backward_entropy": 0.058368285497029625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.04940795898438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03602583333849907,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08395016193389893,
      "backward_entropy": 0.0580389830801222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.42929077148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03606220707297325,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08390638828277588,
      "backward_entropy": 0.05769997835159302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.03003692626953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03609910607337952,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08386292457580566,
      "backward_entropy": 0.06298602951897515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.86709594726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036137402057647705,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08381908535957336,
      "backward_entropy": 0.06273121303982204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.98314666748047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036176469177007675,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08377596735954285,
      "backward_entropy": 0.05669405725267199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.92282104492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03621375933289528,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08373452425003051,
      "backward_entropy": 0.05635987387763129,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.4840087890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03625377267599106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08369221091270447,
      "backward_entropy": 0.05603000852796766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.74665069580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036293983459472656,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0836503267288208,
      "backward_entropy": 0.05569757355584039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.80418395996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036333855241537094,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08360860347747803,
      "backward_entropy": 0.05535306202040778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.35083770751953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036374058574438095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08356742262840271,
      "backward_entropy": 0.055011596944597035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.55900573730469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03641580045223236,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08352632522583008,
      "backward_entropy": 0.06092566914028592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.81702423095703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03645728528499603,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08348557949066163,
      "backward_entropy": 0.07631922430462307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.989723205566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036496490240097046,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08344637155532837,
      "backward_entropy": 0.06039157840940687,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.89325714111328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036532409489154816,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08340899348258972,
      "backward_entropy": 0.06010822455088297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.89679718017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036569803953170776,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08337113857269288,
      "backward_entropy": 0.05325835280948215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.43016815185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036610282957553864,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08333286046981811,
      "backward_entropy": 0.05290777484575907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.41644287109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03665108606219292,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08329516053199768,
      "backward_entropy": 0.05255738894144694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.2634048461914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036696791648864746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08325622081756592,
      "backward_entropy": 0.059018227789137095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.6264877319336,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03674578666687012,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08321670889854431,
      "backward_entropy": 0.07624681790669759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.64429473876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036793965846300125,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08317787647247314,
      "backward_entropy": 0.05850532319810656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.39680480957031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036842744797468185,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08313944935798645,
      "backward_entropy": 0.05824283758799235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.673988342285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03689207136631012,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08310143947601319,
      "backward_entropy": 0.05089126692877875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.9041748046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0369386225938797,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08306527137756348,
      "backward_entropy": 0.05770674016740587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.23976135253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03698595613241196,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08302935361862182,
      "backward_entropy": 0.05743704239527384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.62792205810547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03703715279698372,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08299237489700317,
      "backward_entropy": 0.05717445082134671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.6307144165039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03708622604608536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08295662403106689,
      "backward_entropy": 0.05689847469329834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.23611068725586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037133704870939255,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08292219638824463,
      "backward_entropy": 0.05661388238271078,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.18766784667969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03717749938368797,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08288976550102234,
      "backward_entropy": 0.04878089825312296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.21029663085938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03722391277551651,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08285734057426453,
      "backward_entropy": 0.05602936612235175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.38516235351562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03727390617132187,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08282472491264344,
      "backward_entropy": 0.07622569137149388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.6502914428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0373246856033802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08279261589050294,
      "backward_entropy": 0.04776879813936022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.25313949584961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03737597540020943,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08276084065437317,
      "backward_entropy": 0.047439356644948326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.47203826904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03742426633834839,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08273046016693116,
      "backward_entropy": 0.047091891368230186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.40446472167969,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03747190162539482,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0827004611492157,
      "backward_entropy": 0.07622521453433567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.90534973144531,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03752017766237259,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08267062902450562,
      "backward_entropy": 0.046369449959860906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.64120483398438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03756872937083244,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08264073729515076,
      "backward_entropy": 0.04599913623597887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.38233184814453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037617944180965424,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08261120319366455,
      "backward_entropy": 0.05370314253701104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.57099914550781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03766776621341705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08258200883865356,
      "backward_entropy": 0.045264879862467446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.35226440429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037717126309871674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08255353569984436,
      "backward_entropy": 0.04489577147695753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.85111999511719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03776688873767853,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08252518773078918,
      "backward_entropy": 0.052783932950761586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.56686401367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03781265765428543,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08249834775924683,
      "backward_entropy": 0.05245572328567505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.11579132080078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03786051645874977,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08247145414352416,
      "backward_entropy": 0.052134964201185435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.9142074584961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037903573364019394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08244635462760926,
      "backward_entropy": 0.05179503228929308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.40098571777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03795183077454567,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08242089748382568,
      "backward_entropy": 0.05147877335548401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.33538055419922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03799547255039215,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08239729404449463,
      "backward_entropy": 0.04259871111975776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.08529663085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03804033622145653,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08237367868423462,
      "backward_entropy": 0.042221751477983266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.497989654541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038086313754320145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08235019445419312,
      "backward_entropy": 0.05049066411124335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.70038986206055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038129158318042755,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08232696056365967,
      "backward_entropy": 0.05014376176728143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.88111114501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03816990181803703,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08230452537536621,
      "backward_entropy": 0.04103627469804552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.581748962402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03821239992976189,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08228224515914917,
      "backward_entropy": 0.0494414038128323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.65852355957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038252752274274826,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08226051926612854,
      "backward_entropy": 0.04023275110456678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.679908752441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038293711841106415,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08223911523818969,
      "backward_entropy": 0.04873090982437134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.56211853027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038332510739564896,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08221815824508667,
      "backward_entropy": 0.04836647378073798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.45077514648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03837239369750023,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.082197767496109,
      "backward_entropy": 0.03902034295929803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.36503601074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03841638192534447,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08217698335647583,
      "backward_entropy": 0.03864044613308377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.16657257080078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03846397250890732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0821559488773346,
      "backward_entropy": 0.03827448685963949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.92330932617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0385127067565918,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08213510513305664,
      "backward_entropy": 0.047025700410207115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.70807647705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03856348246335983,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08211443424224854,
      "backward_entropy": 0.03756016161706713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.41128540039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03861360624432564,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08209384679794311,
      "backward_entropy": 0.0463917420970069,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.87351989746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03866364434361458,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08207390308380128,
      "backward_entropy": 0.03683845533265008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.18290710449219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03871231898665428,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.082054603099823,
      "backward_entropy": 0.045739180511898465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.5766830444336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038760483264923096,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08203537464141845,
      "backward_entropy": 0.04540418254004584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.75776672363281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03880979120731354,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08201650381088257,
      "backward_entropy": 0.035742190149095326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.63505554199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03886083886027336,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08199745416641235,
      "backward_entropy": 0.035380780696868896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.19274139404297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03891017287969589,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08197886347770691,
      "backward_entropy": 0.03501021862030029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.29403686523438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03895897790789604,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08196041584014893,
      "backward_entropy": 0.03463278545273675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.1051254272461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03900850936770439,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08194200992584229,
      "backward_entropy": 0.034255888726976186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.54395294189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03905780613422394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08192405700683594,
      "backward_entropy": 0.03388128015730116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.57470703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039107900112867355,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08190624713897705,
      "backward_entropy": 0.033510155147976346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.878787994384766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039160389453172684,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0818889856338501,
      "backward_entropy": 0.04274983207384745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.90564727783203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03920920193195343,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08187270760536194,
      "backward_entropy": 0.04242019852002462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.25830841064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039256710559129715,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08185688257217408,
      "backward_entropy": 0.04208217726813422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.118457794189453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039307888597249985,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08184083700180053,
      "backward_entropy": 0.041758719417783946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.8166732788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039351895451545715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08182584047317505,
      "backward_entropy": 0.03172445959515042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.71772003173828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03939482569694519,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08181082010269165,
      "backward_entropy": 0.04104177157084147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.39427185058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03943798318505287,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08179574012756348,
      "backward_entropy": 0.030972070164150663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.703739166259766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03948018327355385,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08178068399429321,
      "backward_entropy": 0.030590451425976224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.58729553222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039519358426332474,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08176596760749817,
      "backward_entropy": 0.030200041002697416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.87545394897461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03955701366066933,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0817514717578888,
      "backward_entropy": 0.03957529862721761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.04511642456055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03959242254495621,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08173742294311523,
      "backward_entropy": 0.03919385539160834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.31769561767578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03962738811969757,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08172432184219361,
      "backward_entropy": 0.0290313892894321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.8247184753418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03966522216796875,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08171160221099853,
      "backward_entropy": 0.03846648997730679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.113527297973633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039699483662843704,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08169904947280884,
      "backward_entropy": 0.028302099969651964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.62503814697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03972974419593811,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0816870927810669,
      "backward_entropy": 0.037720408704545766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.06526184082031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03976282477378845,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08167484998703003,
      "backward_entropy": 0.027558361490567524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.77693176269531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03980197384953499,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08166235685348511,
      "backward_entropy": 0.03702167669932047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.27042007446289,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039841122925281525,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0816501259803772,
      "backward_entropy": 0.0756911039352417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.90311050415039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039880476891994476,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08163832426071167,
      "backward_entropy": 0.026546029580963984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.2038803100586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03991740569472313,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0816266655921936,
      "backward_entropy": 0.026205516523785062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.6068115234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03995773568749428,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08161484003067017,
      "backward_entropy": 0.025875677665074665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.3110122680664,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03999558836221695,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08160317540168763,
      "backward_entropy": 0.07565081119537354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.031005859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04003900662064552,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08159149885177612,
      "backward_entropy": 0.025218902362717524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.45164489746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04008084163069725,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08157975673675537,
      "backward_entropy": 0.03474101755354139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.50334930419922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04011894017457962,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08156803846359253,
      "backward_entropy": 0.03440636396408081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.2936553955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040162187069654465,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0815569281578064,
      "backward_entropy": 0.024268731474876404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.040035247802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04021279141306877,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08154560923576355,
      "backward_entropy": 0.03383665614657932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.26236343383789,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04026135057210922,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08153461217880249,
      "backward_entropy": 0.07568526268005371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.069698333740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040306925773620605,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08152387142181397,
      "backward_entropy": 0.03327311409844293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.96372985839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04035159572958946,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0815128207206726,
      "backward_entropy": 0.023146463765038386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.47099685668945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04039369896054268,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0815020203590393,
      "backward_entropy": 0.022850735319985285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.910770416259766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0404345765709877,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08149143457412719,
      "backward_entropy": 0.02255625029404958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.87820816040039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04047463834285736,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08148131370544434,
      "backward_entropy": 0.03206819295883179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.89468383789062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040513891726732254,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08147152662277221,
      "backward_entropy": 0.03176981872982449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.17179870605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040556758642196655,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08146190643310547,
      "backward_entropy": 0.03149230281511942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.815811157226562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04060393571853638,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08145238757133484,
      "backward_entropy": 0.03123545977804396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.20150756835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040646471083164215,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08144335746765137,
      "backward_entropy": 0.0309653017255995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.15642547607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04069012776017189,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08143450021743774,
      "backward_entropy": 0.030702435308032565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.83381652832031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04073673114180565,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0814257025718689,
      "backward_entropy": 0.07576062944200304,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.69181823730469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04078739136457443,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08141669631004333,
      "backward_entropy": 0.03021152483092414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.7399673461914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04083961993455887,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08140814304351807,
      "backward_entropy": 0.02998234166039361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.797786712646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04089169576764107,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08139933943748474,
      "backward_entropy": 0.020061790943145752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.21175384521484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04094155132770538,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08139035105705261,
      "backward_entropy": 0.019829546411832173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.206729888916016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04099412262439728,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08138157725334168,
      "backward_entropy": 0.02926850650045607,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.25228881835938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041044630110263824,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08137280941009521,
      "backward_entropy": 0.019381860891977947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.52836608886719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041096851229667664,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08136439323425293,
      "backward_entropy": 0.01916612188021342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.285865783691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04115018993616104,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08135583400726318,
      "backward_entropy": 0.01895064115524292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.18900299072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0412021242082119,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08134686946868896,
      "backward_entropy": 0.018723984559377033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.18380737304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041255053132772446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08133774399757385,
      "backward_entropy": 0.01850030654006534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.4200668334961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04130948707461357,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08132907152175903,
      "backward_entropy": 0.02786098255051507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.89628982543945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04136373847723007,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08132034540176392,
      "backward_entropy": 0.027634688549571566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.59977722167969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04141698032617569,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08131176233291626,
      "backward_entropy": 0.01785923706160651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.50347137451172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04146725684404373,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0813034176826477,
      "backward_entropy": 0.02717214160495334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.89384460449219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04151900112628937,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08129526376724243,
      "backward_entropy": 0.026947673824098375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.241939544677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04157539829611778,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08128735423088074,
      "backward_entropy": 0.017236068844795227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.23296356201172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04163045063614845,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08127939701080322,
      "backward_entropy": 0.026532020833757188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.54758071899414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04168880730867386,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08127171993255615,
      "backward_entropy": 0.02633564008606805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.09633255004883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041744958609342575,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08126440048217773,
      "backward_entropy": 0.01666441559791565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.74825668334961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0417993888258934,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08125732541084289,
      "backward_entropy": 0.07618753115336101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.108028411865234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04185330122709274,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08125075101852416,
      "backward_entropy": 0.01631060242652893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.98954010009766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04190540313720703,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08124420642852784,
      "backward_entropy": 0.025541454553604126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.092308044433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04195994883775711,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08123764991760254,
      "backward_entropy": 0.015960309240553115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.32899475097656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04201308265328407,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0812304973602295,
      "backward_entropy": 0.015774210294087727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.77313995361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042066216468811035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08122308254241943,
      "backward_entropy": 0.015585283438364664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.9923324584961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042122721672058105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08121575117111206,
      "backward_entropy": 0.024738066726260714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.979896545410156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.042180415242910385,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.081208735704422,
      "backward_entropy": 0.07629258102840847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.29355239868164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042236682027578354,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08120161890983582,
      "backward_entropy": 0.02435628076394399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.632755279541016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04229216277599335,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08119457960128784,
      "backward_entropy": 0.07631893952687581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.489559173583984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04234795644879341,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08118789196014405,
      "backward_entropy": 0.023984301421377394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.053245544433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04240238294005394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0811808705329895,
      "backward_entropy": 0.02379343907038371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.57714080810547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04245583713054657,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08117377758026123,
      "backward_entropy": 0.07634800010257298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.647098541259766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042506828904151917,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08116697072982788,
      "backward_entropy": 0.014204556743303934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.336483001708984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042557213455438614,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08116008043289184,
      "backward_entropy": 0.023216101858350966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.59210968017578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042605746537446976,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08115284442901612,
      "backward_entropy": 0.013855050835344527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.26875305175781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04265187308192253,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08114550113677979,
      "backward_entropy": 0.022813727458318073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.676429748535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04269907996058464,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08113824129104615,
      "backward_entropy": 0.01350083781613244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.119163513183594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04274636134505272,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0811310887336731,
      "backward_entropy": 0.013331340418921577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.50047302246094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042791493237018585,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08112374544143677,
      "backward_entropy": 0.02223133709695604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.316078186035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04283669963479042,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08111623525619507,
      "backward_entropy": 0.012990286780728234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.30579376220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04288198798894882,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08110872507095337,
      "backward_entropy": 0.021847728225919936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.639564514160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04293457418680191,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08110160827636718,
      "backward_entropy": 0.012673952513270907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.81331634521484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042985700070858,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08109448552131653,
      "backward_entropy": 0.021516645948092144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.2482795715332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04303976893424988,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08108772039413452,
      "backward_entropy": 0.021366222037209406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.43897247314453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043092336505651474,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08108099699020385,
      "backward_entropy": 0.021216147475772433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.127784729003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04314853250980377,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08107463121414185,
      "backward_entropy": 0.012127565840880076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.70991516113281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04320378601551056,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08106776475906372,
      "backward_entropy": 0.01199874199099011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.48033142089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04325850307941437,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.081061190366745,
      "backward_entropy": 0.011873858670393625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.50840759277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043315671384334564,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08105478286743165,
      "backward_entropy": 0.020672544836997986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.16896057128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04337092489004135,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08104825615882874,
      "backward_entropy": 0.011635503835148282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.42060852050781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04342730715870857,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0810416042804718,
      "backward_entropy": 0.011515070994695028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.00161361694336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043480679392814636,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0810346007347107,
      "backward_entropy": 0.02027484940157996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.956701278686523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04353254660964012,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08102749586105347,
      "backward_entropy": 0.020136492119895086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.021324157714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04358217865228653,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08102033138275147,
      "backward_entropy": 0.019998224245177373,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.59593200683594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04363308846950531,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08101353645324708,
      "backward_entropy": 0.019873776369624667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.60948181152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04368671029806137,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08100684881210327,
      "backward_entropy": 0.010912020173337724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.5902099609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04374174028635025,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08100006580352784,
      "backward_entropy": 0.010804305473963419,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.2584285736084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04379715397953987,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08099350333213806,
      "backward_entropy": 0.019536563091807894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.44549560546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043850112706422806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08098669052124023,
      "backward_entropy": 0.010595012042257521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.792537689208984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04390278831124306,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08098000288009644,
      "backward_entropy": 0.01049330665005578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.874786376953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043953970074653625,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08097305297851562,
      "backward_entropy": 0.010388675663206313,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.805489540100098,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04400086775422096,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08096568584442139,
      "backward_entropy": 0.010278186864323087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.11573314666748,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04404392093420029,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08095791935920715,
      "backward_entropy": 0.018970143463876512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.12130355834961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04408271238207817,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08094985485076904,
      "backward_entropy": 0.018840408987469144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.9760627746582,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04412363842129707,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08094201683998108,
      "backward_entropy": 0.07653364870283338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.412425994873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044164519757032394,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08093424439430237,
      "backward_entropy": 0.01860803034570482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.689353942871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04420251026749611,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08092641830444336,
      "backward_entropy": 0.009723935690191057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.232789993286133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04424084350466728,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08091871738433838,
      "backward_entropy": 0.018389968408478632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.89030456542969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0442783385515213,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08091092705726624,
      "backward_entropy": 0.018284698327382404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.194358825683594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044317129999399185,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08090324997901917,
      "backward_entropy": 0.01818607250849406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.98401641845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044357914477586746,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08089567422866821,
      "backward_entropy": 0.01809202465746138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.49667358398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044397518038749695,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08088778257369995,
      "backward_entropy": 0.017995168765385944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.34087371826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04444100707769394,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08088015913963317,
      "backward_entropy": 0.009143504003683725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.46917724609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04448510333895683,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08087245225906373,
      "backward_entropy": 0.017823389834827848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.9577522277832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04453069716691971,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08086474537849427,
      "backward_entropy": 0.017742082476615906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.50088119506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04457683861255646,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08085706830024719,
      "backward_entropy": 0.008885873688591851,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.041561126708984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04462267458438873,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08084947466850281,
      "backward_entropy": 0.008806938098536598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.471221923828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044666096568107605,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08084166049957275,
      "backward_entropy": 0.008727209435568916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.361114501953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04471030458807945,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08083388209342957,
      "backward_entropy": 0.008650533854961395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.379886627197266,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04475509747862816,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0808260440826416,
      "backward_entropy": 0.07656876908408271,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.36331939697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044802550226449966,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08081853985786439,
      "backward_entropy": 0.017339888546201918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.1327896118164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044853076338768005,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08081093430519104,
      "backward_entropy": 0.017288171582751803,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.701560974121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04490860179066658,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08080382347106933,
      "backward_entropy": 0.017251319355434842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.43936538696289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04496241360902786,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08079637289047241,
      "backward_entropy": 0.017206266522407532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.224510192871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04501698538661003,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08078902959823608,
      "backward_entropy": 0.01716652512550354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.93509674072266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04507224261760712,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08078176975250244,
      "backward_entropy": 0.008198884626229605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.049427032470703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045130759477615356,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08077465295791626,
      "backward_entropy": 0.008143339720037248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.70213317871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0451866053044796,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08076730966567994,
      "backward_entropy": 0.008086341122786203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.4042854309082,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045241959393024445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0807599425315857,
      "backward_entropy": 0.017030904690424602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.59331512451172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04529772698879242,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08075252771377564,
      "backward_entropy": 0.016997228066126507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.697887420654297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04535135626792908,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08074508905410767,
      "backward_entropy": 0.00792013771004147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.940773010253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04540201649069786,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08073748350143432,
      "backward_entropy": 0.016936507489946153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.04930877685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04545282572507858,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08072993755340577,
      "backward_entropy": 0.007813851866457198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.635868072509766,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04550272971391678,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08072230815887452,
      "backward_entropy": 0.07675717936621772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.413530349731445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04555274173617363,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0807146430015564,
      "backward_entropy": 0.00770920928981569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.274391174316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04559992626309395,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0807066798210144,
      "backward_entropy": 0.016816344526078966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.07496643066406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04564760625362396,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08069879412651063,
      "backward_entropy": 0.07677861717012194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.38213348388672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04569840803742409,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08069109916687012,
      "backward_entropy": 0.007550164229340023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.37016677856445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045748207718133926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08068325519561767,
      "backward_entropy": 0.007497953871885936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.648536682128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04579915478825569,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08067550659179687,
      "backward_entropy": 0.007448294096522861,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.07451629638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04585005342960358,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08066772222518921,
      "backward_entropy": 0.016672505272759333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.19028091430664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04590582102537155,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806603729724884,
      "backward_entropy": 0.00735580176115036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.06670379638672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04596336558461189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08065330982208252,
      "backward_entropy": 0.007319418920411004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.933807373046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04602036625146866,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806462287902832,
      "backward_entropy": 0.007283470696873135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.900697708129883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04607665538787842,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08063905239105225,
      "backward_entropy": 0.0072445546587308245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.133670806884766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046130552887916565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08063157200813294,
      "backward_entropy": 0.007203267680274116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.08525466918945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04618336260318756,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806240975856781,
      "backward_entropy": 0.00716306103600396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.8176212310791,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.046237822622060776,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08061671257019043,
      "backward_entropy": 0.0768665009074741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.449316024780273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04629124328494072,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08060935735702515,
      "backward_entropy": 0.016591623425483704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.37436294555664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046342458575963974,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0806017279624939,
      "backward_entropy": 0.007046860125329759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.44397735595703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046395573765039444,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08059426546096801,
      "backward_entropy": 0.007009252905845642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.61825942993164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0464475117623806,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08058667182922363,
      "backward_entropy": 0.006970578597651588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.7213020324707,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.046495821326971054,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08057893514633178,
      "backward_entropy": 0.07689111762576634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.891889572143555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04654625430703163,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08057118654251098,
      "backward_entropy": 0.016527560022142198,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.640974044799805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046594735234975815,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08056325912475586,
      "backward_entropy": 0.016508976618448894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.776472091674805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04664057493209839,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0805550456047058,
      "backward_entropy": 0.006810792618327671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.630598068237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04668591916561127,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08054675459861756,
      "backward_entropy": 0.006767010937134425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.478099822998047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04673092067241669,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08053842782974244,
      "backward_entropy": 0.006723976383606593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.378660202026367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04677576199173927,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08053016662597656,
      "backward_entropy": 0.006683596306376987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.23273277282715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046819183975458145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08052167892456055,
      "backward_entropy": 0.016385674476623535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.12443542480469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046862609684467316,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08051326274871826,
      "backward_entropy": 0.006600222239891688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.115060329437256,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04690682142972946,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08050490617752075,
      "backward_entropy": 0.016346477799945407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.998214721679688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04694733768701553,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08049633502960205,
      "backward_entropy": 0.016329675912857056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.882038116455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04698624461889267,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08048766851425171,
      "backward_entropy": 0.006486071480645074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.583045959472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0470244437456131,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08047887086868286,
      "backward_entropy": 0.006447311490774155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.3373908996582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047063954174518585,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08047018051147461,
      "backward_entropy": 0.01627835300233629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.1310920715332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04710552468895912,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08046160340309143,
      "backward_entropy": 0.006375255270136727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.19371032714844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04714926704764366,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08045328855514526,
      "backward_entropy": 0.016261696815490723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.786300659179688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04719358682632446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08044494986534119,
      "backward_entropy": 0.006315626617934968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.26253890991211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04723488166928291,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08043634295463561,
      "backward_entropy": 0.006282180547714233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.42173957824707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04727983474731445,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08042798042297364,
      "backward_entropy": 0.016233046849568684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.091331481933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04732285067439079,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08041952848434449,
      "backward_entropy": 0.006220930566390355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.18721389770508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047365039587020874,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0804110586643219,
      "backward_entropy": 0.006192751228809357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.02492904663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04740896075963974,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08040268421173095,
      "backward_entropy": 0.016221799784236483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.87193298339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04745439067482948,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08039435744285583,
      "backward_entropy": 0.006138489892085393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.32004165649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04750100523233414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08038599491119384,
      "backward_entropy": 0.006109516653749678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.238073348999023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047549761831760406,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0803777515888214,
      "backward_entropy": 0.00608149915933609,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.317917823791504,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04759781062602997,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08036946058273316,
      "backward_entropy": 0.016198741065131292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.970500946044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0476425476372242,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08036094307899475,
      "backward_entropy": 0.006023830009831322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.95122528076172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0476870983839035,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08035246133804322,
      "backward_entropy": 0.005996007472276688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.833656311035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04773319885134697,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08034408092498779,
      "backward_entropy": 0.005969964795642429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.58859634399414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04778236150741577,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08033584356307984,
      "backward_entropy": 0.016177838047345478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.88166046142578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0478360541164875,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08032783269882202,
      "backward_entropy": 0.005923111405637529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.17048263549805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04789101332426071,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08031984567642211,
      "backward_entropy": 0.005899743901358711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.57245635986328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04794660955667496,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08031195402145386,
      "backward_entropy": 0.016187823481029935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.466758728027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04800168424844742,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08030405044555664,
      "backward_entropy": 0.01619458364115821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.22893524169922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0480521097779274,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08029600381851196,
      "backward_entropy": 0.005840130978160434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.438472747802734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.048102591186761856,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.08028782010078431,
      "backward_entropy": 0.07695278856489393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.517913818359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04815104231238365,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.080279541015625,
      "backward_entropy": 0.016210527883635625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.478540420532227,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04820150136947632,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08027138113975525,
      "backward_entropy": 0.005778887619574864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.3461856842041,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04825107753276825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08026317358016968,
      "backward_entropy": 0.01621824171808031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.769864082336426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04829985275864601,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08025493621826171,
      "backward_entropy": 0.005738748858372371,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.68307113647461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04834610968828201,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0802465558052063,
      "backward_entropy": 0.01622320546044244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.86867904663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04839024320244789,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08023810386657715,
      "backward_entropy": 0.016225611170132954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.685426712036133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048438332974910736,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08022977709770203,
      "backward_entropy": 0.005677903691927592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.104408264160156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04848499223589897,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08022142052650452,
      "backward_entropy": 0.005659423768520355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.685155868530273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04853369668126106,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08021313548088074,
      "backward_entropy": 0.00564202004008823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.62083435058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048582639545202255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08020490407943726,
      "backward_entropy": 0.005626283586025238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.220077514648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04863244667649269,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0801966905593872,
      "backward_entropy": 0.005609828978776932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.174232482910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04867979884147644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08018840551376342,
      "backward_entropy": 0.005593905846277873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.074359893798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04872573912143707,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08018004894256592,
      "backward_entropy": 0.016284594933191936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.992698669433594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04877043515443802,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08017166852951049,
      "backward_entropy": 0.005562873350249397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.84637451171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04881380498409271,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08016320466995239,
      "backward_entropy": 0.005546023448308309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.799116134643555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04885697364807129,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08015471696853638,
      "backward_entropy": 0.005529720750119951,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.623510360717773,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048899095505476,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08014622926712037,
      "backward_entropy": 0.016318976879119873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.838767051696777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04894106835126877,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08013770580291749,
      "backward_entropy": 0.005496867001056671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.00529479980469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048980459570884705,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08012909889221191,
      "backward_entropy": 0.005480544434653388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.99079895019531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04902258887887001,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08012058734893798,
      "backward_entropy": 0.016341613398657903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.529496192932129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04906616732478142,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08011210560798646,
      "backward_entropy": 0.0054503315024905736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.070812225341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04910779371857643,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08010357022285461,
      "backward_entropy": 0.01635419163439009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.17443084716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04914924129843712,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08009510040283203,
      "backward_entropy": 0.016356945037841797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.36650466918945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04918979853391647,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08008639812469483,
      "backward_entropy": 0.01635942194196913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.263336181640625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04923196882009506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08007785081863403,
      "backward_entropy": 0.016360956761572096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.192174911499023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04927229508757591,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08006923198699951,
      "backward_entropy": 0.00536657828423712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.519725799560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0493110790848732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08006060123443604,
      "backward_entropy": 0.00535064438978831,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.7960090637207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04935010150074959,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08005197048187256,
      "backward_entropy": 0.01636965572834015,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.60696029663086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049393218010663986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08004341721534729,
      "backward_entropy": 0.005319886737399631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.55836296081543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04943769425153732,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0800348937511444,
      "backward_entropy": 0.005304414365026686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.88426208496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04948093369603157,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08002635240554809,
      "backward_entropy": 0.005289052095678117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.371803283691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04952637106180191,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.08001784682273864,
      "backward_entropy": 0.016379644473393757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.721750259399414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049570415169000626,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08000930547714233,
      "backward_entropy": 0.005260183579391903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.183027267456055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049612339586019516,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.08000073432922364,
      "backward_entropy": 0.005244261688656277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.07242488861084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04965335130691528,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07999215126037598,
      "backward_entropy": 0.016384056872791715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.00587272644043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04969176650047302,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07998351454734802,
      "backward_entropy": 0.016384204228719074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.460497856140137,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04973110929131508,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07997490763664246,
      "backward_entropy": 0.0051984551052252454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.303068161010742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04976885765790939,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07996628284454346,
      "backward_entropy": 0.016383181015650432,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.631969451904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0498068705201149,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07995764017105103,
      "backward_entropy": 0.005168096886740791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.51068687438965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04984602332115173,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07994903326034546,
      "backward_entropy": 0.005155538519223531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.603015899658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04988609254360199,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07994043827056885,
      "backward_entropy": 0.005143783986568451,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.887941360473633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04992528632283211,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07993184328079224,
      "backward_entropy": 0.005131505015823577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.38279390335083,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04996458441019058,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07992324829101563,
      "backward_entropy": 0.005119838648372226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.68308639526367,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05000076815485954,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07991465330123901,
      "backward_entropy": 0.0051083606150415205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.895795822143555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05003970116376877,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.079906165599823,
      "backward_entropy": 0.0050973594188690186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.511924743652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05007956549525261,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07989745140075684,
      "backward_entropy": 0.0050871529512935216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.363508224487305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050124943256378174,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07988885641098023,
      "backward_entropy": 0.01644987530178494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.002300262451172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050169687718153,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07988027930259704,
      "backward_entropy": 0.016457216607199773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.469972610473633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05021309480071068,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07987170219421387,
      "backward_entropy": 0.0164638575580385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.221778869628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050253719091415405,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07986313104629517,
      "backward_entropy": 0.005045205354690552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.092302322387695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050295066088438034,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07985457181930541,
      "backward_entropy": 0.016478349765141804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.649824142456055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050337035208940506,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07984598278999329,
      "backward_entropy": 0.005025966299904717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.69982147216797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05037795007228851,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07983743548393249,
      "backward_entropy": 0.016496835483445063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.70514678955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05041862279176712,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982889413833619,
      "backward_entropy": 0.01650401618745592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.57379722595215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05045991763472557,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07982034683227539,
      "backward_entropy": 0.016511191924413044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.163208961486816,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050501853227615356,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07981178760528565,
      "backward_entropy": 0.004988287885983785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.36961555480957,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05054120719432831,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07980324029922485,
      "backward_entropy": 0.07699709468417698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.192588806152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05058211088180542,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07979468703269958,
      "backward_entropy": 0.0165444811185201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.063352584838867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05062360316514969,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07978613376617431,
      "backward_entropy": 0.004964747776587804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.961041450500488,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05066560208797455,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07977756261825561,
      "backward_entropy": 0.01656761434343126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.912845611572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050706565380096436,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07976902127265931,
      "backward_entropy": 0.016580416096581355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.794440269470215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05074575915932655,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07976049780845643,
      "backward_entropy": 0.004942794226937824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.639841079711914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050784166902303696,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07975199818611145,
      "backward_entropy": 0.01660595999823676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.538726806640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050822529941797256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.079743492603302,
      "backward_entropy": 0.0049282875325944685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.555118560791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050860922783613205,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07973501086235046,
      "backward_entropy": 0.004920833226707246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.205726623535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05089855566620827,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07972654104232788,
      "backward_entropy": 0.004913378092977736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.774669647216797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050937023013830185,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07971807718276977,
      "backward_entropy": 0.004905931651592255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.78179168701172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05097777768969536,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07970952987670898,
      "backward_entropy": 0.016656508048375446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8189733028411865,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05101983994245529,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07970095276832581,
      "backward_entropy": 0.016666667328940496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.580687999725342,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05105847492814064,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07969245910644532,
      "backward_entropy": 0.004884984758165147,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.832109451293945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051094766706228256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07968406081199646,
      "backward_entropy": 0.004877857036060757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.224498748779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05113121494650841,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07967566251754761,
      "backward_entropy": 0.004870720207691193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.709800720214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05116943269968033,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07966718077659607,
      "backward_entropy": 0.0167082945505778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.228404998779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05121295526623726,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07965850830078125,
      "backward_entropy": 0.004859384149312973,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.053963661193848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051256678998470306,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07964982986450195,
      "backward_entropy": 0.00485394439763493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.30619239807129,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05129832401871681,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07964124679565429,
      "backward_entropy": 0.004848526583777534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.749698638916016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05133962631225586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07963265776634217,
      "backward_entropy": 0.004843519793616401,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.93214225769043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05138359218835831,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07962395548820496,
      "backward_entropy": 0.004838758044772678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.37947940826416,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05142921581864357,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07961518168449402,
      "backward_entropy": 0.00483440069688691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.28955078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051473308354616165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07960647344589233,
      "backward_entropy": 0.016799603899319965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.201321601867676,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05151600018143654,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07959784269332885,
      "backward_entropy": 0.004825873921314876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.80207443237305,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0515575036406517,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07958922982215881,
      "backward_entropy": 0.00482256958882014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.529130935668945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05160307511687279,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07958041429519654,
      "backward_entropy": 0.004819602602057987,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.376867294311523,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051647767424583435,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07957165241241455,
      "backward_entropy": 0.0048158011502689784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.756874084472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0516931414604187,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0795628309249878,
      "backward_entropy": 0.004811423106325997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.054279327392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05173845961689949,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07955400943756104,
      "backward_entropy": 0.004807348466581768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.30327033996582,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051784418523311615,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07954519391059875,
      "backward_entropy": 0.004803188973002964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3984811305999756,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.051831651479005814,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07953620553016663,
      "backward_entropy": 0.07701035340627034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.205278396606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051874879747629166,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07952749729156494,
      "backward_entropy": 0.004793057011233436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.0743408203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051918189972639084,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0795187532901764,
      "backward_entropy": 0.01691548029581706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.586688995361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05196153372526169,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07951000332832336,
      "backward_entropy": 0.004782583564519882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.406343460083008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052006397396326065,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07950115203857422,
      "backward_entropy": 0.004777853273683124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.391563415527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0520525723695755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07949221134185791,
      "backward_entropy": 0.004773019916481442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.276992797851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05209771916270256,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07948330640792847,
      "backward_entropy": 0.004767663776874542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.934557914733887,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05214199796319008,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07947444915771484,
      "backward_entropy": 0.016946261127789814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.8480863571167,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052184730768203735,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07946569919586181,
      "backward_entropy": 0.004757612115807003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.144062042236328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05222610384225845,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07945703268051148,
      "backward_entropy": 0.01695822344885932,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.682053565979004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05226762965321541,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07944834232330322,
      "backward_entropy": 0.004747856822278764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.897554397583008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0523078478872776,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07943972945213318,
      "backward_entropy": 0.01696887281205919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.647872924804688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052348341792821884,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07943109273910523,
      "backward_entropy": 0.004737644145886104,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.439227104187012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05238834023475647,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0794224739074707,
      "backward_entropy": 0.00473251814643542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.272869110107422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052427202463150024,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07941393852233887,
      "backward_entropy": 0.004727557301521301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.287145614624023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052464310079813004,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07940554618835449,
      "backward_entropy": 0.016987986034817167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.318359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05250053480267525,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07939721345901489,
      "backward_entropy": 0.0047174278232786394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.207454681396484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052537381649017334,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07938878536224366,
      "backward_entropy": 0.004712591982550091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.066006660461426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05257481336593628,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0793803334236145,
      "backward_entropy": 0.004708322799868054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.9896240234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052611302584409714,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07937193512916565,
      "backward_entropy": 0.017008759909205966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.856401443481445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052647653967142105,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07936354875564575,
      "backward_entropy": 0.01701413757271237,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.805075645446777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052685242146253586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07935502529144287,
      "backward_entropy": 0.004694857945044835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.65336036682129,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052722569555044174,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07934651970863342,
      "backward_entropy": 0.017022964027192857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.772760391235352,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05276034399867058,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07933799028396607,
      "backward_entropy": 0.017027379737959966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.33721351623535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052796442061662674,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07932959794998169,
      "backward_entropy": 0.004681547482808431,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.321067810058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052833788096904755,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07932108044624328,
      "backward_entropy": 0.004677397923337089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.20879364013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05287158116698265,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07931248545646667,
      "backward_entropy": 0.004673744241396586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.398574829101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052909739315509796,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07930386066436768,
      "backward_entropy": 0.017047661874029372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.6646928787231445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05294681712985039,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0792953372001648,
      "backward_entropy": 0.004666130161947674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.256056785583496,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05298156291246414,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07928704619407653,
      "backward_entropy": 0.004662062972784042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.169221878051758,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0530155673623085,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07927883863449096,
      "backward_entropy": 0.07701140642166138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.559720993041992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.053052302449941635,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.079270339012146,
      "backward_entropy": 0.01706210606627994,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.525835990905762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053086765110492706,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07926205396652222,
      "backward_entropy": 0.004649897830353843,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.218761444091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05311914533376694,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07925399541854858,
      "backward_entropy": 0.004645981308486726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.645026206970215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05315304920077324,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07924576997756957,
      "backward_entropy": 0.00464225560426712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.696388244628906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05318698287010193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07923750877380371,
      "backward_entropy": 0.01707818607489268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.861553192138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05322296544909477,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07922899723052979,
      "backward_entropy": 0.004636044303576152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.734121322631836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05326003581285477,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07922039031982422,
      "backward_entropy": 0.017088568872875638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.317569255828857,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053298089653253555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0792116403579712,
      "backward_entropy": 0.004629410803318024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.282618999481201,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05333368107676506,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07920316457748414,
      "backward_entropy": 0.017095161808861628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.743643760681152,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053367048501968384,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07919498682022094,
      "backward_entropy": 0.004622153523895476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.430161476135254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05340108275413513,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07918668985366821,
      "backward_entropy": 0.0046186016665564645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.774441719055176,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05343436822295189,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07917850017547608,
      "backward_entropy": 0.004614931013849046,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.30246639251709,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0534663125872612,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07917046546936035,
      "backward_entropy": 0.017103650503688388,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.681456089019775,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05349772050976753,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07916251420974732,
      "backward_entropy": 0.0171053773827023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.726755142211914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05352797731757164,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07915470004081726,
      "backward_entropy": 0.017107180423206754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.592397689819336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05355854704976082,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07914683222770691,
      "backward_entropy": 0.0046007757385571795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.15904426574707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05358804017305374,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0791391134262085,
      "backward_entropy": 0.01711372368865543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.498571395874023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05362115055322647,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07913087010383606,
      "backward_entropy": 0.017117240362697177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.966744422912598,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053656864911317825,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07912226915359497,
      "backward_entropy": 0.004592131823301315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.798829078674316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053690314292907715,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07911399006843567,
      "backward_entropy": 0.004589359793398116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.800667762756348,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05372432991862297,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07910562157630921,
      "backward_entropy": 0.00458662791384591,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.170185089111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05375756323337555,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07909734845161438,
      "backward_entropy": 0.004584009034766091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.76080894470215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05379071086645126,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07908909320831299,
      "backward_entropy": 0.017134292258156672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.802799701690674,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05382639169692993,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07908042669296264,
      "backward_entropy": 0.004578976167572869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.539496421813965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053859785199165344,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07907212972640991,
      "backward_entropy": 0.0045763105154037476,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.739160537719727,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053892429918050766,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07906389236450195,
      "backward_entropy": 0.00457375082704756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.126097679138184,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0539231076836586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07905598878860473,
      "backward_entropy": 0.0045711758236090345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.696712493896484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053954582661390305,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07904796600341797,
      "backward_entropy": 0.0045688483450147845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.621172904968262,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05398609861731529,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07903989553451538,
      "backward_entropy": 0.01715245180659824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.545825004577637,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05401768162846565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07903181314468384,
      "backward_entropy": 0.004564152823554145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.882641315460205,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054049283266067505,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07902373075485229,
      "backward_entropy": 0.004561874601576064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.5598297119140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05407966300845146,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07901585102081299,
      "backward_entropy": 0.004559906820456187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.329912185668945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054108306765556335,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07900822162628174,
      "backward_entropy": 0.004558160487148497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.765166282653809,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054137248545885086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07900053262710571,
      "backward_entropy": 0.004556356618801753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.951323509216309,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05416771396994591,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0789925992488861,
      "backward_entropy": 0.01717456512980991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.117773056030273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054197635501623154,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0789847731590271,
      "backward_entropy": 0.01717750562561883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.6283183097839355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05422772839665413,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07897687554359437,
      "backward_entropy": 0.01718119117948744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.368918418884277,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05425669997930527,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.0789691686630249,
      "backward_entropy": 0.07701238658693102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.906017303466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0542871356010437,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07896122932434083,
      "backward_entropy": 0.00454745234714614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.167313575744629,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054317671805620193,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07895324230194092,
      "backward_entropy": 0.017190885212686326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.304075241088867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054349515587091446,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07894500494003295,
      "backward_entropy": 0.004544542481501897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.826355934143066,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054379429668188095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0789371132850647,
      "backward_entropy": 0.0045430900322066415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.370386123657227,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05441005155444145,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07892907857894897,
      "backward_entropy": 0.01720077958371904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.328935623168945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05443945527076721,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0789212703704834,
      "backward_entropy": 0.0045396362741788225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.385157585144043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054467760026454926,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07891368269920349,
      "backward_entropy": 0.004537672632270389,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.249823093414307,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05449570342898369,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07890614867210388,
      "backward_entropy": 0.0045358650386333466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0709500312805176,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05452267453074455,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07889879941940307,
      "backward_entropy": 0.017205981744660273,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.294397354125977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05454757437109947,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07889187335968018,
      "backward_entropy": 0.017206731769773696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.234938621520996,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05457305535674095,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07888481020927429,
      "backward_entropy": 0.07701269785563152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.069948196411133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054599035531282425,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07887762784957886,
      "backward_entropy": 0.01720972028043535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.186908721923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05462366342544556,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07887073755264282,
      "backward_entropy": 0.0045270245108339525,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.043437004089355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0546506829559803,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07886332869529725,
      "backward_entropy": 0.004525822897752126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.995385646820068,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05467741936445236,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07885602116584778,
      "backward_entropy": 0.017216384410858154,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.932029724121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054703302681446075,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07884883880615234,
      "backward_entropy": 0.004523186633984248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.922774791717529,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054729606956243515,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07884159088134765,
      "backward_entropy": 0.01721976035171085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.81204605102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054755084216594696,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07883450984954835,
      "backward_entropy": 0.0045204295052422416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.8513875007629395,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05478101596236229,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07882730960845948,
      "backward_entropy": 0.01722235812081231,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.755237102508545,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054806146770715714,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07882030010223388,
      "backward_entropy": 0.004517659544944763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.636269569396973,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05483115836977959,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07881328463554382,
      "backward_entropy": 0.0045162799457709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.662039279937744,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.054856643080711365,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07880618572235107,
      "backward_entropy": 0.01722451713350084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.615339279174805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05488195642828941,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07879912853240967,
      "backward_entropy": 0.017224851581785414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.245694160461426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05490713194012642,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07879209518432617,
      "backward_entropy": 0.01722573737303416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.519836902618408,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05493393540382385,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07878469228744507,
      "backward_entropy": 0.0172268764840232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.942376136779785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05496043339371681,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07877733707427978,
      "backward_entropy": 0.01722813645998637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.128460884094238,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05498898774385452,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0787695050239563,
      "backward_entropy": 0.00450918326775233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.365649700164795,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055018216371536255,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07876153588294983,
      "backward_entropy": 0.004508323139614529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.141765594482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05504687502980232,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07875365018844604,
      "backward_entropy": 0.00450751930475235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.261202812194824,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055075593292713165,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07874575853347779,
      "backward_entropy": 0.017232976026005216,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.20999002456665,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05510377511382103,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07873799800872802,
      "backward_entropy": 0.004505860308806102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.369880199432373,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05513147637248039,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07873032093048096,
      "backward_entropy": 0.004505082964897156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.111506938934326,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05515814945101738,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07872291803359985,
      "backward_entropy": 0.01723557710647583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.361817359924316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05518447607755661,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07871555089950562,
      "backward_entropy": 0.00450343100561036,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.520828247070312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05521220341324806,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07870782017707825,
      "backward_entropy": 0.01723692648940616,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.222082138061523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055240608751773834,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07869993448257447,
      "backward_entropy": 0.017237590418921575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.91371488571167,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.055267903953790665,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07869232296943665,
      "backward_entropy": 0.07701332039303249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.730947494506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05529475957155228,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07868481874465942,
      "backward_entropy": 0.004500581572453181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.517550468444824,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055323489010334015,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07867677807807923,
      "backward_entropy": 0.00449992385175493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.762759685516357,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05535219609737396,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07866877317428589,
      "backward_entropy": 0.004499319526884291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.068886756896973,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05538030341267586,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07866086959838867,
      "backward_entropy": 0.0044987524549166364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.326897621154785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05540899932384491,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07865284681320191,
      "backward_entropy": 0.00449828017089102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.915961265563965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05543765053153038,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0786448061466217,
      "backward_entropy": 0.00449783561958207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.558960437774658,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05546680837869644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.078636634349823,
      "backward_entropy": 0.004497385687298245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.881428241729736,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05549529567360878,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07862859964370728,
      "backward_entropy": 0.0044968898097674055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.380897521972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0555226169526577,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07862084507942199,
      "backward_entropy": 0.004496347159147263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.803987979888916,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055553872138261795,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07861201763153076,
      "backward_entropy": 0.017239666647381253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.352978706359863,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05558367446064949,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07860356569290161,
      "backward_entropy": 0.004495547049575382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.3021016120910645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.055612713098526,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07859534025192261,
      "backward_entropy": 0.004495076421234343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1262803077697754,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0556410551071167,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07858723402023315,
      "backward_entropy": 0.004494624005423652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.309112548828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05566766858100891,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07857962846755981,
      "backward_entropy": 0.017237590418921575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.158633708953857,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055694907903671265,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07857180833816528,
      "backward_entropy": 0.017236736085679796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.16828441619873,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05572161078453064,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07856414318084717,
      "backward_entropy": 0.017235706249872845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.129626274108887,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05574890971183777,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07855629324913024,
      "backward_entropy": 0.004492589996920692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.014767646789551,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05577781796455383,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0785479187965393,
      "backward_entropy": 0.0044921665555901,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.457592964172363,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055806003510951996,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07853972315788268,
      "backward_entropy": 0.01723234024312761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.397117614746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05583406984806061,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07853156328201294,
      "backward_entropy": 0.017231027285257976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.271819114685059,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05586201697587967,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07852343320846558,
      "backward_entropy": 0.017229626576105755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.45502507686615,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05589091777801514,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07851496934890748,
      "backward_entropy": 0.004490497211615245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.218197822570801,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05591747164726257,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0785072386264801,
      "backward_entropy": 0.004490043967962265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.593753814697266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05594403296709061,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07849940061569213,
      "backward_entropy": 0.01722466614511278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.682992935180664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05597111955285072,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07849145531654358,
      "backward_entropy": 0.017222788598802354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.818972110748291,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055997613817453384,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07848361134529114,
      "backward_entropy": 0.01722084979216258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.1967267990112305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0560225211083889,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07847626805305481,
      "backward_entropy": 0.017218708992004395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7774994373321533,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056046515703201294,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07846916317939759,
      "backward_entropy": 0.017216424147288006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7583391666412354,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056069161742925644,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07846249341964721,
      "backward_entropy": 0.004487377901871999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.480416774749756,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0560905896127224,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07845619916915894,
      "backward_entropy": 0.004486850152413051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.444363594055176,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05611196160316467,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.0784498929977417,
      "backward_entropy": 0.01720837586455875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.704242706298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05613327771425247,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07844357490539551,
      "backward_entropy": 0.01720563405089908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.030832290649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05615350231528282,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07843758463859558,
      "backward_entropy": 0.004485439095232222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.005947589874268,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056173257529735565,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07843177318572998,
      "backward_entropy": 0.004484981298446655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.9631757736206055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056192584335803986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07842609286308289,
      "backward_entropy": 0.004484512739711338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9558680057525635,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05621306598186493,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07841997742652893,
      "backward_entropy": 0.017193723056051467,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9307892322540283,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05623304098844528,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07841407060623169,
      "backward_entropy": 0.017190640171368916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.208423614501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05625256150960922,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07840826511383056,
      "backward_entropy": 0.0044834497902128435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.88179874420166,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05627217888832092,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07840237617492676,
      "backward_entropy": 0.004483213027318318,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.429684638977051,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0562913678586483,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07839663028717041,
      "backward_entropy": 0.0171823071108924,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.943859100341797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05631117895245552,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07839068174362182,
      "backward_entropy": 0.004482796622647179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.075305461883545,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056332554668188095,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07838422060012817,
      "backward_entropy": 0.004482624431451161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.56005859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056353822350502014,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07837773561477661,
      "backward_entropy": 0.004482501910792457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7525713443756104,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05637599155306816,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07837096452713013,
      "backward_entropy": 0.004482470452785492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.209949016571045,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0563974604010582,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07836434841156006,
      "backward_entropy": 0.01716830829779307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.1650166511535645,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05641929805278778,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07835763692855835,
      "backward_entropy": 0.004482501910792457,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.567285537719727,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05644145980477333,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07835075855255128,
      "backward_entropy": 0.07701359192530315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.285800457000732,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05646489933133125,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07834343910217285,
      "backward_entropy": 0.01716133952140808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.022317886352539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05648897960782051,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07833580374717712,
      "backward_entropy": 0.004483090506659614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.778487205505371,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05651313439011574,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07832821011543274,
      "backward_entropy": 0.004483324372106128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.924849033355713,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056536853313446045,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07832071781158448,
      "backward_entropy": 0.004483529676993688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.051854133605957,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05656066909432411,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07831315994262696,
      "backward_entropy": 0.004483795000447167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4963860511779785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056585054844617844,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07830535173416138,
      "backward_entropy": 0.017149719927046034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.3120832443237305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056608475744724274,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07829789519309997,
      "backward_entropy": 0.017147423492537603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.882743835449219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05663053318858147,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07829089164733886,
      "backward_entropy": 0.004484713077545166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.965826034545898,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05665329471230507,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07828366756439209,
      "backward_entropy": 0.004484917140669293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.770459175109863,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05667716637253761,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07827597260475158,
      "backward_entropy": 0.017138962944348652,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.474666118621826,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056701548397541046,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07826809883117676,
      "backward_entropy": 0.004485450271103118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.654901504516602,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056725431233644485,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07826035022735596,
      "backward_entropy": 0.004485812452104356,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.397871971130371,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056749798357486725,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0782524049282074,
      "backward_entropy": 0.004486149383915795,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.899996757507324,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05677364766597748,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0782446026802063,
      "backward_entropy": 0.0044865306052896715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.318633556365967,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05679986998438835,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07823586463928223,
      "backward_entropy": 0.01712523897488912,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.626448631286621,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05682538077235222,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07822736501693725,
      "backward_entropy": 0.004487564580308067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.2947163581848145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056852586567401886,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07821817994117737,
      "backward_entropy": 0.0044881126119030845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.193848133087158,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056879427284002304,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07820909023284912,
      "backward_entropy": 0.01711809304025438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.191523551940918,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056905459612607956,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07820026874542237,
      "backward_entropy": 0.004489150312211778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.113153457641602,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05693122372031212,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07819154858589172,
      "backward_entropy": 0.017112336224979825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.074417591094971,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05695628374814987,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07818306684494018,
      "backward_entropy": 0.004490150345696343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.0458502769470215,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0569806843996048,
      "trajectory_length": 11,
      "branch_chosen": 0,
      "forward_entropy": 0.07817479372024536,
      "backward_entropy": 0.07701396279864842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.999080181121826,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05700493976473808,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07816658616065979,
      "backward_entropy": 0.004490891264544593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.914754867553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05702906474471092,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07815847396850586,
      "backward_entropy": 0.004491151620944341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9226572513580322,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05705488845705986,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07814950942993164,
      "backward_entropy": 0.004491178939739863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9133291244506836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05707995593547821,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.0781408667564392,
      "backward_entropy": 0.004491221573617723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8860526084899902,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05710387974977493,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07813265323638915,
      "backward_entropy": 0.01708480053477817,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.672919750213623,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057126760482788086,
      "trajectory_length": 11,
      "branch_chosen": 2,
      "forward_entropy": 0.07812490463256835,
      "backward_entropy": 0.004491230265961753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.665776252746582,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05715050548315048,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07811669111251832,
      "backward_entropy": 0.017074833313624065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.611297130584717,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05717456713318825,
      "trajectory_length": 11,
      "branch_chosen": 1,
      "forward_entropy": 0.07810834646224976,
      "backward_entropy": 0.017070109645525616,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 6.7530770528316495,
    "avg_log_Z": -0.05596572011709213,
    "success_rate": 1.0,
    "avg_reward": 55.3,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.03,
      "1": 0.4,
      "2": 0.57
    },
    "avg_forward_entropy": 0.07848572409152985,
    "avg_backward_entropy": 0.011749700444440046,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}