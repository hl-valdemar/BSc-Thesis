{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 0.45454545454545453
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.1386169195175171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.1386169195175171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.1386169195175171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13862650394439696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.1386169195175171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.7736358642578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18295049667358398,
      "backward_entropy": 0.13859789371490477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.38864135742188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -9.99999901978299e-05,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293917179107666,
      "backward_entropy": 0.13861795663833618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.09840393066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00019947138207498938,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18292750914891562,
      "backward_entropy": 0.13861889839172364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.72068786621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0002992924419231713,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1829154094060262,
      "backward_entropy": 0.1386197566986084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.5983123779297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00039870516047813,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18290293216705322,
      "backward_entropy": 0.13862897157669068,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.05210876464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.000498746638186276,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1828900376955668,
      "backward_entropy": 0.13862922191619872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.06163024902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005981758586131036,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287678559621176,
      "backward_entropy": 0.13858246803283691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.10484313964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0006982904160395265,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18286307652791342,
      "backward_entropy": 0.13857994079589844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.90921020507812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0007978607900440693,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18284900983174643,
      "backward_entropy": 0.13862942457199096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.13162231445312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0008969019982032478,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1828346053759257,
      "backward_entropy": 0.13862295150756837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.5406951904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.000994742731563747,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1828199028968811,
      "backward_entropy": 0.1386231780052185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 225.06283569335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0010931201977655292,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18280476331710815,
      "backward_entropy": 0.13862329721450806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.98887634277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0011924043064936996,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18278916676839194,
      "backward_entropy": 0.13862321376800538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.0974884033203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0012884116731584072,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18277345101038614,
      "backward_entropy": 0.138629150390625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.3392791748047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0013844608329236507,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18275735775629678,
      "backward_entropy": 0.138629150390625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.71031188964844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0014797232579439878,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18274096647898355,
      "backward_entropy": 0.138629150390625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.10621643066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0015759337693452835,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18272411823272705,
      "backward_entropy": 0.13862211704254152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.60777282714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001672327984124422,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18270681301752725,
      "backward_entropy": 0.13862165212631225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 258.4913635253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.001769510330632329,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1826888918876648,
      "backward_entropy": 0.13862110376358033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.08438110351562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018691079458221793,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1826703945795695,
      "backward_entropy": 0.1385582685470581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.4990234375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0019679637625813484,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18265159924825033,
      "backward_entropy": 0.1386293888092041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.3975372314453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002065476728603244,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18263254563013712,
      "backward_entropy": 0.13861857652664183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.1924591064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0021635578013956547,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18261297543843588,
      "backward_entropy": 0.1386176347732544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.562744140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002261993708088994,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18259302775065103,
      "backward_entropy": 0.13861660957336425,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.83753967285156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002360669197514653,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18257288138071695,
      "backward_entropy": 0.13855421543121338,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 208.4558868408203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0024587283842265606,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1825523575146993,
      "backward_entropy": 0.1386293888092041,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.5561065673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002557090949267149,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18253139654795328,
      "backward_entropy": 0.13855273723602296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.7601776123047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002655862597748637,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18250993887583414,
      "backward_entropy": 0.13861172199249266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 257.41644287109375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0027533420361578465,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18248820304870605,
      "backward_entropy": 0.13862931728363037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.00987243652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.002853595884516835,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18246535460154215,
      "backward_entropy": 0.1385495662689209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.24118041992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0029529077000916004,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18244210879007974,
      "backward_entropy": 0.13854846954345704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.7764434814453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0030495962128043175,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18241870403289795,
      "backward_entropy": 0.13862931728363037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.82167053222656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003144037676975131,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18239512046178183,
      "backward_entropy": 0.1386293053627014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.5398406982422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003241294529289007,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1823706030845642,
      "backward_entropy": 0.13854434490203857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 240.80276489257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0033381949178874493,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18234564860661825,
      "backward_entropy": 0.13854262828826905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.446533203125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00343725411221385,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18231985966364542,
      "backward_entropy": 0.13862937688827515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.6756134033203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003533559152856469,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.182293971379598,
      "backward_entropy": 0.13853960037231444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 206.9524688720703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0036268290132284164,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1822681427001953,
      "backward_entropy": 0.13862941265106202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 254.63670349121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0037211254239082336,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1822416385014852,
      "backward_entropy": 0.13859777450561522,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.63166046142578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.003819203469902277,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1822140614191691,
      "backward_entropy": 0.13862943649291992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.3878936767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003912567161023617,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18218676249186197,
      "backward_entropy": 0.13853204250335693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.20675659179688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004003905691206455,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18215932448705038,
      "backward_entropy": 0.1385939598083496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 207.09451293945312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004094196483492851,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1821315884590149,
      "backward_entropy": 0.13852880001068116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.36007690429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0041857934556901455,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1821031173070272,
      "backward_entropy": 0.13858978748321532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.46527099609375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0042788865976035595,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18207391103108725,
      "backward_entropy": 0.13858770132064818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 223.5109405517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004368734546005726,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18204480409622192,
      "backward_entropy": 0.1385251522064209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.7090606689453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004460920579731464,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18201476335525513,
      "backward_entropy": 0.13852431774139404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 239.1276092529297,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004551301244646311,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18198458353678384,
      "backward_entropy": 0.13862941265106202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.6209716796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004645067732781172,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18195321162541708,
      "backward_entropy": 0.13857660293579102,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.41368103027344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.004736848175525665,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18192172050476074,
      "backward_entropy": 0.1385735273361206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 255.91189575195312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004826669115573168,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18189013004302979,
      "backward_entropy": 0.13862931728363037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.15975952148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004920823499560356,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18185706933339438,
      "backward_entropy": 0.1385201930999756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.0522918701172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0050125583074986935,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1818239688873291,
      "backward_entropy": 0.13862922191619872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 222.179931640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005101616960018873,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18179072936375937,
      "backward_entropy": 0.13862917423248292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.315185546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005193366203457117,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18175639708836874,
      "backward_entropy": 0.1385549545288086,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.6786346435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005284299608319998,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1817222237586975,
      "backward_entropy": 0.13855105638504028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.93031311035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005376925226300955,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18168717622756958,
      "backward_entropy": 0.13862907886505127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 238.04347229003906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005467291455715895,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18165196975072226,
      "backward_entropy": 0.13851509094238282,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.58676147460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005561301950365305,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18161539236704508,
      "backward_entropy": 0.13851392269134521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.47994995117188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005653184838593006,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18157869577407837,
      "backward_entropy": 0.13862907886505127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.86561584472656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005744151305407286,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18154162168502808,
      "backward_entropy": 0.1385119676589966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.58970642089844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005835830233991146,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1815038522084554,
      "backward_entropy": 0.13862910270690917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.74082946777344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005930435378104448,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1814648707707723,
      "backward_entropy": 0.13862917423248292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.86558532714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006023741792887449,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18142555157343546,
      "backward_entropy": 0.13851964473724365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.88723754882812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006118179764598608,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18138529856999716,
      "backward_entropy": 0.1385150671005249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.65968322753906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00621472392231226,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18134387334187826,
      "backward_entropy": 0.13850464820861816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.6949005126953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00631091371178627,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18130183219909668,
      "backward_entropy": 0.13862919807434082,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.68124389648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006405823864042759,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18125949303309122,
      "backward_entropy": 0.13850396871566772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.00001525878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006502808071672916,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1812159220377604,
      "backward_entropy": 0.13850393295288085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.76145935058594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00659832963719964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18117205301920572,
      "backward_entropy": 0.13848388195037842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.58644104003906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006693990435451269,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18112754821777344,
      "backward_entropy": 0.13847682476043702,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.7632598876953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0067907292395830154,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18108208974202475,
      "backward_entropy": 0.1386289119720459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.74244689941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006888153031468391,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18103567759195963,
      "backward_entropy": 0.13846147060394287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.26969909667969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006987302098423243,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18098803361256918,
      "backward_entropy": 0.1384523630142212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.1848602294922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0070788622833788395,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1809418797492981,
      "backward_entropy": 0.13862855434417726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 237.97073364257812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00717288488522172,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18089377880096436,
      "backward_entropy": 0.1384307861328125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 237.88990783691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00727001391351223,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1808433731396993,
      "backward_entropy": 0.1385064482688904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.4830780029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007369880564510822,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18079102039337158,
      "backward_entropy": 0.13840723037719727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.18345642089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0074679250828921795,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18073825041453043,
      "backward_entropy": 0.1383941650390625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.09095764160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007564627565443516,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18068504333496094,
      "backward_entropy": 0.1386277437210083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.9046173095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007656480185687542,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18063261111577353,
      "backward_entropy": 0.13851051330566405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 221.06568908691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007750939577817917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1805793046951294,
      "backward_entropy": 0.13835382461547852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.5861053466797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00784743670374155,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18052450815836588,
      "backward_entropy": 0.1385115385055542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.29347229003906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007943542674183846,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18046887715657553,
      "backward_entropy": 0.13862723112106323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.4827880859375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008038409985601902,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1804124116897583,
      "backward_entropy": 0.13862712383270265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.90780639648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008130667731165886,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1803557276725769,
      "backward_entropy": 0.13829314708709717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.72531127929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008226546458899975,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18029681841532388,
      "backward_entropy": 0.13827747106552124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.10740661621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008322007954120636,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18023695548375449,
      "backward_entropy": 0.13851666450500488,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.4663848876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008413858711719513,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18017741044362387,
      "backward_entropy": 0.13824162483215333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.6462860107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008502362295985222,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18011701107025146,
      "backward_entropy": 0.13852005004882811,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.95016479492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0085875503718853,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18005645275115967,
      "backward_entropy": 0.13852261304855346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.3760528564453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008670172654092312,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17999577522277832,
      "backward_entropy": 0.13817436695098878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.6049041748047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008753827773034573,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1799333691596985,
      "backward_entropy": 0.13862302303314208,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.08677673339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008836142718791962,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17987056573232016,
      "backward_entropy": 0.13853023052215577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.66891479492188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008914596401154995,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17980974912643433,
      "backward_entropy": 0.13862075805664062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.6111602783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008992467075586319,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1797482967376709,
      "backward_entropy": 0.13806734085083008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.29000854492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009073266759514809,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17968459924062094,
      "backward_entropy": 0.13804047107696532,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.54647827148438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00915431883186102,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1796201467514038,
      "backward_entropy": 0.13854029178619384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.3062973022461,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009236756712198257,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17955493927001953,
      "backward_entropy": 0.13861740827560426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.7159881591797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009314451366662979,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17949088414510092,
      "backward_entropy": 0.1385441541671753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.25552368164062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009395051747560501,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1794246037801107,
      "backward_entropy": 0.13861602544784546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 218.22653198242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009475655853748322,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17935721079508463,
      "backward_entropy": 0.1385481357574463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.7117919921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009560402482748032,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17928711573282877,
      "backward_entropy": 0.1378700017929077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.29783630371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009644837118685246,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17921612660090128,
      "backward_entropy": 0.13861517906188964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.57215881347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009730564430356026,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1791436274846395,
      "backward_entropy": 0.13855149745941162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.41419982910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00981589499861002,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17907039324442545,
      "backward_entropy": 0.13855286836624145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.2104949951172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00989991519600153,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1789969801902771,
      "backward_entropy": 0.13855438232421874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.06785583496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009984046220779419,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17892257372538248,
      "backward_entropy": 0.13855578899383544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.3892059326172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010070490650832653,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1788459817568461,
      "backward_entropy": 0.1386149764060974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 235.390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010158906690776348,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17876736323038736,
      "backward_entropy": 0.13855867385864257,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.45248413085938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0102514224126935,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17868558565775552,
      "backward_entropy": 0.13861498832702637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.87521362304688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010346373543143272,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1786014437675476,
      "backward_entropy": 0.13861503601074218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.18905639648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010443171486258507,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1785154938697815,
      "backward_entropy": 0.13755254745483397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.7104949951172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010541514493525028,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1784287691116333,
      "backward_entropy": 0.1386171817779541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 186.62832641601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010637963190674782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17834224303563437,
      "backward_entropy": 0.13856133222579955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 249.7443389892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01073417067527771,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17825591564178467,
      "backward_entropy": 0.13746278285980223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.67141723632812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01083497516810894,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17816664775212607,
      "backward_entropy": 0.13743003606796264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 201.75169372558594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010931387543678284,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17807841300964355,
      "backward_entropy": 0.13861827850341796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.924072265625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011028874665498734,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17798839012781778,
      "backward_entropy": 0.13861804008483886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 232.7061767578125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011122720316052437,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1778993010520935,
      "backward_entropy": 0.13856761455535888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.34207153320312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011220431886613369,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17780701319376627,
      "backward_entropy": 0.13856887817382812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 234.1002960205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011316492222249508,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1777142882347107,
      "backward_entropy": 0.137220561504364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 231.56893920898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01141577772796154,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1776184638341268,
      "backward_entropy": 0.13857266902923585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.7723846435547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011518497951328754,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1775196592013041,
      "backward_entropy": 0.13713086843490602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.7400360107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011621682904660702,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17741936445236206,
      "backward_entropy": 0.13857460021972656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.99262237548828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01172386109828949,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17731829484303793,
      "backward_entropy": 0.13857595920562743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.9790802001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011819304898381233,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17722010612487793,
      "backward_entropy": 0.13857793807983398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 216.48963928222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011911377310752869,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1771228313446045,
      "backward_entropy": 0.13857994079589844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.04061889648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012006167322397232,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17702259620030722,
      "backward_entropy": 0.1385817050933838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.67820739746094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012097859755158424,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17692331473032633,
      "backward_entropy": 0.13861043453216554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.8009033203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012187528423964977,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17682401339213052,
      "backward_entropy": 0.13858506679534913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.55867767333984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012275593355298042,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17672459284464517,
      "backward_entropy": 0.13670735359191893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 199.88619995117188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012359305284917355,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1766265630722046,
      "backward_entropy": 0.13860507011413575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.17733764648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012445530854165554,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17652563254038492,
      "backward_entropy": 0.1365760326385498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.96426391601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012530094012618065,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17642438411712646,
      "backward_entropy": 0.13859338760375978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 215.24681091308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012614668346941471,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17632184425989786,
      "backward_entropy": 0.13859549760818482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.39451599121094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012702937237918377,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17621554931004843,
      "backward_entropy": 0.13859702348709108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 200.5828857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012790944427251816,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1761082410812378,
      "backward_entropy": 0.13630127906799316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.08572387695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012880736030638218,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1759981910387675,
      "backward_entropy": 0.13623206615447997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 198.77601623535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012972737662494183,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17588547865549722,
      "backward_entropy": 0.13860056400299073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.24478149414062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01306651160120964,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17577022314071655,
      "backward_entropy": 0.13859269618988038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 230.17135620117188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013160312548279762,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17565343777338663,
      "backward_entropy": 0.13604540824890138,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 197.64903259277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013258048333227634,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1755325198173523,
      "backward_entropy": 0.1359861135482788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.19483184814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013357139192521572,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17540941635767618,
      "backward_entropy": 0.13593214750289917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.27940368652344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013450004160404205,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17528990904490152,
      "backward_entropy": 0.13587031364440919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.5457763671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013541056774556637,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17517054080963135,
      "backward_entropy": 0.13860249519348145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.33642578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013631031848490238,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17505029837290445,
      "backward_entropy": 0.1357342004776001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.51670837402344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01371798850595951,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17493108908335367,
      "backward_entropy": 0.13860464096069336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.4450454711914,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013805682770907879,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1748096744219462,
      "backward_entropy": 0.13858935832977295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.16880798339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013889603316783905,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17469072341918945,
      "backward_entropy": 0.13860743045806884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.17414093017578,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013969913125038147,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17457425594329834,
      "backward_entropy": 0.1385842442512512,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.00865173339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014045748859643936,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17446033159891763,
      "backward_entropy": 0.13528153896331788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.00682067871094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014122634194791317,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17434422175089517,
      "backward_entropy": 0.1351797342300415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.7127227783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0141992699354887,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17422711849212646,
      "backward_entropy": 0.13507901430130004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.91477966308594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014275475405156612,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1741089423497518,
      "backward_entropy": 0.13857027292251586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.05203247070312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014349760487675667,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17399082581202188,
      "backward_entropy": 0.1385658025741577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.92117309570312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014422542415559292,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17387264966964722,
      "backward_entropy": 0.1347405195236206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.762939453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014499243348836899,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17374980449676514,
      "backward_entropy": 0.13462860584259034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.93405151367188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014578588306903839,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17362372080485025,
      "backward_entropy": 0.13862051963806152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.19920349121094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01465825829654932,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1734956900278727,
      "backward_entropy": 0.13442411422729492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.73573303222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014737234450876713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17336692412694296,
      "backward_entropy": 0.1386220097541809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.01158142089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014816590584814548,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1732362707455953,
      "backward_entropy": 0.13862287998199463,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.66786193847656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014894404448568821,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17310607433319092,
      "backward_entropy": 0.1386236310005188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.70640563964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01496916450560093,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17297744750976562,
      "backward_entropy": 0.13396596908569336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.7300567626953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015036956407129765,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17285404602686563,
      "backward_entropy": 0.13862565755844117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.85377502441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015106191858649254,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17272762457529703,
      "backward_entropy": 0.13367931842803954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 196.17007446289062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015176954679191113,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17259836196899414,
      "backward_entropy": 0.1386275053024292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 181.82127380371094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015251852571964264,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17246381441752115,
      "backward_entropy": 0.13851869106292725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.53660583496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015328822657465935,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1723257303237915,
      "backward_entropy": 0.13862849473953248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.7770767211914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015410800464451313,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.172181765238444,
      "backward_entropy": 0.13313498497009277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.69630432128906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015487974509596825,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17204147577285767,
      "backward_entropy": 0.13300139904022218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.2216796875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015567212365567684,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17189755042394003,
      "backward_entropy": 0.13850793838500977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.189453125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015646986663341522,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1717517375946045,
      "backward_entropy": 0.13850520849227904,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.4008331298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01572434790432453,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17160677909851074,
      "backward_entropy": 0.1325746774673462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.35302734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015800315886735916,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17146233717600504,
      "backward_entropy": 0.1384960412979126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 178.56155395507812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015877615660429,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17131447792053223,
      "backward_entropy": 0.13227441310882568,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 179.20486450195312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01595744490623474,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17116053899129233,
      "backward_entropy": 0.1321418523788452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.65238189697266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016039330512285233,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17100210984547934,
      "backward_entropy": 0.13201439380645752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 180.09788513183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016117475926876068,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17084556818008423,
      "backward_entropy": 0.13862918615341185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.15989685058594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01619753986597061,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17068483432133993,
      "backward_entropy": 0.138488245010376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.9233856201172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01627843640744686,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17052157719930014,
      "backward_entropy": 0.13848689794540406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.0592498779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016361521556973457,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17035452524820963,
      "backward_entropy": 0.13862872123718262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.68258666992188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01644481159746647,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17018540700276694,
      "backward_entropy": 0.13862857818603516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.81887817382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016525818035006523,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17001728216807047,
      "backward_entropy": 0.13114078044891359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.09732055664062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016604997217655182,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16985003153483072,
      "backward_entropy": 0.13848433494567872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.82798767089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016682427376508713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1696836551030477,
      "backward_entropy": 0.1386276364326477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.8102569580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016759758815169334,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16951616605122885,
      "backward_entropy": 0.13862720727920533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 224.440185546875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01683780923485756,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16934605439503989,
      "backward_entropy": 0.13047879934310913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.23690795898438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016921814531087875,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16916757822036743,
      "backward_entropy": 0.13862606287002563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.61738586425781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017004840075969696,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16898890336354574,
      "backward_entropy": 0.13014286756515503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.37417602539062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01708289049565792,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16881489753723145,
      "backward_entropy": 0.13862428665161133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 192.30931091308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017160464078187943,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16863999764124551,
      "backward_entropy": 0.13862297534942628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.2543182373047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01724175177514553,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16845935583114624,
      "backward_entropy": 0.13862184286117554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.38294982910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01732279174029827,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1682781974474589,
      "backward_entropy": 0.13862075805664062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.02743530273438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017402954399585724,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16809654235839844,
      "backward_entropy": 0.12920022010803223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.7607421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017481302842497826,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16791603962580362,
      "backward_entropy": 0.1290028214454651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.0596466064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017559166997671127,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16773486137390137,
      "backward_entropy": 0.12879517078399658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.6951141357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017635680735111237,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16755437850952148,
      "backward_entropy": 0.128586745262146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.5845489501953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017710762098431587,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16737459103266397,
      "backward_entropy": 0.13861070871353148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.16293334960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017784561961889267,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16719533999760947,
      "backward_entropy": 0.1281459093093872,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.6269989013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017851974815130234,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16702383756637573,
      "backward_entropy": 0.12790315151214598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.33062744140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017919110134243965,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16685175895690918,
      "backward_entropy": 0.13859951496124268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.03585815429688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017982153221964836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16668460766474405,
      "backward_entropy": 0.1274205446243286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.0669708251953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018047355115413666,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16651256879170737,
      "backward_entropy": 0.12716668844223022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.83677673339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018116431310772896,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1663341522216797,
      "backward_entropy": 0.13858480453491212,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 220.4556427001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018185200169682503,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16615535815556845,
      "backward_entropy": 0.13858011960983277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.07789611816406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018261319026350975,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16596555709838867,
      "backward_entropy": 0.12649219036102294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.27146911621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01833597756922245,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16577659050623575,
      "backward_entropy": 0.12627100944519043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.42276000976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018408164381980896,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1655901869138082,
      "backward_entropy": 0.1385671854019165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.9708023071289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01848079450428486,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16540205478668213,
      "backward_entropy": 0.13856163024902343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 219.1277313232422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018551502376794815,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16521633664766947,
      "backward_entropy": 0.12555961608886718,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.75918579101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01862950064241886,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16501973072687784,
      "backward_entropy": 0.13855140209197997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.21351623535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01870609261095524,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1648242473602295,
      "backward_entropy": 0.13854646682739258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.6459503173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018778523430228233,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16463368137677512,
      "backward_entropy": 0.12488377094268799,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.93508911132812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018856730312108994,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16443398594856262,
      "backward_entropy": 0.13835911750793456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.64564514160156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018935715779662132,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16423179705937704,
      "backward_entropy": 0.13835766315460205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.9787139892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01901380904018879,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16402926047643027,
      "backward_entropy": 0.1385211706161499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.6616973876953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01909259334206581,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16382439931233725,
      "backward_entropy": 0.12385721206665039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.69020080566406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01917266473174095,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16361706455548605,
      "backward_entropy": 0.12358131408691406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.2125244140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01925252377986908,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16340951124827066,
      "backward_entropy": 0.12330634593963623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.68206787109375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01933460496366024,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16319791475931802,
      "backward_entropy": 0.1230326771736145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.99628448486328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019414562731981277,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.162988543510437,
      "backward_entropy": 0.12273933887481689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.87615966796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019491782411932945,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16278292735417685,
      "backward_entropy": 0.12244373559951782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.69798278808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019568542018532753,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16257663567860922,
      "backward_entropy": 0.1384548783302307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.24400329589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.019639622420072556,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16237803300221762,
      "backward_entropy": 0.13843973875045776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.53174591064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01971258968114853,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16217573483784994,
      "backward_entropy": 0.12144532203674316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 187.72862243652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01978353224694729,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16197619835535684,
      "backward_entropy": 0.13840935230255128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.80812072753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019858699291944504,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16176901261011759,
      "backward_entropy": 0.12077411413192748,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.47539520263672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019935021176934242,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16155902544657388,
      "backward_entropy": 0.12043259143829346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.78772735595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02000921592116356,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16135247548421225,
      "backward_entropy": 0.13836255073547363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.26058197021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020078115165233612,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16115369399388632,
      "backward_entropy": 0.1197373628616333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.77749633789062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020143035799264908,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1609614690144857,
      "backward_entropy": 0.11937642097473145,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 185.1732635498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020209340378642082,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16076648235321045,
      "backward_entropy": 0.11902234554290772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.60588836669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020280832424759865,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16056275367736816,
      "backward_entropy": 0.13828141689300538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.43160247802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020351100713014603,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1603600283463796,
      "backward_entropy": 0.13825982809066772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.20799255371094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020420607179403305,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16015777985254923,
      "backward_entropy": 0.13823652267456055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.32329559326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020490726456046104,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15995399157206217,
      "backward_entropy": 0.13821221590042115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.14934539794922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020557846873998642,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15975512067476907,
      "backward_entropy": 0.117184579372406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.48428344726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02062290534377098,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15955842534701029,
      "backward_entropy": 0.138156521320343,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.55229187011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02069162204861641,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15935476620992026,
      "backward_entropy": 0.1163698673248291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.24580383300781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020758239552378654,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15915407737096152,
      "backward_entropy": 0.11594523191452026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.39596557617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020821774378418922,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1589579383532206,
      "backward_entropy": 0.13806174993515014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.2496337890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020890865474939346,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15875258048375449,
      "backward_entropy": 0.13803181648254395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.14796447753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020961837843060493,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15854335824648538,
      "backward_entropy": 0.1146746277809143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.56343841552734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021032029762864113,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15833526849746704,
      "backward_entropy": 0.1379697322845459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.52012634277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02109605260193348,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1581373612085978,
      "backward_entropy": 0.13793138265609742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.3011703491211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02116011455655098,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1579392155011495,
      "backward_entropy": 0.1378921627998352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.38896179199219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021223021671175957,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15774296720822653,
      "backward_entropy": 0.13785167932510375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.01753997802734,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021282371133565903,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15755297740300497,
      "backward_entropy": 0.13796682357788087,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.3000946044922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0213403832167387,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1573643684387207,
      "backward_entropy": 0.11191613674163818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.8356475830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02140015922486782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15717138846715292,
      "backward_entropy": 0.13771388530731202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.29747009277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02146197110414505,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15697477261225382,
      "backward_entropy": 0.11095964908599854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 210.1193084716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021521123126149178,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15678219000498453,
      "backward_entropy": 0.13761905431747437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.99742889404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021589063107967377,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1565754016240438,
      "backward_entropy": 0.1375787377357483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.28682708740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02165648154914379,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15636932849884033,
      "backward_entropy": 0.1375359535217285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.52327728271484,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021720917895436287,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15616842110951742,
      "backward_entropy": 0.13783862590789794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.94486999511719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021781012415885925,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15597492456436157,
      "backward_entropy": 0.13743176460266113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.62359619140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02184007503092289,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15578287839889526,
      "backward_entropy": 0.1373720645904541,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.35725402832031,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02189522609114647,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15559691190719604,
      "backward_entropy": 0.13775928020477296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.58172035217285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02194996364414692,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15541136264801025,
      "backward_entropy": 0.1372380018234253,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.09709167480469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021995730698108673,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15524129072825113,
      "backward_entropy": 0.10637083053588867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.20917510986328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022042028605937958,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15507012605667114,
      "backward_entropy": 0.10578348636627197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.40738677978516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02208837866783142,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15489818652470908,
      "backward_entropy": 0.10518058538436889,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.93562316894531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022136883810162544,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15472286939620972,
      "backward_entropy": 0.1369174003601074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.6565933227539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022185243666172028,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15454748272895813,
      "backward_entropy": 0.10400135517120361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02223123610019684,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15437716245651245,
      "backward_entropy": 0.13748834133148194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.35852813720703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022280890494585037,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15420102079709372,
      "backward_entropy": 0.1374588966369629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.8382110595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022327737882733345,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1540298859278361,
      "backward_entropy": 0.1365823984146118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.53941345214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022380406036973,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15384885668754578,
      "backward_entropy": 0.13650269508361818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.13456726074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022440148517489433,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15365705887476602,
      "backward_entropy": 0.10118805170059204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.4561538696289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022496430203318596,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1534719467163086,
      "backward_entropy": 0.13636291027069092,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.43760681152344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02255238965153694,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15328733126322427,
      "backward_entropy": 0.13628923892974854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.2913360595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02260619029402733,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15310672918955484,
      "backward_entropy": 0.13620733022689818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 148.9602813720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022662075236439705,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15292235215504965,
      "backward_entropy": 0.09899601936340333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.45801544189453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.022721698507666588,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15273231267929077,
      "backward_entropy": 0.1360452651977539,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.4110107421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02278192900121212,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15254193544387817,
      "backward_entropy": 0.13596670627593993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 176.84890747070312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02284446358680725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15234865744908652,
      "backward_entropy": 0.09736261963844299,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.88819885253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02291271649301052,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15214677651723227,
      "backward_entropy": 0.09685859680175782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.68199157714844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022978266701102257,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15195034941037497,
      "backward_entropy": 0.09634932279586791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.15489196777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023045284673571587,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15175243218739828,
      "backward_entropy": 0.09584336280822754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.09729766845703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023116162046790123,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1515489419301351,
      "backward_entropy": 0.0953447937965393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.91007232666016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02318665198981762,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15134753783543906,
      "backward_entropy": 0.13554391860961915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.6014862060547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023256799206137657,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15114720662434897,
      "backward_entropy": 0.13546913862228394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.9125747680664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023329799994826317,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15094364682833353,
      "backward_entropy": 0.1354031205177307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.24784851074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02340169996023178,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15074257055918375,
      "backward_entropy": 0.09338510632514954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.60124969482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02347179688513279,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15054540832837424,
      "backward_entropy": 0.09286736845970153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.86016082763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02354181371629238,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15034972627957663,
      "backward_entropy": 0.13515883684158325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.01470947265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02360888570547104,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1501600742340088,
      "backward_entropy": 0.09183076620101929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.16653060913086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023678945377469063,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14996644854545593,
      "backward_entropy": 0.13497729301452638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.51681137084961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02374274842441082,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14978363116582236,
      "backward_entropy": 0.13486874103546143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 144.35110473632812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02380112186074257,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14961020151774088,
      "backward_entropy": 0.13474318981170655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.24066162109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023863380774855614,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1494312286376953,
      "backward_entropy": 0.13462648391723633,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.58480072021484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02392740733921528,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14924987157185873,
      "backward_entropy": 0.08901011943817139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.56240844726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.023987561464309692,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14907532930374146,
      "backward_entropy": 0.08841818571090698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.92070388793945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024049809202551842,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14889825383822122,
      "backward_entropy": 0.08783079385757446,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.2460479736328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024107221513986588,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1487296223640442,
      "backward_entropy": 0.13720741271972656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.99391174316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024168651551008224,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14855565627415976,
      "backward_entropy": 0.133992600440979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.73487854003906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.024229491129517555,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14838337898254395,
      "backward_entropy": 0.13717890977859498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.76309204101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024292385205626488,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14820883671442667,
      "backward_entropy": 0.13372304439544677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.2103500366211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024357430636882782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1480321486790975,
      "backward_entropy": 0.13359150886535645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.36585998535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024419602006673813,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.147860715786616,
      "backward_entropy": 0.13344271183013917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.63236999511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024486081674695015,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14768371979395548,
      "backward_entropy": 0.08377243280410766,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.75135803222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024551300331950188,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1475097338358561,
      "backward_entropy": 0.08315684795379638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.841796875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024612396955490112,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14734326799710593,
      "backward_entropy": 0.1329725503921509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.583251953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024671731516718864,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14718050758043924,
      "backward_entropy": 0.13279805183410645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.86497497558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024730348959565163,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14701964457829794,
      "backward_entropy": 0.13261823654174804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.75315856933594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024794546887278557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14685213565826416,
      "backward_entropy": 0.08061683177947998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.53226852416992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024855341762304306,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1466908057530721,
      "backward_entropy": 0.13229464292526244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.28602600097656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024909475818276405,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14654014507929483,
      "backward_entropy": 0.13209908008575438,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.96034240722656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024966038763523102,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14638644456863403,
      "backward_entropy": 0.07871065735816955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.24232482910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02502565272152424,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1462294061978658,
      "backward_entropy": 0.13171967267990112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 99.8476791381836,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025090115144848824,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1460664669672648,
      "backward_entropy": 0.13688886165618896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.29499053955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025153594091534615,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14590622981389365,
      "backward_entropy": 0.1313631057739258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.76334381103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02521437406539917,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1457510987917582,
      "backward_entropy": 0.07620384693145751,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 112.97833251953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02527329884469509,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14559975266456604,
      "backward_entropy": 0.13094743490219116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.76132202148438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025333072990179062,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1454482078552246,
      "backward_entropy": 0.07487152814865113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.6240692138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025396673008799553,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14529242118199667,
      "backward_entropy": 0.07424300909042358,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.28717803955078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025464080274105072,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14513303836186728,
      "backward_entropy": 0.13035824298858642,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.67125701904297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025533029809594154,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14497307936350504,
      "backward_entropy": 0.13018126487731935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.31895446777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025602037087082863,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1448149879773458,
      "backward_entropy": 0.13000247478485108,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.83338165283203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02566966786980629,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14466010530789694,
      "backward_entropy": 0.12981164455413818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.44152069091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025730649009346962,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14451555411020914,
      "backward_entropy": 0.07132201194763184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.80406188964844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.025792323052883148,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1443711519241333,
      "backward_entropy": 0.13670629262924194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.99246597290039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025854812934994698,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1442269285519918,
      "backward_entropy": 0.12913166284561156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.61091613769531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025912456214427948,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1440904140472412,
      "backward_entropy": 0.12887675762176515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.16973876953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025972679257392883,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1439514954884847,
      "backward_entropy": 0.06878026127815247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.61397933959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02603531815111637,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14381057024002075,
      "backward_entropy": 0.06816597580909729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.06080627441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026093123480677605,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14367697636286417,
      "backward_entropy": 0.06752439737319946,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.57723236083984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026154853403568268,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14353948831558228,
      "backward_entropy": 0.12787413597106934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.67296600341797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026216009631752968,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14340386788050333,
      "backward_entropy": 0.12761459350585938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.081787109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026275573298335075,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14327174425125122,
      "backward_entropy": 0.12734603881835938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.5443115234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02633623406291008,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14313948154449463,
      "backward_entropy": 0.12707893848419188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.41947555541992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0263994038105011,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14300525188446045,
      "backward_entropy": 0.12682244777679444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.86220359802246,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02645748294889927,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14287877082824707,
      "backward_entropy": 0.1265309453010559,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.65415954589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026508426293730736,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14276239275932312,
      "backward_entropy": 0.12620203495025634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.00598907470703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026562966406345367,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14264237880706787,
      "backward_entropy": 0.1258919954299927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.52116012573242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02661915123462677,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1425214409828186,
      "backward_entropy": 0.06189577579498291,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.8797607421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02667115442454815,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14240679144859314,
      "backward_entropy": 0.06124252676963806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.54624938964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02672625705599785,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14228946963946024,
      "backward_entropy": 0.12493314743041992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.50276184082031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026782898232340813,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14217126369476318,
      "backward_entropy": 0.12461464405059815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.8998031616211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026840850710868835,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14205268025398254,
      "backward_entropy": 0.059339553117752075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.03468322753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02689713053405285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.141937255859375,
      "backward_entropy": 0.05869007110595703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.8440170288086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026953518390655518,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1418226659297943,
      "backward_entropy": 0.12362194061279297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.081275939941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027008743956685066,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1417104403177897,
      "backward_entropy": 0.0574077308177948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.17290496826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027060220018029213,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14160370826721191,
      "backward_entropy": 0.05676591396331787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.91426849365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027110721915960312,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14149936040242514,
      "backward_entropy": 0.12252707481384277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.2156753540039,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02716212160885334,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1413948436578115,
      "backward_entropy": 0.05547720193862915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.41804504394531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02721410244703293,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14129079381624857,
      "backward_entropy": 0.12177650928497315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.45454406738281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02726350538432598,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14119072755177817,
      "backward_entropy": 0.05418267846107483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.6485595703125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02731100097298622,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1410936713218689,
      "backward_entropy": 0.1209564447402954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.80809783935547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027358314022421837,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14099743962287903,
      "backward_entropy": 0.052859872579574585,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.3891830444336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02740699052810669,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14090035359064737,
      "backward_entropy": 0.05223628282546997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.65812301635742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02745652198791504,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1408036152521769,
      "backward_entropy": 0.05161204934120178,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.69612884521484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027501462027430534,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14071277777353922,
      "backward_entropy": 0.05097190141677856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.50435638427734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027547813951969147,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1406215031941732,
      "backward_entropy": 0.11882438659667968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.807353973388672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027595452964305878,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14052976171175638,
      "backward_entropy": 0.049735790491104125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.597808837890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027637364342808723,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14044499397277832,
      "backward_entropy": 0.11791292428970337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.24987030029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027676943689584732,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14036319653193155,
      "backward_entropy": 0.11741855144500732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.34705352783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027715658769011497,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14028310775756836,
      "backward_entropy": 0.047844219207763675,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.89445495605469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027756713330745697,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1402009924252828,
      "backward_entropy": 0.04724453389644623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.5490493774414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02780110016465187,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14011645317077637,
      "backward_entropy": 0.04666876792907715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.68443298339844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02784857526421547,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14002968867619833,
      "backward_entropy": 0.11555722951889039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.38752746582031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.027902809903025627,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1399377187093099,
      "backward_entropy": 0.11517170667648316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.16361999511719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027957454323768616,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1398471792538961,
      "backward_entropy": 0.04508572816848755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.64493560791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02801148220896721,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13975809017817178,
      "backward_entropy": 0.04457010328769684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.74932098388672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02806767262518406,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1396677295366923,
      "backward_entropy": 0.11398804187774658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.87066650390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028121616691350937,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13958056767781576,
      "backward_entropy": 0.11357367038726807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.94313049316406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028174908831715584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13949508468310037,
      "backward_entropy": 0.11314899921417236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.7474136352539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028228985145688057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13941027720769247,
      "backward_entropy": 0.11273353099822998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.629051208496094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02828228659927845,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1393274466196696,
      "backward_entropy": 0.042050087451934816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.77955627441406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028333958238363266,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13924654324849448,
      "backward_entropy": 0.11187442541122436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.03413391113281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02838546596467495,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13916643460591635,
      "backward_entropy": 0.11143782138824462,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.29669189453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02843514084815979,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13908918698628744,
      "backward_entropy": 0.11098473072052002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.267154693603516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028483517467975616,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13901360829671225,
      "backward_entropy": 0.11052486896514893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.29193115234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028529241681098938,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13894112904866537,
      "backward_entropy": 0.11004511117935181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.857295989990234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028576740995049477,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1388679544130961,
      "backward_entropy": 0.03912170231342316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.64373779296875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028623200953006744,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1387959122657776,
      "backward_entropy": 0.10910973548889161,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.10723114013672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028670165687799454,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13872379064559937,
      "backward_entropy": 0.108638334274292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.3930206298828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028715847060084343,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13865394393603006,
      "backward_entropy": 0.10813953876495361,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.68770217895508,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02876882255077362,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13857871294021606,
      "backward_entropy": 0.1347139835357666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.157318115234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028820011764764786,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13850579659144083,
      "backward_entropy": 0.0368682473897934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.03931427001953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028866561129689217,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1384382446606954,
      "backward_entropy": 0.03639177083969116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.81217193603516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028916124254465103,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1383693814277649,
      "backward_entropy": 0.035934290289878844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.05788040161133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02896583452820778,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1383009652296702,
      "backward_entropy": 0.10575494766235352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.06671905517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02901414781808853,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13823451598485312,
      "backward_entropy": 0.10526158809661865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.37681579589844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029063012450933456,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13816750049591064,
      "backward_entropy": 0.13458807468414308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.70463562011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0291120782494545,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13810078303019205,
      "backward_entropy": 0.034205251932144166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.700676918029785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02916410192847252,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13803214828173319,
      "backward_entropy": 0.10382485389709473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.998680114746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029208866879343987,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1379708449045817,
      "backward_entropy": 0.10328514575958252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.67241668701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02925298921763897,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13791040579477945,
      "backward_entropy": 0.032928654551506044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.41510772705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02929784543812275,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13785005609194437,
      "backward_entropy": 0.032505196332931516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.636146545410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02934871055185795,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13778620958328247,
      "backward_entropy": 0.03210705518722534,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.39594268798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029398314654827118,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13772383332252502,
      "backward_entropy": 0.10123748779296875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.5788459777832,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029447978362441063,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1376625100771586,
      "backward_entropy": 0.10073908567428588,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.90435791015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02949635684490204,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1376030445098877,
      "backward_entropy": 0.1002266526222229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.3424301147461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029547877609729767,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.137541393438975,
      "backward_entropy": 0.09974749088287353,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.37566375732422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029599659144878387,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13747934500376383,
      "backward_entropy": 0.0992700695991516,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.22418975830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029648758471012115,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1374201774597168,
      "backward_entropy": 0.029769247770309447,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.459022521972656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029698219150304794,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1373609701792399,
      "backward_entropy": 0.029397022724151612,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.460304260253906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029746728017926216,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13730262716611227,
      "backward_entropy": 0.1343700885772705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.91356658935547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02979428321123123,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13724555571873984,
      "backward_entropy": 0.09721893072128296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.14290618896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029842237010598183,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13718929886817932,
      "backward_entropy": 0.09669371843338012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.136112213134766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02989358641207218,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13713075717290243,
      "backward_entropy": 0.09620274901390076,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.2254867553711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029943643137812614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13707407315572104,
      "backward_entropy": 0.027584573626518248,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.43296813964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029995622113347054,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1370156208674113,
      "backward_entropy": 0.09521271586418152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.92012405395508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03005308471620083,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1369542678197225,
      "backward_entropy": 0.09477885961532592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.77197265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03010604903101921,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1368968884150187,
      "backward_entropy": 0.09429710507392883,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.17205047607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030158966779708862,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13684060176213583,
      "backward_entropy": 0.09381048679351807,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.63188552856445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030210619792342186,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13678574562072754,
      "backward_entropy": 0.0933111310005188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.89667892456055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030259909108281136,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13673258821169534,
      "backward_entropy": 0.02560380697250366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.28302764892578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030306775122880936,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1366825302441915,
      "backward_entropy": 0.02525812089443207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.23966598510742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030360020697116852,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13662739594777426,
      "backward_entropy": 0.024952906370162963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.01905059814453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03041079454123974,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13657418886820474,
      "backward_entropy": 0.024646005034446715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.09420394897461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030467316508293152,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13651766379674277,
      "backward_entropy": 0.09080857038497925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.10405349731445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03052097000181675,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1364636222521464,
      "backward_entropy": 0.09032261371612549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.762840270996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030573347583413124,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13641109069188437,
      "backward_entropy": 0.023760679364204406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.73069763183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030623354017734528,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13636034727096558,
      "backward_entropy": 0.08929359316825866,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.40849304199219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03067524917423725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13630890846252441,
      "backward_entropy": 0.023161251842975617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.18341064453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03073028475046158,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13625519474347433,
      "backward_entropy": 0.08831244707107544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.06703186035156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030786829069256783,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13620037833849588,
      "backward_entropy": 0.0878506362438202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.2134246826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03084336780011654,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13614566127459207,
      "backward_entropy": 0.0873873472213745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.87993240356445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030909208580851555,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13608642419179282,
      "backward_entropy": 0.08701317310333252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.6677017211914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030972709879279137,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13602900505065918,
      "backward_entropy": 0.08661359548568726,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.5930061340332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031035345047712326,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13597275813420615,
      "backward_entropy": 0.02165728509426117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.3026008605957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031094752252101898,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13591831922531128,
      "backward_entropy": 0.08574638962745666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.28839111328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031152542680501938,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13586504260698953,
      "backward_entropy": 0.08527846932411194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.6009521484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03121298737823963,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13581034541130066,
      "backward_entropy": 0.020932014286518096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.94043731689453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03127315267920494,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13575570782025656,
      "backward_entropy": 0.13479900360107422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.20470428466797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03133152797818184,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13570286830266318,
      "backward_entropy": 0.08393003940582275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.20120239257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03138985484838486,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1356502374013265,
      "backward_entropy": 0.08346885442733765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.75957489013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031447965651750565,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13559863964716592,
      "backward_entropy": 0.02000892609357834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.261810302734375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031503260135650635,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13554920752843222,
      "backward_entropy": 0.019769372045993806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.959381103515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03155731037259102,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13550116618474325,
      "backward_entropy": 0.08200219869613648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.35103225708008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0316103920340538,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1354538599650065,
      "backward_entropy": 0.08148784041404725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.06495666503906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03166120871901512,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13540842135747275,
      "backward_entropy": 0.08095533847808838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.98101043701172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03171281889081001,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13536210854848227,
      "backward_entropy": 0.01879516541957855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.31827545166016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03176764026284218,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13531482219696045,
      "backward_entropy": 0.07994804382324219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.3940658569336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0318240150809288,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13526680072148642,
      "backward_entropy": 0.07947508096694947,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.148372650146484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03188323602080345,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13521631558736166,
      "backward_entropy": 0.018133968114852905,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.87368392944336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031940896064043045,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13516759872436523,
      "backward_entropy": 0.017914938926696777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.53467559814453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03199322894215584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1351232131322225,
      "backward_entropy": 0.07803094983100892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.984142303466797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03204729035496712,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13507833083470663,
      "backward_entropy": 0.01744799464941025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.2786979675293,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03209777921438217,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13503535588582358,
      "backward_entropy": 0.017209741473197936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.44274139404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032146256417036057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13499494393666586,
      "backward_entropy": 0.0764258325099945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.137046813964844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0321958102285862,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13495399554570517,
      "backward_entropy": 0.13486878871917723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.25224304199219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03224492445588112,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13491439819335938,
      "backward_entropy": 0.0753649652004242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.0056381225586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03229643031954765,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1348729431629181,
      "backward_entropy": 0.07487111687660217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.62296676635742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03235014155507088,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13482905427614847,
      "backward_entropy": 0.07440440654754639,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.30501174926758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03240309655666351,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13478553295135498,
      "backward_entropy": 0.01588987708091736,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.36711120605469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032454125583171844,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1347434918085734,
      "backward_entropy": 0.07344959974288941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.044063568115234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03250744938850403,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13469892740249634,
      "backward_entropy": 0.015498226881027222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.019378662109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032558850944042206,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13465567429860434,
      "backward_entropy": 0.07249968647956848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.83197021484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03260975331068039,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13461315631866455,
      "backward_entropy": 0.07201275825500489,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.642255783081055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03266026824712753,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13457128405570984,
      "backward_entropy": 0.014922164380550385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.72329330444336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03270793706178665,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1345309019088745,
      "backward_entropy": 0.07102658152580262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.52805709838867,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03275282680988312,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1344930132230123,
      "backward_entropy": 0.01453164517879486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.355201721191406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03279779478907585,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.134455939133962,
      "backward_entropy": 0.0699937641620636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.40369987487793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03284171223640442,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1344198981920878,
      "backward_entropy": 0.014137296378612519,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.90729904174805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032883334904909134,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13438615202903748,
      "backward_entropy": 0.06896735429763794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.929656982421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0329245887696743,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13435137271881104,
      "backward_entropy": 0.06846250891685486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.336271286010742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.032965343445539474,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13431692123413086,
      "backward_entropy": 0.06796561479568482,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.096967697143555,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033002860844135284,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13428544998168945,
      "backward_entropy": 0.013389351963996887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.39737701416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03303774446249008,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13425536950429282,
      "backward_entropy": 0.013207116723060608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.50481414794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03307940810918808,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13421996434529623,
      "backward_entropy": 0.06646620631217956,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.83151626586914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033120620995759964,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1341846783955892,
      "backward_entropy": 0.012890608608722686,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.035064697265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03316405788064003,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13414676984151205,
      "backward_entropy": 0.06559703350067139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.66619873046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03320812061429024,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13410876194636026,
      "backward_entropy": 0.06518396139144897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.72111511230469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033253904432058334,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1340703765551249,
      "backward_entropy": 0.06478210687637329,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.606712341308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03330394998192787,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13402841488520303,
      "backward_entropy": 0.06441482305526733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.35382843017578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0333540104329586,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1339860459168752,
      "backward_entropy": 0.06405131816864014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.247520446777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03340427577495575,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13394216696421304,
      "backward_entropy": 0.06369879245758056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.139976501464844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03345467150211334,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13389780124028525,
      "backward_entropy": 0.0633537232875824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.89421844482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03350512683391571,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13385387261708578,
      "backward_entropy": 0.011822377145290375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.31601333618164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03355582058429718,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13380877176920572,
      "backward_entropy": 0.011711639165878297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.25007247924805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03360787779092789,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13376239935557047,
      "backward_entropy": 0.011605523526668549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.07430648803711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03365855664014816,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13371713956197104,
      "backward_entropy": 0.062085962295532225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.34769058227539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0337081104516983,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1336723268032074,
      "backward_entropy": 0.011390729248523713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.858367919921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033757973462343216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13362661004066467,
      "backward_entropy": 0.06146658658981323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.50556564331055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03380678594112396,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13358193635940552,
      "backward_entropy": 0.011185736954212188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.03933334350586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03385710343718529,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1335357924302419,
      "backward_entropy": 0.06086153388023376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.47603607177734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.033907450735569,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13349011540412903,
      "backward_entropy": 0.010983540117740631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.63998794555664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03396039828658104,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13344271977742514,
      "backward_entropy": 0.060294175148010255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.72211456298828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03401333838701248,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1333946386973063,
      "backward_entropy": 0.01079111620783806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.788145065307617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034072525799274445,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1333413024743398,
      "backward_entropy": 0.010708438605070114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.53973388671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03412710502743721,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1332923173904419,
      "backward_entropy": 0.05955836772918701,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.063385009765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0341840460896492,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13324089845021567,
      "backward_entropy": 0.05932486057281494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.3656997680664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034240514039993286,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1331895391146342,
      "backward_entropy": 0.059086966514587405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.57335662841797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03430161997675896,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13313390811284384,
      "backward_entropy": 0.05887788534164429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.81519317626953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03436075896024704,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13307924071947733,
      "backward_entropy": 0.05866349339485168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.33440399169922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03442556783556938,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13301917910575867,
      "backward_entropy": 0.05848592519760132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.235280990600586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03448798507452011,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13296069701512656,
      "backward_entropy": 0.05829689502716064,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.981605529785156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03454693779349327,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1329051653544108,
      "backward_entropy": 0.058087676763534546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.92905807495117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03460661694407463,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13284830252329508,
      "backward_entropy": 0.010007308423519134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.71294021606445,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.034665547311306,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13279184699058533,
      "backward_entropy": 0.009934231638908386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.493614196777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03472394123673439,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13273470600446066,
      "backward_entropy": 0.05746650695800781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.39523696899414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03478293493390083,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13267717758814493,
      "backward_entropy": 0.057256770133972165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.931917190551758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034841399639844894,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13261980811754862,
      "backward_entropy": 0.057046568393707274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.101314544677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03489537164568901,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13256744543711343,
      "backward_entropy": 0.009636305272579193,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.21906280517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03494929522275925,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13251431783040366,
      "backward_entropy": 0.009557436406612396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.70482635498047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035006847232580185,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13245803117752075,
      "backward_entropy": 0.056356221437454224,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.305843353271484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03506885841488838,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13239693641662598,
      "backward_entropy": 0.05616695880889892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.06998825073242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0351312980055809,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13233468929926553,
      "backward_entropy": 0.055982422828674314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.65999221801758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03519422188401222,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1322703460852305,
      "backward_entropy": 0.009285636246204376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.713226318359375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035255033522844315,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1322076121966044,
      "backward_entropy": 0.00922238752245903,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.066219329833984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03531640022993088,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13214337825775146,
      "backward_entropy": 0.05544461607933045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.275333404541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035379521548748016,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1320763329664866,
      "backward_entropy": 0.055273938179016116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.118896484375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03544049710035324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13201109568277994,
      "backward_entropy": 0.05509330034255981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.08187484741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035506825894117355,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13194085160891214,
      "backward_entropy": 0.008984990417957306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.862953186035156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03556825593113899,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13187543551127115,
      "backward_entropy": 0.008925615251064301,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.544429779052734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03562774881720543,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13181060552597046,
      "backward_entropy": 0.008865648508071899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.04014205932617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03568790480494499,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13174349069595337,
      "backward_entropy": 0.008806335180997849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.849199295043945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035747282207012177,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13167748848597208,
      "backward_entropy": 0.00874393731355667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.39151382446289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03580238297581673,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13161701957384744,
      "backward_entropy": 0.053988033533096315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.33954620361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035856109112501144,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1315577824910482,
      "backward_entropy": 0.008610601723194122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.13901138305664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03591359779238701,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13149295250574747,
      "backward_entropy": 0.053586375713348386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.42776107788086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03596952185034752,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13142947355906168,
      "backward_entropy": 0.053395557403564456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.894649505615234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03602655231952667,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13136416673660278,
      "backward_entropy": 0.053214430809020996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.2087516784668,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03608207777142525,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13130023082097372,
      "backward_entropy": 0.008370708674192429,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.90165328979492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036140020936727524,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1312316656112671,
      "backward_entropy": 0.05285874009132385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.62394332885742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03619880974292755,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1311622361342112,
      "backward_entropy": 0.008262208849191665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.408260345458984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03625713661313057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13109264771143594,
      "backward_entropy": 0.052527773380279544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.48385620117188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036313820630311966,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1310242215792338,
      "backward_entropy": 0.052354675531387326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.17437744140625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03637498617172241,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1309498349825541,
      "backward_entropy": 0.008099696040153504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.025306701660156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036432940512895584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1308798591295878,
      "backward_entropy": 0.05201855897903442,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.83641815185547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03649042919278145,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13081030050913492,
      "backward_entropy": 0.137199068069458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.57506561279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03654763847589493,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13073993722597757,
      "backward_entropy": 0.05165717601776123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.26782989501953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03660588338971138,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13066757718722025,
      "backward_entropy": 0.05148977041244507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.07758712768555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036666277796030045,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13059106469154358,
      "backward_entropy": 0.05134078860282898,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.83309555053711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036728471517562866,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13051126400629678,
      "backward_entropy": 0.051200932264328,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.01288604736328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03679234907031059,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1304281751314799,
      "backward_entropy": 0.05107195377349853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.8709831237793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03685541823506355,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13034494717915854,
      "backward_entropy": 0.050946831703186035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.50035858154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036917660385370255,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13026217619578043,
      "backward_entropy": 0.00764719694852829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.92644119262695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0369800329208374,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13018113374710083,
      "backward_entropy": 0.050686174631118776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.04067611694336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037043992429971695,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13009564081827799,
      "backward_entropy": 0.00755508616566658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.963449478149414,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.037108175456523895,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13000796238581339,
      "backward_entropy": 0.13746535778045654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.073909759521484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03716904670000076,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12992507219314575,
      "backward_entropy": 0.050296878814697264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.748672485351562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037229180335998535,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12984317541122437,
      "backward_entropy": 0.05015721321105957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.26517105102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03728651627898216,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12976439793904623,
      "backward_entropy": 0.00736992359161377,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.604141235351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03734476864337921,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12968347469965616,
      "backward_entropy": 0.007325271517038346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.022589683532715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03740021958947182,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12960670391718546,
      "backward_entropy": 0.04975079894065857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.19916534423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0374520905315876,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12953492005666098,
      "backward_entropy": 0.007230973988771439,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.887303352355957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03750648349523544,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12945849696795145,
      "backward_entropy": 0.04946689605712891,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.77281951904297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03755751997232437,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12938615679740906,
      "backward_entropy": 0.007137957215309143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.547372817993164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037611331790685654,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1293091376622518,
      "backward_entropy": 0.007095611840486527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.026187896728516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03766387328505516,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1292341649532318,
      "backward_entropy": 0.13760582208633423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.5620231628418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03771798685193062,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12915444374084473,
      "backward_entropy": 0.007010058313608169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.42906188964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037772297859191895,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12907332181930542,
      "backward_entropy": 0.006972131133079529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.02205467224121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037826720625162125,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12899126609166464,
      "backward_entropy": 0.048764771223068236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.588104248046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037880197167396545,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1289097468058268,
      "backward_entropy": 0.006900659203529358,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.37017822265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.037936095148324966,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12882320086161295,
      "backward_entropy": 0.006866530328989029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.32350540161133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03799423202872276,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1287317176659902,
      "backward_entropy": 0.04851757884025574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.18936014175415,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03805544972419739,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12863421440124512,
      "backward_entropy": 0.04844619035720825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.63772964477539,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03811144083738327,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1285457412401835,
      "backward_entropy": 0.1377389669418335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.59838104248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03816843777894974,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1284549335638682,
      "backward_entropy": 0.04829239845275879,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.257362365722656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038228537887334824,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12835739056269327,
      "backward_entropy": 0.04821912050247192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.064647674560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038287922739982605,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12826093037923178,
      "backward_entropy": 0.04813776910305023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.942031860351562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038344595581293106,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12816878159840903,
      "backward_entropy": 0.006632670760154724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.71579360961914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03839999809861183,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.128078023592631,
      "backward_entropy": 0.04798310995101929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.909451484680176,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03845648467540741,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12798410654067993,
      "backward_entropy": 0.04791377186775207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.600479125976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03850921615958214,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12789748112360635,
      "backward_entropy": 0.006530580669641494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.613767623901367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038561005145311356,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12781246503194174,
      "backward_entropy": 0.04774947166442871,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.559444427490234,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03861095756292343,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12772939602533975,
      "backward_entropy": 0.1378587603569031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.886960029602051,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038659051060676575,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12764869133631387,
      "backward_entropy": 0.047597193717956544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.53251266479492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0387030765414238,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12757625182469687,
      "backward_entropy": 0.006390654295682907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.3472957611084,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03875036910176277,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12749653061230978,
      "backward_entropy": 0.0474225103855133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.178855895996094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038795944303274155,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12742010752360025,
      "backward_entropy": 0.04733085036277771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.19099998474121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03884463012218475,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12733603517214456,
      "backward_entropy": 0.047247585654258725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.151885986328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038891468197107315,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12725563844045004,
      "backward_entropy": 0.04715724289417267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.84954071044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03894371539354324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1271622578303019,
      "backward_entropy": 0.04709089398384094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.532428741455078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03900415077805519,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12705073753992716,
      "backward_entropy": 0.04704965353012085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.822233200073242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03906285762786865,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12694201866785684,
      "backward_entropy": 0.04700818955898285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.732038497924805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03911886364221573,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.126838485399882,
      "backward_entropy": 0.04696366786956787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.658493041992188,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03917243704199791,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12673956155776978,
      "backward_entropy": 0.13794138431549072,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.55959701538086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03922368213534355,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12664528687795004,
      "backward_entropy": 0.006065451353788376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.897762298583984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03927524760365486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12655038634936014,
      "backward_entropy": 0.006034400314092636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.374401092529297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03932816907763481,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12644980351130167,
      "backward_entropy": 0.04674308300018311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.386924743652344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03937917947769165,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12635170420010886,
      "backward_entropy": 0.04670016765594483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.14700698852539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03943391889333725,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12624535957972208,
      "backward_entropy": 0.04667320251464844,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.117551803588867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03949207067489624,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12612960735956827,
      "backward_entropy": 0.04666141867637634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.691471099853516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039547719061374664,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12601918975512186,
      "backward_entropy": 0.04665391743183136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.247528076171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03960322588682175,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12590720256169638,
      "backward_entropy": 0.04664592146873474,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.47380447387695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03965747728943825,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12579776843388876,
      "backward_entropy": 0.04663650989532471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.009902954101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039716120809316635,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1256766120592753,
      "backward_entropy": 0.04663425981998444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.30002975463867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03977314755320549,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1255584160486857,
      "backward_entropy": 0.04662767052650452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.11005783081055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039830952882766724,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1254372000694275,
      "backward_entropy": 0.04662184417247772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.49679183959961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03988951817154884,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12531359990437826,
      "backward_entropy": 0.04662151634693146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.396982192993164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.039945345371961594,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12519548336664835,
      "backward_entropy": 0.005759024247527123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.399524688720703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03999884054064751,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12508187691370645,
      "backward_entropy": 0.04660234749317169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.51801300048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04005131497979164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12496920426686604,
      "backward_entropy": 0.04659459590911865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.196285247802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04010729864239693,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12484695514043172,
      "backward_entropy": 0.04659818112850189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.046817779541016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04016311094164848,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12472386161486308,
      "backward_entropy": 0.046605145931243895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.95966911315918,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04022093489766121,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12459405263264973,
      "backward_entropy": 0.04661730229854584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.812191009521484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0402761772274971,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12447019418080647,
      "backward_entropy": 0.04662780165672302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.42770767211914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04033016040921211,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12434883912404378,
      "backward_entropy": 0.04663650989532471,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.58283805847168,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04038626700639725,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12422051032384236,
      "backward_entropy": 0.005609118938446045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.45405387878418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04044094681739807,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12409452597300212,
      "backward_entropy": 0.005589987337589264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.701309204101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040494512766599655,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1239703893661499,
      "backward_entropy": 0.04665177762508392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.62894058227539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04054480418562889,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12385501464207967,
      "backward_entropy": 0.046654558181762694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.591300010681152,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04059750959277153,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12373149394989014,
      "backward_entropy": 0.04665828943252563,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.765838623046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04064702242612839,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1236163079738617,
      "backward_entropy": 0.046658456325531006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.20974349975586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04069682955741882,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1234992245833079,
      "backward_entropy": 0.04665584564208984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.496320724487305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04074463993310928,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12338732679684956,
      "backward_entropy": 0.04664197266101837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.708690643310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04079301282763481,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1232723593711853,
      "backward_entropy": 0.04663408994674682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.958707809448242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04084073007106781,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12315853436787923,
      "backward_entropy": 0.046624937653541566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.73430252075195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04088694602251053,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12304860353469849,
      "backward_entropy": 0.04662407040596008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.98541831970215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04093477129936218,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12293165922164917,
      "backward_entropy": 0.04661523699760437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.54047393798828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04098310321569443,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12281254927317302,
      "backward_entropy": 0.13829052448272705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.17544174194336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04103497043251991,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12268118063608806,
      "backward_entropy": 0.04660351872444153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.080787658691406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041085995733737946,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12255122264226277,
      "backward_entropy": 0.046607178449630735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.542454242706299,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041138265281915665,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12241637706756592,
      "backward_entropy": 0.04661392867565155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.86115074157715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04118611663579941,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12229543924331665,
      "backward_entropy": 0.04660342931747437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.328861236572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04123328998684883,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1221747597058614,
      "backward_entropy": 0.04658636450767517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.655576705932617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0412788912653923,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12205855051676433,
      "backward_entropy": 0.04657180607318878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.05438995361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041324108839035034,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12194269895553589,
      "backward_entropy": 0.046559381484985354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.104394912719727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04137328267097473,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12181260188420613,
      "backward_entropy": 0.04655897617340088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.677492141723633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04142060503363609,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12168743213017781,
      "backward_entropy": 0.04655484855175018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.53761863708496,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04146827757358551,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12156013647715251,
      "backward_entropy": 0.13832263946533202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.683929443359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04151648283004761,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12142995993296306,
      "backward_entropy": 0.046542292833328246,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.251277923583984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041566070169210434,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12129376331965129,
      "backward_entropy": 0.04654037952423096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.99142837524414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04162009432911873,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1211412747701009,
      "backward_entropy": 0.04654812812805176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.151540756225586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041678015142679214,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12097439169883728,
      "backward_entropy": 0.04655732214450836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.368498802185059,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04173649847507477,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12080400188763936,
      "backward_entropy": 0.046577030420303346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.038055419921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04179101064801216,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12064663569132487,
      "backward_entropy": 0.005053002759814263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.428497314453125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04184830188751221,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12047818303108215,
      "backward_entropy": 0.005034584552049637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.30843734741211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041903864592313766,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12031465768814087,
      "backward_entropy": 0.04658620059490204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.21388816833496,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04195786267518997,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12015726168950398,
      "backward_entropy": 0.13836097717285156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.06959056854248,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042011719197034836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11999672651290894,
      "backward_entropy": 0.04658704400062561,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.011459350585938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04206320270895958,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1198438008626302,
      "backward_entropy": 0.046589577198028566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.904581069946289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04211150482296944,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11970238884290059,
      "backward_entropy": 0.04658783674240112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.67823600769043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042158059775829315,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11956633130709331,
      "backward_entropy": 0.0049186859279870985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.65867805480957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04220502823591232,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11942771077156067,
      "backward_entropy": 0.046591299772262576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.797589302062988,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04225117340683937,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.119291086991628,
      "backward_entropy": 0.04658212661743164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.603095054626465,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0422949381172657,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11916268865267436,
      "backward_entropy": 0.04658975303173065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.01028823852539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04233747348189354,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11903778711954753,
      "backward_entropy": 0.0466059148311615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.45796012878418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04238202050328255,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11890364686648051,
      "backward_entropy": 0.046640002727508546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.73141098022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04242537170648575,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11877175172170003,
      "backward_entropy": 0.046686413884162906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.321869850158691,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042470477521419525,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11862908800443013,
      "backward_entropy": 0.0048026107251644135,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.248717308044434,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042514123022556305,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11849143107732137,
      "backward_entropy": 0.04677457809448242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.88669776916504,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0425565242767334,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1183574100335439,
      "backward_entropy": 0.04682211279869079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.427033424377441,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04259886220097542,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11822181940078735,
      "backward_entropy": 0.04687816500663757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.38386058807373,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.042639054358005524,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11809431513150533,
      "backward_entropy": 0.13840901851654053,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.982818603515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04267730936408043,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1179741124312083,
      "backward_entropy": 0.04699285328388214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.177703857421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0427149161696434,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11785537997881572,
      "backward_entropy": 0.0047323167324066166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.470077514648438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042753688991069794,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11772952477137248,
      "backward_entropy": 0.04711233973503113,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.96553611755371,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04279252886772156,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11760207017262776,
      "backward_entropy": 0.04716060161590576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.74042797088623,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042832475155591965,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11746768156687419,
      "backward_entropy": 0.04720702171325684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.284021377563477,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042871344834566116,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11733720699946086,
      "backward_entropy": 0.04724582731723785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.6117525100708,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.042912472039461136,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11719496051470439,
      "backward_entropy": 0.0046681281179189685,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.546686172485352,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04295235499739647,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11705764134724934,
      "backward_entropy": 0.00465385690331459,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.00731086730957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04299111291766167,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1169235606988271,
      "backward_entropy": 0.04735803008079529,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.227909088134766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04302792623639107,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11679783463478088,
      "backward_entropy": 0.0046213164925575255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.9416389465332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043068092316389084,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11665440599123637,
      "backward_entropy": 0.0474076509475708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.09126853942871,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0431131049990654,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11648652950922649,
      "backward_entropy": 0.04743555188179016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.350040435791016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04315858706831932,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11631524562835693,
      "backward_entropy": 0.04746530652046203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.129590034484863,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043205440044403076,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11613549788792928,
      "backward_entropy": 0.04749332666397095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.386898040771484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04325052723288536,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11596355835596721,
      "backward_entropy": 0.04751681685447693,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.295528411865234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043295010924339294,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11579339702924092,
      "backward_entropy": 0.04753823876380921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.756061553955078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04333878308534622,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11562579870223999,
      "backward_entropy": 0.004513036087155342,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.125423431396484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0433841347694397,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1154484748840332,
      "backward_entropy": 0.04756283760070801,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.523351669311523,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04343278706073761,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11525337894757588,
      "backward_entropy": 0.04758826792240143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.884193420410156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04347855970263481,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11507196227709453,
      "backward_entropy": 0.04761542677879334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.608846664428711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043523747473955154,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1148922046025594,
      "backward_entropy": 0.004451838880777359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.851367950439453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043567292392253876,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11471994717915852,
      "backward_entropy": 0.0476797878742218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.864704132080078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04361135885119438,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.11454339822133382,
      "backward_entropy": 0.13845702409744262,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.178978443145752,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043656859546899796,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11435772975285848,
      "backward_entropy": 0.047744056582450865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.314423561096191,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04369858279824257,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1141919195652008,
      "backward_entropy": 0.04776073098182678,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.191892623901367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043739236891269684,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11403059959411621,
      "backward_entropy": 0.04779369831085205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.239574432373047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04377773404121399,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11388134956359863,
      "backward_entropy": 0.04782413244247437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.214645385742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043816108256578445,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11373079816500346,
      "backward_entropy": 0.04784500002861023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.05661964416504,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04385744780302048,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11356011033058167,
      "backward_entropy": 0.004330147057771683,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.99188232421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043901439756155014,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11337513724962871,
      "backward_entropy": 0.047911149263381955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.963439464569092,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0439438596367836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11319779356320699,
      "backward_entropy": 0.04794262945652008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.64851188659668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043984100222587585,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11303164561589558,
      "backward_entropy": 0.04798377156257629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.6092529296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0440259724855423,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11285500725110371,
      "backward_entropy": 0.004274845123291016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.594779014587402,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04406821355223656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11267506082852681,
      "backward_entropy": 0.004258808493614197,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.517048835754395,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0441102497279644,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11249484618504842,
      "backward_entropy": 0.04807973802089691,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.939983367919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04415170103311539,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11231690645217896,
      "backward_entropy": 0.00422939658164978,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.131254196166992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04419552907347679,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11212384700775146,
      "backward_entropy": 0.04813347458839416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.439620018005371,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04423992335796356,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.11192588011423747,
      "backward_entropy": 0.004199789091944695,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.363676071166992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04428265243768692,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1117363174756368,
      "backward_entropy": 0.0041854001581668855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.28572654724121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04432404786348343,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11155310273170471,
      "backward_entropy": 0.04826856255531311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.236387252807617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044367674738168716,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11135660608609517,
      "backward_entropy": 0.04830360412597656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.56399154663086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044409554451704025,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11116989453633626,
      "backward_entropy": 0.048322749137878415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.408184051513672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04445170238614082,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11097826560338338,
      "backward_entropy": 0.0483268141746521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.683287620544434,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04449174180626869,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11079930265744527,
      "backward_entropy": 0.048351836204528806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.847867965698242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044531360268592834,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1106222669283549,
      "backward_entropy": 0.004091366752982139,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.888900756835938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04457273706793785,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11043273409207661,
      "backward_entropy": 0.048404616117477414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.592601776123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04461287334561348,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1102494200070699,
      "backward_entropy": 0.04844846725463867,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.888593673706055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044654376804828644,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.11005644003550212,
      "backward_entropy": 0.04847979843616486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.77742576599121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044696301221847534,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1098591685295105,
      "backward_entropy": 0.04850894212722778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.136716842651367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04473850503563881,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10965936382611592,
      "backward_entropy": 0.04852984547615051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.536657333374023,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044780246913433075,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10946120818456014,
      "backward_entropy": 0.048559832572937014,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.478086471557617,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044824112206697464,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10924947261810303,
      "backward_entropy": 0.003987063467502594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.962005615234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044866450130939484,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10904552539189656,
      "backward_entropy": 0.048620188236236574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.628313064575195,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04490648955106735,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10885514815648396,
      "backward_entropy": 0.003958715870976448,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.679253578186035,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044947899878025055,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10865298906962077,
      "backward_entropy": 0.003944383561611175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.83488130569458,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044988926500082016,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10845335324605306,
      "backward_entropy": 0.04873871207237244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.591630935668945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04502762854099274,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10826831062634786,
      "backward_entropy": 0.003917424380779267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.76151466369629,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04506875202059746,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10806570450464885,
      "backward_entropy": 0.04881587028503418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.963518142700195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04511021822690964,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10785938302675883,
      "backward_entropy": 0.048851731419563296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.08319801092147827,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04515289142727852,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10764385263125102,
      "backward_entropy": 0.04888925552368164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.348111391067505,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045191045850515366,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10745913783709209,
      "backward_entropy": 0.048903930187225345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.576345920562744,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04522617906332016,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10729459921518962,
      "backward_entropy": 0.04891120195388794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.770864486694336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04525959491729736,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1071414053440094,
      "backward_entropy": 0.04892116487026214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.79766273498535,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045292407274246216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10699049631754558,
      "backward_entropy": 0.04894008040428162,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.245962142944336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04532903432846069,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10681257645289104,
      "backward_entropy": 0.04896828532218933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.13443946838379,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04536733031272888,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10662187139193217,
      "backward_entropy": 0.049002605676651004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.155275344848633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04540674015879631,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10642295082410176,
      "backward_entropy": 0.04901230931282043,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.588020324707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04544807970523834,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10620914896329244,
      "backward_entropy": 0.049005132913589475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.2857232093811035,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04548899084329605,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10599692662556966,
      "backward_entropy": 0.04901799559593201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.59473991394043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045527685433626175,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10580101609230042,
      "backward_entropy": 0.04904079139232635,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.333683967590332,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04556773230433464,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10559016466140747,
      "backward_entropy": 0.0036871686577796938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.336315155029297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04560726508498192,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10538432002067566,
      "backward_entropy": 0.049088907241821286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.201557159423828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045647993683815,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10516923666000366,
      "backward_entropy": 0.003655508905649185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.069208145141602,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04568982869386673,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10494464635848999,
      "backward_entropy": 0.04914060533046723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0548973083496094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04573172330856323,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10471826791763306,
      "backward_entropy": 0.04916492700576782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.840439796447754,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045770205557346344,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1045164664586385,
      "backward_entropy": 0.04918107688426972,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.87131404876709,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04580934718251228,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10430857539176941,
      "backward_entropy": 0.049220573902130124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.72563648223877,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04584728181362152,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10410861174265544,
      "backward_entropy": 0.04927195608615875,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9642512798309326,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04588489234447479,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10391085346539815,
      "backward_entropy": 0.0035666868090629576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.441630363464355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0459197461605072,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1037330428759257,
      "backward_entropy": 0.04938955307006836,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.207548141479492,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04595547541975975,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10354407628377278,
      "backward_entropy": 0.04945709705352783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.412659645080566,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.045992642641067505,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.10334293047587077,
      "backward_entropy": 0.13847981691360473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.521556854248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04602957144379616,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1031441589196523,
      "backward_entropy": 0.04957496225833893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.05848503112793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04606536403298378,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10295325517654419,
      "backward_entropy": 0.0035079814493656158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.742952346801758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04610173776745796,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10275681813557942,
      "backward_entropy": 0.04969149827957153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.868618965148926,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04613932967185974,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10255024830500285,
      "backward_entropy": 0.049732100963592527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.553929328918457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04617710039019585,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10234109560648601,
      "backward_entropy": 0.0034656178206205366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.221128463745117,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04621266946196556,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10214847326278687,
      "backward_entropy": 0.04976922869682312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.86231803894043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04624735191464424,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10196157296498616,
      "backward_entropy": 0.003433224558830261,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.157190322875977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046282071620225906,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10177344083786011,
      "backward_entropy": 0.04985198378562927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.03705406188965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04631820693612099,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10157310962677002,
      "backward_entropy": 0.049897730350494385,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.645051956176758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04635569825768471,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.10136109590530396,
      "backward_entropy": 0.0033927954733371734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.04049301147461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046392567455768585,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10115282734235127,
      "backward_entropy": 0.04998195171356201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.877058982849121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0464322604238987,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10092136263847351,
      "backward_entropy": 0.05001824498176575,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.395345687866211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04647040739655495,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1007016400496165,
      "backward_entropy": 0.05004989504814148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.867850303649902,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04650793597102165,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10048611958821614,
      "backward_entropy": 0.05007550120353699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.22553825378418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04654582962393761,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.10026661554972331,
      "backward_entropy": 0.05010814070701599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.671430587768555,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04658331349492073,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.10004892945289612,
      "backward_entropy": 0.13848599195480346,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.061223030090332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046621035784482956,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09982861081759135,
      "backward_entropy": 0.050191950798034665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.037577152252197,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04665844514966011,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09960987170537312,
      "backward_entropy": 0.0502522349357605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.381991386413574,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04669371619820595,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09940949082374573,
      "backward_entropy": 0.05030713081359863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.395333290100098,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046729352325201035,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09920292099316914,
      "backward_entropy": 0.050349414348602295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.7758207321167,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046763986349105835,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09900246063868205,
      "backward_entropy": 0.05040435791015625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.502345085144043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046798206865787506,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09880579511324565,
      "backward_entropy": 0.05044490098953247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.61196231842041,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0468338280916214,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09859702984491985,
      "backward_entropy": 0.050493812561035155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.821564674377441,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04686928167939186,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09838810563087463,
      "backward_entropy": 0.05056267976760864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.137142658233643,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046902645379304886,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09819672505060832,
      "backward_entropy": 0.0506151556968689,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.404752731323242,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04693487659096718,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09801453351974487,
      "backward_entropy": 0.05064898133277893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.028021812438965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04696714133024216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09783066312472026,
      "backward_entropy": 0.05069335699081421,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.978676795959473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0469985231757164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09765358765920003,
      "backward_entropy": 0.05073578357696533,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.212181091308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047029104083776474,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09748253226280212,
      "backward_entropy": 0.05077649354934692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.363678455352783,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04705975577235222,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09731006622314453,
      "backward_entropy": 0.05081863403320312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.843758583068848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.047087810933589935,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09715997179349263,
      "backward_entropy": 0.0030996862798929213,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.026311874389648,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047118499875068665,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09698631366093953,
      "backward_entropy": 0.050849545001983645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.740570068359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0471491739153862,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09681197007497151,
      "backward_entropy": 0.05086984634399414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.688521862030029,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04717909172177315,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09664297103881836,
      "backward_entropy": 0.050891512632369997,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.464114189147949,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047208428382873535,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09647812445958455,
      "backward_entropy": 0.05092322826385498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.603104591369629,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04723624140024185,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09632762273152669,
      "backward_entropy": 0.05094234943389893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.72568416595459,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04726357012987137,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09618039925893147,
      "backward_entropy": 0.0030044648796319962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.663111686706543,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04729108139872551,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0960295299688975,
      "backward_entropy": 0.050969135761260984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.337917804718018,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04731885716319084,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09587509433428447,
      "backward_entropy": 0.002970851957798004,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.546610832214355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047345470637083054,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09573144714037578,
      "backward_entropy": 0.0509906530380249,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.498496055603027,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0473724789917469,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09558311104774475,
      "backward_entropy": 0.051012170314788816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.250813961029053,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04739963263273239,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09543373187383015,
      "backward_entropy": 0.05102150440216065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.302136421203613,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047425732016563416,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09529242912928264,
      "backward_entropy": 0.051043647527694705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.325153350830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04745154455304146,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09515249729156494,
      "backward_entropy": 0.051070773601531984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.2344818115234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04747781157493591,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09500768780708313,
      "backward_entropy": 0.051100021600723265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.297468185424805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047503408044576645,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09486892819404602,
      "backward_entropy": 0.051102054119110105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.178155899047852,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04753068834543228,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09471426407496135,
      "backward_entropy": 0.0028468713164329527,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.102094650268555,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04755907878279686,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09454944729804993,
      "backward_entropy": 0.1384488344192505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.039083480834961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047587595880031586,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09438344836235046,
      "backward_entropy": 0.051113462448120116,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.938254356384277,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047616321593523026,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09421366453170776,
      "backward_entropy": 0.05113880038261413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.837320327758789,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04764653369784355,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0940294861793518,
      "backward_entropy": 0.051160669326782225,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.577653884887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047678276896476746,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09383065501848857,
      "backward_entropy": 0.05119788646697998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.924478769302368,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047713421285152435,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0936012864112854,
      "backward_entropy": 0.051240742206573486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.711582183837891,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0477466844022274,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09338873624801636,
      "backward_entropy": 0.051294898986816405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9694244861602783,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0477796345949173,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09317814310391744,
      "backward_entropy": 0.05135518908500671,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.70682430267334,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047810059040784836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09299075603485107,
      "backward_entropy": 0.051411676406860354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.39465045928955,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0478396862745285,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09281033277511597,
      "backward_entropy": 0.05146976113319397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.460397243499756,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04786980524659157,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09262534976005554,
      "backward_entropy": 0.05151234865188599,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.746098279953003,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04789992794394493,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09243951241175334,
      "backward_entropy": 0.05156621932983398,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.167055130004883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04792836681008339,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09226887424786885,
      "backward_entropy": 0.05160553455352783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.679884195327759,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04795747995376587,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09209178884824117,
      "backward_entropy": 0.13846440315246583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.238112449645996,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04798520728945732,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09192705154418945,
      "backward_entropy": 0.13846560716629028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6219751834869385,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048012875020504,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09176275134086609,
      "backward_entropy": 0.051719486713409424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.875848770141602,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04803934693336487,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09160878260930379,
      "backward_entropy": 0.051764070987701416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5762407779693604,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04806695878505707,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09144278367360432,
      "backward_entropy": 0.05183024406433105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.022706031799316,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04809315502643585,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09128975868225098,
      "backward_entropy": 0.05188477635383606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.406293869018555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04811940714716911,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09113581975301106,
      "backward_entropy": 0.051924514770507815,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.199221611022949,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0481472946703434,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09096548954645793,
      "backward_entropy": 0.05197203755378723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.154434680938721,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048174548894166946,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09080057342847188,
      "backward_entropy": 0.05202180743217468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.810473442077637,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04820132628083229,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.09063969055811565,
      "backward_entropy": 0.0025754982605576517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.072943687438965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04822801053524017,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09048013885815938,
      "backward_entropy": 0.0521253764629364,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7338224649429321,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04825431481003761,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09032311042149861,
      "backward_entropy": 0.05218384861946106,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.292132377624512,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04827864095568657,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.09018508593241374,
      "backward_entropy": 0.052231442928314206,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.597530841827393,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.048304006457328796,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.09003645181655884,
      "backward_entropy": 0.138482403755188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.40046215057373,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04832964763045311,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08988432089487712,
      "backward_entropy": 0.0523370623588562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.680708885192871,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048357632011175156,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08971011638641357,
      "backward_entropy": 0.05240540504455567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.8487701416015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04838350787758827,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08955628673235576,
      "backward_entropy": 0.002502911165356636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6465282440185547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048408929258584976,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08940495053927104,
      "backward_entropy": 0.052529698610305785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6394933462142944,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04843270033597946,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08926928043365479,
      "backward_entropy": 0.052606475353240964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.982259750366211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04845487326383591,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08914914727210999,
      "backward_entropy": 0.052683156728744504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.7172956466674805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04847954958677292,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08900105953216553,
      "backward_entropy": 0.05275682210922241,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.823308944702148,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04850370064377785,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08885887265205383,
      "backward_entropy": 0.0024581225588917732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5970090627670288,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04853002354502678,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08869729439417522,
      "backward_entropy": 0.052868545055389404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.121263027191162,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048554372042417526,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08855436245600383,
      "backward_entropy": 0.05291339159011841,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.569354057312012,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048578839749097824,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08841000994046529,
      "backward_entropy": 0.05294637084007263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.043991804122925,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04860414192080498,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0882571538289388,
      "backward_entropy": 0.05297638177871704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.973556995391846,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0486283004283905,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08811508615811665,
      "backward_entropy": 0.053010189533233644,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.318655014038086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04865267872810364,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08797011772791545,
      "backward_entropy": 0.053042268753051756,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.416423797607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048679169267416,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08780479431152344,
      "backward_entropy": 0.053072237968444826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.2728142738342285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04870516434311867,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08764358361562093,
      "backward_entropy": 0.05311815738677979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.350613117218018,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04873163253068924,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08747750520706177,
      "backward_entropy": 0.05314353108406067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.732931613922119,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04875738173723221,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08731823166211446,
      "backward_entropy": 0.05316483974456787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.283467292785645,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048782989382743835,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08716050783793132,
      "backward_entropy": 0.05317269563674927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4630916118621826,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04881112277507782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08697820703188579,
      "backward_entropy": 0.05318056344985962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4528216123580933,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048837028443813324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08681774139404297,
      "backward_entropy": 0.05318372249603272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.258879661560059,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04886087775230408,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08667771021525066,
      "backward_entropy": 0.05317397713661194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.190570831298828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04888606071472168,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.08652559916178386,
      "backward_entropy": 0.13848428726196288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.085198402404785,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04891233146190643,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.0863606830437978,
      "backward_entropy": 0.13847928047180175,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.390464186668396,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048937976360321045,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08620155851046245,
      "backward_entropy": 0.05311257243156433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3872731924057007,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04896191135048866,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08605907360712688,
      "backward_entropy": 0.05311666131019592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.587454319000244,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048984095454216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0859334369500478,
      "backward_entropy": 0.05311978459358215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9482715129852295,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04900744557380676,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08579587936401367,
      "backward_entropy": 0.05315083861351013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6314163208007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049030497670173645,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08566095431645711,
      "backward_entropy": 0.05319526195526123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.166473388671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049052681773900986,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08553388714790344,
      "backward_entropy": 0.053252899646759035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5962445735931396,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04907510057091713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08540412783622742,
      "backward_entropy": 0.0533014714717865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.086096286773682,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04909655079245567,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0852844516436259,
      "backward_entropy": 0.05334931015968323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.048216819763184,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0491182915866375,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08516079187393188,
      "backward_entropy": 0.05339013934135437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.702828407287598,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049140237271785736,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08503443002700806,
      "backward_entropy": 0.05341765880584717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.175775051116943,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049164362251758575,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08488682905832927,
      "backward_entropy": 0.053457236289978026,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6998231410980225,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04918932542204857,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08473057548205058,
      "backward_entropy": 0.05351718068122864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.664710283279419,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049213722348213196,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08457934856414795,
      "backward_entropy": 0.053582966327667236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.0142951011657715,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04923764988780022,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08443212509155273,
      "backward_entropy": 0.05365709066390991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.961295127868652,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04926229268312454,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08427728215853374,
      "backward_entropy": 0.053736555576324466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.731894493103027,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04928751289844513,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08411673704783122,
      "backward_entropy": 0.05381447672843933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.9996209144592285,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0493127703666687,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08395617206891377,
      "backward_entropy": 0.053902387619018555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.926787853240967,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049339231103658676,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08378156026204427,
      "backward_entropy": 0.05399665832519531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.591476917266846,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04936680942773819,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.083596666653951,
      "backward_entropy": 0.05410321354866028,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.556624889373779,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0493941456079483,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08341371019681294,
      "backward_entropy": 0.05421350002288818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.726802349090576,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04942098259925842,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08323635160923004,
      "backward_entropy": 0.05429909825325012,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.07400151342153549,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04944851994514465,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08305246631304423,
      "backward_entropy": 0.05436422824859619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.412734508514404,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04947323724627495,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08289687832196553,
      "backward_entropy": 0.05441218614578247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.282395601272583,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04949777573347092,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08274307350317638,
      "backward_entropy": 0.054454123973846434,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.316417694091797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04952181130647659,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08259375393390656,
      "backward_entropy": 0.054511868953704835,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.332619667053223,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049545928835868835,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08244289954503377,
      "backward_entropy": 0.054582488536834714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.229044437408447,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04957059770822525,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08228607972462972,
      "backward_entropy": 0.054657620191574094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.170997142791748,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049595315009355545,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0821277250846227,
      "backward_entropy": 0.054749476909637454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1405417919158936,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049619197845458984,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08197806278864543,
      "backward_entropy": 0.054822832345962524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1033575534820557,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04964231699705124,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08183627327283223,
      "backward_entropy": 0.05488113760948181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0750648975372314,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04966409504413605,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08170823256174724,
      "backward_entropy": 0.054915958642959596,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0588936805725098,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04968482255935669,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08159046371777852,
      "backward_entropy": 0.054945772886276244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.990253925323486,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04970502480864525,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0814779798189799,
      "backward_entropy": 0.001987088285386562,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.972938299179077,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049726393073797226,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08135354518890381,
      "backward_entropy": 0.05495394468307495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9326837062835693,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049747854471206665,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08122776945432027,
      "backward_entropy": 0.05495057702064514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.94795823097229,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04976944625377655,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.08109996219476064,
      "backward_entropy": 0.13852362632751464,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8563079833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049790412187576294,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08097851276397705,
      "backward_entropy": 0.054942095279693605,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.822824716567993,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049811601638793945,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08085453013579051,
      "backward_entropy": 0.05494545698165894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9166686534881592,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04983293265104294,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08072894811630249,
      "backward_entropy": 0.0549547016620636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.822816848754883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049853455275297165,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08061116437117259,
      "backward_entropy": 0.0019108030945062637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.636009216308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04987375810742378,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.08049400647481282,
      "backward_entropy": 0.055036789178848265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6831512451171875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049894724041223526,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08037046094735463,
      "backward_entropy": 0.0018945962190628051,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9633294343948364,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049915820360183716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.08024531602859497,
      "backward_entropy": 0.0018863638862967492,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.726064682006836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04993533715605736,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.080136239528656,
      "backward_entropy": 0.0551527738571167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6033661365509033,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049954600632190704,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0800292541583379,
      "backward_entropy": 0.0551926851272583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6726157665252686,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04997376725077629,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07992403705914815,
      "backward_entropy": 0.05519832372665405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.528550863265991,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04999281093478203,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07981937130292256,
      "backward_entropy": 0.055225038528442384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.633077621459961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05001198127865791,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07971317569414775,
      "backward_entropy": 0.138509202003479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9162115454673767,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05003086104989052,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07960948348045349,
      "backward_entropy": 0.05525510311126709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.58268404006958,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05004832521080971,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07952005167802174,
      "backward_entropy": 0.05526156425476074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.073081016540527,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050065770745277405,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07943021257718404,
      "backward_entropy": 0.05528386235237122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5552728176116943,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05008462071418762,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0793258398771286,
      "backward_entropy": 0.001801268756389618,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.512982130050659,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05010294169187546,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07922723889350891,
      "backward_entropy": 0.05530707836151123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5025551319122314,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05012119561433792,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07912939786911011,
      "backward_entropy": 0.05532863140106201,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.059873759746551514,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05013912916183472,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07903386652469635,
      "backward_entropy": 0.05534156560897827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2586662769317627,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05015530809760094,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07895609736442566,
      "backward_entropy": 0.001762515865266323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.025498390197754,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050171904265880585,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07887275516986847,
      "backward_entropy": 0.05536311268806458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9837260246276855,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05018935352563858,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07878082990646362,
      "backward_entropy": 0.05536607503890991,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.173685073852539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05020771920681,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07867985963821411,
      "backward_entropy": 0.055381250381469724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.377835750579834,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05022626742720604,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07857728997866313,
      "backward_entropy": 0.0553969144821167,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.642568111419678,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050244349986314774,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07847938934961955,
      "backward_entropy": 0.055397802591323854,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8135895133018494,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05026363581418991,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07836948831876119,
      "backward_entropy": 0.055402332544326784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0509438514709473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05028152838349342,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07827322681744893,
      "backward_entropy": 0.055406808853149414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.031464099884033,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05029973015189171,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07817343374093373,
      "backward_entropy": 0.05542177557945251,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.0003020763397217,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05031795799732208,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07807311415672302,
      "backward_entropy": 0.05542058944702148,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.972602367401123,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05033625289797783,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07797216872374217,
      "backward_entropy": 0.05540921688079834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6541247367858887,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05035455897450447,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07787119845549266,
      "backward_entropy": 0.0553861141204834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.192028522491455,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050373625010252,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07776244481404622,
      "backward_entropy": 0.05538362264633179,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1666481494903564,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05039234831929207,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07765663663546245,
      "backward_entropy": 0.05539129972457886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4519898891448975,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050410859286785126,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07755254705746968,
      "backward_entropy": 0.055418777465820315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1261281967163086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050428520888090134,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07745699087778728,
      "backward_entropy": 0.05544466376304626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4213823080062866,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05044601112604141,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07736287514368693,
      "backward_entropy": 0.05548308491706848,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4084078073501587,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05046284571290016,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07727523644765218,
      "backward_entropy": 0.05552986264228821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.7669148445129395,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05047908052802086,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07719422380129497,
      "backward_entropy": 0.00159971434623003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.049497127532959,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05049697682261467,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07709403336048126,
      "backward_entropy": 0.05562634468078613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6879780292510986,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050516001880168915,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07698351641496022,
      "backward_entropy": 0.001586068980395794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7004598379135132,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050535086542367935,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07687212030092876,
      "backward_entropy": 0.05572729110717774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.922766923904419,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050552885979413986,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07677343487739563,
      "backward_entropy": 0.05578747987747192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2461040019989014,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050571829080581665,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07666322588920593,
      "backward_entropy": 0.05585645437240601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.5780041217803955,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050591204315423965,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07654810448487599,
      "backward_entropy": 0.05591671466827393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6719961762428284,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05061051622033119,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0764338622490565,
      "backward_entropy": 0.05597240328788757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1438019275665283,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05062849819660187,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07633269826571147,
      "backward_entropy": 0.056035560369491574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.106832981109619,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05064692348241806,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0762270341316859,
      "backward_entropy": 0.05608695149421692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.4738047122955322,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050665825605392456,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0761165718237559,
      "backward_entropy": 0.056138885021209714,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6476425528526306,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05068453028798103,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07600816090901692,
      "backward_entropy": 0.05617080926895142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.8221310377120972,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05070185661315918,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0759131908416748,
      "backward_entropy": 0.056199264526367185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2223365306854248,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05071888491511345,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07582097252209981,
      "backward_entropy": 0.05623345971107483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7857919931411743,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050735075026750565,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07573736210664113,
      "backward_entropy": 0.0562596321105957,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4876186847686768,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05075107142329216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07565538585186005,
      "backward_entropy": 0.05629099607467651,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.5895538330078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05076828971505165,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0755607138077418,
      "backward_entropy": 0.05633742809295654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8578197956085205,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05078738555312157,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07544710238774617,
      "backward_entropy": 0.05639207363128662,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.2652509212493896,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050806671380996704,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07533210515975952,
      "backward_entropy": 0.056428998708724976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1401078701019287,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05082579702138901,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07521810134251912,
      "backward_entropy": 0.05646033883094788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.668919563293457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0508439876139164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07511341571807861,
      "backward_entropy": 0.056497079133987424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.7180912494659424,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05086174234747887,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07501286764939626,
      "backward_entropy": 0.056537199020385745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2155673503875732,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05088000372052193,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.07490694026152293,
      "backward_entropy": 0.0014586818404495716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5609280467033386,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050899118185043335,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07479230562845866,
      "backward_entropy": 0.05664582252502441,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.139458417892456,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05091701075434685,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07468985517819722,
      "backward_entropy": 0.056722217798233034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.103055715560913,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050935760140419006,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07457752525806427,
      "backward_entropy": 0.05679899454116821,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5449808239936829,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050955213606357574,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0744587779045105,
      "backward_entropy": 0.05687244534492493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0356268882751465,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05097329989075661,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07435358067353566,
      "backward_entropy": 0.05695198774337769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5345979332923889,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0509912371635437,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07424969971179962,
      "backward_entropy": 0.05701985359191895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.5346075892448425,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05100790038704872,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0741582711537679,
      "backward_entropy": 0.057089030742645264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.484194040298462,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05102326720952988,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0740793248017629,
      "backward_entropy": 0.0571446418762207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9927586317062378,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05103842914104462,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.074002206325531,
      "backward_entropy": 0.05720366239547729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.869769811630249,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0510530099272728,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0739305317401886,
      "backward_entropy": 0.057266765832901,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3020682334899902,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05106860771775246,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07384901245435078,
      "backward_entropy": 0.05732635259628296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.49262842535972595,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051085587590932846,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0737533022960027,
      "backward_entropy": 0.05739071369171143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.317399501800537,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05110159143805504,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07366642355918884,
      "backward_entropy": 0.05748072862625122,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.950564444065094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051118094474077225,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07357449332873027,
      "backward_entropy": 0.05756826996803284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.49254968762397766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051133617758750916,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07349269092082977,
      "backward_entropy": 0.05762632489204407,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.368465781211853,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05114791914820671,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07342294851938884,
      "backward_entropy": 0.05766798853874207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7857744693756104,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05116191878914833,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07335583368937175,
      "backward_entropy": 0.05769628286361694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6285157203674316,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051176153123378754,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07328587770462036,
      "backward_entropy": 0.05772641897201538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3220866918563843,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051191430538892746,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07320511341094971,
      "backward_entropy": 0.057764995098114016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8962315320968628,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05120638385415077,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07312723497549693,
      "backward_entropy": 0.05780031681060791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8735219836235046,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051220402121543884,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07305920620759328,
      "backward_entropy": 0.05780385732650757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1056787967681885,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051233891397714615,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0729956825574239,
      "backward_entropy": 0.057815569639205935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2689220905303955,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05124804750084877,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07292513052622478,
      "backward_entropy": 0.05783489942550659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8552404046058655,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05126193165779114,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0728572408358256,
      "backward_entropy": 0.05784909129142761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6457617282867432,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05127507448196411,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0727969507376353,
      "backward_entropy": 0.05784672498703003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.8291333913803101,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051288362592458725,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07273537417252858,
      "backward_entropy": 0.057837390899658205,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9990423917770386,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05130118876695633,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07267819841702779,
      "backward_entropy": 0.0578389048576355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.1467437744140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05131471902132034,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07261292139689128,
      "backward_entropy": 0.057849693298339847,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5742652416229248,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05132989585399628,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.07253134250640869,
      "backward_entropy": 0.1384857177734375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1725765466690063,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05134504288434982,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07245044906934102,
      "backward_entropy": 0.057864928245544435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.7792057394981384,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05135989189147949,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07237217823664348,
      "backward_entropy": 0.057878369092941286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5178543329238892,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051374245434999466,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07229799032211304,
      "backward_entropy": 0.0579146146774292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.5003478527069092,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05138867720961571,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07222266991933186,
      "backward_entropy": 0.0579487681388855,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1191880702972412,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051403168588876724,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07214668393135071,
      "backward_entropy": 0.05798068642616272,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.39456504583358765,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05141740292310715,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.0720730721950531,
      "backward_entropy": 0.0012646732851862908,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.1000422239303589,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0514305904507637,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07200942436854045,
      "backward_entropy": 0.058050537109375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4345591068267822,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051443472504615784,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0719486524661382,
      "backward_entropy": 0.0580696702003479,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.7641671895980835,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05145654082298279,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07188604275385539,
      "backward_entropy": 0.058091658353805545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.4001026153564453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05147015303373337,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.0718181977669398,
      "backward_entropy": 0.058118772506713864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.0465837717056274,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05148395150899887,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07174807290236156,
      "backward_entropy": 0.058156222105026245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3719518184661865,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05149753391742706,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07167965670426686,
      "backward_entropy": 0.05819864273071289,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.36362552642822266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051511168479919434,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07161065936088562,
      "backward_entropy": 0.05823352932929993,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6660312414169312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0515238456428051,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07155077159404755,
      "backward_entropy": 0.05826644897460938,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9950911402702332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05153706297278404,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07148551444212596,
      "backward_entropy": 0.0583019495010376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.3099457025527954,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05155026540160179,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07141966621081035,
      "backward_entropy": 0.058361905813217166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.978812575340271,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0515635684132576,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07135264078776042,
      "backward_entropy": 0.05841843485832214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.9664878845214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05157662555575371,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07128775119781494,
      "backward_entropy": 0.05847371220588684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.650446891784668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05158950015902519,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07122440139452617,
      "backward_entropy": 0.058530235290527345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.1693663597106934,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05160175636410713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07116688787937164,
      "backward_entropy": 0.058575892448425294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.2360470294952393,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05161513388156891,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07109845181306203,
      "backward_entropy": 0.05861436128616333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6194332242012024,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05162859708070755,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.07102913657824199,
      "backward_entropy": 0.05865628719329834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.6197442412376404,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05164165422320366,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.070963054895401,
      "backward_entropy": 0.058722174167633055,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 1.955539919734001,
    "avg_log_Z": -0.050888846591115,
    "success_rate": 1.0,
    "avg_reward": 14.7,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.01,
      "1": 0.93,
      "2": 0.06
    },
    "avg_forward_entropy": 0.07492445901036263,
    "avg_backward_entropy": 0.05434562860522419,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}