{
  "metrics_history": [
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 0.45454545454545453
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13862730264663697,
      "exploration_ratio": 0.7272727272727273
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13862730264663697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13862730264663697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13862730264663697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.0,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13861948251724243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.27861785888672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": 0.0,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18293519814809164,
      "backward_entropy": 0.13859922885894777,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.9298210144043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -9.999999747378752e-05,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1829282840092977,
      "backward_entropy": 0.13859540224075317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.25218963623047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -4.016435195808299e-05,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1829212705294291,
      "backward_entropy": 0.13861539363861083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.4082260131836,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -7.156515493988991e-06,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1829139788945516,
      "backward_entropy": 0.13862895965576172,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.643798828125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -3.367973113199696e-05,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18290644884109497,
      "backward_entropy": 0.1386106252670288,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.70237731933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -5.970263737253845e-05,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18289875984191895,
      "backward_entropy": 0.13860812187194824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.77954864501953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00011245644418522716,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18289087216059366,
      "backward_entropy": 0.13857293128967285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.18412017822266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00016478076577186584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18288280566533408,
      "backward_entropy": 0.13860275745391845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.431270599365234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00021515684784390032,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18287460009256998,
      "backward_entropy": 0.13859986066818236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 93.47601318359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00023883616086095572,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18286633491516113,
      "backward_entropy": 0.13862890005111694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.72551727294922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0002812579332385212,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1828578313191732,
      "backward_entropy": 0.13855249881744386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.7589111328125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0003241749946027994,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18284916877746582,
      "backward_entropy": 0.13859031200408936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.1580581665039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0003851571527775377,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18284024794896445,
      "backward_entropy": 0.1385871171951294,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.539424896240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0004575324128381908,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1828310489654541,
      "backward_entropy": 0.13858399391174317,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.12074279785156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0005148194031789899,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1828218698501587,
      "backward_entropy": 0.13862674236297606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.1278305053711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0005835355841554701,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18281237284342447,
      "backward_entropy": 0.13857738971710204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.24903106689453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.000657775322906673,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18280271689097086,
      "backward_entropy": 0.13862568140029907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.66566467285156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0007399207097478211,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18279282251993814,
      "backward_entropy": 0.13850953578948974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.04463195800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.000829240889288485,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18278255065282187,
      "backward_entropy": 0.13856635093688965,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.35807800292969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0009218279155902565,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1827720801035563,
      "backward_entropy": 0.13856229782104493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.16881561279297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.001018043956719339,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18276151021321616,
      "backward_entropy": 0.13855811357498168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.73819732666016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00110784568823874,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18275090058644614,
      "backward_entropy": 0.13862321376800538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.13465881347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0011923924321308732,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18274029095967612,
      "backward_entropy": 0.1385491371154785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.16419982910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0012813815847039223,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18272942304611206,
      "backward_entropy": 0.13854444026947021,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.43207550048828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0013714360538870096,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18271843592325845,
      "backward_entropy": 0.13853957653045654,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.41351318359375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0014661109307780862,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18270726998647055,
      "backward_entropy": 0.13862123489379882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.9787826538086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0015646154060959816,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18269588549931845,
      "backward_entropy": 0.13852951526641846,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.0394515991211,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0016665314324200153,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1826843023300171,
      "backward_entropy": 0.13862048387527465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.18854522705078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0017700929893180728,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18267250061035156,
      "backward_entropy": 0.1384281873703003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.18572998046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0018755531636998057,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18266046047210693,
      "backward_entropy": 0.1385135293006897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.61209869384766,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0019669868052005768,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1826485594113668,
      "backward_entropy": 0.13861931562423707,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.18286895751953,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002052716910839081,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18263665835062662,
      "backward_entropy": 0.13861879110336303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.98403930664062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002133730100467801,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18262471755345663,
      "backward_entropy": 0.13839371204376222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.32484436035156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002226360607892275,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18261218070983887,
      "backward_entropy": 0.13861777782440185,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.8406524658203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0023141882847994566,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18259966373443604,
      "backward_entropy": 0.13861727714538574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 142.23098754882812,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.002410450717434287,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18258665005366007,
      "backward_entropy": 0.13861690759658812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.03673553466797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0025133562739938498,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18257315953572592,
      "backward_entropy": 0.13846967220306397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.26899719238281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0026171556673943996,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18255949020385742,
      "backward_entropy": 0.13846302032470703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.07363891601562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0027246838435530663,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18254550298055014,
      "backward_entropy": 0.13861608505249023,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.0390167236328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002840806031599641,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18253080050150552,
      "backward_entropy": 0.13832807540893555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.54000091552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.002963666571304202,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1825154423713684,
      "backward_entropy": 0.13831980228424073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.7265167236328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0030838188249617815,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18250004450480142,
      "backward_entropy": 0.1386164426803589,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.73538970947266,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0032089618034660816,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1824841300646464,
      "backward_entropy": 0.1386167049407959,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 191.1373748779297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0033333059400320053,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18246805667877197,
      "backward_entropy": 0.13842474222183226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 158.6346893310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0034626866690814495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18245136737823486,
      "backward_entropy": 0.13828636407852174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.47618103027344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0035933384206146,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18243426084518433,
      "backward_entropy": 0.1384121894836426,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.23580169677734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.003724190639331937,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1824169953664144,
      "backward_entropy": 0.1384056806564331,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.9355010986328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0038501154631376266,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18239974975585938,
      "backward_entropy": 0.1386181354522705,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.90066528320312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.003979503642767668,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18238202730814615,
      "backward_entropy": 0.13825088739395142,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.75421142578125,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004111696500331163,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18236378828684488,
      "backward_entropy": 0.13861860036849977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 190.87161254882812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004238673951476812,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18234566847483316,
      "backward_entropy": 0.13837735652923583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.99623107910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004369879607111216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18232699235280356,
      "backward_entropy": 0.13836997747421265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.89920043945312,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004501482471823692,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.182308296362559,
      "backward_entropy": 0.13861907720565797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.8997344970703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.004634052515029907,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18228914340337118,
      "backward_entropy": 0.1386192798614502,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.4501495361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.004765057470649481,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18226969242095947,
      "backward_entropy": 0.13834695816040038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.2525177001953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00489848991855979,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18224968512852988,
      "backward_entropy": 0.13833909034729003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 174.02926635742188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0050298976711928844,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18223037322362265,
      "backward_entropy": 0.1383309245109558,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.3627166748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005163002759218216,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18221056461334229,
      "backward_entropy": 0.13832271099090576,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.18048858642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.005299251060932875,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18219021956125894,
      "backward_entropy": 0.13814845085144042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.7251091003418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005426255986094475,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18217023213704428,
      "backward_entropy": 0.1383056163787842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.87173461914062,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.005540631711483002,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18215092023213705,
      "backward_entropy": 0.1386197566986084,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.66228485107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0056552160531282425,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1821313500404358,
      "backward_entropy": 0.13828577995300292,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.8191680908203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0057677896693348885,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1821116805076599,
      "backward_entropy": 0.138275408744812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.78907775878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.005882464814931154,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18209153413772583,
      "backward_entropy": 0.13826489448547363,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.00297546386719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.00599723169580102,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1820711294809977,
      "backward_entropy": 0.13806686401367188,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.32241821289062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006107752677053213,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1820508042971293,
      "backward_entropy": 0.13805148601531983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.37029266357422,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0062189833261072636,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1820302406946818,
      "backward_entropy": 0.13861796855926514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 141.32196044921875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006326108705252409,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18200981616973877,
      "backward_entropy": 0.13801969289779664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.72087860107422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.00643344409763813,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18198899428049722,
      "backward_entropy": 0.1382066011428833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.50296783447266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006538864690810442,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1819679339726766,
      "backward_entropy": 0.13798409700393677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.6706771850586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006639058701694012,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18194709221522012,
      "backward_entropy": 0.13818025588989258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.13272094726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006738094612956047,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18192591269810995,
      "backward_entropy": 0.13816637992858888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 205.37474060058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.006840970367193222,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18190407752990723,
      "backward_entropy": 0.13815250396728515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.29908752441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.006952631752938032,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18188109000523886,
      "backward_entropy": 0.13790764808654785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.85586547851562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007065930403769016,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18185752630233765,
      "backward_entropy": 0.13788881301879882,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.38736724853516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007180806715041399,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1818334460258484,
      "backward_entropy": 0.13786954879760743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.60296630859375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0072893500328063965,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18180978298187256,
      "backward_entropy": 0.13784949779510497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.07518768310547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0073987157084047794,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18178580204645792,
      "backward_entropy": 0.13782970905303954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.72300720214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.007505875546485186,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18176166216532388,
      "backward_entropy": 0.1378087043762207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 189.35777282714844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007615272421389818,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18173688650131226,
      "backward_entropy": 0.13861119747161865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.0927505493164,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007730396464467049,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18171111742655435,
      "backward_entropy": 0.13861083984375,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.84139251708984,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.007839738391339779,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18168548742930093,
      "backward_entropy": 0.1386102557182312,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.76930236816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.007944440469145775,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1816599170366923,
      "backward_entropy": 0.13800349235534667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.55184936523438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.008051865734159946,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1816337505976359,
      "backward_entropy": 0.13860890865325928,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.47241973876953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008161340840160847,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18160698811213175,
      "backward_entropy": 0.1379694700241089,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.10945892333984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008268613368272781,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18158012628555298,
      "backward_entropy": 0.13764355182647706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.74752807617188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008372155949473381,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18155340353647867,
      "backward_entropy": 0.1379333257675171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.37451171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.00847583170980215,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18152624368667603,
      "backward_entropy": 0.13860626220703126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.68148803710938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008582801558077335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18149844805399576,
      "backward_entropy": 0.13756469488143921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.43850708007812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008697683922946453,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18146932125091553,
      "backward_entropy": 0.137876296043396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.708984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.008815594017505646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18143850564956665,
      "backward_entropy": 0.13751544952392578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.95095825195312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.008933976292610168,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18140713373819986,
      "backward_entropy": 0.1378377914428711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.4440460205078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009051341563463211,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1813753048578898,
      "backward_entropy": 0.13746438026428223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.3897705078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009168868884444237,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1813430388768514,
      "backward_entropy": 0.13779692649841307,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.33187866210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009283461607992649,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18131144841512045,
      "backward_entropy": 0.13740882873535157,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.62344360351562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009399309754371643,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18127942085266113,
      "backward_entropy": 0.1386045455932617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.92231750488281,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009514004923403263,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18124735355377197,
      "backward_entropy": 0.1386042594909668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.1672821044922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.009625314734876156,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1812152067820231,
      "backward_entropy": 0.13770599365234376,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.85860443115234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009741834364831448,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18118180831273398,
      "backward_entropy": 0.13728911876678468,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.82643127441406,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.009854722768068314,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18114837010701498,
      "backward_entropy": 0.1386030435562134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.83888244628906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.009964348748326302,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18111483256022134,
      "backward_entropy": 0.1372220039367676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.35538482666016,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010079002007842064,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18108004331588745,
      "backward_entropy": 0.13760597705841066,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 157.04714965820312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010190335102379322,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1810452143351237,
      "backward_entropy": 0.13715245723724365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 203.52378845214844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.010302279144525528,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18100965023040771,
      "backward_entropy": 0.13860087394714354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.35819244384766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.010421191342175007,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18097259600957236,
      "backward_entropy": 0.13752691745758056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.39893341064453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010532164946198463,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18093629678090414,
      "backward_entropy": 0.13704349994659423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 188.5463104248047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010637343861162663,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18090025583902994,
      "backward_entropy": 0.13700330257415771,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.71888732910156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.010747857391834259,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18086407581965128,
      "backward_entropy": 0.1369635581970215,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.3667449951172,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01085330918431282,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18082849184672037,
      "backward_entropy": 0.1385979175567627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 156.7594757080078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01095789298415184,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18079247077306113,
      "backward_entropy": 0.13859692811965943,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.0165023803711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011063641868531704,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1807556947072347,
      "backward_entropy": 0.13734763860702515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.79454040527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011162337847054005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18071951468785605,
      "backward_entropy": 0.1367854118347168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.81619262695312,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011257506906986237,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1806835730870565,
      "backward_entropy": 0.13673702478408814,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.95133209228516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01135305780917406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18064705530802408,
      "backward_entropy": 0.1366874933242798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.14203643798828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011444858275353909,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18061081568400064,
      "backward_entropy": 0.13663601875305176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.90025329589844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011535012163221836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1805742383003235,
      "backward_entropy": 0.13716793060302734,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.8650665283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.011626451276242733,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18053845564524332,
      "backward_entropy": 0.13652875423431396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.4375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011721203103661537,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18050195773442587,
      "backward_entropy": 0.13858327865600586,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.80156707763672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.011820321902632713,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18046418825785318,
      "backward_entropy": 0.13858165740966796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.17243194580078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.011917569674551487,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18042629957199097,
      "backward_entropy": 0.13700907230377196,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.9443588256836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012012964114546776,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18038823207219443,
      "backward_entropy": 0.13696694374084473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.1434326171875,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012104987166821957,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18034984668095908,
      "backward_entropy": 0.13857595920562743,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.44015502929688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012191109359264374,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18031187852223715,
      "backward_entropy": 0.1361885905265808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.68937683105469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012279482558369637,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18027313550313315,
      "backward_entropy": 0.13683054447174073,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.45550537109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012362503446638584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1802347699801127,
      "backward_entropy": 0.13678207397460937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 204.02281188964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012441268190741539,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18019676208496094,
      "backward_entropy": 0.135997211933136,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.8577117919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012530447915196419,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.18015623092651367,
      "backward_entropy": 0.1359328031539917,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.27081298828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0126238614320755,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1801145076751709,
      "backward_entropy": 0.13587191104888915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.72183990478516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.012717841193079948,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.18007208903630575,
      "backward_entropy": 0.1385603666305542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.54782104492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01280765887349844,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.18002994855244955,
      "backward_entropy": 0.13653428554534913,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.05034637451172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.012900441884994507,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1799865961074829,
      "backward_entropy": 0.136482572555542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.42107391357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.012989922426640987,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1799434224764506,
      "backward_entropy": 0.13560222387313842,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.4489517211914,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013076252304017544,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17990034818649292,
      "backward_entropy": 0.13637275695800782,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.29618072509766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013157423585653305,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1798577904701233,
      "backward_entropy": 0.13631478548049927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.32793426513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013238590210676193,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17981471618016562,
      "backward_entropy": 0.1353721261024475,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.89546966552734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013317459262907505,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17977156241734824,
      "backward_entropy": 0.13529106378555297,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.8646011352539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013394458219408989,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17972832918167114,
      "backward_entropy": 0.1361328125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.43775939941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013469785451889038,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17968493700027466,
      "backward_entropy": 0.13512263298034669,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.95970153808594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.013542860746383667,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17964136600494385,
      "backward_entropy": 0.13600389957427977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 155.11802673339844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013621382415294647,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1795961062113444,
      "backward_entropy": 0.13494348526000977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.05158233642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013704226352274418,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17954923709233603,
      "backward_entropy": 0.13485342264175415,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.10315704345703,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.013786034658551216,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1795018513997396,
      "backward_entropy": 0.13852417469024658,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.76783752441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01386647392064333,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17945432662963867,
      "backward_entropy": 0.1346631407737732,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.53536987304688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.013942571356892586,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1794071594874064,
      "backward_entropy": 0.13456385135650634,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.28964233398438,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014023487456142902,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17935816446940103,
      "backward_entropy": 0.13557236194610595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.56864166259766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01410924643278122,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1793074607849121,
      "backward_entropy": 0.13549474477767945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.10617065429688,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.014192502945661545,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17925679683685303,
      "backward_entropy": 0.13850947618484497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.8596649169922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014277760870754719,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17920496066411337,
      "backward_entropy": 0.13533324003219604,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.976318359375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014364451169967651,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1791520118713379,
      "backward_entropy": 0.13524994850158692,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.82273864746094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01445758156478405,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17909693717956543,
      "backward_entropy": 0.13516757488250733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.71963500976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014547287486493587,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17904206116994223,
      "backward_entropy": 0.13508057594299316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.41864013671875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01463796105235815,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1789862116177877,
      "backward_entropy": 0.13499112129211427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.51573944091797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014728337526321411,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17892994483311972,
      "backward_entropy": 0.13363091945648192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.24070739746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.014816147275269032,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17887383699417114,
      "backward_entropy": 0.13351850509643554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.48396301269531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014907934702932835,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17881596088409424,
      "backward_entropy": 0.1347111463546753,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.6463623046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.014998768456280231,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17875764767328897,
      "backward_entropy": 0.1346142530441284,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.58655548095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01508912444114685,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17869887749354044,
      "backward_entropy": 0.13317737579345704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.58706665039062,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015178128145635128,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1786396304766337,
      "backward_entropy": 0.13441404104232788,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.43204498291016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015268588438630104,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1785792112350464,
      "backward_entropy": 0.1329301118850708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.00013732910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015353183262050152,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1785196860631307,
      "backward_entropy": 0.13420259952545166,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.27601623535156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015432687476277351,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17846083641052246,
      "backward_entropy": 0.13265998363494874,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 184.85008239746094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.015518632717430592,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17839918533960977,
      "backward_entropy": 0.13252182006835939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.67973327636719,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.015613002702593803,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17833439509073892,
      "backward_entropy": 0.13846845626831056,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 138.07363891601562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01570092886686325,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17827065785725912,
      "backward_entropy": 0.13224562406539916,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.42584228515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015790043398737907,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1782057285308838,
      "backward_entropy": 0.1336413025856018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.39482879638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015878312289714813,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17814032236735025,
      "backward_entropy": 0.13352079391479493,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.7690658569336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.015964072197675705,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17807507514953613,
      "backward_entropy": 0.13339595794677733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.357139587402344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01604560576379299,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17801068226496378,
      "backward_entropy": 0.13844958543777466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.5992431640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016116052865982056,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17794938882191977,
      "backward_entropy": 0.13147406578063964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.75498962402344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016194459050893784,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17788469791412354,
      "backward_entropy": 0.1313126802444458,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.62955474853516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016277708113193512,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17781758308410645,
      "backward_entropy": 0.13286135196685792,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.92694091796875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016354205086827278,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17775209744771323,
      "backward_entropy": 0.13098409175872802,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.04402923583984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016429007053375244,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17768637339274088,
      "backward_entropy": 0.13081188201904298,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.21361541748047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016504459083080292,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1776195764541626,
      "backward_entropy": 0.13841967582702636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.57655334472656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016580885276198387,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17755172650019327,
      "backward_entropy": 0.1304576277732849,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.1861572265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016655271872878075,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17748361825942993,
      "backward_entropy": 0.13027238845825195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.3451690673828,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.016734646633267403,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17741280794143677,
      "backward_entropy": 0.13840461969375611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.28849411010742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.016818853095173836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17733951409657797,
      "backward_entropy": 0.12989907264709472,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 136.87046813964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.016891157254576683,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17726983626683554,
      "backward_entropy": 0.13166327476501466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.56580352783203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01696654222905636,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1771981120109558,
      "backward_entropy": 0.13838770389556884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.9190216064453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017040396109223366,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17712618907292685,
      "backward_entropy": 0.13838067054748535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.3744659423828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017116636037826538,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1770522197087606,
      "backward_entropy": 0.13113627433776856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.70054626464844,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017200512811541557,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17697461446126303,
      "backward_entropy": 0.13836941719055176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 102.44063568115234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017279647290706635,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17689812183380127,
      "backward_entropy": 0.13077399730682374,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.70850372314453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017357859760522842,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1768216093381246,
      "backward_entropy": 0.13835787773132324,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.04393768310547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017434101551771164,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1767449975013733,
      "backward_entropy": 0.13038508892059325,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 183.1107177734375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017508389428257942,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17666814724604288,
      "backward_entropy": 0.13834424018859864,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.4832305908203,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.017592642456293106,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17658650875091553,
      "backward_entropy": 0.13833949565887452,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.0281524658203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.017678245902061462,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1765031615893046,
      "backward_entropy": 0.12754138708114623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.89720916748047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.017769968137145042,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.176416277885437,
      "backward_entropy": 0.12958011627197266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 153.72152709960938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01786033809185028,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17632891734441122,
      "backward_entropy": 0.1383267641067505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.69619750976562,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.01795334555208683,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1762391726175944,
      "backward_entropy": 0.1383222222328186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.41781616210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018039340153336525,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17615211009979248,
      "backward_entropy": 0.12893540859222413,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.82510375976562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01811862550675869,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17606741189956665,
      "backward_entropy": 0.12870194911956787,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.85702514648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018203025683760643,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1759796937306722,
      "backward_entropy": 0.12598018646240233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.4372100830078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0182893518358469,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17589006821314493,
      "backward_entropy": 0.12823475599288942,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 150.98043823242188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018379688262939453,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1757976214090983,
      "backward_entropy": 0.12799900770187378,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.3878936767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0184737928211689,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17570261160532633,
      "backward_entropy": 0.1277613639831543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.42376708984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.018571989610791206,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17560521761576334,
      "backward_entropy": 0.13828437328338622,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.86341094970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01866914890706539,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17550756533940634,
      "backward_entropy": 0.12453484535217285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 165.52725219726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018762504681944847,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17541064818700156,
      "backward_entropy": 0.12423129081726074,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.56759643554688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.018861891701817513,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17531015475591025,
      "backward_entropy": 0.12677197456359862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 166.1002655029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.018966101109981537,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17520634333292642,
      "backward_entropy": 0.12363433837890625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 167.87986755371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.01907476596534252,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17509937286376953,
      "backward_entropy": 0.12333366870880128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.43943786621094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019186506047844887,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17498948176701865,
      "backward_entropy": 0.12302136421203613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.97624969482422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019299155101180077,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1748778223991394,
      "backward_entropy": 0.12269831895828247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.53803253173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.019407181069254875,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17476836840311685,
      "backward_entropy": 0.12236764430999755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 230.1419219970703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01951124146580696,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1746592124303182,
      "backward_entropy": 0.12517991065979003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.76061248779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0196270439773798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1745432217915853,
      "backward_entropy": 0.12170263528823852,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.04872131347656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019732726737856865,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17443142334620157,
      "backward_entropy": 0.13825215101242067,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 134.96263122558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.01983785070478916,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17431851228078207,
      "backward_entropy": 0.12431485652923584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 151.15773010253906,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.019942475482821465,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17420450846354166,
      "backward_entropy": 0.13824148178100587,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.50421142578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02004850097000599,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17408831914265951,
      "backward_entropy": 0.12023711204528809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.74144744873047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.020150555297732353,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17397365967432657,
      "backward_entropy": 0.12338860034942627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.52439880371094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020245825871825218,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17386152346928915,
      "backward_entropy": 0.11944451332092285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.36732482910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02034067176282406,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17374918858210245,
      "backward_entropy": 0.12272148132324219,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.35460662841797,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020441558212041855,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17363286018371582,
      "backward_entropy": 0.11865962743759155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.80735778808594,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02053692191839218,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17351909478505453,
      "backward_entropy": 0.1381988763809204,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 118.40314483642578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020632093772292137,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17340346177419028,
      "backward_entropy": 0.11785287857055664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.02056121826172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02072577364742756,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1732872724533081,
      "backward_entropy": 0.12134460210800171,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.06819915771484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.020816780626773834,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1731717586517334,
      "backward_entropy": 0.117000412940979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.7649154663086,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020900214090943336,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17305954297383627,
      "backward_entropy": 0.1381608486175537,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 131.3935089111328,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.020980533212423325,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.172947367032369,
      "backward_entropy": 0.13814808130264283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.4586181640625,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021064279600977898,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17283268769582114,
      "backward_entropy": 0.13813751935958862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.87100219726562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021146029233932495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17271822690963745,
      "backward_entropy": 0.11517864465713501,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.86217498779297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02123126946389675,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1726012627283732,
      "backward_entropy": 0.11473021507263184,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.95165252685547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021312084048986435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17248602708180746,
      "backward_entropy": 0.11426552534103393,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.10258483886719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02139086276292801,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17237085103988647,
      "backward_entropy": 0.11378544569015503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.0181121826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021468764171004295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17225571473439535,
      "backward_entropy": 0.11330535411834716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 132.927001953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.021549338474869728,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17213745911916098,
      "backward_entropy": 0.11282026767730713,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 195.56234741210938,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021632323041558266,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1720163623491923,
      "backward_entropy": 0.13806228637695311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 133.4374237060547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0217257272452116,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17188711961110434,
      "backward_entropy": 0.11184024810791016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.58245086669922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.021819842979311943,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17175604899724325,
      "backward_entropy": 0.11618961095809936,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.0251007080078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.021912550553679466,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1716247002283732,
      "backward_entropy": 0.13803914785385132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.8210906982422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02201046794652939,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.17148907979329428,
      "backward_entropy": 0.11533185243606567,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 162.3954315185547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02211121656000614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17134942611058554,
      "backward_entropy": 0.10975769758224488,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.9014434814453,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022217005491256714,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1712061564127604,
      "backward_entropy": 0.13802027702331543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.98724365234375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022325139492750168,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1710604429244995,
      "backward_entropy": 0.10872883796691894,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.20905303955078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.022426702082157135,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17091874281565347,
      "backward_entropy": 0.1380107045173645,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.44968032836914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022520259022712708,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17078198989232382,
      "backward_entropy": 0.1076418399810791,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.41178131103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02260448783636093,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17065083980560303,
      "backward_entropy": 0.10707337856292724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.02396392822266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02268226072192192,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17052326599756876,
      "backward_entropy": 0.10648874044418336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.61792755126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022760717198252678,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17039386431376138,
      "backward_entropy": 0.10589821338653564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.82630920410156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02283698506653309,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.17026424407958984,
      "backward_entropy": 0.1379352331161499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 164.03160095214844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02291468158364296,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.17013285557428995,
      "backward_entropy": 0.10468671321868897,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.3905029296875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.022998545318841934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16999471187591553,
      "backward_entropy": 0.10407230854034424,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.24528503417969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02307588793337345,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16986064116160074,
      "backward_entropy": 0.10957975387573242,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 161.98719787597656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023151222616434097,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16972657044728598,
      "backward_entropy": 0.13787033557891845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.90576171875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023233911022543907,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1695858637491862,
      "backward_entropy": 0.10852339267730712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.57086944580078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02332080528140068,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16944066683451334,
      "backward_entropy": 0.10152051448822022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.4062957763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023407403379678726,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16929473479588827,
      "backward_entropy": 0.10745759010314941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.96621704101562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02349616214632988,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1691463589668274,
      "backward_entropy": 0.10691776275634765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 146.72572326660156,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.023587053641676903,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16899599631627402,
      "backward_entropy": 0.13781862258911132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.2231903076172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023681173101067543,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16884177923202515,
      "backward_entropy": 0.10582909584045411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 177.5696563720703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023781226947903633,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1686821977297465,
      "backward_entropy": 0.10529228448867797,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.71949768066406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023887481540441513,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1685161590576172,
      "backward_entropy": 0.1047590970993042,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.4000473022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.023989861831068993,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16835288206736246,
      "backward_entropy": 0.10421148538589478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 160.61325073242188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024086538702249527,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16819381713867188,
      "backward_entropy": 0.09635759592056274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.09546661376953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02418835274875164,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16802942752838135,
      "backward_entropy": 0.0957025170326233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.00033569335938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.024286961182951927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1678674817085266,
      "backward_entropy": 0.09504439830780029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.74417114257812,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024386737495660782,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16770402590433756,
      "backward_entropy": 0.10194149017333984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.27594757080078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02448990009725094,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16753721237182617,
      "backward_entropy": 0.10137321949005126,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.28938293457031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02458832412958145,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1673727035522461,
      "backward_entropy": 0.10078665018081664,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.96837615966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024678925052285194,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.167214035987854,
      "backward_entropy": 0.1001855969429016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.2190399169922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02476438693702221,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16705856720606485,
      "backward_entropy": 0.1377615213394165,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 175.18844604492188,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024851378053426743,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16690009832382202,
      "backward_entropy": 0.09894683957099915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.62250518798828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.024946395307779312,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16673338413238525,
      "backward_entropy": 0.09834114313125611,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.23057556152344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025034194812178612,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16657270987828574,
      "backward_entropy": 0.08946070671081544,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.7787628173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02511569671332836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16641738017400107,
      "backward_entropy": 0.08871452808380127,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.94522857666016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02519972436130047,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16625851392745972,
      "backward_entropy": 0.1377042055130005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 127.9281234741211,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02527732588350773,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16610409816106161,
      "backward_entropy": 0.137687611579895,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.251708984375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025358254089951515,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16594459613164267,
      "backward_entropy": 0.08645449876785279,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.19508361816406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025435730814933777,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16578819354375204,
      "backward_entropy": 0.09445928931236267,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.34329986572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025522874668240547,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16562193632125854,
      "backward_entropy": 0.09382029771804809,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.42668151855469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0256096962839365,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16545487443606058,
      "backward_entropy": 0.08421334028244018,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 173.56784057617188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02569511905312538,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16528924306233725,
      "backward_entropy": 0.08346990942955017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.34181213378906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025788865983486176,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16511452198028564,
      "backward_entropy": 0.0827396035194397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 143.44113159179688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.025879839435219765,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16494188706080118,
      "backward_entropy": 0.09123729467391968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.57781982421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.025974150747060776,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16476458311080933,
      "backward_entropy": 0.08125691413879395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.4277572631836,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026070361956954002,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16458545128504434,
      "backward_entropy": 0.0805203914642334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 125.74394989013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026166142895817757,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1644065777460734,
      "backward_entropy": 0.0797868013381958,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.44158172607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026264052838087082,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1642258365948995,
      "backward_entropy": 0.08864811062812805,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.85426330566406,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.026361223310232162,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16404535373051962,
      "backward_entropy": 0.08799699544906617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 97.01905822753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026457404717803,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1638649900754293,
      "backward_entropy": 0.07761720418930054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 171.6807098388672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026550371199846268,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16368667284647623,
      "backward_entropy": 0.07686596512794494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.86212921142578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02665112540125847,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1634999712308248,
      "backward_entropy": 0.08601879477500915,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.76421356201172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026743855327367783,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16332020362218222,
      "backward_entropy": 0.0753732979297638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.92391586303711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.026837920770049095,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16313788294792175,
      "backward_entropy": 0.07460849285125733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 140.0703887939453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02692299708724022,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16296527783075967,
      "backward_entropy": 0.07382426261901856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.36107635498047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.02701321803033352,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1627860963344574,
      "backward_entropy": 0.07306935787200927,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 170.83580017089844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027103634551167488,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16260689496994019,
      "backward_entropy": 0.07232327461242676,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 111.10163116455078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027201995253562927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16241932908693948,
      "backward_entropy": 0.07158691287040711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.69841003417969,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027299068868160248,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1622310678164164,
      "backward_entropy": 0.0708299458026886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.18580627441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027395181357860565,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16204402844111124,
      "backward_entropy": 0.07005448937416077,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 124.2616195678711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02748934179544449,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1618596911430359,
      "backward_entropy": 0.07999749183654785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 139.93380737304688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02758568897843361,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16167320807774863,
      "backward_entropy": 0.07933706045150757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 110.34392547607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027685439214110374,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16148245334625244,
      "backward_entropy": 0.06778856515884399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.46514129638672,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02778388187289238,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.16129211584726968,
      "backward_entropy": 0.13752493858337403,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.70414733886719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02788168005645275,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16110219558080038,
      "backward_entropy": 0.07736827135086059,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.49199676513672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.027974650263786316,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.16091710329055786,
      "backward_entropy": 0.06549099683761597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.68978118896484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028070030733942986,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1607297956943512,
      "backward_entropy": 0.07603206634521484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 152.96348571777344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028160713613033295,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16054735581080118,
      "backward_entropy": 0.07535288333892823,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 154.160400390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028258074074983597,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1603576143582662,
      "backward_entropy": 0.07470139861106873,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.34581756591797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028360430151224136,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.16016002496083578,
      "backward_entropy": 0.07406262755393982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.21170043945312,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028462037444114685,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15996346871058145,
      "backward_entropy": 0.07342402338981628,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.49188995361328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0285564586520195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15977428356806436,
      "backward_entropy": 0.061009657382965085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.77989959716797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028650658205151558,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1595850388209025,
      "backward_entropy": 0.07210984230041503,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.00956726074219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.028744354844093323,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15939557552337646,
      "backward_entropy": 0.059504568576812744,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.1874237060547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02884070575237274,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15920366843541464,
      "backward_entropy": 0.1375034809112549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 92.33915710449219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.028941024094820023,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15900731086730957,
      "backward_entropy": 0.07018480300903321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 137.03823852539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029039185494184494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15881397326787314,
      "backward_entropy": 0.057373225688934326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 106.92637634277344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029140997678041458,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1586165428161621,
      "backward_entropy": 0.13752219676971436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.4095230102539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029242467135190964,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15841962893803915,
      "backward_entropy": 0.06832677125930786,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.4432601928711,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.029345709830522537,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1582214037577311,
      "backward_entropy": 0.1375403046607971,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 194.92904663085938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029447857290506363,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15802407264709473,
      "backward_entropy": 0.05466402769088745,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.71868896484375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.02956107258796692,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15781458218892416,
      "backward_entropy": 0.13756521940231323,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.77122497558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029669027775526047,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15761212507883707,
      "backward_entropy": 0.06594763994216919,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 91.43099212646484,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.029773864895105362,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1574136515458425,
      "backward_entropy": 0.06535238027572632,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 135.33380126953125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.029876084998250008,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15721864501635233,
      "backward_entropy": 0.05208402276039124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.11821365356445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.02998177334666252,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15701969464619955,
      "backward_entropy": 0.06415140628814697,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 149.8675994873047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030078057199716568,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15683195988337198,
      "backward_entropy": 0.05078428387641907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 120.07740783691406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03018013946712017,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15663699309031168,
      "backward_entropy": 0.05013859272003174,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.60554504394531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030283883213996887,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15644062558809915,
      "backward_entropy": 0.06231279373168945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.993568420410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030382854864001274,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1562494933605194,
      "backward_entropy": 0.048863974213600156,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.7498550415039,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03047548606991768,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15606619914372763,
      "backward_entropy": 0.061081111431121826,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 90.13227844238281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03057066537439823,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1558801531791687,
      "backward_entropy": 0.06046933531761169,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.02442169189453,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030664563179016113,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1556965410709381,
      "backward_entropy": 0.04694270491600037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.3394775390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.030755767598748207,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1555175483226776,
      "backward_entropy": 0.04632962048053742,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 119.08158874511719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03084663115441799,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15534021457036337,
      "backward_entropy": 0.058672189712524414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.94242858886719,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.030940331518650055,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15515978137652078,
      "backward_entropy": 0.058095937967300414,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.75605773925781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031032701954245567,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1549805005391439,
      "backward_entropy": 0.04456965029239655,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.71012115478516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03112153895199299,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15480566024780273,
      "backward_entropy": 0.056935304403305055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.36894226074219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03121195174753666,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.154629647731781,
      "backward_entropy": 0.05636295080184937,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.37984848022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03130161762237549,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15445502599080405,
      "backward_entropy": 0.055794477462768555,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.6109848022461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03138669952750206,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15428680181503296,
      "backward_entropy": 0.04224480390548706,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 88.659912109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03146975487470627,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15412181615829468,
      "backward_entropy": 0.05463627576828003,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.46285247802734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.031553223729133606,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15395657221476236,
      "backward_entropy": 0.05407381057739258,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.42195892333984,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.031640827655792236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15378697713216147,
      "backward_entropy": 0.040593850612640384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 103.47398376464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03172609210014343,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1536207894484202,
      "backward_entropy": 0.04005807638168335,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.97158813476562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03181272745132446,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15345269441604614,
      "backward_entropy": 0.05243939161300659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.47415161132812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03190675377845764,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15327597657839456,
      "backward_entropy": 0.03900043368339538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 145.267578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03200768679380417,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15309152007102966,
      "backward_entropy": 0.03850751519203186,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 159.70469665527344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03211478888988495,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15290086468060812,
      "backward_entropy": 0.03803815245628357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 117.2789306640625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03222886100411415,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15270274877548218,
      "backward_entropy": 0.03757719099521637,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.37903594970703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03234293311834335,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15250476201375326,
      "backward_entropy": 0.037096285820007326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 172.9891357421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03245776519179344,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15230700373649597,
      "backward_entropy": 0.03661895990371704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 73.27571105957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.032580725848674774,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15210102001825967,
      "backward_entropy": 0.036165791749954226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 101.9338607788086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03269797936081886,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1519035299619039,
      "backward_entropy": 0.03570154905319214,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 129.71153259277344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03281344100832939,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15170876185099283,
      "backward_entropy": 0.04821090996265411,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.98445701599121,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03293145075440407,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1515122652053833,
      "backward_entropy": 0.03476313948631286,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.590492248535156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03303789347410202,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15133056044578552,
      "backward_entropy": 0.047280806303024295,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.54906463623047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03313641622662544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1511593759059906,
      "backward_entropy": 0.033794951438903806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 72.61634826660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03323392942547798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.15099084377288818,
      "backward_entropy": 0.03332563638687134,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 100.06220245361328,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03332829847931862,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15082629521687826,
      "backward_entropy": 0.04584360718727112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 128.41697692871094,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03342438116669655,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.15066099166870117,
      "backward_entropy": 0.1378396987915039,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.5822982788086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03352527320384979,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15049081047375998,
      "backward_entropy": 0.044977009296417236,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 114.62895965576172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03362813964486122,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.15031854311625162,
      "backward_entropy": 0.04456426501274109,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.49320220947266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03373250365257263,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1501446564992269,
      "backward_entropy": 0.0441554456949234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 85.579833984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.033835697919130325,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14997398853302002,
      "backward_entropy": 0.043762382864952085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.7313232421875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03393757343292236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14980627099672952,
      "backward_entropy": 0.030360198020935057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.59305953979492,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03404153883457184,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1496368646621704,
      "backward_entropy": 0.029973715543746948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.94098663330078,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.034140124917030334,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1494751274585724,
      "backward_entropy": 0.13793570995330812,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 168.67762756347656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03424349054694176,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14930913845698038,
      "backward_entropy": 0.029206284880638124,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.93514251708984,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0343562588095665,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14913345376650491,
      "backward_entropy": 0.1379695177078247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 113.00440216064453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03446841612458229,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1489601731300354,
      "backward_entropy": 0.04155214726924896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.66694641113281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03458135575056076,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14878620704015097,
      "backward_entropy": 0.04119970202445984,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.03047943115234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.034690335392951965,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1486189862092336,
      "backward_entropy": 0.04084980487823486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 126.20209503173828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03479506075382233,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14845764636993408,
      "backward_entropy": 0.02740219235420227,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.53761672973633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03490324690937996,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14829285939534506,
      "backward_entropy": 0.040124592185020444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.54429626464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03500611335039139,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14813605944315592,
      "backward_entropy": 0.026668429374694824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 98.04618835449219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03510555997490883,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1479840874671936,
      "backward_entropy": 0.026302281022071838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.64588928222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03520546853542328,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14783203601837158,
      "backward_entropy": 0.039037668704986574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.67159080505371,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03530174866318703,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1476842761039734,
      "backward_entropy": 0.038669875264167784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.368534088134766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03539030998945236,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14754706621170044,
      "backward_entropy": 0.03830868899822235,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.60778045654297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035473741590976715,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14741702874501547,
      "backward_entropy": 0.0379556268453598,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.542964935302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03555642440915108,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14728895823160806,
      "backward_entropy": 0.02452318072319031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 83.42430877685547,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.03563406318426132,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1471670667330424,
      "backward_entropy": 0.13802146911621094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 69.45468139648438,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03571280092000961,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14704410235087076,
      "backward_entropy": 0.023841510713100433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.380985260009766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.035791072994470596,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14692246913909912,
      "backward_entropy": 0.03663546741008759,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.68428039550781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.035864707082509995,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14680667718251547,
      "backward_entropy": 0.023181180655956268,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.21056365966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03594063222408295,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14668906728426614,
      "backward_entropy": 0.036013805866241456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.38356018066406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036020390689373016,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14656831820805868,
      "backward_entropy": 0.022578980028629302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.21666717529297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03610200434923172,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14644724130630493,
      "backward_entropy": 0.022307524085044862,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 163.87525939941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03618637099862099,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1463234027226766,
      "backward_entropy": 0.02203705906867981,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.85247039794922,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036282017827034,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14618796110153198,
      "backward_entropy": 0.035004603862762454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.72279357910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03638279438018799,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14604821801185608,
      "backward_entropy": 0.03478497862815857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.88695526123047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036480799317359924,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1459125280380249,
      "backward_entropy": 0.034558871388435365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 109.00392150878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03657499700784683,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1457828183968862,
      "backward_entropy": 0.021023502945899962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.36438751220703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03667229041457176,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1456504464149475,
      "backward_entropy": 0.03411790132522583,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.77034759521484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.036767225712537766,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14552191893259683,
      "backward_entropy": 0.020513716340065002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.28740692138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.036861661821603775,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14539474248886108,
      "backward_entropy": 0.03366138637065887,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 94.63826751708984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03695645555853844,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14526883761088052,
      "backward_entropy": 0.03344695568084717,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.13206481933594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03705316409468651,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14514226714769998,
      "backward_entropy": 0.03324238061904907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 147.8078155517578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037155359983444214,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14501150449117026,
      "backward_entropy": 0.033059507608413696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 107.67173767089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037265513092279434,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.144873837629954,
      "backward_entropy": 0.03289183974266052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.84703063964844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03737730160355568,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14473563432693481,
      "backward_entropy": 0.032721465826034545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.383678436279297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03748708218336105,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14460082848866782,
      "backward_entropy": 0.018921294808387758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.31099700927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03758835792541504,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1444766124089559,
      "backward_entropy": 0.01870608627796173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.31614685058594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03768935054540634,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14435418446858725,
      "backward_entropy": 0.032198792695999144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.49041748046875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0377897247672081,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14423354466756186,
      "backward_entropy": 0.018297670781612395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.349365234375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037886664271354675,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14411818981170654,
      "backward_entropy": 0.03189523816108704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.69635009765625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.037978313863277435,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14400944113731384,
      "backward_entropy": 0.03175482153892517,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.87117767333984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038070958107709885,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14390109976132712,
      "backward_entropy": 0.03163455724716187,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.44275665283203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03816363960504532,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14379323522249857,
      "backward_entropy": 0.017561227083206177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.345123291015625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03825505077838898,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14368769526481628,
      "backward_entropy": 0.03138095140457153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 105.55122375488281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03834313154220581,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1435860594113668,
      "backward_entropy": 0.031247550249099733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.46558380126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03843572735786438,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14348095655441284,
      "backward_entropy": 0.017029204964637758,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.99330520629883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03852805867791176,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1433763305346171,
      "backward_entropy": 0.016849374771118163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.013126373291016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.038617271929979324,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14327571789423624,
      "backward_entropy": 0.01667264997959137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.813413619995117,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03870332986116409,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14317870140075684,
      "backward_entropy": 0.0164897620677948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.76937866210938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03878314420580864,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14308851957321167,
      "backward_entropy": 0.01630347967147827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 130.72726440429688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03886285424232483,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14299906293551126,
      "backward_entropy": 0.03044247031211853,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.56159210205078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.038950979709625244,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14290207624435425,
      "backward_entropy": 0.030314621329307557,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.36211395263672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03903479874134064,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14281030495961508,
      "backward_entropy": 0.03019123077392578,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.3746337890625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0391233004629612,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1427148381868998,
      "backward_entropy": 0.015612581372261047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.9375228881836,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.039210762828588486,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.14262117942174277,
      "backward_entropy": 0.13838595151901245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.12538146972656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03930431604385376,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14252283175786337,
      "backward_entropy": 0.02985154092311859,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 116.59801483154297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.039397791028022766,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1424252986907959,
      "backward_entropy": 0.029736465215682982,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.78843688964844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03949650749564171,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14232408006985983,
      "backward_entropy": 0.01497468501329422,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.7218246459961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0395948700606823,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14222431182861328,
      "backward_entropy": 0.014816115796566009,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.79046630859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03969258815050125,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14212598403294882,
      "backward_entropy": 0.029376679658889772,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.76752471923828,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03978697583079338,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14203187823295593,
      "backward_entropy": 0.014496470987796783,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.37138366699219,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.03987804427742958,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14194166660308838,
      "backward_entropy": 0.02914491295814514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 115.31189727783203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.03996826708316803,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1418534517288208,
      "backward_entropy": 0.014194311201572418,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.82828521728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04006443917751312,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14176122347513834,
      "backward_entropy": 0.01405775099992752,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.346229553222656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04016125202178955,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14167000850041708,
      "backward_entropy": 0.02889397144317627,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.9171142578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040254779160022736,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14158268769582114,
      "backward_entropy": 0.013810966908931733,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 76.4685287475586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04034724831581116,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1414974331855774,
      "backward_entropy": 0.028775525093078614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.73097610473633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04044053331017494,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14141265551249185,
      "backward_entropy": 0.013580793142318725,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.94392013549805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.040532585233449936,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14132980505625406,
      "backward_entropy": 0.013468711078166962,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.30682373046875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04062209278345108,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14125016331672668,
      "backward_entropy": 0.028643590211868287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.805816650390625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040707219392061234,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14117484291394553,
      "backward_entropy": 0.02859829068183899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.67387008666992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04079030081629753,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14110193649927774,
      "backward_entropy": 0.01313856840133667,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.624629974365234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04087187349796295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14103122552235922,
      "backward_entropy": 0.013034957647323608,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.54449462890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.040951889008283615,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1409625212351481,
      "backward_entropy": 0.028496050834655763,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.48012924194336,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04103051498532295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14089560508728027,
      "backward_entropy": 0.01283542662858963,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 75.4274673461914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041107792407274246,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14083033800125122,
      "backward_entropy": 0.01273820549249649,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.33473587036133,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.041187092661857605,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14076387882232666,
      "backward_entropy": 0.028421074151992798,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.241695404052734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04126489534974098,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14069912830988565,
      "backward_entropy": 0.02839283347129822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.61378860473633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04134146869182587,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14063597718874613,
      "backward_entropy": 0.012444843351840974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.30846405029297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041418612003326416,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1405729055404663,
      "backward_entropy": 0.012349721789360047,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.82756805419922,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04150018095970154,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1405074199040731,
      "backward_entropy": 0.13852994441986083,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.33317947387695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.041583482176065445,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14044121901194254,
      "backward_entropy": 0.012182106077671052,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.9532699584961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04166649654507637,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14037571350733438,
      "backward_entropy": 0.012096192687749863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.4095687866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04175274819135666,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14030847946802774,
      "backward_entropy": 0.012011047452688217,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 123.63790130615234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04184066131711006,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1402409871419271,
      "backward_entropy": 0.011932572722434998,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.47895812988281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04193626344203949,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14016874631245932,
      "backward_entropy": 0.011854816228151321,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.016845703125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04203389957547188,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.14009598890940347,
      "backward_entropy": 0.01177811473608017,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.62862014770508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04213183373212814,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.14002407590548197,
      "backward_entropy": 0.028277939558029173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.31285858154297,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04222821071743965,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1399540901184082,
      "backward_entropy": 0.011629832535982132,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.35063934326172,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04232107847929001,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13988702495892844,
      "backward_entropy": 0.02825677990913391,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.23643112182617,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0424133837223053,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13982141017913818,
      "backward_entropy": 0.028256163001060486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 122.03784942626953,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.042505182325839996,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13975695768992105,
      "backward_entropy": 0.028270992636680602,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.01456069946289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04260393977165222,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13968839248021445,
      "backward_entropy": 0.028272253274917603,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.803775787353516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04270134121179581,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13962181409200033,
      "backward_entropy": 0.028281718492507935,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.72733688354492,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.04279543459415436,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13955809672673544,
      "backward_entropy": 0.1385892868041992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 84.82330322265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04288628324866295,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13949699203173319,
      "backward_entropy": 0.011132345348596574,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 108.75727844238281,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04297972470521927,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13943501313527426,
      "backward_entropy": 0.01106076017022133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.42635726928711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04307886213064194,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13937032222747803,
      "backward_entropy": 0.010994607955217362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 96.31779479980469,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04317642003297806,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13930757840474448,
      "backward_entropy": 0.010930243134498595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.1859245300293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04327746108174324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13924360275268555,
      "backward_entropy": 0.02828548550605774,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.053794860839844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04337519779801369,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13918273647626242,
      "backward_entropy": 0.010809913277626038,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.9106559753418,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043471403419971466,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13912356893221536,
      "backward_entropy": 0.01075114607810974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.931251525878906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04356658458709717,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1390660802523295,
      "backward_entropy": 0.028342804312705992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 95.3409423828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043658267706632614,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13901095589001974,
      "backward_entropy": 0.010634784400463105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.702178955078125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.043754007667303085,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1389542520046234,
      "backward_entropy": 0.028357154130935668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 47.6008415222168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04384680464863777,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13890006144841513,
      "backward_entropy": 0.02836858630180359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.33395767211914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.043937042355537415,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13884812593460083,
      "backward_entropy": 0.010461726784706115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.60771942138672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044026341289281845,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13879727323849997,
      "backward_entropy": 0.010404200851917266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.09674072265625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04411206766963005,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13874930143356323,
      "backward_entropy": 0.01035270020365715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.73419952392578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.044197481125593185,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13870206475257874,
      "backward_entropy": 0.010301095247268677,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 82.3219223022461,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04428422451019287,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13865462938944498,
      "backward_entropy": 0.028462621569633483,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.35383605957031,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04437383636832237,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1386062502861023,
      "backward_entropy": 0.010197345912456513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.94994354248047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04445918649435043,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1385606328646342,
      "backward_entropy": 0.02849045693874359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.549028396606445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0445428192615509,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1385166049003601,
      "backward_entropy": 0.028508079051971436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.427555084228516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.044621340930461884,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1384757955869039,
      "backward_entropy": 0.028526511788368226,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 104.79052734375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04470004141330719,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13843520482381186,
      "backward_entropy": 0.02853728234767914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.6095085144043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04478560760617256,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13839159409205118,
      "backward_entropy": 0.02854604125022888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.23603820800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04486896097660065,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13834955294926962,
      "backward_entropy": 0.02854676842689514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 81.07133483886719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04495544359087944,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13830647865931192,
      "backward_entropy": 0.009812036156654358,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.85002517700195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04504480957984924,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.138262669245402,
      "backward_entropy": 0.02855488657951355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 80.73857116699219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045133352279663086,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13821975390116373,
      "backward_entropy": 0.009705158323049546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.60797882080078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04522433504462242,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13817626237869263,
      "backward_entropy": 0.028562766313552857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.93281555175781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04531402885913849,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1381338139375051,
      "backward_entropy": 0.009592701494693757,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.922218322753906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04540491849184036,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13809161384900412,
      "backward_entropy": 0.009540856629610062,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 68.6424560546875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0454934686422348,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13805119196573892,
      "backward_entropy": 0.028577852249145507,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.574551582336426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04558272659778595,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1380109190940857,
      "backward_entropy": 0.02858469486236572,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.99616622924805,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045664772391319275,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13797454039255777,
      "backward_entropy": 0.028597816824913025,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.54269790649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04574689269065857,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13793861865997314,
      "backward_entropy": 0.00934344008564949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.40557861328125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.045827414840459824,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13790390888849893,
      "backward_entropy": 0.00929674357175827,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.94939422607422,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045911483466625214,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13786816596984863,
      "backward_entropy": 0.028658279776573183,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.26349639892578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.045997295528650284,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13783228397369385,
      "backward_entropy": 0.028696027398109437,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.6670913696289,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.046081285923719406,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13779776295026144,
      "backward_entropy": 0.028740668296813966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.845619201660156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04616687819361687,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13776308298110962,
      "backward_entropy": 0.009145939350128173,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.977684020996094,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04624864459037781,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13773036003112793,
      "backward_entropy": 0.0091094970703125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.4328842163086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04632849618792534,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13769869009653726,
      "backward_entropy": 0.028859901428222656,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.796661376953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04641179367899895,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13766604661941528,
      "backward_entropy": 0.028888022899627684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.56251525878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046493448317050934,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13763455549875894,
      "backward_entropy": 0.008993008732795715,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 66.83736419677734,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04657154530286789,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13760475317637125,
      "backward_entropy": 0.02894251048564911,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.340301513671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046651482582092285,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1375745634237925,
      "backward_entropy": 0.008908801525831223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.43072509765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046726860105991364,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1375465989112854,
      "backward_entropy": 0.008868364244699478,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.400901794433594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0468011237680912,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13751937945683798,
      "backward_entropy": 0.029000642895698547,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 121.45731353759766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04687637835741043,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1374922196070353,
      "backward_entropy": 0.02902928590774536,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.16743850708008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.046961959451436996,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13746158281962076,
      "backward_entropy": 0.008756022155284881,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 87.98884582519531,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04704580083489418,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1374321977297465,
      "backward_entropy": 0.029107245802879333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 65.87381744384766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04713433235883713,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1374016006787618,
      "backward_entropy": 0.02915399968624115,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.86838912963867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.047223757952451706,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1373711625734965,
      "backward_entropy": 0.029199355840682985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.6692008972168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04731110483407974,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13734199603398642,
      "backward_entropy": 0.029257190227508546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.54607009887695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04739801958203316,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1373134652773539,
      "backward_entropy": 0.008623915910720825,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.559688568115234,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04748459905385971,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13728549083073935,
      "backward_entropy": 0.008603739738464355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.63667297363281,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04756905138492584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1372586190700531,
      "backward_entropy": 0.029465526342391968,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.6077880859375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04765026271343231,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1372332771619161,
      "backward_entropy": 0.02954401969909668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.057865142822266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04773636534810066,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13720669349034628,
      "backward_entropy": 0.029621803760528566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.169620513916016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04782218486070633,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13718063632647196,
      "backward_entropy": 0.008529893308877944,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.5473403930664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04790599271655083,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13715556263923645,
      "backward_entropy": 0.0297842413187027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.683372497558594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04799129068851471,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13713039954503378,
      "backward_entropy": 0.029867452383041383,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.558067321777344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04807617887854576,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13710569341977438,
      "backward_entropy": 0.008480054885149002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 74.76282501220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04816063120961189,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13708142439524332,
      "backward_entropy": 0.008461330085992813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 53.307220458984375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048247966915369034,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13705662886301676,
      "backward_entropy": 0.03008873462677002,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.955902099609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04833466559648514,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13703236977259317,
      "backward_entropy": 0.030156612396240234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.47694778442383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04841766506433487,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13700956106185913,
      "backward_entropy": 0.00840473100543022,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.378623962402344,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.048499248921871185,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13698757688204447,
      "backward_entropy": 0.1386292099952698,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.82109832763672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.048579491674900055,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13696635762850443,
      "backward_entropy": 0.030403843522071837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.183692932128906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04865995794534683,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13694536685943604,
      "backward_entropy": 0.030499711632728577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.07796859741211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.048739004880189896,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13692504167556763,
      "backward_entropy": 0.008357147872447967,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.518314361572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04881993308663368,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13690446813901266,
      "backward_entropy": 0.030691221356391907,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.794307708740234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04889758676290512,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1368850270907084,
      "backward_entropy": 0.030778098106384277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.2247200012207,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04897724464535713,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1368652880191803,
      "backward_entropy": 0.008319777250289918,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.10546112060547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.049056973308324814,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13684574762980142,
      "backward_entropy": 0.008304952085018158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.60959243774414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04913681745529175,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1368264158566793,
      "backward_entropy": 0.0310213565826416,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.862327575683594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04921526089310646,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13680771986643472,
      "backward_entropy": 0.008275307714939117,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 62.07746124267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04929371550679207,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13678919275601706,
      "backward_entropy": 0.008257516473531724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 61.92782211303711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04937410727143288,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13677040735880533,
      "backward_entropy": 0.008241468667984008,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.93815040588379,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04945606738328934,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13675149281819662,
      "backward_entropy": 0.03130887150764465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.88932037353516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04953477904200554,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13673365116119385,
      "backward_entropy": 0.00820842683315277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.02034378051758,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.04961680620908737,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1367151935895284,
      "backward_entropy": 0.031446152925491334,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.709095001220703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04969717562198639,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13669743140538534,
      "backward_entropy": 0.008175066858530044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.006038665771484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04977412521839142,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1366806427637736,
      "backward_entropy": 0.00815485641360283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.56867027282715,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.04985135793685913,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13666399319966635,
      "backward_entropy": 0.00813433825969696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.770877838134766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.049925796687603,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1366482377052307,
      "backward_entropy": 0.03168594837188721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.199654579162598,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0500008687376976,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13663252194722494,
      "backward_entropy": 0.031745216250419615,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.44953155517578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05007008835673332,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13661839564641318,
      "backward_entropy": 0.008076396584510804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 70.57093048095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.050138652324676514,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1366045077641805,
      "backward_entropy": 0.03185204565525055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.26990509033203,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05021120235323906,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.136589785416921,
      "backward_entropy": 0.008028732985258103,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.230220794677734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.050282761454582214,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13657542069753012,
      "backward_entropy": 0.00800217017531395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.08418083190918,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05035671591758728,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13656068841616312,
      "backward_entropy": 0.03194311559200287,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 49.97860336303711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05042659118771553,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13654707868893942,
      "backward_entropy": 0.03198052644729614,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.90678787231445,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0504976361989975,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13653339942296347,
      "backward_entropy": 0.03202859759330749,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 79.55152130126953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05056808888912201,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1365200181802114,
      "backward_entropy": 0.007911021262407303,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 89.26556396484375,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05064414069056511,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13650556405385336,
      "backward_entropy": 0.007891425490379333,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.60720443725586,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.050726667046546936,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13648990790049234,
      "backward_entropy": 0.13862905502319336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.508121490478516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05080719292163849,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1364748477935791,
      "backward_entropy": 0.007848686724901199,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.40292739868164,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05088615417480469,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13646034399668375,
      "backward_entropy": 0.007828354090452194,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.53644561767578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05096356198191643,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13644635677337646,
      "backward_entropy": 0.032305991649627684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.76091766357422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051045842468738556,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13643145561218262,
      "backward_entropy": 0.007788725942373276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 58.60183334350586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05112941563129425,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13641653458277384,
      "backward_entropy": 0.007770579308271408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 48.709224700927734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05121427774429321,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13640159368515015,
      "backward_entropy": 0.007755307108163833,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.167909622192383,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05129872262477875,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13638694087664285,
      "backward_entropy": 0.007742143422365189,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.4151668548584,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05137950927019119,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13637318213780722,
      "backward_entropy": 0.007728390395641327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.01919174194336,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05145546793937683,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1363605260848999,
      "backward_entropy": 0.032660096883773804,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.44783782958984,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05152883380651474,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1363485852877299,
      "backward_entropy": 0.03273506164550781,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 86.4703369140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.051605891436338425,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13633598883946738,
      "backward_entropy": 0.032811909914016724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.20914649963379,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05168912187218666,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1363223393758138,
      "backward_entropy": 0.007680244743824005,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.70720100402832,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.051767174154520035,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13630982240041098,
      "backward_entropy": 0.007666391134262085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.637237548828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05184217169880867,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13629802068074545,
      "backward_entropy": 0.007652247697114945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.066802978515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05191463604569435,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13628681500752768,
      "backward_entropy": 0.007640679180622101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 56.92155838012695,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05198642238974571,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1362758974234263,
      "backward_entropy": 0.1386289954185486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.86913299560547,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05206044390797615,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13626463214556375,
      "backward_entropy": 0.0332354485988617,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.773231506347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.052133359014987946,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13625365495681763,
      "backward_entropy": 0.033313214778900146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.460049629211426,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05220531299710274,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.136242945988973,
      "backward_entropy": 0.033389809727668765,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.971744537353516,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05227161571383476,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13623333970705667,
      "backward_entropy": 0.13862861394882203,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.141117095947266,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05233912169933319,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13622358441352844,
      "backward_entropy": 0.007580914348363876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.410037994384766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052404843270778656,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13621424635251364,
      "backward_entropy": 0.007571389526128769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.64231491088867,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05247025936841965,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1362050175666809,
      "backward_entropy": 0.03367545902729034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.2348747253418,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05253711715340614,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1361956000328064,
      "backward_entropy": 0.03375447392463684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.41823196411133,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05260371044278145,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1361863116423289,
      "backward_entropy": 0.007545606791973114,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 64.80094146728516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.052671611309051514,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13617684443791708,
      "backward_entropy": 0.007539745420217514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.50282859802246,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05274348333477974,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13616669178009033,
      "backward_entropy": 0.034005007147789,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.86547088623047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05281137675046921,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13615728418032327,
      "backward_entropy": 0.03408144116401672,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.770225524902344,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05287899076938629,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1361479957898458,
      "backward_entropy": 0.034169015288352964,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.67959213256836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0529460683465004,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13613881667455038,
      "backward_entropy": 0.03425154685974121,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.45383644104004,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05301273241639137,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13612975676854452,
      "backward_entropy": 0.007503477483987808,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.387985229492188,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05307754874229431,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13612105449040732,
      "backward_entropy": 0.0074967719614505764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.41199493408203,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05314081534743309,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13611270984013876,
      "backward_entropy": 0.03450714945793152,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.115386009216309,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05320395529270172,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13610440492630005,
      "backward_entropy": 0.007485765218734741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.39445877075195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05326239764690399,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13609695434570312,
      "backward_entropy": 0.03467294871807099,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.242210388183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05332576856017113,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13608863949775696,
      "backward_entropy": 0.0347480297088623,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 54.07136917114258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053393710404634476,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1360795497894287,
      "backward_entropy": 0.007462385296821594,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.984333038330078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05346395820379257,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1360700527826945,
      "backward_entropy": 0.007451700419187546,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.95583724975586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05353188142180443,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13606101274490356,
      "backward_entropy": 0.007440921664237976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.844974517822266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0535961277782917,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13605263829231262,
      "backward_entropy": 0.035012534260749816,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.041278328746557236,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05365856736898422,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.136044571797053,
      "backward_entropy": 0.007416591793298721,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.50569534301758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.053714919835329056,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13603763779004416,
      "backward_entropy": 0.007404705882072449,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.402626037597656,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.053773291409015656,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13603035608927408,
      "backward_entropy": 0.13862664699554444,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.593435287475586,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05383343622088432,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13602274656295776,
      "backward_entropy": 0.007378385215997696,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.36132049560547,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05389221012592316,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13601536552111307,
      "backward_entropy": 0.007364065200090408,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 44.08655548095703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05395125225186348,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13600793480873108,
      "backward_entropy": 0.03532012403011322,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.402753829956055,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05401196330785751,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13600019613901773,
      "backward_entropy": 0.007332721352577209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.10549545288086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05407138168811798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1359927256902059,
      "backward_entropy": 0.00731809139251709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.01886749267578,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05413106828927994,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13598516583442688,
      "backward_entropy": 0.007303950190544128,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 78.55616760253906,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05419081822037697,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13597760597864786,
      "backward_entropy": 0.03548071086406708,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.93894577026367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05425826832652092,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13596874475479126,
      "backward_entropy": 0.03551641404628754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 52.08379364013672,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054329827427864075,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13595924774805704,
      "backward_entropy": 0.007260464131832123,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.92435836791992,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05440342053771019,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13594931364059448,
      "backward_entropy": 0.0072492346167564396,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 77.62663269042969,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05447870120406151,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1359391212463379,
      "backward_entropy": 0.035651442408561704,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 60.17961883544922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05455995723605156,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13592795530954996,
      "backward_entropy": 0.007222381234169006,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.845298767089844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0546436682343483,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1359163522720337,
      "backward_entropy": 0.035719701647758485,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.629425048828125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0547267347574234,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13590488831202188,
      "backward_entropy": 0.007195484638214111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.0472412109375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05480603128671646,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13589408000310263,
      "backward_entropy": 0.03579631149768829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.93595504760742,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.054883409291505814,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1358835498491923,
      "backward_entropy": 0.007166634500026703,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.826393127441406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05495915189385414,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13587329785029092,
      "backward_entropy": 0.007151993364095688,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 42.139488220214844,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055033449083566666,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1358633041381836,
      "backward_entropy": 0.03589929044246674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.214345932006836,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05510784313082695,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13585323095321655,
      "backward_entropy": 0.0359350711107254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.13686180114746,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05517931655049324,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13584364453951517,
      "backward_entropy": 0.035968101024627684,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.747947692871094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05524825304746628,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1358344852924347,
      "backward_entropy": 0.03600403666496277,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.62122344970703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05531780794262886,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13582517703374228,
      "backward_entropy": 0.036038607358932495,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.49095916748047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05538797751069069,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13581570982933044,
      "backward_entropy": 0.13862943649291992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 57.89558029174805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05545853450894356,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1358061134815216,
      "backward_entropy": 0.007052223384380341,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.507585525512695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05553247407078743,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13579585154851279,
      "backward_entropy": 0.03613373041152954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 41.095523834228516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05560211464762688,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13578635454177856,
      "backward_entropy": 0.007022541016340256,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.77968215942383,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05567212030291557,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1357766886552175,
      "backward_entropy": 0.03618710339069366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.841819763183594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.055741142481565475,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13576714197794595,
      "backward_entropy": 0.03621032238006592,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.437841415405273,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05581091344356537,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13575746615727743,
      "backward_entropy": 0.03624426424503326,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.474609375,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05587831512093544,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13574816783269247,
      "backward_entropy": 0.03628539741039276,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 40.46142578125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05594507232308388,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1357390284538269,
      "backward_entropy": 0.006954506784677505,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.14962387084961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05601263418793678,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1357294718424479,
      "backward_entropy": 0.006944309175014496,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.063741683959961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05607648938894272,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13572070995966592,
      "backward_entropy": 0.006933964788913727,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.07356834411621,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05613554269075394,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13571284214655557,
      "backward_entropy": 0.006923840939998626,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.98782730102539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056193187832832336,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13570515314737955,
      "backward_entropy": 0.006913576275110245,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 71.76875305175781,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05625542253255844,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1356965204079946,
      "backward_entropy": 0.00690305233001709,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.80734634399414,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056324753910303116,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13568639755249023,
      "backward_entropy": 0.036609572172164914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.78228759765625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056393202394247055,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1356764237085978,
      "backward_entropy": 0.006886334717273712,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.814834594726562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.056459199637174606,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1356668472290039,
      "backward_entropy": 0.03670703172683716,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.38106155395508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0565217100083828,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13565791646639505,
      "backward_entropy": 0.03675611019134521,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.871108531951904,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05658533051609993,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1356486678123474,
      "backward_entropy": 0.006859691441059112,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.325130462646484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05664418265223503,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1356403330961863,
      "backward_entropy": 0.006851052492856979,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.236188888549805,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0567031130194664,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13563192884127298,
      "backward_entropy": 0.006842512637376785,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.366453170776367,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05676213279366493,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13562341531117758,
      "backward_entropy": 0.03695065975189209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 46.57589340209961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.056819841265678406,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13561511039733887,
      "backward_entropy": 0.006827467679977417,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.967269897460938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05688051879405975,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13560612003008524,
      "backward_entropy": 0.03705476522445679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.58635711669922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05694115161895752,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.135597030321757,
      "backward_entropy": 0.006811708956956863,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.400858879089355,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05700303614139557,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13558761278788248,
      "backward_entropy": 0.00680379644036293,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.35625076293945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0570618100464344,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1355787714322408,
      "backward_entropy": 0.03720861971378327,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.308916091918945,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057122036814689636,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13556949297587076,
      "backward_entropy": 0.0067887909710407255,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.886714935302734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05717925727367401,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13556082050005594,
      "backward_entropy": 0.006781170517206192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.02379608154297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05723518133163452,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13555236657460532,
      "backward_entropy": 0.03735637664794922,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.913490295410156,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05729282647371292,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13554340600967407,
      "backward_entropy": 0.0067644953727722164,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 30.24190902709961,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.05735209211707115,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1355340083440145,
      "backward_entropy": 0.138629150390625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.61793327331543,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05741122364997864,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13552450140317282,
      "backward_entropy": 0.0067471139132976535,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 45.08400344848633,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057468850165605545,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1355152726173401,
      "backward_entropy": 0.037528985738754274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.50567626953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05752955749630928,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13550521930058798,
      "backward_entropy": 0.037571981549263,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.950480461120605,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0575857050716877,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13549615939458212,
      "backward_entropy": 0.006720476597547531,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 67.02146911621094,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05763933062553406,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13548760612805685,
      "backward_entropy": 0.03766344785690308,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.122562408447266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.057700734585523605,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13547722498575845,
      "backward_entropy": 0.03771757185459137,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 51.795047760009766,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05776327848434448,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13546645641326904,
      "backward_entropy": 0.037768658995628354,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.130035400390625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05782976746559143,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13545464475949606,
      "backward_entropy": 0.0066952861845493315,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.401819229125977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05789398401975632,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13544326027234396,
      "backward_entropy": 0.03786826133728027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.97992515563965,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.057957690209150314,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13543188571929932,
      "backward_entropy": 0.006682839244604111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.502220153808594,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05801931396126747,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1354208787282308,
      "backward_entropy": 0.006675202399492264,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.106857299804688,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058081965893507004,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13540947437286377,
      "backward_entropy": 0.03800349235534668,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.265442371368408,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05814409628510475,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13539805014928183,
      "backward_entropy": 0.03803831040859222,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.91891860961914,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05820157751441002,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13538771867752075,
      "backward_entropy": 0.006648851186037063,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.43811798095703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05825904384255409,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13537724812825522,
      "backward_entropy": 0.006639262288808822,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.374351501464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05832080543041229,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13536555568377176,
      "backward_entropy": 0.006629675626754761,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.79430389404297,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058379389345645905,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1353545884291331,
      "backward_entropy": 0.03817896544933319,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.54397964477539,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.058439262211322784,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1353431542714437,
      "backward_entropy": 0.00661158338189125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.34229850769043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05849888548254967,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13533164064089456,
      "backward_entropy": 0.006601843237876892,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.35723304748535,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05855701491236687,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1353204051653544,
      "backward_entropy": 0.006593777239322663,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 63.578800201416016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05861503258347511,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1353090504805247,
      "backward_entropy": 0.0065848052501678465,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.162384033203125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05868010222911835,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13529566923777261,
      "backward_entropy": 0.03834979832172394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.049646377563477,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0587444007396698,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1352824568748474,
      "backward_entropy": 0.03838523626327515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.94779968261719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05880666896700859,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13526960213979086,
      "backward_entropy": 0.006561183929443359,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.82187271118164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.058869827538728714,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13525633017222086,
      "backward_entropy": 0.038463401794433597,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 55.50365447998047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05893373116850853,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1352426807085673,
      "backward_entropy": 0.03849552273750305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.64961051940918,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059002604335546494,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13522752126057944,
      "backward_entropy": 0.03853056132793427,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.540882110595703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05907033383846283,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13521249095598856,
      "backward_entropy": 0.038567906618118285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.290626525878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05913693457841873,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1351975897947947,
      "backward_entropy": 0.006521120667457581,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.499303817749023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05920403450727463,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13518240054448447,
      "backward_entropy": 0.006513632088899613,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.22426414489746,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05926869064569473,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13516770799954733,
      "backward_entropy": 0.006505787372589111,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.568663597106934,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05933254957199097,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13515305519104004,
      "backward_entropy": 0.0064978539943695065,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.272125244140625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.059392962604761124,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13513930638631186,
      "backward_entropy": 0.03874466121196747,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.47291088104248,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05945156142115593,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13512597481409708,
      "backward_entropy": 0.03878016471862793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.841262817382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059507206082344055,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13511339823404947,
      "backward_entropy": 0.006475391983985901,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.752090454101562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05956288427114487,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13510067264238992,
      "backward_entropy": 0.00646696612238884,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 59.972267150878906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05961865186691284,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1350876490275065,
      "backward_entropy": 0.006458771228790283,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.19717788696289,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059681378304958344,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13507231076558432,
      "backward_entropy": 0.006450512260198593,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.68026351928711,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.05974480137228966,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13505655527114868,
      "backward_entropy": 0.03894184827804566,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.937950134277344,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.059810228645801544,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13503992557525635,
      "backward_entropy": 0.0064327090978622435,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.80297088623047,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.0598762147128582,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13502293825149536,
      "backward_entropy": 0.13862940073013305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.138137817382812,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.05994249880313873,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13500567277272543,
      "backward_entropy": 0.006416899710893631,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.032657623291016,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.060007743537425995,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13498846689860025,
      "backward_entropy": 0.13862941265106202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 32.40717315673828,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060072094202041626,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1349713901678721,
      "backward_entropy": 0.0390909492969513,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.465582847595215,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060137029737234116,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13495385646820068,
      "backward_entropy": 0.03912849724292755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.441570281982422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06019693240523338,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13493794202804565,
      "backward_entropy": 0.006388130784034729,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.44684982299805,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.060252346098423004,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13492348790168762,
      "backward_entropy": 0.13862941265106202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.540592193603516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060310542583465576,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13490787148475647,
      "backward_entropy": 0.03925233483314514,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.088659286499023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06036844104528427,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13489209612210593,
      "backward_entropy": 0.0063691698014736176,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.68321418762207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0604247972369194,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1348767081896464,
      "backward_entropy": 0.03933369517326355,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 37.89554214477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06047825142741203,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1348621944586436,
      "backward_entropy": 0.0393707126379013,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.176076889038086,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06053468585014343,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13484638929367065,
      "backward_entropy": 0.03941151797771454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.547181129455566,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06059110164642334,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13483039538065592,
      "backward_entropy": 0.03945780694484711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.2419490814209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06064457818865776,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13481531540552774,
      "backward_entropy": 0.006337652355432511,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 31.12873649597168,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06069963797926903,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1347994605700175,
      "backward_entropy": 0.03953790068626404,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.81321907043457,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.060756050050258636,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1347828507423401,
      "backward_entropy": 0.0063255175948143,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.36646556854248,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06081237271428108,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13476606210072836,
      "backward_entropy": 0.006319813430309296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 43.098114013671875,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06086588278412819,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1347502072652181,
      "backward_entropy": 0.006314324587583542,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.537433624267578,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.060923606157302856,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13473248481750488,
      "backward_entropy": 0.03970680236816406,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.442922592163086,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06098100170493126,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13471466302871704,
      "backward_entropy": 0.006302936375141144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 36.51698303222656,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06103809177875519,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13469680150349936,
      "backward_entropy": 0.00629599466919899,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.131011962890625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06109754368662834,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13467751940091452,
      "backward_entropy": 0.03980971872806549,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.157381057739258,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06115378439426422,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1346594492594401,
      "backward_entropy": 0.03983685374259949,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.051185607910156,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06120983883738518,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13464123010635376,
      "backward_entropy": 0.039863419532775876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.973669052124023,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061264365911483765,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13462344805399576,
      "backward_entropy": 0.006262592971324921,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.851367950439453,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06131882593035698,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13460542758305868,
      "backward_entropy": 0.03991231918334961,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.79007339477539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06137460097670555,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13458668192227682,
      "backward_entropy": 0.03993566930294037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.776540756225586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06143014878034592,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13456769784291586,
      "backward_entropy": 0.0399545282125473,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 29.506214141845703,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06148426979780197,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13454918066660562,
      "backward_entropy": 0.03997780084609985,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.762652397155762,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0615396574139595,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1345298190911611,
      "backward_entropy": 0.00621538758277893,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.426868438720703,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06159217655658722,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13451154033342996,
      "backward_entropy": 0.0062049426138401035,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.507640838623047,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061644814908504486,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13449307282765707,
      "backward_entropy": 0.006194479390978813,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.252853393554688,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06169633939862251,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13447473446528116,
      "backward_entropy": 0.006185780093073845,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.954252243041992,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06174813210964203,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1344561775525411,
      "backward_entropy": 0.040082722902297974,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.07626724243164,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06180141121149063,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13443662722905478,
      "backward_entropy": 0.04010552167892456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.754807472229004,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.061854712665081024,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13441681861877441,
      "backward_entropy": 0.04012588262557983,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.350162506103516,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.061904069036245346,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1343987782796224,
      "backward_entropy": 0.006152289360761643,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.412651062011719,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06195642054080963,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13437891006469727,
      "backward_entropy": 0.006143060326576233,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.09261703491211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06200616806745529,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1343601644039154,
      "backward_entropy": 0.04017821848392487,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.301362991333008,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06205889582633972,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13433976968129477,
      "backward_entropy": 0.04019086360931397,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.280656814575195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06211301311850548,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13431833187739053,
      "backward_entropy": 0.040205517411231996,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.305904388427734,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062164369970560074,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1342980662981669,
      "backward_entropy": 0.006103590503334999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 50.32223129272461,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.062219832092523575,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13427545626958212,
      "backward_entropy": 0.0060938775539398195,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.830875396728516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.062281545251607895,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1342493693033854,
      "backward_entropy": 0.04024127721786499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.549015045166016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06234363466501236,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13422282536824545,
      "backward_entropy": 0.00607401616871357,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.55213737487793,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0624009408056736,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1341985563437144,
      "backward_entropy": 0.00606481209397316,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.480920791625977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06245650723576546,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13417493303616843,
      "backward_entropy": 0.04027976989746094,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.881410598754883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0625104233622551,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13415189584096274,
      "backward_entropy": 0.04029189050197601,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 38.12533187866211,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06256423145532608,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13412867983182272,
      "backward_entropy": 0.006036896631121636,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.694103240966797,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06262180209159851,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1341030796368917,
      "backward_entropy": 0.040321508049964906,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.200780868530273,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06267894804477692,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13407746950785318,
      "backward_entropy": 0.006020167842507362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.504566192626953,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06273438781499863,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13405253489812216,
      "backward_entropy": 0.006013092771172524,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.41107749938965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06278961896896362,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13402753074963888,
      "backward_entropy": 0.040402159094810486,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.991518020629883,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06284461915493011,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1340023080507914,
      "backward_entropy": 0.006002356857061386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 26.532917022705078,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06289810687303543,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1339777112007141,
      "backward_entropy": 0.0404832661151886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 21.136627197265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06295280903577805,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13395212093989053,
      "backward_entropy": 0.040531909465789794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.527832984924316,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06300732493400574,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13392635186513266,
      "backward_entropy": 0.040584754943847653,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.484931945800781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06305907666683197,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1339019536972046,
      "backward_entropy": 0.0406402587890625,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.87353515625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0631083995103836,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1338789463043213,
      "backward_entropy": 0.005985875427722931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.205819606781006,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06315790116786957,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13385538260142008,
      "backward_entropy": 0.04075618088245392,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.361604690551758,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0632038339972496,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13383390506108603,
      "backward_entropy": 0.005979076772928238,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.482328414916992,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06324775516986847,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13381346066792807,
      "backward_entropy": 0.13862932920455934,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.565902709960938,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06329119205474854,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13379305601119995,
      "backward_entropy": 0.005970380455255509,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.250391960144043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06333540380001068,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13377185662587485,
      "backward_entropy": 0.005965125560760498,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.419843673706055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06337776780128479,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13375160098075867,
      "backward_entropy": 0.040995067358016966,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.093536853790283,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0634210854768753,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1337304711341858,
      "backward_entropy": 0.04103729724884033,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.2781982421875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06346134841442108,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1337113082408905,
      "backward_entropy": 0.04107603430747986,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.109334945678711,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06350280344486237,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13369087378184,
      "backward_entropy": 0.0059421233832836155,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.139986038208008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06354282796382904,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13367138306299844,
      "backward_entropy": 0.005938194692134857,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.084247589111328,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06358407437801361,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13365081946055093,
      "backward_entropy": 0.005935138836503029,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.99853515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06362756341695786,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13362843791643778,
      "backward_entropy": 0.04128277599811554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.922405242919922,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06367048621177673,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13360613584518433,
      "backward_entropy": 0.005925995111465454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.84817886352539,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06371423602104187,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1335830291112264,
      "backward_entropy": 0.0413706511259079,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.83087158203125,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06375881284475327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1335590680440267,
      "backward_entropy": 0.005916247144341469,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.853239059448242,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06380273401737213,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13353532552719116,
      "backward_entropy": 0.0059111595153808595,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.528823852539062,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06384488940238953,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13351265589396158,
      "backward_entropy": 0.005906473845243454,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.893884181976318,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06388922780752182,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.133488138516744,
      "backward_entropy": 0.041556897759437564,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.611001014709473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06393038481473923,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13346568743387857,
      "backward_entropy": 0.04159880578517914,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 33.958351135253906,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06397118419408798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1334432860215505,
      "backward_entropy": 0.005890220403671265,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.160892486572266,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06401672214269638,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13341705004374185,
      "backward_entropy": 0.041668689250946044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.819258213043213,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06406401842832565,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13338921467463175,
      "backward_entropy": 0.005876825004816055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.174072265625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06410792469978333,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1333637237548828,
      "backward_entropy": 0.041736292839050296,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.008895592764019966,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06415249407291412,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13333743810653687,
      "backward_entropy": 0.041771745681762694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.273934364318848,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06419264525175095,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1333144704500834,
      "backward_entropy": 0.00585746131837368,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 23.697696685791016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06423262506723404,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13329134384791055,
      "backward_entropy": 0.005851408839225769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.448187828063965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06427482515573502,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13326619068781534,
      "backward_entropy": 0.04187701642513275,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 28.22271156311035,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06431534886360168,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13324211041132608,
      "backward_entropy": 0.04190884232521057,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.6919708251953125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0643593817949295,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13321498036384583,
      "backward_entropy": 0.04194367825984955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.340081214904785,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06440027803182602,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13319015502929688,
      "backward_entropy": 0.041976708173751834,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.306367874145508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0644395649433136,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13316640257835388,
      "backward_entropy": 0.04200507402420044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.273783683776855,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06447743624448776,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13314352432886759,
      "backward_entropy": 0.005808214470744133,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 27.709413528442383,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06451404839754105,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13312151034673056,
      "backward_entropy": 0.13862929344177247,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.40617561340332,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06455438584089279,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13309611876805624,
      "backward_entropy": 0.042083358764648436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.33455467224121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06459566950798035,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1330696145693461,
      "backward_entropy": 0.042105501890182494,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.135255813598633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06463773548603058,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13304214676221213,
      "backward_entropy": 0.005774100869894027,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.737939834594727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0646781399846077,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13301587104797363,
      "backward_entropy": 0.04214381575584412,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.11845588684082,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06472068279981613,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13298743963241577,
      "backward_entropy": 0.04216398000717163,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.551982879638672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06476394087076187,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13295807441075644,
      "backward_entropy": 0.04218568503856659,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 35.922794342041016,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06480898708105087,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13292684157689413,
      "backward_entropy": 0.005739150568842888,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.410700798034668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06485936045646667,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1328907012939453,
      "backward_entropy": 0.04222097098827362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.347517967224121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06490840762853622,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1328552464644114,
      "backward_entropy": 0.04224078953266144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.711162567138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06495632231235504,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13282057642936707,
      "backward_entropy": 0.042267441749572754,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.817864418029785,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06500444561243057,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13278536995251974,
      "backward_entropy": 0.005708727985620499,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.163954734802246,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06505022197961807,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13275199135144553,
      "backward_entropy": 0.005701986700296402,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.105988502502441,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06509515643119812,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1327191392580668,
      "backward_entropy": 0.04236516952514648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.395917892456055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06513935327529907,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13268660505612692,
      "backward_entropy": 0.04240450859069824,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 34.6288948059082,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0651841089129448,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1326532562573751,
      "backward_entropy": 0.00568605624139309,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.314599514007568,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06523407995700836,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13261459271113077,
      "backward_entropy": 0.04249143898487091,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 25.72620964050293,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06528037041425705,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13257922728856406,
      "backward_entropy": 0.042537498474121097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.06907081604004,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06532938778400421,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13254094123840332,
      "backward_entropy": 0.04258790910243988,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.985437393188477,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06537829339504242,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1325021286805471,
      "backward_entropy": 0.042631027102470395,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.45448112487793,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06542716920375824,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13246301809946695,
      "backward_entropy": 0.042672759294509886,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.414895057678223,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06547348201274872,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1324260433514913,
      "backward_entropy": 0.005657380819320679,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.746065139770508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06551755219697952,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13239099582036337,
      "backward_entropy": 0.04273029267787933,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.669343948364258,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06556208431720734,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13235512375831604,
      "backward_entropy": 0.005643077939748764,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.446030616760254,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06560700386762619,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1323184370994568,
      "backward_entropy": 0.00563654899597168,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.515987396240234,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0656510517001152,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1322822868824005,
      "backward_entropy": 0.04282604455947876,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.655231475830078,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06569544225931168,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13224542140960693,
      "backward_entropy": 0.005622351169586181,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.27100944519043,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06574254482984543,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13220520814259848,
      "backward_entropy": 0.04287919998168945,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.279090881347656,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06578857451677322,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13216580947240195,
      "backward_entropy": 0.042908000946044925,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.104096412658691,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06583476811647415,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13212581475575766,
      "backward_entropy": 0.04293547868728638,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.066154479980469,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06587879359722137,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13208780686060587,
      "backward_entropy": 0.04296714961528778,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.02999210357666,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06592085212469101,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1320516069730123,
      "backward_entropy": 0.0430009663105011,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.988667488098145,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06596112996339798,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13201715548833212,
      "backward_entropy": 0.005580496788024902,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.9373140335083,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06600090861320496,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1319828430811564,
      "backward_entropy": 0.04306728541851044,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.886635780334473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06604023277759552,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13194870948791504,
      "backward_entropy": 0.04309604167938232,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 39.434898376464844,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06607919931411743,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13191463549931845,
      "backward_entropy": 0.005558696016669274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.853092193603516,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06612598896026611,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1318712830543518,
      "backward_entropy": 0.04315411448478699,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.718558311462402,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06617041677236557,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13183029492696127,
      "backward_entropy": 0.043177202343940735,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8929781913757324,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06621398031711578,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13178990284601846,
      "backward_entropy": 0.043204179406166075,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.47734546661377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06625441461801529,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13175288836161295,
      "backward_entropy": 0.005528714135289192,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.406829833984375,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06629548221826553,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13171480099360147,
      "backward_entropy": 0.13862943649291992,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.99886703491211,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06633715331554413,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13167555133501688,
      "backward_entropy": 0.04329626560211182,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.071107864379883,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06638174504041672,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13163242737452188,
      "backward_entropy": 0.04333583116531372,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.594020843505859,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06642765551805496,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1315872073173523,
      "backward_entropy": 0.005502815917134285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.556765079498291,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06647132337093353,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13154438138008118,
      "backward_entropy": 0.04340327084064484,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7646706104278564,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06651303172111511,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13150362173716226,
      "backward_entropy": 0.005490200221538543,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.706933975219727,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06655179709196091,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13146628936131796,
      "backward_entropy": 0.043481874465942386,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.175196647644043,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06659252196550369,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13142607609430948,
      "backward_entropy": 0.005479185655713081,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7134311199188232,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06663266569375992,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13138622045516968,
      "backward_entropy": 0.04356665909290314,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.764915466308594,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0666699931025505,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13134966293970743,
      "backward_entropy": 0.04360831081867218,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 22.045330047607422,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06670814752578735,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1313116451104482,
      "backward_entropy": 0.0054614488035440445,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.629326820373535,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06674933433532715,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13126924633979797,
      "backward_entropy": 0.04367419481277466,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 18.194841384887695,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06679119169712067,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13122556606928507,
      "backward_entropy": 0.04372069239616394,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.864655494689941,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06683456152677536,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13117945194244385,
      "backward_entropy": 0.043761545419692995,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.209534168243408,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06687706708908081,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.131134033203125,
      "backward_entropy": 0.04380311071872711,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.758200645446777,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06691762804985046,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13109085957209268,
      "backward_entropy": 0.005433068796992302,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.707015991210938,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06695757806301117,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1310481826464335,
      "backward_entropy": 0.043882182240486144,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.107168674468994,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0669969692826271,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13100582361221313,
      "backward_entropy": 0.04392043948173523,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 17.674787521362305,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06703486293554306,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13096521298090616,
      "backward_entropy": 0.043970787525177,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.041009426116943,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06707455962896347,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1309216320514679,
      "backward_entropy": 0.04401150941848755,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.5083847045898438,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06711249053478241,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.13088025649388632,
      "backward_entropy": 0.13862926959991456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.94847297668457,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06714781373739243,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1308422088623047,
      "backward_entropy": 0.04407760202884674,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 20.825529098510742,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06718415766954422,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13080238302548727,
      "backward_entropy": 0.044114810228347776,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.3662748336792,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06722354888916016,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13075764973958334,
      "backward_entropy": 0.044144070148468016,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4442520141601562,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06726237386465073,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13071329394976297,
      "backward_entropy": 0.04417122602462768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.428790330886841,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06729844957590103,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13067272305488586,
      "backward_entropy": 0.04419735074043274,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.226912498474121,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06733203679323196,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1306356688340505,
      "backward_entropy": 0.04422059953212738,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.183725357055664,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06736556440591812,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13059820731480917,
      "backward_entropy": 0.044238466024398806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.51809024810791,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06739909201860428,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1305603583653768,
      "backward_entropy": 0.005338509008288384,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.821176528930664,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06743381172418594,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13052038351694742,
      "backward_entropy": 0.005330083891749382,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.742290496826172,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06747058033943176,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13047675291697183,
      "backward_entropy": 0.005320912227034569,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.998847961425781,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06750916689634323,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1304299533367157,
      "backward_entropy": 0.044312238693237305,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.3220016956329346,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06754723936319351,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13038352131843567,
      "backward_entropy": 0.04432580471038818,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.904390335083008,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06758271157741547,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13034086426099142,
      "backward_entropy": 0.005292290449142456,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.85918140411377,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06761804223060608,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1302980879942576,
      "backward_entropy": 0.005284552276134491,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.082765579223633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06765328347682953,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13025514284769693,
      "backward_entropy": 0.005278229713439941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.021135330200195,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06768946349620819,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.13021027048428854,
      "backward_entropy": 0.0444558173418045,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.720621109008789,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06772644817829132,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1301636596520742,
      "backward_entropy": 0.005266163498163223,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.451842308044434,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0677630603313446,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.13011721769968668,
      "backward_entropy": 0.005259861052036285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.2148947715759277,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06779821217060089,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1300728221734365,
      "backward_entropy": 0.04456858932971954,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.587214469909668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06783097982406616,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1300321122010549,
      "backward_entropy": 0.0446040004491806,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.366092681884766,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06786378473043442,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12999099493026733,
      "backward_entropy": 0.005239706858992577,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.339236259460449,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06789544224739075,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12995141744613647,
      "backward_entropy": 0.005231930315494538,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.31298303604126,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06792621314525604,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12991303205490112,
      "backward_entropy": 0.044705310463905336,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.567545890808105,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0679561048746109,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12987587849299112,
      "backward_entropy": 0.04474038779735565,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.640944480895996,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06798742711544037,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12983583410580954,
      "backward_entropy": 0.044779342412948606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.344817161560059,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0680210068821907,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12979151805241904,
      "backward_entropy": 0.04481179118156433,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.301979064941406,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06805460155010223,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12974671522776285,
      "backward_entropy": 0.005198920890688896,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.175381183624268,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0680881142616272,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12970176339149475,
      "backward_entropy": 0.04489275813102722,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.21816349029541,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06812045723199844,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12965849041938782,
      "backward_entropy": 0.044931480288505556,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.289948463439941,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06815286725759506,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12961475054423013,
      "backward_entropy": 0.005180775001645088,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.174822807312012,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06818743795156479,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12956666946411133,
      "backward_entropy": 0.005175061523914337,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 24.222002029418945,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06822286546230316,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12951666116714478,
      "backward_entropy": 0.04505331516265869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.05580997467041,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06826341897249222,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1294569472471873,
      "backward_entropy": 0.04509761929512024,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.975398063659668,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06830530613660812,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12939433256785074,
      "backward_entropy": 0.04514138698577881,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.9817562103271484,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06834729015827179,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1293310523033142,
      "backward_entropy": 0.005153633281588554,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.796719551086426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.0683862492442131,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12927289803822836,
      "backward_entropy": 0.005148856341838837,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.889321327209473,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06842662394046783,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12921160459518433,
      "backward_entropy": 0.045269259810447694,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.781441688537598,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06846509873867035,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12915346026420593,
      "backward_entropy": 0.045307266712188723,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.550996780395508,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06850296258926392,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12909603118896484,
      "backward_entropy": 0.0051314570009708405,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.684351921081543,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06854236871004105,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.129035085439682,
      "backward_entropy": 0.04538852572441101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.883687734603882,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06858091801404953,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12897533178329468,
      "backward_entropy": 0.04541823565959931,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.312108039855957,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06861671060323715,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1289205551147461,
      "backward_entropy": 0.04544989764690399,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.6979265213012695,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06865410506725311,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12886208295822144,
      "backward_entropy": 0.0051033273339271545,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.327914237976074,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06868990510702133,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12880641222000122,
      "backward_entropy": 0.005095575004816055,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.8226964473724365,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06872629374265671,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12874906261761984,
      "backward_entropy": 0.13862931728363037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.010259628295898,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06876006722450256,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12869654099146524,
      "backward_entropy": 0.045547330379486085,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.72074317932129,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06879564374685287,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12864011526107788,
      "backward_entropy": 0.04556259512901306,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.085440635681152,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06883388012647629,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1285772224267324,
      "backward_entropy": 0.005059113353490829,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.773520469665527,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06887241452932358,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.128513365983963,
      "backward_entropy": 0.04558723270893097,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 16.425996780395508,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06891230493783951,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12844626108805338,
      "backward_entropy": 0.04560100734233856,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.164193153381348,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06895444542169571,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12837387124697366,
      "backward_entropy": 0.005030181631445885,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.515401840209961,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06899537146091461,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12830336888631186,
      "backward_entropy": 0.005019510537385941,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.427390098571777,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06903740018606186,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12822997570037842,
      "backward_entropy": 0.045637696981430054,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.671711921691895,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06908043473958969,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1281538705031077,
      "backward_entropy": 0.045657801628112796,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.952358245849609,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06912337243556976,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12807724873224893,
      "backward_entropy": 0.045687702298164365,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.6391539573669434,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06916510313749313,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12800268332163492,
      "backward_entropy": 0.04571811556816101,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.466926574707031,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0692036896944046,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12793463468551636,
      "backward_entropy": 0.045746833086013794,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.402589797973633,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06924255192279816,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12786532441775003,
      "backward_entropy": 0.0049709644168615345,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.590934991836548,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06928160786628723,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12779506047566733,
      "backward_entropy": 0.045813518762588504,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.710743427276611,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06931786239147186,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12773065765698752,
      "backward_entropy": 0.004956934601068497,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.666728973388672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0693536028265953,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12766688068707785,
      "backward_entropy": 0.0458966851234436,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.161450386047363,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06938878446817398,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12760386864344278,
      "backward_entropy": 0.04593750238418579,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.055584907531738,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06942447274923325,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12753909826278687,
      "backward_entropy": 0.004937357828021049,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.046407699584961,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06945868581533432,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12747740745544434,
      "backward_entropy": 0.04601970911026001,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.999283313751221,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06949352473020554,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.1274136702219645,
      "backward_entropy": 0.13862942457199096,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.453158855438232,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0695267766714096,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12735319137573242,
      "backward_entropy": 0.04609622955322266,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.008645071648061275,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06955968588590622,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12729299068450928,
      "backward_entropy": 0.04612495005130768,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 19.654470443725586,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06958937644958496,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1272403597831726,
      "backward_entropy": 0.04615872204303741,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.21649169921875,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06962407380342484,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12717499335606894,
      "backward_entropy": 0.046194449067115784,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.142138481140137,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06966014951467514,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12710569302241007,
      "backward_entropy": 0.04621798098087311,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.831516742706299,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06969756633043289,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12703239917755127,
      "backward_entropy": 0.004875572398304939,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.802161693572998,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06973318010568619,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1269630789756775,
      "backward_entropy": 0.046259832382202146,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.539801597595215,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06976722925901413,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12689721584320068,
      "backward_entropy": 0.004856442287564278,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.48308277130127,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06980175524950027,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1268297036488851,
      "backward_entropy": 0.04629854261875153,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.42575740814209,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06983674317598343,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12676038344701132,
      "backward_entropy": 0.004836558178067207,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.047182083129883,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.06987210363149643,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12668955326080322,
      "backward_entropy": 0.13862931728363037,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.657487392425537,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06990974396467209,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12661216656366983,
      "backward_entropy": 0.04634379744529724,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.247445106506348,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.06994558125734329,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12653902173042297,
      "backward_entropy": 0.04635767340660095,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.188407897949219,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.06998176127672195,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12646435697873434,
      "backward_entropy": 0.004795107245445252,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.409072875976562,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07001832872629166,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1263880729675293,
      "backward_entropy": 0.004786998778581619,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.067841529846191,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07005614042282104,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12630788485209146,
      "backward_entropy": 0.04644896686077118,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.75722074508667,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07009407132863998,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12622672319412231,
      "backward_entropy": 0.04649093747138977,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.946847915649414,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07013110816478729,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12614739934603372,
      "backward_entropy": 0.00476635955274105,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 15.54514217376709,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07016827911138535,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12606704235076904,
      "backward_entropy": 0.004759056866168976,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 0.009355500340461731,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07020845264196396,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12597785393397012,
      "backward_entropy": 0.004752244055271149,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.5754828453063965,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07024461030960083,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12589921553929648,
      "backward_entropy": 0.04664885401725769,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 13.056593894958496,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0702800378203392,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1258222460746765,
      "backward_entropy": 0.04668731689453125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.3282880783081055,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07031752169132233,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1257386306921641,
      "backward_entropy": 0.046716877818107606,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.7353515625,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.070353202521801,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12565972407658896,
      "backward_entropy": 0.0467499703168869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.139861822128296,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07038998603820801,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12557689348856607,
      "backward_entropy": 0.046777147054672244,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.47663402557373,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07042409479618073,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12550127506256104,
      "backward_entropy": 0.004702721536159515,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.215714454650879,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07045860588550568,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1254238486289978,
      "backward_entropy": 0.04684704542160034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 12.549229621887207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07049152255058289,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1253506342569987,
      "backward_entropy": 0.04688040018081665,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.161461353302002,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07052672654390335,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12526996930440268,
      "backward_entropy": 0.04691134095191955,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.259936332702637,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0705602765083313,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12519371509552002,
      "backward_entropy": 0.04694154560565948,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.255228996276855,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0705941766500473,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1251157522201538,
      "backward_entropy": 0.046970400214195254,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 2.0445735454559326,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07062927633523941,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12503352761268616,
      "backward_entropy": 0.04699443280696869,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.054287910461426,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07066194713115692,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1249581774075826,
      "backward_entropy": 0.004642428830265999,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.049242973327637,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07069312781095505,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12488681077957153,
      "backward_entropy": 0.047062304615974423,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 4.004467010498047,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07072490453720093,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12481311957041423,
      "backward_entropy": 0.04709498584270477,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.965713977813721,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07075529545545578,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12474318345387776,
      "backward_entropy": 0.004614964872598648,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.903564453125,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07078540325164795,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12467368443806966,
      "backward_entropy": 0.04714899063110352,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9708043336868286,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07081614434719086,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.1246014932791392,
      "backward_entropy": 0.004595744982361793,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.9097025394439697,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07084483653306961,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12453540166219075,
      "backward_entropy": 0.04721202552318573,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 9.706018447875977,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07087252289056778,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12447203199068706,
      "backward_entropy": 0.047251859307289125,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.792725563049316,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07090188562870026,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12440268198649089,
      "backward_entropy": 0.004569913446903229,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.9257404804229736,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07093101739883423,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12433356046676636,
      "backward_entropy": 0.04730713367462158,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.8206708431243896,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07095811516046524,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1242706576983134,
      "backward_entropy": 0.047330087423324584,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.695537567138672,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0709843784570694,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12421011924743652,
      "backward_entropy": 0.04736015200614929,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.7794957160949707,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07101079821586609,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12414835890134175,
      "backward_entropy": 0.047399044036865234,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.634725093841553,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07103635370731354,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12408908208211263,
      "backward_entropy": 0.04743445515632629,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.6046295166015625,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07106203585863113,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12402894099553426,
      "backward_entropy": 0.004514927789568901,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 11.139936447143555,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07108786702156067,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12396779656410217,
      "backward_entropy": 0.047503703832626344,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.541031837463379,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07111652195453644,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12389673789342244,
      "backward_entropy": 0.04754469394683838,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.50785493850708,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07114499807357788,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1238257884979248,
      "backward_entropy": 0.047584259510040285,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.29693078994751,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07117332518100739,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12375469009081523,
      "backward_entropy": 0.047625237703323366,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 10.873543739318848,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07120244950056076,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1236803929011027,
      "backward_entropy": 0.04767325818538666,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.6061160564422607,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07123401761054993,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12359721461931865,
      "backward_entropy": 0.04772632718086243,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.369872570037842,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07126419991254807,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12351831793785095,
      "backward_entropy": 0.04777717590332031,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 7.111123561859131,
      "terminal_state_reached": true,
      "terminal_reward": 0,
      "log_Z": -0.07129401713609695,
      "trajectory_length": 7,
      "branch_chosen": 0,
      "forward_entropy": 0.12344010670979817,
      "backward_entropy": 0.13862941265106202,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 14.119841575622559,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07132431119680405,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12335960070292155,
      "backward_entropy": 0.04786636233329773,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 8.76246166229248,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07135861366987228,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1232645610968272,
      "backward_entropy": 0.04790964722633362,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4832539558410645,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07139384746551514,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12316563725471497,
      "backward_entropy": 0.0479511559009552,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.4581456184387207,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.0714273601770401,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12307223677635193,
      "backward_entropy": 0.047996622323989865,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 5.145605564117432,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07145914435386658,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12298444906870525,
      "backward_entropy": 0.004420108720660209,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.8104472160339355,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07149039208889008,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12289798259735107,
      "backward_entropy": 0.04806146323680878,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 1.6975553035736084,
      "terminal_state_reached": true,
      "terminal_reward": 90,
      "log_Z": -0.07152188569307327,
      "trajectory_length": 7,
      "branch_chosen": 2,
      "forward_entropy": 0.12281003594398499,
      "backward_entropy": 0.004402278363704682,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 6.717825412750244,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07155103981494904,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.1227302352587382,
      "backward_entropy": 0.048102593421936034,
      "exploration_ratio": 1.0
    },
    {
      "trajectory_balance_loss": 3.34065580368042,
      "terminal_state_reached": true,
      "terminal_reward": 10,
      "log_Z": -0.07158058136701584,
      "trajectory_length": 7,
      "branch_chosen": 1,
      "forward_entropy": 0.12264826893806458,
      "backward_entropy": 0.0481075644493103,
      "exploration_ratio": 1.0
    }
  ],
  "summary_stats": {
    "avg_loss": 8.219648347785697,
    "avg_log_Z": -0.06995881125330924,
    "success_rate": 1.0,
    "avg_reward": 32.8,
    "branch_dist": {
      "-1": 0.0,
      "0": 0.04,
      "1": 0.67,
      "2": 0.29
    },
    "avg_forward_entropy": 0.12631440937519073,
    "avg_backward_entropy": 0.038180594015866515,
    "avg_exploration_ratio": 1.0
  },
  "window_size": 100
}